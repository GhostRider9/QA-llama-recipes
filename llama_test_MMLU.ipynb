{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to python llama_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import datasets\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaTokenizer\n",
    "from inference.safety_utils import get_safety_checker\n",
    "from inference.model_utils import load_model, load_peft_model, load_llama_from_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    model_name: str='/root/Model/llama_2_13B_chat'\n",
    "    # peft_model: str='/root/Codes/QA-llama-recipes/llama2_lora_autotrain_2/checkpoint-463'\n",
    "    peft_model: str=None\n",
    "    quantization: bool=True\n",
    "    max_new_tokens: int=6 #The maximum numbers of tokens to generate\n",
    "    seed: int=42 #seed value for reproducibility\n",
    "    do_sample: bool=True #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "    min_length: int=None #The minimum length of the sequence to be generated, input prompt + min_new_tokens\n",
    "    use_cache: bool=True  #[optional] Whether or not the model should use the past last key/values attentions Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
    "    top_p: float=0.9 # [optional] If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "    temperature: float=0.01 # [optional] The value used to modulate the next token probabilities.\n",
    "    top_k: int=50 # [optional] The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "    repetition_penalty: float=2.0 #The parameter for repetition penalty. 1.0 means no penalty.\n",
    "    length_penalty: int=1 #[optional] Exponential penalty to the length that is used with beam-based generation. \n",
    "\n",
    "\n",
    "conf = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b612b67168fd4772b804d7f153f640e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/micromamba/envs/llm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/root/micromamba/envs/llm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seeds for reproducibility\n",
    "torch.cuda.manual_seed(conf.seed)\n",
    "torch.manual_seed(conf.seed)\n",
    "\n",
    "model = load_model(conf.model_name, conf.quantization)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(conf.model_name)\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"pad_token\": \"<PAD>\",\n",
    "    }\n",
    ")\n",
    "\n",
    "if conf.peft_model:\n",
    "    model = load_peft_model(model, conf.peft_model)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8dc1e31ffc642d88df773b16b448401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_test_dataset(file):\n",
    "    test_df = pd.read_csv(file)\n",
    "    dataset = datasets.Dataset.from_pandas(test_df)\n",
    "\n",
    "    def func(r):\n",
    "        return {\n",
    "            \"text\": r['input'],\n",
    "            \"answer\": r['output']\n",
    "        }\n",
    "\n",
    "    return dataset.map(func, remove_columns=list(dataset.features))\n",
    "\n",
    "test_dataset = load_test_dataset('./data/autotrain/valid.csv')\n",
    "# test_dataset = load_test_dataset('./data/autotrain/valid_no_context.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "### Instruction:\n",
      "According to the context, select the most accurate answer for the quesiton.\n",
      "### Question:\n",
      "Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\n",
      "### Options:\n",
      "(A) MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"\n",
      "(B) MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.\n",
      "(C) MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.\n",
      "(D) MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.\n",
      "(E) MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.\n",
      "### Context:\n",
      "MOND Modified Newtonian Dynamics (MOND) is a relatively modern proposal to explain the galaxy rotation problem based on a variation of Newton's Second Law of Dynamics at low accelerations. This would produce a large-scale variation of Newton's universal theory of gravity. A modification of Newton's theory would also imply a modification of general relativistic cosmology in as much as Newtonian cosmology is the limit of Friedman cosmology. While almost all astrophysicists today reject MOND in favor of dark matter, a small number of researchers continue to enhance it, recently incorporating Bransâ€“Dicke theories into treatments that attempt to account for cosmological observations.\n",
      "[/INST]\n",
      "The most accurate answer is (\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [09:18<00:00,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "def precision_at_k(r, k):\n",
    "    \"\"\"Precision at k\"\"\"\n",
    "    assert k <= len(r)\n",
    "    assert k != 0\n",
    "    return sum(int(x) for x in r[:k]) / k\n",
    "\n",
    "def MAP_at_3(predictions, true_items):\n",
    "    \"\"\"Score is mean average precision at 3\"\"\"\n",
    "    U = len(predictions)\n",
    "    map_at_3 = 0.0\n",
    "    for u in range(U):\n",
    "        user_preds = predictions[u]\n",
    "        user_true = true_items[u]\n",
    "        user_results = [1 if item == user_true else 0 for item in user_preds]\n",
    "        for k in range(min(len(user_preds), 3)):\n",
    "            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n",
    "    return map_at_3 / U\n",
    "\n",
    "\n",
    "def get_ans(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    logits = model(input_ids=inputs['input_ids'].cuda(), attention_mask=inputs['attention_mask'].cuda()).logits[0, -1]\n",
    "    \n",
    "    # Create a list of tuples having (logit, 'option') format\n",
    "    options_list = zip([logits[tokenizer(x).input_ids[-1]] for x in 'ABCDE'], list('ABCDE'))\n",
    "\n",
    "    del inputs\n",
    "    del logits\n",
    "\n",
    "    options_list = sorted(options_list, reverse=True)\n",
    "    ans_list = []\n",
    "    for i in range(3):\n",
    "        ans_list.append(options_list[i][1])\n",
    "    \n",
    "    del options_list\n",
    "\n",
    "    return ans_list\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "preds = []\n",
    "count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(test_dataset['text'], total=len(test_dataset)):\n",
    "        if count % 10 == 0:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        pred = ' '.join(get_ans(x))\n",
    "        preds.append(pred)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6933333333333329\n"
     ]
    }
   ],
   "source": [
    "print(MAP_at_3(preds, test_dataset['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.645"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score([x[0] for x in preds], test_dataset['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

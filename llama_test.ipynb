{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to python llama_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import datasets\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List\n",
    "from transformers import LlamaTokenizer\n",
    "from inference.safety_utils import get_safety_checker\n",
    "from inference.model_utils import load_model, load_peft_model, load_llama_from_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    model_name: str='/root/Model/llama-2-7b'\n",
    "    peft_model: str='/root/Model/llama-output-01/checkpoint-850'\n",
    "    # peft_model: str=None\n",
    "    quantization: bool=True\n",
    "    max_new_tokens: int=6 #The maximum numbers of tokens to generate\n",
    "    seed: int=42 #seed value for reproducibility\n",
    "    do_sample: bool=True #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "    min_length: int=None #The minimum length of the sequence to be generated, input prompt + min_new_tokens\n",
    "    use_cache: bool=True  #[optional] Whether or not the model should use the past last key/values attentions Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
    "    top_p: float=0.9 # [optional] If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "    temperature: float=0.01 # [optional] The value used to modulate the next token probabilities.\n",
    "    top_k: int=50 # [optional] The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "    repetition_penalty: float=2.0 #The parameter for repetition penalty. 1.0 means no penalty.\n",
    "    length_penalty: int=1 #[optional] Exponential penalty to the length that is used with beam-based generation. \n",
    "\n",
    "\n",
    "conf = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b85f4085b7e4130aaa5233989f6dbe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seeds for reproducibility\n",
    "torch.cuda.manual_seed(conf.seed)\n",
    "torch.manual_seed(conf.seed)\n",
    "\n",
    "model = load_model(conf.model_name, conf.quantization)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(conf.model_name)\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"pad_token\": \"<PAD>\",\n",
    "    }\n",
    ")\n",
    "\n",
    "if conf.peft_model:\n",
    "    model = load_peft_model(model, conf.peft_model)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(user_prompt):\n",
    "    batch = tokenizer(user_prompt, return_tensors=\"pt\")\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=conf.max_new_tokens,\n",
    "            do_sample=conf.do_sample,\n",
    "            top_p=conf.top_p,\n",
    "            temperature=conf.temperature,\n",
    "            min_length=conf.min_length,\n",
    "            use_cache=conf.use_cache,\n",
    "            top_k=conf.top_k,\n",
    "            repetition_penalty=conf.repetition_penalty,\n",
    "            length_penalty=conf.length_penalty,\n",
    "        )\n",
    "    e2e_inference_time = (time.perf_counter()-start)*1000\n",
    "    # print(f\"the inference time is {e2e_inference_time} ms\")\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(r'Answer:\\nOption[ :-]*\\(*([\\w])\\)*')\n",
    "\n",
    "def get_result(response):\n",
    "    # extract the answer\n",
    "    res = pattern.findall(response.strip())\n",
    "    if len(res) == 1:\n",
    "        answer = res[0]  # 'A', 'B', ...\n",
    "    else:\n",
    "        answer = \"FAILED\"\n",
    "\n",
    "    return answer.capitalize()\n",
    "\n",
    "get_result('Answer:\\nOption ((C).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset():\n",
    "    dataset = datasets.load_dataset(\"metaeval/ScienceQA_text_only\", split='test')\n",
    "    choice_prefixes = [chr(ord('A') + i) for i in range(26)] # A-Z\n",
    "\n",
    "    prompt = '''Context: {hint}\\nQuestion: {question}\\nOptions: {options}\\n---\\nAnswer:\\n'''\n",
    "\n",
    "    def format_options(options):\n",
    "        return '\\n'.join([f'({c}) {o}' for c, o in zip(choice_prefixes, options)])\n",
    "\n",
    "    def apply_prompt_template(r):\n",
    "        options = format_options(r['choices'])\n",
    "        return {\n",
    "            \"text\": prompt.format(\n",
    "                hint=r[\"hint\"],\n",
    "                question=r[\"question\"],\n",
    "                options=options,\n",
    "            ),\n",
    "            \"answer\": f\"({choice_prefixes[r['answer']]})\"\n",
    "        }\n",
    "\n",
    "    return dataset.map(apply_prompt_template, remove_columns=list(dataset.features))\n",
    "        \n",
    "test_dataset = load_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Read the description of a trait.\n",
      "Michelle has wavy hair.\n",
      "Question: What information supports the conclusion that Michelle inherited this trait?\n",
      "Options: (A) Michelle's parents were born with wavy hair. They passed down this trait to Michelle.\n",
      "(B) Michelle and her mother both have short hair.\n",
      "---\n",
      "Answer:\n",
      "Option B\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 100\n",
    "print(test_dataset['answer'][id])\n",
    "\n",
    "response = inference(test_dataset['text'][id])\n",
    "print(response)\n",
    "get_result(response.split('---')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2224/2224 [35:52<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "responses, preds = [], []\n",
    "\n",
    "for x in tqdm(test_dataset['text']):\n",
    "    response = inference(x).split('---')[1]\n",
    "    responses.append(response)\n",
    "    preds.append(get_result(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6344424460431655\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score([x[1:2] for x in test_dataset['answer']], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6344424460431655"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_preds = [get_result(x) for x in responses]\n",
    "accuracy_score([x[1:2] for x in test_dataset['answer']], new_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Failed Answer:Observe trevors physical\n",
      "189 Failed Answer:Future\n",
      "262 Failed Answer:Option \n",
      "288 Failed Answer:Option \n",
      "417 Failed Answer:Option ***\n",
      "440 Failed Answer:Metaphore\n",
      "521 Failed Answer:Option \n",
      "567 Failed Answer:The liquid in this pressure cook\n",
      "664 Failed Answer:Mass can be measured in\n",
      "736 Failed Answer:Option\n",
      "737 Failed Answer:The correct answer is Option B\n",
      "860 Failed Answer:Option \n",
      "1033 Failed Answer:Carbon is a chemical comp\n",
      "1197 Failed Answer:Option \n",
      "1318 Failed Answer:Option \n",
      "1418 Failed Answer:Option--\n",
      "1688 Failed Answer:Option ***\n",
      "1754 Failed Answer:Option\n",
      "1776 Failed Answer:\n",
      "1925 Failed Answer:Pouring out water from\n",
      "1983 Failed Answer:Option \n",
      "2008 Failed Answer:The gravitational force exert\n",
      "2113 Failed Answer:Lawyer\n",
      "2156 Failed Answer:Cell walls are made of dead\n",
      "2167 Failed Answer:Option \n"
     ]
    }
   ],
   "source": [
    "for i, x, y in zip(range(len(new_preds)), new_preds, responses):\n",
    "    if len(x) > 1:\n",
    "        print(i,x, y.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

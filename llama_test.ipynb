{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import datasets\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List\n",
    "from transformers import LlamaTokenizer\n",
    "from inference.safety_utils import get_safety_checker\n",
    "from inference.model_utils import load_model, load_peft_model, load_llama_from_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    model_name: str='/root/Model/llama-2-7b'\n",
    "    peft_model: str='/root/Model/llama-output/checkpoint-384'\n",
    "    # peft_model: str=None\n",
    "    quantization: bool=True\n",
    "    max_new_tokens: int=6 #The maximum numbers of tokens to generate\n",
    "    seed: int=42 #seed value for reproducibility\n",
    "    do_sample: bool=True #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "    min_length: int=None #The minimum length of the sequence to be generated, input prompt + min_new_tokens\n",
    "    use_cache: bool=True  #[optional] Whether or not the model should use the past last key/values attentions Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
    "    top_p: float=0.9 # [optional] If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "    temperature: float=0.01 # [optional] The value used to modulate the next token probabilities.\n",
    "    top_k: int=50 # [optional] The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "    repetition_penalty: float=2.0 #The parameter for repetition penalty. 1.0 means no penalty.\n",
    "    length_penalty: int=1 #[optional] Exponential penalty to the length that is used with beam-based generation. \n",
    "\n",
    "\n",
    "conf = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fccdb26a32413da491744ddb332016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seeds for reproducibility\n",
    "torch.cuda.manual_seed(conf.seed)\n",
    "torch.manual_seed(conf.seed)\n",
    "\n",
    "model = load_model(conf.model_name, conf.quantization)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(conf.model_name)\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"pad_token\": \"<PAD>\",\n",
    "    }\n",
    ")\n",
    "\n",
    "if conf.peft_model:\n",
    "    model = load_peft_model(model, conf.peft_model)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(user_prompt):\n",
    "    batch = tokenizer(user_prompt, return_tensors=\"pt\")\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=conf.max_new_tokens,\n",
    "            do_sample=conf.do_sample,\n",
    "            top_p=conf.top_p,\n",
    "            temperature=conf.temperature,\n",
    "            min_length=conf.min_length,\n",
    "            use_cache=conf.use_cache,\n",
    "            top_k=conf.top_k,\n",
    "            repetition_penalty=conf.repetition_penalty,\n",
    "            length_penalty=conf.length_penalty,\n",
    "        )\n",
    "    e2e_inference_time = (time.perf_counter()-start)*1000\n",
    "    # print(f\"the inference time is {e2e_inference_time} ms\")\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(r'Answer:\\nOption[ :-]*\\(*([\\w])\\)*')\n",
    "\n",
    "def get_result(response):\n",
    "    # extract the answer\n",
    "    res = pattern.findall(response.strip())\n",
    "    if len(res) == 1:\n",
    "        answer = res[0]  # 'A', 'B', ...\n",
    "    else:\n",
    "        answer = \"FAILED\"\n",
    "\n",
    "    return answer.capitalize()\n",
    "\n",
    "get_result('Answer:\\nOption ((C).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset():\n",
    "    dataset = datasets.load_dataset(\"metaeval/ScienceQA_text_only\", split='test')\n",
    "    choice_prefixes = [chr(ord('A') + i) for i in range(26)] # A-Z\n",
    "\n",
    "    prompt = '''Context: {hint}\\nQuestion: {question}\\nOptions: {options}\\n---\\nAnswer:\\n'''\n",
    "\n",
    "    def format_options(options):\n",
    "        return '\\n'.join([f'({c}) {o}' for c, o in zip(choice_prefixes, options)])\n",
    "\n",
    "    def apply_prompt_template(r):\n",
    "        options = format_options(r['choices'])\n",
    "        return {\n",
    "            \"text\": prompt.format(\n",
    "                hint=r[\"hint\"],\n",
    "                question=r[\"question\"],\n",
    "                options=options,\n",
    "            ),\n",
    "            \"answer\": f\"({choice_prefixes[r['answer']]})\"\n",
    "        }\n",
    "\n",
    "    return dataset.map(apply_prompt_template, remove_columns=list(dataset.features))\n",
    "        \n",
    "test_dataset = load_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Read the description of a trait.\n",
      "Michelle has wavy hair.\n",
      "Question: What information supports the conclusion that Michelle inherited this trait?\n",
      "Options: (A) Michelle's parents were born with wavy hair. They passed down this trait to Michelle.\n",
      "(B) Michelle and her mother both have short hair.\n",
      "---\n",
      "Answer:\n",
      "Option A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 100\n",
    "print(test_dataset['answer'][id])\n",
    "\n",
    "response = inference(test_dataset['text'][id])\n",
    "print(response)\n",
    "get_result(response.split('---')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2224/2224 [38:23<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "responses, preds = [], []\n",
    "\n",
    "for x in tqdm(test_dataset['text']):\n",
    "    response = inference(x).split('---')[1]\n",
    "    responses.append(response)\n",
    "    preds.append(get_result(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2972122302158273"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score([x[1:2] for x in test_dataset['answer']], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3035071942446043"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_preds = [get_result(x) for x in responses]\n",
    "accuracy_score([x[1:2] for x in test_dataset['answer']], new_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 C\n",
      "1 B\n",
      "2 B\n",
      "3 A\n",
      "4 C\n",
      "5 A\n",
      "6 A\n",
      "7 D\n",
      "8 B\n",
      "9 B\n",
      "10 Failed\n",
      "11 A\n",
      "12 E\n",
      "13 Failed\n",
      "14 B\n",
      "15 B\n",
      "16 E\n",
      "17 C\n",
      "18 Failed\n",
      "19 Failed\n",
      "20 A\n",
      "21 C\n",
      "22 Failed\n",
      "23 B\n",
      "24 A\n",
      "25 Failed\n",
      "26 A\n",
      "27 C\n",
      "28 A\n",
      "29 B\n",
      "30 B\n",
      "31 Failed\n",
      "32 B\n",
      "33 B\n",
      "34 B\n",
      "35 A\n",
      "36 Failed\n",
      "37 Failed\n",
      "38 C\n",
      "39 C\n",
      "40 A\n",
      "41 B\n",
      "42 A\n",
      "43 C\n",
      "44 C\n",
      "45 C\n",
      "46 Failed\n",
      "47 B\n",
      "48 B\n",
      "49 Failed\n",
      "50 Failed\n",
      "51 Failed\n",
      "52 B\n",
      "53 C\n",
      "54 Failed\n",
      "55 B\n",
      "56 C\n",
      "57 A\n",
      "58 Failed\n",
      "59 B\n",
      "60 C\n",
      "61 Failed\n",
      "62 B\n",
      "63 Failed\n",
      "64 B\n",
      "65 B\n",
      "66 C\n",
      "67 Failed\n",
      "68 A\n",
      "69 Failed\n",
      "70 B\n",
      "71 B\n",
      "72 A\n",
      "73 C\n",
      "74 E\n",
      "75 Failed\n",
      "76 C\n",
      "77 A\n",
      "78 B\n",
      "79 B\n",
      "80 A\n",
      "81 B\n",
      "82 Failed\n",
      "83 B\n",
      "84 A\n",
      "85 A\n",
      "86 Failed\n",
      "87 C\n",
      "88 A\n",
      "89 Failed\n",
      "90 D\n",
      "91 C\n",
      "92 B\n",
      "93 D\n",
      "94 A\n",
      "95 Failed\n",
      "96 B\n",
      "97 Failed\n",
      "98 C\n",
      "99 Failed\n"
     ]
    }
   ],
   "source": [
    "for i, x in zip(range(100), new_preds[:100]):\n",
    "    print(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Answer:Option ((C)}.\n",
      "1 Answer:Option B\n",
      "2 Answer:Option B\n",
      "3 Answer:Option A\n",
      "4 Answer:Option ((C).\n",
      "5 Answer:Option A\n",
      "6 Answer:Option A\n",
      "7 Answer:Option d\n",
      "8 Answer:Option B\n",
      "9 Answer:Option B\n",
      "10 Answer:Streaked is between ser\n",
      "11 Answer:Option A\n",
      "12 Answer:Option ((E))\n",
      "13 Answer:Option ()\n",
      "14 Answer:Option B\n",
      "15 Answer:Option B\n",
      "16 Answer:Option ((Earth's\n",
      "17 Answer:Option ((C).\n",
      "18 Answer:## Answer\n",
      "19 Answer:This question is phrased as\n",
      "20 Answer:Option A\n",
      "21 Answer:Option ((C).\n",
      "22 Answer:Option ()\n",
      "23 Answer:Option B\n",
      "24 Answer:Option A\n",
      "25 Answer:Option ()\n",
      "26 Answer:Option A\n",
      "27 Answer:Option c\n",
      "28 Answer:Option A\n",
      "29 Answer:Option B\n",
      "30 Answer:Option - B\n",
      "31 Answer:Most mamas have\n",
      "32 Answer:Option b\n",
      "33 Answer:Option B\n",
      "34 Answer:Option B\n",
      "35 Answer:Option A\n",
      "36 Answer:Open SyLLABE -\n",
      "37 Answer:Option ()\n",
      "38 Answer:Option ((C\n",
      "39 Answer:Option ((C))\n",
      "40 Answer:Option A\n",
      "41 Answer:Option b\n",
      "42 Answer:Option ((a))\n",
      "43 Answer:Option ((C).\n",
      "44 Answer:Option:(C)\n",
      "45 Answer:Option ((C).\n",
      "46 Answer:Verify that motion and rest\n",
      "47 Answer:Option B\n",
      "48 Answer:Option B\n",
      "49 Answer:Mable likes red-\n",
      "50 Answer:Option ()\n",
      "51 Answer:Option ***\n",
      "52 Answer:Option B\n",
      "53 Answer:Option ((C).\n",
      "54 Answer:Verify your answer by re\n",
      "55 Answer:Option B\n",
      "56 Answer:Option ((C)}.\n",
      "57 Answer:Option A\n",
      "58 Answer:Option ()\n",
      "59 Answer:Option b\n",
      "60 Answer:Option ((C)}.\n",
      "61 Answer:##\n",
      "62 Answer:Option B\n",
      "63 Answer:This kind word refers to someone\n",
      "64 Answer:Option b\n",
      "65 Answer:Option B\n",
      "66 Answer:Option ((C))\n",
      "67 Answer:## Options\n",
      "68 Answer:Option A\n",
      "69 Answer:Option ***\n",
      "70 Answer:Option B\n",
      "71 Answer:Option B\n",
      "72 Answer:Option A\n",
      "73 Answer:Option ((C).\n",
      "D4 Answer:Option:(E)\n",
      "75 Answer:Option ()\n",
      "76 Answer:Option ((C).\n",
      "77 Answer:Option A —\n",
      "78 Answer:Option b\n",
      "79 Answer:Option B\n",
      "80 Answer:Option A\n",
      "81 Answer:Option B\n",
      "82 Answer:Option : ()\n",
      "83 Answer:Option B\n",
      "84 Answer:Option A\n",
      "85 Answer:Option A\n",
      "86 Answer:Nor is not in this\n",
      "87 Answer:Option ((C)).\n",
      "88 Answer:Option A\n",
      "89 Answer:Option :\n",
      "90 Answer:Option D\n",
      "91 Answer:Option:(C)(Earth\n",
      "92 Answer:Option B—\n",
      "93 Answer:Option - D\n",
      "94 Answer:Option A\n",
      "95 Answer:Option ()\n",
      "96 Answer:Option B\n",
      "97 Answer:Option ()\n",
      "98 Answer:Option ((C)).\n",
      "99 Answer:Mirandanamed her\n"
     ]
    }
   ],
   "source": [
    "for i, x in zip(range(100), responses[:100]):\n",
    "    print(i,x.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

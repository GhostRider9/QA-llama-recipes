prompt,A,B,C,D,E,answer,wiki_fact
"Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed ""missing baryonic mass"" discrepancy in galaxy clusters?","MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called ""fuzzy dark matter.""",MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.,MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.,MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.,MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.,D,"MOND 

Modified Newtonian Dynamics (MOND) is a relatively modern proposal to explain the galaxy rotation problem based on a variation of Newton's Second Law of Dynamics at low accelerations. This would produce a large-scale variation of Newton's universal theory of gravity. A modification of Newton's theory would also imply a modification of general relativistic cosmology in as much as Newtonian cosmology is the limit of Friedman cosmology. While almost all astrophysicists today reject MOND in favor of dark matter, a small number of researchers continue to enhance it, recently incorporating Brans–Dicke theories into treatments that attempt to account for cosmological observations."
Which of the following is an accurate definition of dynamic scaling in self-similar systems?,"Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times exhibits similarity to the respective data taken from snapshots of any earlier or later time. This similarity is tested by a certain time-dependent stochastic variable x.","Dynamic scaling refers to the non-evolution of self-similar systems, where data obtained from snapshots at fixed times is similar to the respective data taken from snapshots of any earlier or later time. This similarity is tested by a certain time-dependent stochastic variable x.","Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times is dissimilar to the respective data taken from snapshots of any earlier or later time. This dissimilarity is tested by a certain time-independent stochastic variable y.","Dynamic scaling refers to the non-evolution of self-similar systems, where data obtained from snapshots at fixed times is dissimilar to the respective data taken from snapshots of any earlier or later time. This dissimilarity is tested by a certain time-independent stochastic variable y.","Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times is independent of the respective data taken from snapshots of any earlier or later time. This independence is tested by a certain time-dependent stochastic variable z.",A,"In the study of partial differential equations, particularly in fluid dynamics, a self-similar solution is a form of solution  which is similar to itself if the independent and dependent variables are appropriately scaled. Self-similar solutions appear whenever the problem lacks a characteristic length or time scale (for example, the Blasius boundary layer of an infinite plate, but not of a finite-length plate). These include, for example, the Blasius boundary layer or the Sedov–Taylor shell.

Concept"
Which of the following statements accurately describes the origin and significance of the triskeles symbol?,"The triskeles symbol was reconstructed as a feminine divine triad by the rulers of Syracuse, and later adopted as an emblem. Its usage may also be related to the Greek name of Sicily, Trinacria, which means ""having three headlands."" The head of Medusa at the center of the Sicilian triskeles represents the three headlands.","The triskeles symbol is a representation of three interlinked spirals, which was adopted as an emblem by the rulers of Syracuse. Its usage in modern flags of Sicily has its origins in the ancient Greek name for the island, Trinacria, which means ""Sicily with three corners."" The head of Medusa at the center is a representation of the island's rich cultural heritage.","The triskeles symbol is a representation of a triple goddess, reconstructed by the rulers of Syracuse, who adopted it as an emblem. Its significance lies in the fact that it represents the Greek name for Sicily, Trinacria, which contains the element ""tria,"" meaning three. The head of Medusa at the center of the Sicilian triskeles represents the three headlands.","The triskeles symbol represents three interlocked spiral arms, which became an emblem for the rulers of Syracuse. Its usage in modern flags of Sicily is due to the island's rich cultural heritage, which dates back to ancient times. The head of Medusa at the center represents the lasting influence of Greek mythology on Sicilian culture.","The triskeles symbol is a representation of the Greek goddess Hecate, reconstructed by the rulers of Syracuse. Its adoption as an emblem was due to its cultural significance, as it represented the ancient Greek name for Sicily, Trinacria. The head of Medusa at the center of the Sicilian triskeles represents the island's central location in the Mediterranean.",A,
What is the significance of regularization in terms of renormalization problems in physics?,"Regularizing the mass-energy of an electron with a finite radius can theoretically simplify calculations involving infinities or singularities, thereby providing explanations that would otherwise be impossible to achieve.",Regularizing the mass-energy of an electron with an infinite radius allows for the breakdown of a theory that is valid under one set of conditions. This approach can be applied to other renormalization problems as well.,Regularizing the mass-energy of an electron with a finite radius is a means of demonstrating that a system below a certain size can be explained without the need for further calculations. This approach can be applied to other renormalization problems as well.,Regularizing the mass-energy of an electron with an infinite radius can be used to provide a highly accurate description of a system under specific conditions. This approach can be transferred to other renormalization problems as well.,Regularizing the mass-energy of an electron with an infinite radius is essential for explaining how a system below a certain size operates. This approach can be applied to other renormalization problems as well.,C,"However, the result usually includes terms proportional to expressions like  which are not well-defined in the limit . Regularization is the first step towards obtaining a completely finite and meaningful result; in quantum field theory it must be usually followed by a related, but independent technique called renormalization. Renormalization is based on the requirement that some physical quantities — expressed by seemingly divergent expressions such as  — are equal to the observed values. Such a constraint allows one to calculate a finite value for many other quantities that looked divergent."
Which of the following statements accurately describes the relationship between the dimensions of a diffracting object and the angular spacing of features in the diffraction pattern?,"The angular spacing of features in the diffraction pattern is indirectly proportional to the dimensions of the object causing the diffraction. Therefore, if the diffracting object is smaller, the resulting diffraction pattern will be narrower.","The angular spacing of features in the diffraction pattern is directly proportional to the dimensions of the object causing the diffraction. Therefore, if the diffracting object is smaller, the resulting diffraction pattern will be narrower.","The angular spacing of features in the diffraction pattern is independent of the dimensions of the object causing the diffraction. Therefore, if the diffracting object is smaller, the resulting diffraction pattern will be the same as if it were big.","The angular spacing of features in the diffraction pattern is inversely proportional to the dimensions of the object causing the diffraction. Therefore, if the diffracting object is smaller, the resulting diffraction pattern will be wider.","The angular spacing of features in the diffraction pattern is directly proportional to the square root of the dimensions of the object causing the diffraction. Therefore, if the diffracting object is smaller, the resulting diffraction pattern will be slightly narrower.",D,"where . However  and integrating from  to ,

where .

Integrating we then get

Letting  where the array length in radians is , then,

Diffraction by a rectangular aperture

The form of the diffraction pattern given by a rectangular aperture is shown in the figure on the right (or above, in tablet format). There is a central semi-rectangular peak, with a series of horizontal and vertical fringes.  The dimensions of the central band are related to the dimensions of the slit by the same relationship as for a single slit so that the larger dimension in the diffracted image corresponds to the smaller dimension in the slit.  The spacing of the fringes is also inversely proportional to the slit dimension."
"Which of the following statements accurately depicts the relationship between Gauss's law, electric flux, electric field, and symmetry in electric fields?","Gauss's law holds only for situations involving symmetric electric fields, like those with spherical or cylindrical symmetry, and doesn't apply to other field types. Electric flux, as an expression of the total electric field passing through a closed surface, is influenced only by charges within the surface and unaffected by distant charges located outside it. The scalar quantity electric flux is strictly measured in SI fundamental quantities as kg·m3·s−3·A.","Gauss's law holds in all cases, but it is most useful for calculations involving symmetric electric fields, like those with spherical or cylindrical symmetry, as they allow for simpler algebraic manipulations. Electric flux is not affected by distant charges outside the closed surface, whereas the net electric field, E, can be influenced by any charges positioned outside of the closed surface. In SI base units, the electric flux is expressed as kg·m3·s−3·A−1.","Gauss's law, which applies equally to all electric fields, is typically most useful when dealing with symmetric field configurations, like those with spherical or cylindrical symmetry, since it makes it easier to calculate the total electric flux. Electric flux, an expression of the total electric field through a closed surface, is unaffected by charges outside the surface, while net electric field, E, may be influenced by charges located outside the closed surface. Electric flux is expressed in SI base units as kg·m3·s−1·C.","Gauss's law only holds for electric fields with cylindrical symmetry, like those of a straight long wire; it is not applicable to fields with other types of symmetry. Electric flux, which measures the total electric field across a closed surface, is influenced by all charges within the surface as well as by those located outside it. The unit of electric flux in SI base units is kg·m2·s−2·A−1.","Gauss's law, which holds for all situations, is most beneficial when applied to electric fields that exhibit higher degrees of symmetry, like those with cylindrical and spherical symmetry. While electric flux is unaffected by charges outside of a given closed surface, the net electric field, E, may be affected by them. The unit of electric flux in SI base units is kg·m2·s−1·C.",B,"In physics and electromagnetism, Gauss's law, also known as Gauss's flux theorem, (or sometimes simply called Gauss's theorem) is a law relating the distribution of electric charge to the resulting electric field. In its integral form, it states that the flux of the electric field out of an arbitrary closed surface is proportional to the electric charge enclosed by the surface, irrespective of how that charge is distributed. Even though the law alone is insufficient to determine the electric field across a surface enclosing any charge distribution, this may be possible in cases where symmetry mandates uniformity of the field. Where no such symmetry exists, Gauss's law can be used in its differential form, which states that the divergence of the electric field is proportional to the local density of charge."
Which of the following statements accurately describes the dimension of an object in a CW complex?,"The dimension of an object in a CW complex is the largest n for which the n-skeleton is nontrivial, where the empty set is considered to have dimension -1 and the boundary of a discrete set of points is the empty set.","The dimension of an object in a CW complex is determined by the number of critical points the object contains. The boundary of a discrete set of points is considered to have dimension 1, while the empty set is given a dimension of 0.","The dimension of an object in a CW complex is the smallest n for which the n-skeleton is nontrivial. The empty set is given a dimension of -1, while the boundary of a discrete set of points is assigned a dimension of 0.","The dimension of an object in a CW complex is calculated by counting the number of cells of all dimensions in the object. The empty set is given a dimension of 0, while the boundary of a discrete set of points is assigned a dimension of -1.",The dimension of an object in a CW complex depends on the number of singularities in the object. The empty set and the boundary of a discrete set of points are both assigned a dimension of 0.,A,"Similarly, for the class of CW complexes, the dimension of an object is the largest  for which the -skeleton is nontrivial.  Intuitively, this can be described as follows: if the original space can be continuously deformed into a collection of higher-dimensional triangles joined at their faces with a complicated surface, then the dimension of the object is the dimension of those triangles.

Hausdorff dimension
The Hausdorff dimension is useful for studying structurally complicated sets, especially fractals. The Hausdorff dimension is defined for all metric spaces and, unlike the dimensions considered above, can also have non-integer real values. The box dimension or Minkowski dimension is a variant of the same idea. In general, there exist more definitions of fractal dimensions that work for highly irregular sets and attain non-integer positive real values."
Which of the following statements accurately describes the blocking temperature of an antiferromagnetic layer in a spin valve?,The blocking temperature of an antiferromagnetic layer in a spin valve is the temperature at which the magnetization of the ferromagnetic layer becomes aligned with the magnetic field. The blocking temperature is typically higher than the Néel temperature.,"The blocking temperature of an antiferromagnetic layer in a spin valve is the temperature below which the layer loses its ability to ""pin"" the magnetization direction of an adjacent ferromagnetic layer. The blocking temperature is typically higher than the Néel temperature.",The blocking temperature of an antiferromagnetic layer in a spin valve is the temperature at which the ferromagnetic layer becomes completely demagnetized. The blocking temperature is typically higher than the Néel temperature.,The blocking temperature of an antiferromagnetic layer in a spin valve is the temperature at or above which the layer ceases to prevent the orientation of an adjacent ferromagnetic layer. The blocking temperature is typically lower than the Néel temperature.,"The blocking temperature of an antiferromagnetic layer in a spin valve is the temperature at which the ferromagnetic layer loses its ability to ""pin"" the magnetization direction of an adjacent antiferromagnetic layer. The blocking temperature is typically higher than the Néel temperature.",D,"Antiferromagnets can couple to ferromagnets, for instance, through a mechanism known as exchange bias, in which the ferromagnetic film is either grown upon the antiferromagnet or annealed in an aligning magnetic field, causing the surface atoms of the ferromagnet to align with the surface atoms of the antiferromagnet. This provides the ability to ""pin"" the orientation of a ferromagnetic film, which provides one of the main uses in so-called spin valves, which are the basis of magnetic sensors including modern hard disk drive read heads. The temperature at or above which an antiferromagnetic layer loses its ability to ""pin"" the magnetization direction of an adjacent ferromagnetic layer is called the blocking temperature of that layer and is usually lower than the Néel temperature.

Geometric frustration"
What is the term used in astrophysics to describe light-matter interactions resulting in energy shifts in the radiation field?,Blueshifting,Redshifting,Reddening,Whitening,Yellowing,C,"The interactions and phenomena summarized in the subjects of radiative transfer and physical optics can result in shifts in the wavelength and frequency of electromagnetic radiation. In such cases, the shifts correspond to a physical energy transfer to matter or other photons rather than being by a transformation between reference frames. Such shifts can be from such physical phenomena as coherence effects or the scattering of electromagnetic radiation whether from charged elementary particles, from particulates, or from fluctuations of the index of refraction in a dielectric medium as occurs in the radio phenomenon of radio whistlers. While such phenomena are sometimes referred to as ""redshifts"" and ""blueshifts"", in astrophysics light-matter interactions that result in energy shifts in the radiation field are generally referred to as ""reddening"" rather than ""redshifting"" which, as a term, is normally reserved for the effects discussed above."
What is the role of axioms in a formal theory?,"Basis statements called axioms form the foundation of a formal theory and, together with the deducing rules, help in deriving a set of statements called theorems using proof theory.",Axioms are supplementary statements added to a formal theory that break down otherwise complex statements into more simple ones.,"Axioms are redundant statements that can be derived from other statements in a formal theory, providing additional perspective to theorems derived from the theory.",The axioms in a theory are used for experimental validation of the theorems derived from the statements in the theory.,"The axioms in a formal theory are added to prove that the statements derived from the theory are true, irrespective of their validity in the real world.",A,"In mathematical logic, an axiom schema (plural: axiom schemata or axiom schemas) generalizes the notion of axiom.

Formal definition
An axiom schema is a formula in the metalanguage of an axiomatic system, in which one or more schematic variables appear. These variables, which are metalinguistic constructs, stand for any term or subformula of the system, which may or may not be required to satisfy certain conditions. Often, such conditions require that certain variables be free, or that certain variables not appear in the subformula or term."
What did Fresnel predict and verify with regards to total internal reflections?,"Fresnel predicted and verified that three total internal reflections at 75°27' would give a precise circular polarization if two of the reflections had water as the external medium and the third had air, but not if the reflecting surfaces were all wet or all dry.","Fresnel predicted and verified that eight total internal reflections at 68°27' would give an accurate circular polarization if four of the reflections had water as the external medium while the other four had air, but not if the reflecting surfaces were all wet or all dry.","Fresnel predicted and verified that four total internal reflections at 30°27' would result in circular polarization if two of the reflections had water as the external medium while the other two had air, regardless if the reflecting surfaces were all wet or all dry.","Fresnel predicted and verified that two total internal reflections at 68°27' would give an accurate linear polarization if one of the reflections had water as the external medium and the other had air, but not if the reflecting surfaces were all wet or all dry.","Fresnel predicted and verified that four total internal reflections at 68°27' would give a precise circular polarization if two of the reflections had water as the external medium while the other two had air, but not if the reflecting surfaces were all wet or all dry.",E,"For added confidence, Fresnel predicted and verified that four total internal reflections at 68°27' would give an accurate circular polarization if two of the reflections had water as the external medium while the other two had air, but not if the reflecting surfaces were all wet or all dry.

Significance 

In summary, the invention of the rhomb was not a single event in Fresnel's career, but a process spanning a large part of it. Arguably, the calculation of the phase shift on total internal reflection marked not only the completion of his theory of the rhomb, but also the essential completion of his reconstruction of physical optics on the transverse-wave hypothesis (see Augustin-Jean Fresnel)."
What is the relationship between the Wigner function and the density matrix operator?,"The Wigner function W(x, p) is the Wigner transform of the density matrix operator ρ̂, and the trace of an operator with the density matrix Wigner-transforms to the equivalent phase-space integral overlap of g(x,p) with the Wigner function.","The Wigner function W(x, p) is a source function used for the density matrix operator ρ̂ and the product of these two functions creates the phase space wave function g(x, p).","The Wigner function W(x, p) is the derivative of the density matrix operator ρ̂ with respect to the phase space coordinate.","The Wigner function W(x, p) represents the Hamiltonian H(x,p) of the density matrix operator ρ̂, while the Moyal bracket {{⋅, ⋅}} represents the Poisson bracket in the phase space.","The Wigner function W(x, p) is the time derivative of the density matrix operator ρ̂ with respect to the phase space coordinate.",A,"Regardless, the Weyl–Wigner transform is a well-defined integral transform between the phase-space and operator representations, and yields insight into the workings of quantum mechanics. Most importantly, the Wigner quasi-probability distribution is the Wigner transform of the quantum density matrix, and, conversely, the density matrix is the Weyl transform of the Wigner function. 

In contrast to Weyl's original intentions in seeking a consistent quantization scheme, this map merely amounts to a change of representation within quantum mechanics; it need not connect ""classical"" with ""quantum"" quantities. For example, the phase-space function may depend explicitly on Planck's constant ħ, as it does in some familiar cases involving angular momentum.  This invertible representation change then allows one to express quantum mechanics in phase space, as was appreciated in the 1940s by Hilbrand J. Groenewold and José Enrique Moyal."
What is one of the examples of the models proposed by cosmologists and theoretical physicists without the cosmological or Copernican principles that can be used to address specific issues in the Lambda-CDM model and distinguish between current models and other possible models?,"The Copernican principle, which proposes that Earth, the Solar System, and the Milky Way are not at the centre of the universe, but instead, the universe is expanding equally in all directions. This principle is a modification of the Lambda-CDM model and has been shown to explain several observational results.","Inhomogeneous cosmology, which states that the universe is entirely homogeneous and isotropic, directly proportional to the density of matter and radiation. This model proposes that everything in the universe is completely uniform, but it does not match observations.","Inhomogeneous cosmology, which models the universe as an extremely large, low-density void, instead of using the concept of dark energy. According to the model, this theory can match the observed accelerating universe and cosmological constant, but it contradicts the Copernican principle.","The cosmological principle, which proposes that Earth, the Solar System, and the Milky Way are at the centre of the universe. This principle is a modification of the Lambda-CDM model and has been shown to explain several observational results.","The principle of dark energy, which proposes that a new form of energy, not previously detected, is responsible for the acceleration of the expansion of the universe. This principle is a modification of the Lambda-CDM model and has been shown to explain several observational results.",C,"Physics without the principle
The standard model of cosmology, the Lambda-CDM model, assumes the Copernican principle and the more general Cosmological principle. The Lambda-CDM model's observations are largely consistent, but there remain unsolved problems.  Some cosmologists and theoretical physicists have created models without the Cosmological or Copernican principles to constrain the values of observational results, to address specific known issues, and to propose tests to distinguish between current models and other possible models.

A prominent example in this context is the observed accelerating universe and cosmological constant.  Instead of using the current accepted idea of dark energy, this model proposes the universe is much more inhomogeneous than currently assumed, and instead, we are in an extremely large low-density void.  To match observations we would have to be very close to the centre of this void, immediately contradicting the Copernican principle.

See also"
What is the Roche limit?,The Roche limit is the distance at which tidal effects would cause an object to rotate since the forces exerted by two massive bodies produce a torque on a third object.,The Roche limit is the distance at which tidal effects would cause an object to unite since differential force from a planet results in parts becoming attracted to one another.,The Roche limit is the distance at which tidal effects would cause a planet to disintegrate since differential force from an object overcomes the planet's core.,The Roche limit is the distance at which tidal effects would cause an object to disintegrate since differential force from a planet overcomes the attraction of the parts between them.,"The Roche limit is the distance at which tidal effects would cause an object to break apart due to differential force from the planet overcoming the attraction of the parts of the object for one another, which depends on the object's density and composition, as well as the mass and size of the planet.",D,"In celestial mechanics, the Roche limit, also called Roche radius, is the distance from a celestial body within which a second celestial body, held together only by its own force of gravity, will disintegrate because the first body's tidal forces exceed the second body's gravitational self-attraction. Inside the Roche limit, orbiting material disperses and forms rings, whereas outside the limit, material tends to coalesce.  The Roche radius depends on the radius of the first body and on the ratio of the bodies' densities.

The term is named after Édouard Roche (,  ), who was the French astronomer who first calculated this theoretical limit in 1848.

Explanation"
What is Martin Heidegger's view on the relationship between time and human existence?,"Martin Heidegger believes that humans exist within a time continuum that is infinite and does not have a defined beginning or end. The relationship to the past involves acknowledging it as a historical era, and the relationship to the future involves creating a world that will endure beyond one's own time.","Martin Heidegger believes that humans do not exist inside time, but that they are time. The relationship to the past is a present awareness of having been, and the relationship to the future involves anticipating a potential possibility, task, or engagement.","Martin Heidegger does not believe in the existence of time or that it has any effect on human consciousness. The relationship to the past and the future is insignificant, and human existence is solely based on the present.",Martin Heidegger believes that the relationship between time and human existence is cyclical. The past and present are interconnected and the future is predetermined. Human beings do not have free will.,"Martin Heidegger believes that time is an illusion, and the past, present, and future are all happening simultaneously. Humans exist outside of this illusion and are guided by a higher power.",B,"In contrast with his former mentor Edmund Husserl, Martin Heidegger (in his Being and Time) put ontology before epistemology and thought that phenomenology would have to be based on an observation and analysis of Dasein (""being-there""), human being, investigating the fundamental ontology of the Lebenswelt (lifeworld, Husserl's term) underlying all so-called regional ontologies of the special sciences. In Heidegger's philosophy, people are thrown into the world in a given situation, but they are also a project towards the future, possibility, freedom, wait, hope, anguish. In contrast with the philosopher Kierkegaard, Heidegger wanted to explore the problem of Dasein existentially (), rather than existentielly () because Heidegger argued Kierkegaard had already described the latter with ""penetrating fashion"". Most existentialist phenomenologists were concerned with how we are constituted by our experiences and yet how we are also free in some respect to modify both ourselves and the"
"What is the ""ultraviolet catastrophe""?",It is a phenomenon that occurs only in multi-mode vibration.,It is the misbehavior of a formula for higher frequencies.,It is the standing wave of a string in harmonic resonance.,It is a flaw in classical physics that results in the misallocation of energy.,It is a disproven theory about the distribution of electromagnetic radiation.,B,"The name comes from the earliest example of such a divergence, the ""ultraviolet catastrophe"" first encountered in understanding blackbody radiation. According to classical physics at the end of the nineteenth century, the quantity of radiation in the form of light released at any specific wavelength should increase with decreasing wavelength—in particular, there should be considerably more ultraviolet light released from a blackbody radiator than infrared light. Measurements showed the opposite, with maximal energy released at intermediate wavelengths, suggesting a failure of classical mechanics. This problem eventually led to the development of quantum mechanics."
What is the most popular explanation for the shower-curtain effect?,The pressure differential between the inside and outside of the shower,The decrease in velocity resulting in an increase in pressure,The movement of air across the outside surface of the shower curtain,The use of cold water,Bernoulli's principle,E,
What is the butterfly effect?,"The butterfly effect is a physical cause that occurs when a massive sphere is caused to roll down a slope starting from a point of unstable equilibrium, and its velocity is assumed to be caused by the force of gravity accelerating it.",The butterfly effect is a distributed causality that opens up the opportunity to understand the relationship between necessary and sufficient conditions in classical (Newtonian) physics.,The butterfly effect is a proportionality between the cause and the effect of a physical phenomenon in classical (Newtonian) physics.,The butterfly effect is a small push that is needed to set a massive sphere into motion when it is caused to roll down a slope starting from a point of unstable equilibrium.,The butterfly effect is a phenomenon that highlights the difference between the application of the notion of causality in physics and a more general use of causality as represented by Mackie's INUS conditions.,E,"The Lorenz system is a system of ordinary differential equations first studied by mathematician and meteorologist Edward Lorenz. It is notable for having chaotic solutions for certain parameter values and initial conditions. In particular, the Lorenz attractor is a set of chaotic solutions of the Lorenz system. In popular media the ""butterfly effect"" stems from the real-world implications of the Lorenz attractor, i.e. that in any physical system, in the absence of perfect knowledge of the initial conditions (even the minuscule disturbance of the air due to a butterfly flapping its wings), our ability to predict its future course will always fail. This underscores that physical systems can be completely deterministic and yet still be inherently unpredictable even in the absence of quantum effects. The shape of the Lorenz attractor itself, when plotted graphically, may also be seen to resemble a butterfly."
What is the 'reactive Leidenfrost effect' observed in non-volatile materials?,"The 'reactive Leidenfrost effect' is a phenomenon where solid particles float above hot surfaces and move erratically, observed in non-volatile materials.","The 'reactive Leidenfrost effect' is a phenomenon where solid particles float above hot surfaces and move erratically, observed in volatile materials.","The 'reactive Leidenfrost effect' is a phenomenon where solid particles sink into hot surfaces and move slowly, observed in non-volatile materials.","The 'reactive Leidenfrost effect' is a phenomenon where solid particles float above cold surfaces and move erratically, observed in non-volatile materials.","The 'reactive Leidenfrost effect' is a phenomenon where solid particles sink into cold surfaces and move slowly, observed in non-volatile materials.",A,"Reactive Leidenfrost effect 

Non-volatile materials were discovered in 2015 to also exhibit a 'reactive Leidenfrost effect,' whereby solid particles were observed to float above hot surfaces and skitter around erratically. Detailed characterization of the reactive Leidenfrost effect was completed for small particles of cellulose (~0.5 mm) on high temperature polished surfaces by high speed photography. Cellulose was shown to decompose to short-chain oligomers which melt and wet smooth surfaces with increasing heat transfer associated with increasing surface temperature. Above , cellulose was observed to exhibit transition boiling with violent bubbling and associated reduction in heat transfer. Liftoff of the cellulose droplet (depicted at the right) was observed to occur above about  associated with a dramatic reduction in heat transfer."
What is reciprocal length or inverse length?,"Reciprocal length or inverse length is a quantity or measurement used in physics and chemistry. It is the reciprocal of time, and common units used for this measurement include the reciprocal second or inverse second (symbol: s−1), the reciprocal minute or inverse minute (symbol: min−1).","Reciprocal length or inverse length is a quantity or measurement used in geography and geology. It is the reciprocal of area, and common units used for this measurement include the reciprocal square metre or inverse square metre (symbol: m−2), the reciprocal square kilometre or inverse square kilometre (symbol: km−2).","Reciprocal length or inverse length is a quantity or measurement used in biology and medicine. It is the reciprocal of mass, and common units used for this measurement include the reciprocal gram or inverse gram (symbol: g−1), the reciprocal kilogram or inverse kilogram (symbol: kg−1).","Reciprocal length or inverse length is a quantity or measurement used in economics and finance. It is the reciprocal of interest rate, and common units used for this measurement include the reciprocal percent or inverse percent (symbol: %−1), the reciprocal basis point or inverse basis point (symbol: bp−1).","Reciprocal length or inverse length is a quantity or measurement used in several branches of science and mathematics. It is the reciprocal of length, and common units used for this measurement include the reciprocal metre or inverse metre (symbol: m−1), the reciprocal centimetre or inverse centimetre (symbol: cm−1).",E,"In mathematics, a multiplicative inverse or reciprocal for a number x, denoted by 1/x or x−1, is a number which when multiplied by x yields the multiplicative identity, 1. The multiplicative inverse of a fraction a/b is b/a. For the multiplicative inverse of a real number, divide 1 by the number. For example, the reciprocal of 5 is one fifth (1/5 or 0.2), and the reciprocal of 0.25 is 1 divided by 0.25, or 4. The reciprocal function, the function f(x) that maps x to 1/x, is one of the simplest examples of a function which is its own inverse (an involution).

Multiplying by a number is the same as dividing by its reciprocal and vice versa. For example, multiplication by 4/5 (or 0.8) will give the same result as division by 5/4 (or 1.25). Therefore, multiplication by a number followed by multiplication by its reciprocal yields the original number (since the product of the number and its reciprocal is 1)."
Which of the following statements is true about the categorization of planetary systems according to their orbital dynamics?,Planetary systems cannot be categorized based on their orbital dynamics.,"Planetary systems can be categorized as resonant, non-resonant-interacting, hierarchical, or some combination of these, but only based on the number of planets in the system.",Planetary systems can only be categorized as resonant or non-resonant-interacting.,"Planetary systems can be categorized as resonant, non-resonant-interacting, hierarchical, or some combination of these.",Planetary systems can only be categorized as hierarchical or non-hierarchical.,D,"Orbital dynamics
Planetary systems can be categorized according to their orbital dynamics as resonant, non-resonant-interacting, hierarchical, or some combination of these. In resonant systems the orbital periods of the planets are in integer ratios. The Kepler-223 system contains four planets in an 8:6:4:3 orbital resonance.
Giant planets are found in mean-motion resonances more often than smaller planets.
In interacting systems the planets orbits are close enough together that they perturb the orbital parameters. The Solar System could be described as weakly interacting. In strongly interacting systems Kepler's laws do not hold.
In hierarchical systems the planets are arranged so that the system can be gravitationally considered as a nested system of two-bodies, e.g. in a star with a close-in hot jupiter with another gas giant much further out, the star and hot jupiter form a pair that appears as a single object to another planet that is far enough out."
What is the propagation constant in sinusoidal waves?,The propagation constant is a measure of the amplitude of the sinusoidal wave that varies with distance.,The propagation constant is a real number that remains constant with distance due to the phase change in the sinusoidal wave.,The propagation constant is a real number that varies with distance due to the phase change in the sinusoidal wave.,The propagation constant is a complex number that varies with distance due to the phase change in the sinusoidal wave.,The propagation constant is a complex number that remains constant with distance due to the phase change in the sinusoidal wave.,D,"In physics, a sinusoidal (or monochromatic) plane wave is a special case of plane wave: a field whose value varies as a sinusoidal function of time and of the distance from some fixed plane.

For any position  in space and any time , the value of such a field can be written as

where  is a unit-length vector, the direction of propagation of the wave, and """" denotes the dot product of two vectors.  The parameter , which may be a scalar or a vector, is called the amplitude of the wave; the coefficient , a positive scalar, its spatial frequency; and the adimensional scalar , an angle in radians, is its initial phase or phase shift.

The scalar quantity  gives the (signed) displacement of the point  from the plane that is perpendicular to  and goes through the origin of the coordinate system.  This quantity is constant over each plane perpendicular to .

At time , the field  varies with the displacement  as a sinusoidal function"
What is the gravitomagnetic interaction?,The gravitomagnetic interaction is a force that is produced by the rotation of atoms in materials with linear properties that enhance time-varying gravitational fields.,"The gravitomagnetic interaction is a force that acts against gravity, produced by materials that have nonlinear properties that enhance time-varying gravitational fields.","The gravitomagnetic interaction is a new force of nature generated by rotating matter, whose intensity is proportional to the rate of spin, according to the general theory of relativity.","The gravitomagnetic interaction is a force that occurs in neutron stars, producing a gravitational analogue of the Meissner effect.",The gravitomagnetic interaction is a force that is produced by the rotation of atoms in materials of different gravitational permeability.,C,"Higher-order effects
Some higher-order gravitomagnetic effects can reproduce effects reminiscent of the interactions of more conventional polarized charges. For instance, if two wheels are spun on a common axis, the mutual gravitational attraction between the two wheels will be greater if they spin in opposite directions than in the same direction. This can be expressed as an attractive or repulsive gravitomagnetic component.

Gravitomagnetic arguments also predict that a flexible or fluid toroidal mass undergoing minor axis rotational acceleration (accelerating ""smoke ring"" rotation) will tend to pull matter through the throat (a case of rotational frame dragging, acting through the throat). In theory, this configuration might be used for accelerating objects (through the throat) without such objects experiencing any g-forces."
What did Newton's manuscripts of the 1660s show?,Newton learned about tangential motion and radially directed force or endeavour from Hooke's work.,Newton's manuscripts did not show any evidence of combining tangential motion with the effects of radially directed force or endeavour.,Newton combined tangential motion with the effects of radially directed force or endeavour and expressed the concept of linear inertia.,Newton's manuscripts showed that he learned about the inverse square law from Hooke's private papers.,"Newton's manuscripts showed that he was indebted to Descartes' work, published in 1644, for the concept of linear inertia.",C,
What is the decay energy for the free neutron decay process?,0.013343 MeV,0.013 MeV,"1,000 MeV",0.782 MeV,0.782343 MeV,E,"For diagrams at several levels of detail, see § Decay process, below.

Energy budget
For the free neutron, the decay energy for this process (based on the rest masses of the neutron, proton and electron) is . That is the difference between the rest mass of the neutron and the sum of the rest masses of the products. That difference has to be carried away as kinetic energy. The maximal energy of the beta decay electron (in the process wherein the neutrino receives a vanishingly small amount of kinetic energy) has been measured at . The latter number is not well-enough measured to determine the comparatively tiny rest mass of the neutrino (which must in theory be subtracted from the maximal electron kinetic energy); furthermore, neutrino mass is constrained by many other methods.

A small fraction (about 1 in 1,000) of free neutrons decay with the same products, but add an extra particle in the form of an emitted gamma ray:"
What is Hesse's principle of transfer in geometry?,Hesse's principle of transfer is a concept in biology that explains the transfer of genetic information from one generation to another.,Hesse's principle of transfer is a concept in chemistry that explains the transfer of electrons between atoms in a chemical reaction.,Hesse's principle of transfer is a concept in physics that explains the transfer of energy from one object to another.,Hesse's principle of transfer is a concept in economics that explains the transfer of wealth from one individual to another.,"Hesse's principle of transfer is a concept in geometry that states that if the points of the projective line P1 are depicted by a rational normal curve in Pn, then the group of the projective transformations of Pn that preserve the curve is isomorphic to the group of the projective transformations of P1.",E,"In geometry, the Hesse configuration, introduced by Colin Maclaurin and studied by ,  is a configuration of 9 points and 12 lines with three points per line and four lines through each point. It can be realized in the complex projective plane as the set of inflection points of an elliptic curve, but it has no realization in the Euclidean plane.

Description
The Hesse configuration has the same incidence relations as the lines and points of the affine plane over the field of 3 elements. That is, the points of the Hesse configuration may be identified with ordered pairs of numbers modulo 3, and the lines of the configuration may correspondingly be identified with the triples of points  satisfying a linear equation . Alternatively, the points of the configuration may be identified by the squares of a tic-tac-toe board, and the lines may be identified with the lines and broken diagonals of the board."
What is the relationship between the Cauchy momentum equation and the Navier-Stokes equation?,"The Navier-Stokes equation can be derived from the Cauchy momentum equation by specifying the stress tensor through a constitutive relation, expressing the shear tensor in terms of viscosity and fluid velocity, and assuming constant density and viscosity.",The Navier-Stokes equation is a simplified version of the Cauchy momentum equation that only applies to situations with constant density and viscosity.,"The Navier-Stokes equation is a special case of the Cauchy momentum equation, which is a more general equation that applies to all non-relativistic momentum conservation situations.",The Cauchy momentum equation and the Navier-Stokes equation are completely unrelated and cannot be used interchangeably in any situation.,"The Cauchy momentum equation is a special case of the Navier-Stokes equation, which is a more general equation that applies to all non-relativistic momentum conservation situations.",A,"Incompressible Navier–Stokes momentum equation

Incompressible Navier–Stokes momentum equation is a Cauchy momentum equation with the Pascal law and Stokes's law being the stress constitutive relations:

in nondimensional convective form it is:

where  is the Reynolds number. Free Navier–Stokes equations are dissipative (non conservative).

Other applications

Ship hydrodynamics

In marine hydrodynamic applications, the Froude number is usually referenced with the notation  and is defined as:

where  is the relative flow velocity between the sea and ship,  is in particular the acceleration due to gravity, and  is the length of the ship at the water line level, or  in some notations. It is an important parameter with respect to the ship's drag, or resistance, especially in terms of wave making resistance."
What is X-ray pulsar-based navigation (XNAV)?,X-ray pulsar-based navigation (XNAV) is a navigation technique that uses the periodic X-ray signals emitted from pulsars to determine the location of a vehicle in the Earth's atmosphere.,"X-ray pulsar-based navigation (XNAV) is a navigation technique that uses the periodic radio signals emitted from pulsars to determine the location of a vehicle in deep space, such as a spacecraft.","X-ray pulsar-based navigation (XNAV) is a navigation technique that uses the periodic X-ray signals emitted from satellites to determine the location of a vehicle in deep space, such as a spacecraft.","X-ray pulsar-based navigation (XNAV) is a navigation technique that uses the periodic X-ray signals emitted from pulsars to determine the location of a vehicle in deep space, such as a spacecraft.","X-ray pulsar-based navigation (XNAV) is a navigation technique that uses the periodic radio signals emitted from satellites to determine the location of a vehicle in deep space, such as a spacecraft.",D,"X-ray pulsar-based navigation and timing (XNAV) or simply pulsar navigation is a navigation technique whereby the periodic X-ray signals emitted from pulsars are used to determine the location of a vehicle, such as a spacecraft in deep space. A vehicle using XNAV would compare received X-ray signals with a database of known pulsar frequencies and locations. Similar to GPS, this comparison would allow the vehicle to calculate its position accurately (±5 km). The advantage of using X-ray signals over radio waves is that X-ray telescopes can be made smaller and lighter. Experimental demonstrations have been reported in 2018.

Spacecraft navigation"
What is the evidence for the existence of a supermassive black hole at the center of the Milky Way galaxy?,"The Milky Way galaxy has a supermassive black hole at its center because of the bright flare activity observed near Sagittarius A*. The radius of the central object must be less than 17 light-hours, because otherwise S2 would collide with it. Observations of the star S14 indicate that the radius is no more than 6.25 light-hours, about the diameter of Uranus' orbit. No known astronomical object other than a black hole can contain 4.0 million M☉ in this volume of space.","The Milky Way galaxy has a supermassive black hole at its center because the star S14 follows an elliptical orbit with a period of 15.2 years and a pericenter of 17 light-hours from the center of the central object. From the motion of star S14, the object's mass can be estimated as 4.0 million M☉, or about 7.96×1036 kg. The radius of the central object must be less than 17 light-hours, because otherwise S14 would collide with it. Observations of the star S2 indicate that the radius is no more than 6.25 light-hours, about the diameter of Uranus' orbit. No known astronomical object other than a black hole can contain 4.0 million M☉ in this volume of space.","The Milky Way galaxy has a supermassive black hole at its center because of the bright flare activity observed near Sagittarius A*. The radius of the central object must be less than 6.25 light-hours, about the diameter of Uranus' orbit. Observations of the star S2 indicate that the radius is no more than 17 light-hours, because otherwise S2 would collide with it. No known astronomical object other than a black hole can contain 4.0 million M☉ in this volume of space.",The Milky Way galaxy has a supermassive black hole at its center because it is the only explanation for the bright flare activity observed near Sagittarius A* at a separation of six to ten times the gravitational radius of the candidate SMBH.,"The star S2 follows an elliptical orbit with a period of 15.2 years and a pericenter of 17 light-hours from the center of the central object. From the motion of star S2, the object's mass can be estimated as 4.0 million M☉, or about 7.96×1036 kg. The radius of the central object must be less than 17 light-hours, because otherwise S2 would collide with it. Observations of the star S14 indicate that the radius is no more than 6.25 light-hours, about the diameter of Uranus' orbit. No known astronomical object other than a black hole can contain 4.0 million M☉ in this volume of space.",E,"Unambiguous dynamical evidence for supermassive black holes exists only in a handful of galaxies; these include the Milky Way, the Local Group galaxies M31 and M32, and a few galaxies beyond the Local Group, e.g. NGC 4395. In these galaxies, the mean square (or rms) velocities of the stars or gas rises proportionally to 1/ near the center, indicating a central point mass. In all other galaxies observed to date, the rms velocities are flat, or even falling, toward the center, making it impossible to state with certainty that a supermassive black hole is present. Nevertheless, it is commonly accepted that the center of nearly every galaxy contains a supermassive black hole. The reason for this assumption is the M–sigma relation, a tight (low scatter) relation between the mass of the hole in the 10 or so galaxies with secure detections, and the velocity dispersion of the stars in the bulges of those galaxies. This correlation, although based on just a handful of galaxies, suggests to"
What is the function of the fibrous cardiac skeleton?,The fibrous cardiac skeleton is a system of blood vessels that supplies oxygen and nutrients to the heart muscle.,"The fibrous cardiac skeleton is responsible for the pumping action of the heart, regulating the flow of blood through the atria and ventricles.","The fibrous cardiac skeleton provides structure to the heart, forming the atrioventricular septum that separates the atria from the ventricles, and the fibrous rings that serve as bases for the four heart valves.",The fibrous cardiac skeleton is a network of nerves that controls the heartbeat and rhythm of the heart.,"The fibrous cardiac skeleton is a protective layer that surrounds the heart, shielding it from external damage.",C,"The fibrous cardiac skeleton gives structure to the heart. It forms the atrioventricular septum, which separates the atria from the ventricles, and the fibrous rings, which serve as bases for the four heart valves. The cardiac skeleton also provides an important boundary in the heart's electrical conduction system since collagen cannot conduct electricity. The interatrial septum separates the atria, and the interventricular septum separates the ventricles. The interventricular septum is much thicker than the interatrial septum since the ventricles need to generate greater pressure when they contract.

Valves

The heart has four valves, which separate its chambers. One valve lies between each atrium and ventricle, and one valve rests at the exit of each ventricle."
What is the Carnot engine?,The Carnot engine is a theoretical engine that operates in the limiting mode of extreme speed known as dynamic. It represents the theoretical maximum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures.,The Carnot engine is an ideal heat engine that operates in the limiting mode of extreme slowness known as quasi-static. It represents the theoretical maximum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures.,The Carnot engine is a real heat engine that operates in the limiting mode of extreme speed known as dynamic. It represents the theoretical minimum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures.,The Carnot engine is a theoretical engine that operates in the limiting mode of extreme slowness known as quasi-static. It represents the theoretical minimum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures.,The Carnot engine is a real engine that operates in the limiting mode of extreme slowness known as quasi-static. It represents the theoretical maximum efficiency of a heat engine operating between any two given thermal or heat reservoirs at different temperatures.,B,"A photo-Carnot engine is a Carnot cycle engine in which the working medium is a photon inside a cavity with perfectly reflecting walls.  Radiation is the working fluid, and the piston is driven by radiation pressure.

A quantum Carnot engine is one in which the atoms in the heat bath are given a small bit of quantum coherence.  The phase of the atomic coherence provides a new control parameter.

The deep physics behind the second law of thermodynamics is not violated; nevertheless, the quantum Carnot engine has certain features that are not possible in a classical engine.

Derivation 
The internal energy of the photo-Carnot engine is proportional to the volume (unlike the ideal-gas equivalent) as well as the 4th power of the temperature (see Stefan–Boltzmann law) using  :

 

The radiation pressure is only proportional to this 4th power of temperature but no other variables, meaning that for this photo-Carnot engine an isotherm is equivalent to an isobar:"
Which mathematical function is commonly used to characterize linear time-invariant systems?,Trigonometric function,Quadratic function,Exponential function,Logarithmic function,Transfer function,E,"If a system is time-invariant then the system block commutes with an arbitrary delay.

If a time-invariant system is also linear, it is the subject of linear time-invariant theory (linear time-invariant) with direct applications in NMR spectroscopy, seismology, circuits, signal processing, control theory, and other technical areas. Nonlinear time-invariant systems lack a comprehensive, governing theory. Discrete time-invariant systems are known as shift-invariant systems. Systems which lack the time-invariant property are studied as time-variant systems.

Simple example 

To demonstrate how to determine if a system is time-invariant, consider the two systems:

 System A: 
 System B: 

Since the System Function  for system A explicitly depends on t outside of , it is not time-invariant because the time-dependence is not explicitly a function of the input function."
What is the second law of thermodynamics?,The second law of thermodynamics is a physical law based on universal experience concerning heat and energy interconversions. It states that heat always moves from colder objects to hotter objects unless energy in some form is supplied to reverse the direction of heat flow.,The second law of thermodynamics is a physical law based on universal experience concerning heat and energy interconversions. It establishes that the internal energy of a thermodynamic system is a physical property that can be used to predict whether processes are forbidden despite obeying the requirement of conservation of energy as expressed in the first law of thermodynamics.,The second law of thermodynamics is a physical law based on universal experience concerning heat and energy interconversions. It establishes that all heat energy can be converted into work in a cyclic process.,"The second law of thermodynamics is a physical law based on universal experience concerning heat and energy interconversions. It states that the entropy of isolated systems left to spontaneous evolution can decrease, as they always arrive at a state of thermodynamic equilibrium where the entropy is highest at the given internal energy.",The second law of thermodynamics is a physical law based on universal experience concerning heat and energy interconversions. It establishes the concept of entropy as a physical property of a thermodynamic system and can be used to predict whether processes are forbidden despite obeying the requirement of conservation of energy as expressed in the first law of thermodynamics.,E,"Though not a widely named ""law,"" it is an axiom of thermodynamics that there exist states of thermodynamic equilibrium. The second law of thermodynamics states that when an isolated body of material starts from an equilibrium state, in which, portions of it are held at different states by more or less permeable or impermeable partitions, and a thermodynamic operation removes or makes the partitions more permeable, then it spontaneously reaches its own, new state of internal thermodynamic equilibrium, and this is accompanied by an increase in the sum of the entropies of the portions.

Overview"
"What are amorphous ferromagnetic metallic alloys, and what are their advantages?","Amorphous ferromagnetic metallic alloys are crystalline alloys that can be made by rapidly cooling a liquid alloy. Their properties are nearly anisotropic, resulting in low coercivity, low hysteresis loss, high permeability, and high electrical resistivity.","Amorphous ferromagnetic metallic alloys are non-crystalline alloys that can be made by slowly heating a solid alloy. Their properties are nearly isotropic, resulting in low coercivity, low hysteresis loss, high permeability, and high electrical resistivity.","Amorphous ferromagnetic metallic alloys are crystalline alloys that can be made by slowly cooling a liquid alloy. Their properties are nearly anisotropic, resulting in high coercivity, high hysteresis loss, low permeability, and low electrical resistivity.","Amorphous ferromagnetic metallic alloys are non-crystalline alloys that can be made by rapidly cooling a liquid alloy. Their properties are nearly isotropic, resulting in low coercivity, low hysteresis loss, high permeability, and high electrical resistivity.","Amorphous ferromagnetic metallic alloys are non-crystalline alloys that can be made by rapidly heating a solid alloy. Their properties are nearly isotropic, resulting in high coercivity, high hysteresis loss, low permeability, and low electrical resistivity.",D,"Amorphous (non-crystalline) ferromagnetic metallic alloys can be made by very rapid quenching (cooling) of a liquid alloy. These have the advantage that their properties are nearly isotropic (not aligned along a crystal axis); this results in low coercivity, low hysteresis loss, high permeability, and high electrical resistivity. One such typical material is a transition metal-metalloid alloy, made from about 80% transition metal (usually Fe, Co, or Ni) and a metalloid component (B, C, Si, P, or Al) that lowers the melting point.

A relatively new class of exceptionally strong ferromagnetic materials are the rare-earth magnets. They contain lanthanide elements that are known for their ability to carry large magnetic moments in well-localized f-orbitals.

The table lists a selection of ferromagnetic and ferrimagnetic compounds, along with the temperature above which they cease to exhibit spontaneous magnetization (see Curie temperature)."
What is the Penrose process?,"The Penrose process is a mechanism through which objects can emerge from the ergosphere with less energy than they entered with, taking energy from the rotational energy of the black hole and speeding up its rotation.","The Penrose process is a mechanism through which objects can emerge from the ergosphere with the same energy as they entered with, taking energy from the rotational energy of the black hole and maintaining its rotation.","The Penrose process is a mechanism through which objects can emerge from the ergosphere with more energy than they entered with, taking extra energy from the rotational energy of the black hole and slowing down its rotation.","The Penrose process is a mechanism through which objects can emerge from the event horizon with less energy than they entered with, taking energy from the rotational energy of the black hole and speeding up its rotation.","The Penrose process is a mechanism through which objects can emerge from the event horizon with more energy than they entered with, taking extra energy from the rotational energy of the black hole and slowing down its rotation.",C,"The Penrose process (also called Penrose mechanism) is theorised by Roger Penrose as a means whereby energy can be extracted from a rotating black hole. The process takes advantage of the ergosphere --- a region of spacetime around the black hole dragged by its rotation faster than the speed of light, meaning that from the point of an outside observer any matter inside is forced to move in the direction of the rotation of the black hole."
What was the aim of the Gravity Probe B (GP-B) mission?,To prove that pressure contributes equally to spacetime curvature as does mass-energy.,"To measure spacetime curvature near Earth, with particular emphasis on gravitomagnetism.",To measure the distribution of Fe and Al on the Moon's surface.,"To confirm the relatively large geodetic effect due to simple spacetime curvature, and is also known as de Sitter precession.",To measure the discrepancy between active and passive mass to about 10−12.,B,"Gravity Probe B (GP-B) was a satellite-based experiment to test two unverified predictions of general relativity: the geodetic effect and frame-dragging. This was to be accomplished by measuring, very precisely, tiny changes in the direction of spin of four gyroscopes contained in an Earth-orbiting satellite at  altitude, crossing directly over the poles.

The satellite was launched on 20 April 2004 on a Delta II rocket. The spaceflight phase lasted until 2005; Its aim was to measure spacetime curvature near Earth, and thereby the stress–energy tensor (which is related to the distribution and the motion of matter in space) in and near Earth. This provided a test of general relativity, gravitomagnetism and related models. The principal investigator was Francis Everitt."
What was Pierre de Fermat's solution to the problem of refraction?,"Fermat supposed that light took the path of least resistance, and that different media offered the same resistance. His eventual solution, described in a letter to La Chambre dated 1 January 1662, construed ""resistance"" as inversely proportional to speed, so that light took the path of least time. That premise yielded the ordinary law of refraction, provided that light traveled more slowly in the optically denser medium.","Fermat supposed that light took the path of least resistance, and that different media offered different resistances. His eventual solution, described in a letter to La Chambre dated 1 January 1662, construed ""resistance"" as directly proportional to speed, so that light took the path of least time. That premise yielded the ordinary law of refraction, provided that light traveled more quickly in the optically denser medium.","Fermat supposed that light took the path of least resistance, and that different media offered the same resistance. His eventual solution, described in a letter to La Chambre dated 1 January 1662, construed ""resistance"" as directly proportional to speed, so that light took the path of least time. That premise yielded the ordinary law of refraction, provided that light traveled more slowly in the optically denser medium.","Fermat supposed that light took the path of least resistance, and that different media offered the same resistance. His eventual solution, described in a letter to La Chambre dated 1 January 1662, construed ""resistance"" as inversely proportional to speed, so that light took the path of least time. That premise yielded the ordinary law of refraction, provided that light traveled more quickly in the optically denser medium.","Fermat supposed that light took the path of least resistance, and that different media offered different resistances. His eventual solution, described in a letter to La Chambre dated 1 January 1662, construed ""resistance"" as inversely proportional to speed, so that light took the path of least time. That premise yielded the ordinary law of refraction, provided that light traveled more slowly in the optically denser medium.",E,"Fermat replied that refraction might be brought into the same framework by supposing that light took the path of least resistance, and that different media offered different resistances. His eventual solution, described in a letter to La Chambre dated 1 January 1662, construed ""resistance"" as inversely proportional to speed, so that light took the path of least time. That premise yielded the ordinary law of refraction, provided that light traveled more slowly in the optically denser medium."
What is the reason behind the adoption of a logarithmic scale of 5√100 ≈ 2.512 between magnitudes in astronomy?,The logarithmic scale was adopted to ensure that five magnitude steps corresponded precisely to a factor of 100 in brightness.,The logarithmic scale was adopted to measure the size of stars.,The logarithmic scale was adopted to measure the intensity of light coming from a star.,The logarithmic scale was adopted to ensure that the apparent sizes of stars were not spurious.,The logarithmic scale was adopted to measure the distance between stars.,A,"Modern definition 

Early photometric measurements (made, for example, by using a light to project an artificial “star” into a telescope's field of view and adjusting it to match real stars in brightness) demonstrated that first magnitude stars are about 100 times brighter than sixth magnitude stars.

Thus in 1856 Norman Pogson of Oxford proposed that a logarithmic scale of  ≈ 2.512 be adopted between magnitudes, so five magnitude steps corresponded precisely to a factor of 100 in brightness. Every interval of one magnitude equates to a variation in brightness of  or roughly 2.512 times. Consequently, a magnitude 1 star is about 2.5 times brighter than a magnitude 2 star, about 2.52 times brighter than a magnitude 3 star, about 2.53 times brighter than a magnitude 4 star, and so on."
What is the spin quantum number?,The spin quantum number is a measure of the distance between an elementary particle and the nucleus of an atom.,The spin quantum number is a measure of the size of an elementary particle.,The spin quantum number is a measure of the charge of an elementary particle.,The spin quantum number is a measure of the speed of an elementary particle's rotation around some axis.,"The spin quantum number is a dimensionless quantity obtained by dividing the spin angular momentum by the reduced Planck constant ħ, which has the same dimensions as angular momentum.",E,"In quantum mechanics, spin is an intrinsic property of all elementary particles. All known fermions, the particles that constitute ordinary matter, have a spin of . The spin number describes how many symmetrical facets a particle has in one full rotation; a spin of  means that the particle must be rotated by two full turns (through 720°) before it has the same configuration as when it started.

Particles having net spin  include the proton, neutron, electron, neutrino, and quarks. The dynamics of spin- objects cannot be accurately described using classical physics; they are among the simplest systems which require quantum mechanics to describe them.  As such, the study of the behavior of spin- systems forms a central part of quantum mechanics.

Stern–Gerlach experiment"
What is the synapstor or synapse transistor?,A device used to demonstrate a neuro-inspired circuit that shows short-term potentiation for learning and inactivity-based forgetting.,A device used to demonstrate a neuro-inspired circuit that shows long-term potentiation for learning and activity-based forgetting.,A device used to demonstrate a neuro-inspired circuit that shows short-term depression for learning and inactivity-based forgetting.,A device used to demonstrate a neuro-inspired circuit that shows short-term potentiation for learning and activity-based forgetting.,A device used to demonstrate a neuro-inspired circuit that shows long-term potentiation for learning and inactivity-based forgetting.,E,"A synaptic transistor is an electrical device that can learn in ways similar to a neural synapse. It optimizes its own properties for the functions it has carried out in the past. The device mimics the behavior of the property of neurons called spike-timing-dependent plasticity, or STDP.

Structure
Its structure is similar to that of a field effect transistor, where an ionic liquid takes the place of the gate insulating layer between the gate electrode and the conducting channel. That channel is composed of samarium nickelate (, or SNO) rather than the field effect transistor's doped silicon."
What is spontaneous symmetry breaking?,"Spontaneous symmetry breaking occurs when the action of a theory has no symmetry, but the vacuum state has a symmetry. In that case, there will exist a local operator that is non-invariant under the symmetry, giving it a nonzero vacuum expectation value.","Spontaneous symmetry breaking occurs when the action of a theory has a symmetry, and the vacuum state also has the same symmetry. In that case, there will exist a local operator that is invariant under the symmetry, giving it a zero vacuum expectation value.","Spontaneous symmetry breaking occurs when the action of a theory has no symmetry, and the vacuum state also has no symmetry. In that case, there will exist a local operator that is invariant under the symmetry, giving it a zero vacuum expectation value.","Spontaneous symmetry breaking occurs when the action of a theory has a symmetry, but the vacuum state violates this symmetry. In that case, there will exist a local operator that is invariant under the symmetry, giving it a zero vacuum expectation value.","Spontaneous symmetry breaking occurs when the action of a theory has a symmetry, but the vacuum state violates this symmetry. In that case, there will exist a local operator that is non-invariant under the symmetry, giving it a nonzero vacuum expectation value.",E,"Spontaneous symmetry breaking is a spontaneous process of symmetry breaking, by which a physical system in a symmetric state ends up in an asymmetric state. In particular, it can describe systems where the equations of motion or the Lagrangian obey symmetries, but the lowest-energy vacuum solutions do not exhibit that same symmetry. When the system goes to one of those vacuum solutions, the symmetry is broken for perturbations around that vacuum even though the entire Lagrangian retains that symmetry.

Overview

By definition, spontaneous symmetry breaking requires the existence of physical laws (e.g. quantum mechanics) which are invariant under a symmetry transformation (such as translation or rotation), so that any pair of outcomes differing only by that transformation have the same probability distribution. For example if measurements of an observable at any two different positions have the same probability distribution, the observable has translational symmetry."
What is the proper distance for a redshift of 8.2?,"The proper distance for a redshift of 8.2 is about 6.2 Gpc, or about 24 billion light-years.","The proper distance for a redshift of 8.2 is about 7.2 Gpc, or about 26 billion light-years.","The proper distance for a redshift of 8.2 is about 9.2 Gpc, or about 30 billion light-years.","The proper distance for a redshift of 8.2 is about 8.2 Gpc, or about 28 billion light-years.","The proper distance for a redshift of 8.2 is about 10.2 Gpc, or about 32 billion light-years.",C,"Together with the scale factor it gives the proper distance at the time:

Transverse comoving distance:

Angular diameter distance:

This formula is strictly correct if neither the solar system nor the object has a peculiar velocity component parallel to the line between them. Otherwise, the redshift that would pertain in that case should be used but  should be corrected for the motion of the solar system by a factor between 0.99867 and 1.00133, depending on the direction. (If one starts to move with velocity  towards an object, at any distance, the angular diameter of that object decreases by a factor of )

Luminosity distance:"
Who was the first to determine the velocity of a star moving away from the Earth using the Doppler effect?,Fraunhofer,William Huggins,Hippolyte Fizeau,Vogel and Scheiner,None of the above,B,"The first Doppler redshift was described by French physicist Hippolyte Fizeau in 1848, who pointed to the shift in spectral lines seen in stars as being due to the Doppler effect. The effect is sometimes called the ""Doppler–Fizeau effect"". In 1868, British astronomer William Huggins was the first to determine the velocity of a star moving away from the Earth by this method. In 1871, optical redshift was confirmed when the phenomenon was observed in Fraunhofer lines using solar rotation, about 0.1 Å in the red. In 1887, Vogel and Scheiner discovered the annual Doppler effect, the yearly change in the Doppler shift of stars located near the ecliptic due to the orbital velocity of the Earth. In 1901, Aristarkh Belopolsky verified optical redshift in the laboratory using a system of rotating mirrors."
What is the information loss paradox in black holes?,"Black holes have an infinite number of internal parameters, so all the information about the matter that went into forming the black hole is preserved. Regardless of the type of matter which goes into a black hole, it appears that all the information is conserved. As black holes evaporate by emitting Hawking radiation, the information is lost forever.","Black holes have an infinite number of internal parameters, so all the information about the matter that went into forming the black hole is preserved. Regardless of the type of matter which goes into a black hole, it appears that all the information is conserved. As black holes evaporate by emitting Hawking radiation, the information is lost temporarily but reappears once the black hole has fully evaporated.","Black holes have only a few internal parameters, so most of the information about the matter that went into forming the black hole is lost. Regardless of the type of matter which goes into a black hole, it appears that only information concerning the total mass, charge, and angular momentum are conserved. As black holes evaporate by emitting Hawking radiation, the information is lost forever.","Black holes have only a few internal parameters, so most of the information about the matter that went into forming the black hole is lost. Regardless of the type of matter which goes into a black hole, it appears that only information concerning the total mass, charge, and angular momentum are conserved. As black holes evaporate by emitting Hawking radiation, the information is preserved and reappears once the black hole has fully evaporated.","Black holes have only a few internal parameters, so most of the information about the matter that went into forming the black hole is preserved. Regardless of the type of matter which goes into a black hole, it appears that all the information is conserved. As black holes evaporate by emitting Hawking radiation, the information is preserved and reappears once the black hole has fully evaporated.",C,"Black hole complementarity is a conjectured solution to the black hole information paradox, proposed by Leonard Susskind, Larus Thorlacius, and Gerard 't Hooft.

Overview
Ever since Stephen Hawking suggested information is lost in an evaporating black hole once it passes through the event horizon and is inevitably destroyed at the singularity, and that this can turn pure quantum states into mixed states, some physicists have wondered if a complete theory of quantum gravity might be able to conserve information with a unitary time evolution. But how can this be possible if information cannot escape the event horizon without traveling faster than light? This seems to rule out Hawking radiation as the carrier of the missing information. It also appears as if information cannot be ""reflected"" at the event horizon as there is nothing special about it locally."
What is the Kutta condition?,"The Kutta condition is a physical requirement that the fluid moving along the lower and upper surfaces of an airfoil meet smoothly, with no fluid moving around the trailing edge of the airfoil.","The Kutta condition is a physical requirement that the fluid moving along the lower and upper surfaces of an airfoil meet smoothly, with no fluid moving around the leading edge of the airfoil.",The Kutta condition is a mathematical requirement that the loop used in applying the Kutta-Joukowski theorem must be chosen outside the boundary layer of the airfoil.,The Kutta condition is a mathematical requirement that the flow can be assumed inviscid in the entire region outside the airfoil provided the Reynolds number is large and the angle of attack is small.,The Kutta condition is a physical requirement that the circulation calculated using the loop corresponding to the surface of the airfoil must be zero for a viscous fluid.,A,"The Kutta condition is a principle in steady-flow fluid dynamics, especially aerodynamics, that is applicable to solid bodies with sharp corners, such as the trailing edges of airfoils.  It is named for German mathematician and aerodynamicist Martin Kutta.

Kuethe and Schetzer state the Kutta condition as follows:
A body with a sharp trailing edge which is moving through a fluid will create about itself a circulation of sufficient strength to hold the rear stagnation point at the trailing edge.

In fluid flow around a body with a sharp corner, the Kutta condition refers to the flow pattern in which fluid approaches the corner from above and below, meets at the corner, and then flows away from the body. None of the fluid flows around the sharp corner."
What is classical mechanics?,"Classical mechanics is the branch of physics that describes the motion of macroscopic objects using concepts such as mass, acceleration, and force. It is based on a three-dimensional Euclidean space with fixed axes, and utilises many equations and mathematical concepts to relate physical quantities to one another.","Classical mechanics is the branch of physics that describes the motion of microscopic objects using concepts such as energy, momentum, and wave-particle duality. It is based on a four-dimensional space-time continuum and utilises many equations and mathematical concepts to relate physical quantities to one another.",Classical mechanics is the branch of physics that studies the behaviour of subatomic particles such as electrons and protons. It is based on the principles of quantum mechanics and utilises many equations and mathematical concepts to describe the properties of these particles.,Classical mechanics is the branch of physics that studies the behaviour of light and electromagnetic radiation. It is based on the principles of wave-particle duality and utilises many equations and mathematical concepts to describe the properties of light.,Classical mechanics is the branch of physics that studies the behaviour of fluids and gases. It is based on the principles of thermodynamics and utilises many equations and mathematical concepts to describe the properties of these substances.,A,"Classical mechanics is a physical theory describing the motion of macroscopic objects, from projectiles to parts of machinery, and astronomical objects, such as spacecraft, planets, stars, and galaxies. For objects governed by classical mechanics, if the present state is known, it is possible to predict how it will move in the future (determinism), and how it has moved in the past (reversibility)."
Who shared the other half of the Nobel Prize with Yoichiro Nambu for discovering the origin of the explicit breaking of CP symmetry in the weak interactions?,Richard Feynman and Julian Schwinger,Makoto Kobayashi and Toshihide Maskawa,Steven Weinberg and Sheldon Glashow,Peter Higgs and Francois Englert,Murray Gell-Mann and George Zweig,B,"Nobel Prize
On October 7, 2008, the Royal Swedish Academy of Sciences awarded the 2008 Nobel Prize in Physics to three scientists for their work in subatomic physics symmetry breaking. Yoichiro Nambu, of the University of Chicago, won half of the prize for the discovery of the mechanism of spontaneous broken symmetry in the context of the strong interactions, specifically chiral symmetry breaking. Physicists Makoto Kobayashi and Toshihide Maskawa, of Kyoto University, shared the other half of the prize for discovering the origin of the explicit breaking of CP symmetry in the weak interactions. This origin is ultimately reliant on the Higgs mechanism, but, so far understood as a ""just so"" feature of Higgs couplings, not a spontaneously broken symmetry phenomenon.

See also"
What are some models that attempt to account for all observations without invoking supplemental non-baryonic matter?,"The Doppler effect, the photoelectric effect, or the Compton effect.","The Higgs boson, the W boson, or the Z boson.","Modified Newtonian dynamics, tensor–vector–scalar gravity, or entropic gravity.","The strong nuclear force, the weak nuclear force, or the electromagnetic force.","Kepler's laws, Newton's laws, or Einstein's theory of general relativity.",C,"Theoretical considerations
Theoretical work simultaneously also showed that ancient MACHOs are not likely to account for the large amounts of dark matter now thought to be present in the universe. The Big Bang as it is currently understood could not have produced enough baryons and still be consistent with the observed elemental abundances, including the abundance of deuterium. Furthermore, separate observations of baryon acoustic oscillations, both in the cosmic microwave background and  large-scale structure of galaxies, set limits on the ratio of baryons to the total amount of matter.  These observations show that a large fraction of non-baryonic matter is necessary regardless of the presence or absence of MACHOs; however MACHO candidates such as primordial black holes could be formed of non-baryonic matter (from pre-baryonic epochs of the early Big Bang)."
What is the purpose of the proximity-focusing design in a RICH detector?,"To emit a cone of Askaryan radiation that traverses a small distance and is detected on the photon detector plane, creating a ring of light whose radius is defined by the Cherenkov emission angle and the proximity gap.","To emit a cone of Bremsstrahlung radiation that traverses a large distance and is detected on the photon detector plane, creating a ring of light whose radius is defined by the Cherenkov emission angle and the proximity gap.","To emit a cone of Cherenkov light that traverses a large distance and is detected on the photon detector plane, creating a ring of light whose radius is defined by the Cherenkov emission angle and the proximity gap.","To emit a cone of Cherenkov light that traverses a small distance and is detected on the photon detector plane, creating a ring of light whose radius is defined by the Cherenkov emission angle and the proximity gap.","To emit a cone of Bremsstrahlung radiation that traverses a small distance and is detected on the photon detector plane, creating a ring of light whose radius is defined by the Cherenkov emission angle and the proximity gap.",D,"gases—due to the larger radiator length needed to create enough photons. In the more compact proximity-focusing design, a thin radiator volume emits a cone of Cherenkov light which traverses a small distance—the proximity gap—and is detected on the photon detector plane. The image is a ring of light whose radius is defined by the Cherenkov emission angle and the proximity gap. The ring thickness is determined by the thickness of the radiator. An example of a proximity gap RICH detector is the High Momentum Particle Identification Detector (HMPID), a detector currently under construction for ALICE (A Large Ion Collider Experiment), one of the six experiments at the LHC (Large Hadron Collider) at CERN."
What is a light-year?,A unit of time used to express astronomical distances that is equivalent to the time that an object moving at the speed of light in vacuum would take to travel in one Julian year: approximately 9.46 trillion seconds (9.46×1012 s) or 5.88 trillion minutes (5.88×1012 min).,A unit of length used to express astronomical distances that is equivalent to the distance that an object moving at the speed of light in vacuum would travel in one Julian year: approximately 9.46 trillion kilometres (9.46×1012 km) or 5.88 trillion miles (5.88×1012 mi).,A unit of temperature used to express astronomical distances that is equivalent to the temperature of an object moving at the speed of light in vacuum in one Julian year: approximately 9.46 trillion Kelvin (9.46×1012 K) or 5.88 trillion Celsius (5.88×1012 °.,A unit of energy used to express astronomical distances that is equivalent to the energy of an object moving at the speed of light in vacuum in one Julian year: approximately 9.46 trillion joules (9.46×1012 J) or 5.88 trillion watt-hours (5.88×1012 Wh).,A unit of mass used to express astronomical distances that is equivalent to the mass of an object moving at the speed of light in vacuum in one Julian year: approximately 9.46 trillion kilograms (9.46×1012 kg) or 5.88 trillion pounds (5.88×1012 lb).,B,"A light-year, alternatively spelt lightyear, is a large unit of length used to express astronomical distances, and is equivalent to about 9.46 trillion kilometers (), or 5.88 trillion miles (). As defined by the International Astronomical Union (IAU), a light-year is the distance that light travels in vacuum in one Julian year (365.25 days). Because it includes the time-measurement word ""year"", the term light-year is sometimes misinterpreted as a unit of time.

The light-year is most often used when expressing distances to stars and other distances on a galactic scale, especially in non-specialist contexts and popular science publications. The unit most commonly used in professional astronomy is the parsec (symbol: pc, about 3.26 light-years) which derives from astrometry; it is the distance at which one astronomical unit subtends an angle of one second of arc."
What is the main advantage of ferroelectric memristors?,"Ferroelectric memristors have a higher resistance than other types of memristors, making them more suitable for high-power applications.","Ferroelectric domain dynamics can be tuned, allowing for the engineering of memristor response, and resistance variations are due to purely electronic phenomena, making the device more reliable.","Ferroelectric memristors have a more complex structure than other types of memristors, allowing for a wider range of applications.",Ferroelectric memristors have a unique piezoelectric field that allows for the creation of non-uniform elastic strain and a more stable structure.,"Ferroelectric memristors are based on vertically aligned carbon nanotubes, which offer a more efficient and faster switching mechanism than other materials.",B,"The ferroelectric memristor is based on a thin ferroelectric barrier sandwiched between two metallic electrodes. Switching the polarization of the ferroelectric material by applying a positive or negative voltage across the junction can lead to a two order of magnitude resistance variation:  (an effect called Tunnel Electro-Resistance). In general, the polarization does not switch abruptly. The reversal occurs gradually through the nucleation and growth of ferroelectric domains with opposite polarization. During this process, the resistance is neither RON or ROFF, but in between. When the voltage is cycled, the ferroelectric domain configuration evolves, allowing a fine tuning of the resistance value. The ferroelectric memristor's main advantages are that ferroelectric domain dynamics can be tuned, offering a way to engineer the memristor response, and that the resistance variations are due to purely electronic phenomena, aiding device reliability, as no deep change to the material"
What is the term used to describe the conduction that occurs in non-crystalline semiconductors by charges quantum tunnelling from one localised site to another?,Intrinsic semiconductors,Electrical impedance tomography,Quantum conduction,Carrier mobility,Variable range hopping,E,"where ,  and  are the so-called Steinhart–Hart coefficients.

This equation is used to calibrate thermistors.

Extrinsic (doped) semiconductors have a far more complicated temperature profile. As temperature increases starting from absolute zero they first decrease steeply in resistance as the carriers leave the donors or acceptors. After most of the donors or acceptors have lost their carriers, the resistance starts to increase again slightly due to the reducing mobility of carriers (much as in a metal). At higher temperatures, they behave like intrinsic semiconductors as the carriers from the donors/acceptors become insignificant compared to the thermally generated carriers.

In non-crystalline semiconductors, conduction can occur by charges quantum tunnelling from one localised site to another. This is known as variable range hopping and has the characteristic form of

where  = 2, 3, 4, depending on the dimensionality of the system."
What is resistivity?,Resistivity is an extrinsic property of a material that describes how difficult it is to make electrical current flow through it. It is measured in ohms and is dependent on the material's shape and size.,Resistivity is a measure of the resistance of a material to electrical current flow. It is measured in ohm-meters and is dependent on the material's shape and size.,Resistivity is an intrinsic property of a material that describes how difficult it is to make electrical current flow through it. It is measured in ohm-meters and is independent of the material's shape and size.,Resistivity is a measure of the electrical current that can flow through a material. It is measured in ohms and is dependent on the material's shape and size.,Resistivity is a measure of the electrical current that can flow through a material. It is measured in ohm-meters and is independent of the material's shape and size.,C,"Resistivity, which is a characteristic of particles in an electric field, is a measure of a particle's resistance to transferring charge (both accepting and giving up charges). Resistivity is a function of a particle's chemical composition as well as flue gas operating conditions such as temperature and moisture. Particles can have high, moderate (normal), or low resistivity.

Bulk resistivity is defined using a more general version of Ohm’s Law, as given in Equation () below:

  

  Where:
  E is the Electric field strength.Unit:-(V/cm);
  j is the Current density.Unit:-(A/cm2); and
  ρ is the Resistivity.Unit:-(Ohm-cm)

A better way of displaying this would be to solve for resistivity as a function of applied voltage and current, as given in Equation () below:"
What did Newton adopt after his correspondence with Hooke in 1679-1680?,The language of inward or centripetal force.,The language of gravitational force.,The language of outward or centrifugal force.,The language of tangential and radial displacements.,The language of electromagnetic force.,A,"Robert Hooke had published his concept of gravitational forces in 1674, stating that all celestial bodies have an attraction or gravitating power towards their own centers, and also attract all the other celestial bodies that are within the sphere of their activity. He further stated that gravitational attraction increases by how much nearer the body wrought upon is to its own center. In correspondence with Isaac Newton from 1679 and 1680, Hooke conjectured that gravitational forces might decrease according to the double of the distance between the two bodies. Hooke urged Newton, who was a pioneer in the development of calculus, to work through the mathematical details of Keplerian orbits to determine if Hooke's hypothesis was correct.  Newton's own investigations verified that Hooke was correct, but due to personal differences between the two men, Newton chose not to reveal this to Hooke.  Isaac Newton kept quiet about his discoveries until 1684, at which time he told a friend,"
What is the metallicity of Kapteyn's star estimated to be?,8 times more than the Sun,8 times less than the Sun,13 light years away from Earth,Unknown,Equal to the Sun,B,"Population II, or metal-poor stars, are those with relatively low metallicity which can have hundreds (e.g. BD +17° 3248) or thousands (e.g. Sneden's Star) times less metallicity than the Sun. These objects formed during an earlier time of the universe. Intermediate population II stars are common in the bulge near the center of the Milky Way, whereas Population II stars found in the galactic halo are older and thus more metal-poor. Globular clusters also contain high numbers of population II stars.
In 2014, the first planets around a halo star were announced around Kapteyn's star, the nearest halo star to Earth, around 13 light years away. However, later research suggests that Kapteyn b is just an artefact of stellar activity and that Kapteyn c needs more study to be confirmed. The metallicity of Kapteyn's star is estimated to be about 8 times less than the Sun."
What is the SI base unit of time and how is it defined?,"The SI base unit of time is the week, which is defined by measuring the electronic transition frequency of caesium atoms.","The SI base unit of time is the second, which is defined by measuring the electronic transition frequency of caesium atoms.","The SI base unit of time is the hour, which is defined by measuring the electronic transition frequency of caesium atoms.","The SI base unit of time is the day, which is defined by measuring the electronic transition frequency of caesium atoms.","The SI base unit of time is the minute, which is defined by measuring the electronic transition frequency of caesium atoms.",B,"A unit of time is any particular time interval, used as a standard way of measuring or expressing duration. The base unit of time in the International System of Units (SI) and by extension most of the Western world, is the second, defined as about 9 billion oscillations of the caesium atom. The exact modern definition, from the National Institute of Standards and Technology is:
""The second, symbol s, is the SI unit of time. It is defined by taking the fixed numerical value of the cesium frequency ΔνCs, the unperturbed ground-state hyperfine transition frequency of the cesium 133 atom, to be  when expressed in the unit Hz, which is equal to s−1."""
What is a planetary system?,A system of planets that are all located in the same solar system.,A system of planets that are all the same size and shape.,Any set of gravitationally bound non-stellar objects in or out of orbit around a star or star system.,A system of planets that are all located in the same galaxy.,A system of planets that are all made of gas.,C,"A planetary system is a set of gravitationally bound non-stellar objects in or out of orbit around a star or star system. Generally speaking, systems with one or more planets constitute a planetary system, although such systems may also consist of bodies such as dwarf planets, asteroids, natural satellites, meteoroids, comets, planetesimals and circumstellar disks. The Sun together with the planetary system revolving around it, including Earth, forms the Solar System. The term exoplanetary system is sometimes used in reference to other planetary systems.

 Debris disks are also known to be common, though other objects are more difficult to observe.

Of particular interest to astrobiology is the habitable zone of planetary systems where planets could have surface liquid water, and thus the capacity to support Earth-like life.

History"
What is the result of the collapse of a cavitation bubble?,"The collapse of a cavitation bubble causes the surrounding liquid to expand, resulting in the formation of a low-pressure vapor bubble.","The collapse of a cavitation bubble causes a decrease in pressure and temperature of the vapor within, releasing a small amount of energy in the form of an acoustic shock wave and visible light.","The collapse of a cavitation bubble causes a sharp increase in pressure and temperature of the vapor within, releasing a significant amount of energy in the form of an acoustic shock wave and visible light.","The collapse of a cavitation bubble causes the surrounding liquid to implode, resulting in the formation of a vacuum.",The collapse of a cavitation bubble has no effect on the surrounding liquid or vapor.,C,"When the cavitation bubbles collapse, they force energetic liquid into very small volumes, thereby creating spots of high temperature and emitting shock waves, the latter of which are a source of noise. The noise created by cavitation is a particular problem for military submarines, as it increases the chances of being detected by passive sonar.

Although the collapse of a small cavity is a relatively low-energy event, highly localized collapses can erode metals, such as steel, over time. The pitting caused by the collapse of cavities produces great wear on components and can dramatically shorten a propeller's or pump's lifetime."
Who was Giordano Bruno?,A German philosopher who supported the Keplerian theory that planets move in elliptical orbits around the Sun and believed that fixed stars are similar to the Sun but have different designs and are not subject to the dominion of One.,An English philosopher who supported the Ptolemaic theory that Earth is the center of the universe and believed that fixed stars are not similar to the Sun and do not have planets orbiting them.,A French philosopher who supported the Aristotelian theory that Earth is at the center of the universe and believed that fixed stars are similar to the Sun but do not have planets orbiting them.,An Italian philosopher who supported the Copernican theory that Earth and other planets orbit the Sun and believed that fixed stars are similar to the Sun and have planets orbiting them.,A Spanish philosopher who supported the Galilean theory that Earth and other planets orbit the Sun and believed that fixed stars are not similar to the Sun and do not have planets orbiting them.,D,"1584 - Giordano Bruno published two important philosophical dialogues (La Cena de le Ceneri and De l'infinito universo et mondi) in which he argued against the planetary spheres and affirmed the Copernican principle. Bruno's infinite universe was filled with a substance—a ""pure air"", aether, or spiritus—that offered no resistance to the heavenly bodies which, in Bruno's view, rather than being fixed, moved under their own impetus (momentum). Most dramatically, he completely abandoned the idea of a hierarchical universe. Bruno's cosmology distinguishes between ""suns"" which produce their own light and heat, and have other bodies moving around them; and ""earths"" which move around suns and receive light and heat from them. Bruno suggested that some, if not all, of the objects classically known as fixed stars are in fact suns, so he was arguably the first person to grasp that ""stars are other suns with their own planets."" Bruno wrote that other worlds ""have no less virtue nor a nature"
What are the Navier-Stokes equations?,"The Navier-Stokes equations are partial differential equations that describe the motion of viscous fluid substances, expressing momentum balance and conservation of mass for Newtonian fluids.","The Navier-Stokes equations are partial differential equations that describe the motion of viscous fluid substances, expressing momentum balance and conservation of mass for non-Newtonian fluids.","The Navier-Stokes equations are partial differential equations that describe the motion of non-viscous fluid substances, expressing momentum balance and conservation of mass for Newtonian fluids.","The Navier-Stokes equations are algebraic equations that describe the motion of non-viscous fluid substances, expressing momentum balance and conservation of mass for Newtonian fluids.","The Navier-Stokes equations are algebraic equations that describe the motion of viscous fluid substances, expressing momentum balance and conservation of mass for Newtonian fluids.",A,"Navier–Stokes equations

The Navier–Stokes equations (named after Claude-Louis Navier and George Gabriel Stokes) are differential equations that describe the force balance at a given point within a fluid. For an incompressible fluid with vector velocity field , the Navier–Stokes equations are 

 .

These differential equations are the analogues for deformable materials to Newton's equations of motion for particles – the Navier–Stokes equations describe changes in momentum (force) in response to pressure  and viscosity, parameterized by the kinematic viscosity  here. Occasionally, body forces, such as the gravitational force or Lorentz force are added to the equations."
What is the revised view of the atmosphere's nature based on the time-varying multistability that is associated with the modulation of large-scale processes and aggregated feedback of small-scale processes?,The atmosphere is a system that is only influenced by large-scale processes and does not exhibit any small-scale feedback.,"The atmosphere possesses both chaos and order, including emerging organized systems and time-varying forcing from recurrent seasons.",The atmosphere is a system that is only influenced by small-scale processes and does not exhibit any large-scale modulation.,The atmosphere is a completely chaotic system with no order or organization.,The atmosphere is a completely ordered system with no chaos or randomness.,B,
What is the reason that it is nearly impossible to see light emitted at the Lyman-alpha transition wavelength from a star farther than a few hundred light years from Earth?,"Far ultraviolet light is absorbed effectively by the charged components of the ISM, including atomic helium, which has a typical absorption wavelength of about 121.5 nanometers, the Lyman-alpha transition.","Far ultraviolet light is absorbed effectively by the neutral components of the ISM, including atomic hydrogen, which has a typical absorption wavelength of about 121.5 nanometers, the Lyman-alpha transition.","Far ultraviolet light is absorbed effectively by the charged components of the ISM, including atomic hydrogen, which has a typical absorption wavelength of about 121.5 nanometers, the Lyman-alpha transition.","Far ultraviolet light is absorbed effectively by the neutral components of the ISM, including atomic helium, which has a typical absorption wavelength of about 121.5 nanometers, the Lyman-alpha transition.","Far ultraviolet light is absorbed effectively by the neutral components of the ISM, including atomic hydrogen, which has a typical absorption wavelength of about 212.5 nanometers, the Lyman-alpha transition.",B,"Interstellar extinction

The ISM is also responsible for extinction and reddening, the decreasing light intensity and shift in the dominant observable wavelengths of light from a star. These effects are caused by scattering and absorption of photons and allow the ISM to be observed with the naked eye in a dark sky. The apparent rifts that can be seen in the band of the Milky Way – a uniform disk of stars – are caused by absorption of background starlight by molecular clouds within a few thousand light years from Earth.

Far ultraviolet light is absorbed effectively by the neutral components of the ISM. For example, a typical absorption wavelength of atomic hydrogen lies at about 121.5 nanometers, the Lyman-alpha transition. Therefore, it is nearly impossible to see light emitted at that wavelength from a star farther than a few hundred light years from Earth, because most of it is absorbed during the trip to Earth by intervening neutral hydrogen.

Heating and cooling"
What is a Schwarzschild black hole?,"A black hole that has mass but neither electric charge nor angular momentum, and is not spherically symmetric, according to Birkhoff's theorem.","A black hole that has mass, electric charge, and angular momentum, and is spherically symmetric, according to Birkhoff's theorem.","A black hole that has mass but neither electric charge nor angular momentum, and is spherically symmetric, according to Birkhoff's theorem.","A black hole that has neither mass nor electric charge nor angular momentum, and is not spherically symmetric, according to Birkhoff's theorem.","A black hole that has mass, electric charge, and angular momentum, and is not spherically symmetric, according to Birkhoff's theorem.",C,"According to Birkhoff's theorem, the Schwarzschild metric is the most general spherically symmetric, vacuum solution of the Einstein field equations. A Schwarzschild black hole or static black hole is a black hole that has no charge or angular momentum. A Schwarzschild black hole is described by the Schwarzschild metric, and cannot be distinguished from any other Schwarzschild black hole except by its mass.

See also
 Differentiable manifold
 Christoffel symbol
 Riemannian geometry
 Ricci calculus
 Differential geometry and topology
 List of differential geometry topics
 General relativity
 Gauge gravitation theory
 General covariant transformations
 Derivations of the Lorentz transformations

Notes

References"
What is the definition of Atomristor?,Atomristor is a flexible memristive device comprising a MoOx/MoS2 heterostructure sandwiched between silver electrodes on a plastic foil.,Atomristor is a prominent memcapacitive effect observed in switches with memristive behavior.,Atomristor is defined as the electrical devices showing memristive behavior in atomically thin nanomaterials or atomic sheets.,Atomristor is a printing and solution-processing technology used to fabricate memristive devices.,Atomristor is a type of two-dimensional layered transition metal dichalcogenides (TMDs) used in the fabrication of memristive devices.,C,
Who published the first theory that was able to encompass previously separate field theories to provide a unifying theory of electromagnetism?,Maxwell,Einstein,Galileo,Faraday,Newton,A,"The development of the independent concept of a field truly began in the nineteenth century with the development of the theory of electromagnetism. In the early stages, André-Marie Ampère and Charles-Augustin de Coulomb could manage with Newton-style laws that expressed the forces between pairs of electric charges or electric currents. However, it became much more natural to take the field approach and express these laws in terms of electric and magnetic fields; in 1849 Michael Faraday became the first to coin the term ""field"".

The independent nature of the field became more apparent with James Clerk Maxwell's discovery that waves in these fields propagated at a finite speed. Consequently, the forces on charges and currents no longer just depended on the positions and velocities of other charges and currents at the same time, but also on their positions and velocities in the past."
What is the relevant type of coherence for the Young's double-slit interferometer?,Visibility,Coherence time,Spatial coherence,Coherence length,Diameter of the coherence area (Ac),E,"White-light interferometer (see also Optical coherence tomography)
White-light scatterplate interferometer (white-light) (microscopy)
Young's double-slit interferometer
Zernike phase-contrast microscopy"
What is the Peierls bracket in canonical quantization?,The Peierls bracket is a mathematical symbol used to represent the Poisson algebra in the canonical quantization method.,The Peierls bracket is a mathematical tool used to generate the Hamiltonian in the canonical quantization method.,The Peierls bracket is a Poisson bracket derived from the action in the canonical quantization method that converts the quotient algebra into a Poisson algebra.,The Peierls bracket is a mathematical symbol used to represent the quotient algebra in the canonical quantization method.,The Peierls bracket is a mathematical tool used to generate the Euler-Lagrange equations in the canonical quantization method.,C,"In theoretical physics, the Peierls bracket is an equivalent description of the Poisson bracket. It can be defined directly from the action and does not require the canonical coordinates and their canonical momenta to be defined in advance.

The bracket

is defined as

,

as the difference between some kind of action of one quantity on the other, minus the flipped term.

In quantum mechanics, the Peierls bracket becomes a commutator i.e. a Lie bracket.

References

Peierls, R. ""The Commutation Laws of Relativistic Field Theory,""
Proc. R. Soc. Lond. August 21, 1952 214 1117 143-157.

Theoretical physics"
What is the isophotal diameter used for in measuring a galaxy's size?,The isophotal diameter is a way of measuring a galaxy's distance from Earth.,The isophotal diameter is a measure of a galaxy's age.,The isophotal diameter is a measure of a galaxy's mass.,The isophotal diameter is a measure of a galaxy's temperature.,The isophotal diameter is a conventional way of measuring a galaxy's size based on its apparent surface brightness.,E,"Galaxy effective radius or half-light radius () is the radius at which half of the total light of a galaxy is emitted. This assumes the galaxy has either intrinsic spherical symmetry or is at least circularly symmetric as viewed in the plane of the sky. Alternatively, a half-light contour, or isophote, may be used for spherically and circularly asymmetric objects.

 is an important length scale in  term in de Vaucouleurs law, which characterizes a specific rate at which surface brightness decreases as a function of radius:

where  is the surface brightness at . At ,

Thus, the central surface brightness is approximately .

See also

References

Physical quantities"
What is the Maxwell's Demon thought experiment?,"A thought experiment in which a demon guards a microscopic trapdoor in a wall separating two parts of a container filled with different gases at equal temperatures. The demon selectively allows molecules to pass from one side to the other, causing an increase in temperature in one part and a decrease in temperature in the other, contrary to the second law of thermodynamics.","A thought experiment in which a demon guards a macroscopic trapdoor in a wall separating two parts of a container filled with different gases at different temperatures. The demon selectively allows molecules to pass from one side to the other, causing a decrease in temperature in one part and an increase in temperature in the other, in accordance with the second law of thermodynamics.","A thought experiment in which a demon guards a microscopic trapdoor in a wall separating two parts of a container filled with the same gas at equal temperatures. The demon selectively allows faster-than-average molecules to pass from one side to the other, causing a decrease in temperature in one part and an increase in temperature in the other, contrary to the second law of thermodynamics.","A thought experiment in which a demon guards a macroscopic trapdoor in a wall separating two parts of a container filled with the same gas at equal temperatures. The demon selectively allows faster-than-average molecules to pass from one side to the other, causing an increase in temperature in one part and a decrease in temperature in the other, contrary to the second law of thermodynamics.","A thought experiment in which a demon guards a microscopic trapdoor in a wall separating two parts of a container filled with the same gas at different temperatures. The demon selectively allows slower-than-average molecules to pass from one side to the other, causing a decrease in temperature in one part and an increase in temperature in the other, in accordance with the second law of thermodynamics.",C,"The question of whether this time-asymmetric dissipation is really inevitable has been considered by many physicists, often in the context of Maxwell's demon. The name comes from a thought experiment described by James Clerk Maxwell in which a microscopic demon guards a gate between two halves of a room. It only lets slow molecules into one half, only fast ones into the other. By eventually making one side of the room cooler than before and the other hotter, it seems to reduce the entropy of the room, and reverse the arrow of time. Many analyses have been made of this; all show that when the entropy of room and demon are taken together, this total entropy does increase. Modern analyses of this problem have taken into account Claude E. Shannon's relation between entropy and information. Many interesting results in modern computing are closely related to this problem — reversible computing, quantum computing and physical limits to computing, are examples. These seemingly metaphysical"
What is the application of Memristor?,"Memristor has applications in the production of electric cars, airplanes, and ships.","Memristor has applications in the production of food, clothing, and shelter.","Memristor has applications in the production of solar panels, wind turbines, and hydroelectric power plants.","Memristor has applications in programmable logic signal processing, Super-resolution imaging, physical neural networks, control systems, reconfigurable computing, in-memory computing, brain–computer interfaces and RFID.","Memristor has applications in optical fiber communication, satellite communication, and wireless communication.",D,"Memristor have applications in programmable logic, signal processing, Super-resolution imaging, physical neural networks, control systems, reconfigurable computing, brain–computer interfaces, and RFID. Memristive devices are potentially used for stateful logic implication, allowing a replacement for CMOS-based logic computation. Several early works have been reported in this direction."
What is the effect generated by a spinning superconductor?,"An electric field, precisely aligned with the spin axis.","A magnetic field, randomly aligned with the spin axis.","A magnetic field, precisely aligned with the spin axis.","A gravitational field, randomly aligned with the spin axis.","A gravitational field, precisely aligned with the spin axis.",C,"In a high-c superconductor, the mechanism is extremely similar to a conventional superconductor, except, in this case, phonons virtually play no role and their role is replaced by spin-density waves. Just as all known conventional superconductors are strong phonon systems, all known high-c superconductors are strong spin-density wave systems, within close vicinity of a magnetic transition to, for example, an antiferromagnet. When an electron moves in a high-c superconductor, its spin creates a spin-density wave around it. This spin-density wave in turn causes a nearby electron to fall into the spin depression created by the first electron (water-bed effect again). Hence, again, a Cooper pair is formed. When the system temperature is lowered, more spin density waves and Cooper pairs are created, eventually leading to superconductivity. Note that in high-c systems, as these systems are magnetic systems due to the Coulomb interaction, there is a strong Coulomb repulsion between"
What is the main focus of cryogenic and noble liquid detectors in dark matter experiments?,Distinguishing background particles from dark matter particles by detecting the heat produced when a particle hits an atom in a crystal absorber or the scintillation produced by a particle collision in liquid xenon or argon.,Detecting the heat produced when a particle hits an atom in a crystal absorber or the scintillation produced by a particle collision in liquid xenon or argon to determine the mass and interaction cross section with electrons of dark matter particles.,Detecting the heat produced when a particle hits an atom in a crystal absorber or the scintillation produced by a particle collision in liquid xenon or argon to distinguish between different types of background particles.,Detecting the heat produced when a particle hits an atom in a crystal absorber or the scintillation produced by a particle collision in liquid xenon or argon to determine the mass and interaction cross section with nucleons of dark matter particles.,Detecting the mass and interaction cross section with nucleons of dark matter particles by detecting the heat produced when a particle hits an atom in a crystal absorber or the scintillation produced by a particle collision in liquid xenon or argon.,A,"The Cryogenic Low-Energy Astrophysics with Noble liquids (CLEAN) experiment by the DEAP/CLEAN collaboration is searching for dark matter using noble gases at the SNOLAB underground facility.  CLEAN has studied neon and argon in the MicroCLEAN prototype, and running the MiniCLEAN detector to test a multi-ton design.

Design 
Dark matter searches in isolated noble gas scintilators with xenon and argon have set limits on WIMP interactions, such as recent cross sections from LUX and XENON. Particles scattering in the target emit photons detected by PMTs, identified via pulse shape discrimination developed on DEAP results. Shielding reduces the cosmic and radiation background. Neon has been studied as a clear, dense, low-background scintilator. CLEAN can use neon or argon and plans runs with both to study nuclear mass dependence of any WIMP signals."
What is a pycnometer?,A device used to measure the density of a gas.,A device used to measure the mass of a liquid.,A device used to measure the volume of a gas.,A device used to determine the density of a liquid.,A device used to determine the volume of a liquid.,D,"This shows that, for small Δx, changes in displacement are approximately proportional to changes in relative density.

Pycnometer

A pycnometer (from Greek: πυκνός () meaning ""dense""), also called pyknometer or specific gravity bottle, is a device used to determine the density of a liquid.  A pycnometer is usually made of glass, with a close-fitting ground glass stopper with a capillary tube through it, so that air bubbles may escape from the apparatus.  This device enables a liquid's density to be measured accurately by reference to an appropriate working fluid, such as water or mercury, using an analytical balance."
"What is the estimated redshift of CEERS-93316, a candidate high-redshift galaxy observed by the James Webb Space Telescope?","Approximately z = 6.0, corresponding to 1 billion years after the Big Bang.","Approximately z = 16.7, corresponding to 235.8 million years after the Big Bang.","Approximately z = 3.0, corresponding to 5 billion years after the Big Bang.","Approximately z = 10.0, corresponding to 13 billion years after the Big Bang.","Approximately z = 13.0, corresponding to 30 billion light-years away from Earth.",B,"Highest redshifts

Currently, the objects with the highest known redshifts are galaxies and the objects producing gamma ray bursts. The most reliable redshifts are from spectroscopic data, and the highest-confirmed spectroscopic redshift of a galaxy is that of GN-z11, with a redshift of , corresponding to 400 million years after the Big Bang. The previous record was held by UDFy-38135539 at a redshift of , corresponding to 600 million years after the Big Bang. Slightly less reliable are Lyman-break redshifts, the highest of which is the lensed galaxy A1689-zD1 at a redshift  and the next highest being . The most distant-observed gamma-ray burst with a spectroscopic redshift measurement was GRB 090423, which had a redshift of . The most distant-known quasar, ULAS J1342+0928, is at . The highest-known redshift radio galaxy (TGSS1530) is at a redshift  and the highest-known redshift molecular material is the detection of emission from the CO molecule from the quasar SDSS J1148+5251 at ."
What is bollard pull primarily used for measuring?,The weight of heavy machinery,The speed of locomotives,The distance traveled by a truck,The strength of tugboats,The height of a ballast tractor,D,
What is the piezoelectric strain coefficient for AT-cut quartz crystals?,d = 1.9·10‑12 m/V,d = 3.1·10‑12 m/V,d = 4.2·10‑12 m/V,d = 2.5·10‑12 m/V,d = 5.8·10‑12 m/V,B,"Amplitude of motion 
The amplitude of lateral displacement rarely exceeds a nanometer. More specifically one has

with u0 the amplitude of lateral displacement, n the overtone order, d the piezoelectric strain coefficient, Q the quality factor, and Uel the amplitude of electrical driving. The piezoelectric strain coefficient is given as d = 3.1·10‑12 m/V for AT-cut quartz crystals. Due to the small amplitude, stress and strain usually are proportional to each other. The QCM operates in the range of linear acoustics.

Effects of temperature and stress 
The resonance frequency of acoustic resonators depends on temperature, pressure, and bending stress.  Temperature-frequency coupling is minimized by employing special crystal cuts. A widely used temperature-compensated cut of quartz is the AT-cut.  Careful control of temperature and stress is essential in the operation of the QCM."
What is the difference between probability mass function (PMF) and probability density function (PDF)?,"PMF is used only for continuous random variables, while PDF is used for both continuous and discrete random variables.","PMF is used for both continuous and discrete random variables, while PDF is used only for continuous random variables.","PMF is used for continuous random variables, while PDF is used for discrete random variables.","PMF is used for discrete random variables, while PDF is used for continuous random variables.",PMF and PDF are interchangeable terms used for the same concept in probability theory.,D,"In statistics, especially in Bayesian statistics, the kernel of a probability density function (pdf) or probability mass function (pmf) is the form of the pdf or pmf in which any factors that are not functions of any of the variables in the domain are omitted.  Note that such factors may well be functions of the parameters of the pdf or pmf.  These factors form part of the normalization factor of the probability distribution, and are unnecessary in many situations.  For example, in pseudo-random number sampling, most sampling algorithms ignore the normalization factor.  In addition, in Bayesian analysis of conjugate prior distributions, the normalization factors are generally ignored during the calculations, and only the kernel considered.  At the end, the form of the kernel is examined, and if it matches a known distribution, the normalization factor can be reinstated.  Otherwise, it may be unnecessary (for example, if the distribution only needs to be sampled from)."
"How do the Lunar Laser Ranging Experiment, radar astronomy, and the Deep Space Network determine distances to the Moon, planets, and spacecraft?",They determine the values of electromagnetic constants.,They measure round-trip transit times.,They measure the actual speed of light waves.,They use interferometry to determine the speed of light.,They separately determine the frequency and wavelength of a light beam.,B,"Lunar Laser Ranging (LLR) is the practice of measuring the distance between the surfaces of the Earth and the Moon using laser ranging. The distance can be calculated from the round-trip time of laser light pulses travelling at the speed of light, which are reflected back to Earth by the Moon's surface or by one of five retroreflectors installed on the Moon during the Apollo program (11, 14, and 15) and Lunokhod 1 and 2 missions.

Although it is possible to reflect light or radio waves directly from the Moon's surface (a process known as EME), a much more precise range measurement can be made using retroreflectors, since because of their small size, the temporal spread in the reflected signal is much smaller.

A review of Lunar Laser Ranging is available.

Laser ranging measurements can also be made with retroreflectors installed on Moon-orbiting satellites such as the LRO.

History"
What is the Ozma Problem?,The Ozma Problem is a chapter in a book that discusses the versatility of carbon and chirality in biochemistry.,"The Ozma Problem is a discussion about time invariance and reversal in particle physics, theoretical physics, and cosmology.","The Ozma Problem is a conundrum that examines whether there is any fundamental asymmetry to the universe. It concerns various aspects of atomic and subatomic physics and how they relate to mirror asymmetry and the related concepts of chirality, antimatter, magnetic and electrical polarity, parity, charge and spin.",The Ozma Problem is a measure of how symmetry and asymmetry have evolved from the beginning of life on Earth.,The Ozma Problem is a comparison between the level of a desired signal and the level of background noise used in science and engineering.,C,"Semaglutide, sold under the brand name Ozempic among others, is an antidiabetic medication used for the treatment of type 2 diabetes and long-term weight management.

Semaglutide acts like human glucagon-like peptide-1 (GLP-1) in that it increases insulin secretion, thereby increasing sugar metabolism. It is distributed as a metered subcutaneous injection in a prefilled pen, or as an oral form. One of its advantages over other antidiabetic drugs is that it has a long duration of action, so a once-a-week injection is sufficient."
What is a Hilbert space in quantum mechanics?,A complex vector space where the state of a classical mechanical system is described by a vector |Ψ⟩.,A physical space where the state of a classical mechanical system is described by a vector |Ψ⟩.,A physical space where the state of a quantum mechanical system is described by a vector |Ψ⟩.,A mathematical space where the state of a classical mechanical system is described by a vector |Ψ⟩.,A complex vector space where the state of a quantum mechanical system is described by a vector |Ψ⟩.,E,"Broadening beyond this simple case, the mathematical formulation of quantum mechanics   developed by Paul Dirac, David Hilbert, John von Neumann, and Hermann Weyl defines the state of a quantum mechanical system to be a vector  belonging to a (separable) Hilbert space . This vector is postulated to be normalized under the Hilbert's space inner product, that is, in Dirac notation it obeys . The exact nature of this Hilbert space is dependent on the system – for example, for describing position and momentum the Hilbert space is the space of complex square-integrable functions , while the Hilbert space for the spin of a single proton is simply the space of two-dimensional complex vectors  with the usual inner product."
What is the significance of the speed of light in vacuum?,The speed of light in vacuum is only relevant when measuring the one-way speed of light.,The speed of light in vacuum is only relevant when measuring the two-way speed of light.,The speed of light in vacuum is independent of the motion of the wave source and the observer's inertial frame of reference.,The speed of light in vacuum is dependent on the motion of the wave source and the observer's inertial frame of reference.,The speed of light in vacuum is only relevant when c appears explicitly in the units of measurement.,C,"Justifications

Casimir vacuum and quantum tunnelling
Special relativity postulates that the speed of light in vacuum is invariant in inertial frames. That is, it will be the same from any frame of reference moving at a constant speed. The equations do not specify any particular value for the speed of light, which is an experimentally determined quantity for a fixed unit of length. Since 1983, the SI unit of length (the meter) has been defined using the speed of light."
What is the term used to describe the proportionality factor to the Stefan-Boltzmann law that is utilized in subsequent evaluations of the radiative behavior of grey bodies?,Emissivity,Wien's displacement law,Reflectance,Black-body radiation,Albedo,A,"The Stefan–Boltzmann law describes the power radiated from a black body in terms of its temperature.  Specifically, the Stefan–Boltzmann law states that the total energy radiated per unit surface area of a black body across all wavelengths per unit time  (also known as the black-body radiant emittance) is directly proportional to the fourth power of the black body's thermodynamic temperature T:

The constant of proportionality σ, called the Stefan–Boltzmann constant, is derived from other known physical constants. Since 2019, the value of the constant is

where k is the Boltzmann constant, h is Planck's constant, and c is the speed of light in a vacuum. The radiance from a specified angle of view (watts per square metre per steradian) is given by

A body that does not absorb all incident radiation (sometimes known as a grey body) emits less total energy than a black body and is characterized by an emissivity, :"
What is the reason for the formation of stars exclusively within molecular clouds?,The formation of stars occurs exclusively outside of molecular clouds.,"The low temperatures and high densities of molecular clouds cause the gravitational force to exceed the internal pressures that are acting ""outward"" to prevent a collapse.","The low temperatures and low densities of molecular clouds cause the gravitational force to be less than the internal pressures that are acting ""outward"" to prevent a collapse.","The high temperatures and low densities of molecular clouds cause the gravitational force to exceed the internal pressures that are acting ""outward"" to prevent a collapse.","The high temperatures and high densities of molecular clouds cause the gravitational force to be less than the internal pressures that are acting ""outward"" to prevent a collapse.",B,"Star formation

The formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force."
What is the identity operation in symmetry groups?,The identity operation leaves the molecule unchanged and forms the identity element in the symmetry group.,The identity operation rotates the molecule about its center of mass.,The identity operation inverts the molecule about its center of inversion.,The identity operation reflects the molecule across a plane of symmetry.,The identity operation translates the molecule in 3-D space.,A,"An action that leaves an object looking the same after it has been carried out is called a symmetry operation . For example rotations, reflections and inversions etc. There is a corresponding symmetry element for each symmetry operation. For instance a point, a line or a plane can be a symmetry element. The symmetry operation is performed with respect to a symmetry element ( a point, line or plane). In the context of molecular symmetry, a symmetry operation is a permutation of atoms such that the molecule or crystal is transformed into a state indistinguishable from the starting state.
Two basic facts follow from this definition, which emphasizes its usefulness.
 Physical properties must be invariant with respect to symmetry operations.
 Symmetry operations can be collected together in groups which are isomorphic to permutation groups."
What is a regular polytope?,A regular polytope is a geometric shape whose symmetry group is transitive on its diagonals.,A regular polytope is a geometric shape whose symmetry group is transitive on its vertices.,A regular polytope is a geometric shape whose symmetry group is transitive on its flags.,A regular polytope is a geometric shape whose symmetry group is transitive on its edges.,A regular polytope is a geometric shape whose symmetry group is transitive on its faces.,C,"In mathematics, a regular polytope is a polytope whose symmetry group acts transitively on its flags, thus giving it the highest degree of symmetry. All its elements or j-faces (for all 0 ≤ j ≤ n, where n is the dimension of the polytope) — cells, faces and so on — are also transitive on the symmetries of the polytope, and are regular polytopes of dimension ≤ n.

Regular polytopes are the generalized analog in any number of dimensions of regular polygons (for example, the square or the regular pentagon) and regular polyhedra (for example, the cube). The strong symmetry of the regular polytopes gives them an aesthetic quality that interests both non-mathematicians and mathematicians.

Classically, a regular polytope in n dimensions may be defined as having regular facets [(n − 1)-faces] and regular vertex figures. These two conditions are sufficient to ensure that all faces are alike and all vertices are alike. Note, however, that this definition does not work for abstract polytopes."
What is the reason behind the largest externally observed electrical effects when two conductors are separated by the smallest distance without touching?,"The surface charge on a conductor depends on the magnitude of the electric field, which in turn depends on the temperature between the surfaces.","The surface charge on a conductor depends on the magnitude of the magnetic field, which in turn depends on the distance between the surfaces.","The surface charge on a conductor depends on the magnitude of the electric field, which in turn depends on the angle between the surfaces.","The surface charge on a conductor depends on the magnitude of the electric field, which in turn depends on the distance between the surfaces.","The surface charge on a conductor depends on the magnitude of the electric field, which in turn depends on the pressure between the surfaces.",D,"Contact electrification If two conducting surfaces are moved relative to each other, and there is potential difference in the space between them, then an electric current will be driven. This is because the surface charge on a conductor depends on the magnitude of the electric field, which in turn depends on the distance between the surfaces. The externally observed electrical effects are largest when the conductors are separated by the smallest distance without touching (once brought into contact, the charge will instead flow internally through the junction between the conductors). Since two conductors in equilibrium can have a built-in potential difference due to work function differences, this means that bringing dissimilar conductors into contact, or pulling them apart, will drive electric currents. These contact currents can damage sensitive microelectronic circuitry and occur even when the conductors would be grounded in the absence of motion."
What is the formalism that angular momentum is associated with in rotational invariance?,Angular momentum is the 1-form Noether charge associated with rotational invariance.,Angular momentum is the 3-form Noether charge associated with rotational invariance.,Angular momentum is the 5-form Noether charge associated with rotational invariance.,Angular momentum is the 2-form Noether charge associated with rotational invariance.,Angular momentum is the 4-form Noether charge associated with rotational invariance.,D,"Rotational invariance

The conservation of the angular momentum L = r × p is  analogous to its linear momentum counterpart. It is assumed that the symmetry of the Lagrangian is rotational, i.e., that the Lagrangian does not depend on the absolute orientation of the physical system in space. For concreteness, assume that the Lagrangian does not change under small rotations of an angle δθ about an axis n; such a rotation transforms the Cartesian coordinates by the equation

Since time is not being transformed, T = 0. Taking δθ as the ε parameter and the Cartesian coordinates r as the generalized coordinates q, the corresponding Q variables are given by

Then Noether's theorem states that the following quantity is conserved,

In other words, the component of the angular momentum L along the n axis is conserved.

If n is arbitrary, i.e., if the system is insensitive to any rotation, then every component of L is conserved; in short, angular momentum is conserved."
"Which hand should be used to apply the right-hand rule when tightening or loosening nuts, screws, bolts, bottle caps, and jar lids?",One's dominant hand,The right hand,Both hands,The left hand,Either hand,B,"To apply the right-hand rule, place one's loosely clenched right hand above the object with the thumb pointing in the direction one wants the screw, nut, bolt, or cap ultimately to move, and the curl of the fingers, from the palm to the tips, will indicate in which way one needs to turn the screw, nut, bolt or cap to achieve the desired result. Almost all threaded objects obey this rule except for a few left-handed exceptions described below.

The reason for the clockwise standard for most screws and bolts is that supination of the arm, which is used by a right-handed person to tighten a screw clockwise, is generally stronger than pronation used to loosen."
What is the Minkowski diagram used for?,The Minkowski diagram is used to define concepts and demonstrate properties of Newtonian mechanics and to provide geometrical interpretation to the generalization of Lorentz transformations to relativistic mechanics.,The Minkowski diagram is used to define concepts and demonstrate properties of general relativity and to provide geometrical interpretation to the generalization of special relativity to relativistic mechanics.,The Minkowski diagram is used to define concepts and demonstrate properties of Lorentz transformations and to provide geometrical interpretation to the generalization of quantum mechanics to relativistic mechanics.,The Minkowski diagram is used to define concepts and demonstrate properties of special relativity and to provide geometrical interpretation to the generalization of general relativity to relativistic mechanics.,The Minkowski diagram is used to define concepts and demonstrate properties of Lorentz transformations and to provide geometrical interpretation to the generalization of Newtonian mechanics to relativistic mechanics.,E,"The Minkowski diagram is drawn in a spacetime plane where the spatial aspect has been restricted to a single dimension. The units of distance and time on such a plane are 
 units of 30 centimetres length and nanoseconds, or
 astronomical units and intervals of 8 minutes and 20 seconds, or
 light years and years.
Each of these scales of coordinates results in photon connections of events along diagonal lines of slope plus or minus one.
Five elements constitute the diagram Hermann Minkowski used to describe the relativity transformations: the unit hyperbola, its conjugate hyperbola, the axes of the hyperbola, a diameter of the unit hyperbola, and the conjugate diameter."
What are the two main interpretations for the disparity between the presence of matter and antimatter in the observable universe?,"The universe began with a small preference for matter, or it was originally perfectly asymmetric, but a set of phenomena contributed to a small imbalance in favor of antimatter over time.","The universe began with a small preference for antimatter, or it was originally perfectly symmetric, but a set of phenomena contributed to a small imbalance in favor of antimatter over time.","The universe began with equal amounts of matter and antimatter, or it was originally perfectly symmetric, but a set of phenomena contributed to a small imbalance in favor of antimatter over time.","The universe began with a small preference for matter, or it was originally perfectly symmetric, but a set of phenomena contributed to a small imbalance in favor of matter over time.","The universe began with equal amounts of matter and antimatter, or it was originally perfectly asymmetric, but a set of phenomena contributed to a small imbalance in favor of matter over time.",D,"One of the outstanding problems in modern physics is the predominance of matter over antimatter in the universe. The universe, as a whole, seems to have a nonzero positive baryon number density. Since it is assumed in cosmology that the particles we see were created using the same physics we measure today, it would normally be expected that the overall baryon number should be zero, as matter and antimatter should have been created in equal amounts. A number of theoretical mechanisms are proposed to account for this discrepancy, namely identifying conditions that favour symmetry breaking and the creation of normal matter (as opposed to antimatter). This imbalance has to be exceptionally small, on the order of 1 in every  (~2×109) particles a small fraction of a second after the Big Bang. After most of the matter and antimatter was annihilated, what remained was all the baryonic matter in the current universe, along with a much greater number of bosons. Experiments reported in 2010 at"
What is the Ramsauer-Townsend effect?,The Ramsauer-Townsend effect is a physical phenomenon that involves the scattering of low-energy electrons by atoms of a non-noble gas. It can be explained by classical mechanics.,The Ramsauer-Townsend effect is a physical phenomenon that involves the scattering of low-energy electrons by atoms of a noble gas. It requires the wave theory of quantum mechanics to be explained.,The Ramsauer-Townsend effect is a physical phenomenon that involves the scattering of high-energy electrons by atoms of a noble gas. It can be explained by classical mechanics.,The Ramsauer-Townsend effect is a physical phenomenon that involves the scattering of high-energy electrons by atoms of a non-noble gas. It requires the wave theory of quantum mechanics to be explained.,The Ramsauer-Townsend effect is a physical phenomenon that involves the scattering of electrons by atoms of any gas. It can be explained by classical mechanics.,B,
What is Minkowski space?,Minkowski space is a physical space where objects move in a straight line unless acted upon by a force.,Minkowski space is a mathematical model that combines inertial space and time manifolds with a non-inertial reference frame of space and time into a four-dimensional model relating a position to the field.,Minkowski space is a mathematical model that combines space and time into a two-dimensional model relating a position to the field.,Minkowski space is a mathematical model that combines space and time into a three-dimensional model relating a position to the field.,Minkowski space is a physical space where objects move in a curved line unless acted upon by a force.,B,"Minkowski spacetime

Minkowski space (or Minkowski spacetime) is a mathematical setting in which special relativity is conveniently formulated. Minkowski space is named for the German mathematician Hermann Minkowski, who around 1907 realized that the theory of special relativity (previously developed by Poincaré and Einstein) could be elegantly described using a four-dimensional spacetime, which combines the dimension of time with the three dimensions of space.

Mathematically there are a number of ways in which the four-dimensions of Minkowski spacetime are commonly represented: as a four-vector with 4 real coordinates, as a four-vector with 3 real and one complex coordinate, or using tensors.

Test theories of special relativity"
What is the Optical Signal-to-Noise Ratio (OSNR)?,"The Optical Signal-to-Noise Ratio (OSNR) is the ratio between the modulation frequency and the carrier frequency of an optical signal, used to describe the signal quality in systems where dynamic range is less than 6.02m.","The Optical Signal-to-Noise Ratio (OSNR) is the ratio between the signal power and the noise power in a given bandwidth, used to describe the signal quality without taking the receiver into account.","The Optical Signal-to-Noise Ratio (OSNR) is the ratio between the signal power and the noise power in a given bandwidth, used to describe the signal quality in situations where the dynamic range is less than 6.02m.","The Optical Signal-to-Noise Ratio (OSNR) is the ratio between the signal power and the noise power in a fixed bandwidth of 6.02m, used to describe the signal quality in systems where dynamic range is less than 6.02m.","The Optical Signal-to-Noise Ratio (OSNR) is the ratio between the signal power and the noise power in a given bandwidth, used to describe the signal quality in situations where the dynamic range is large or unpredictable.",B,"Signal-to-noise ratio (SNR or S/N) is a measure used in science and engineering that compares the level of a desired signal to the level of background noise. SNR is defined as the ratio of signal power to the noise power, often expressed in decibels. A ratio higher than 1:1 (greater than 0 dB) indicates more signal than noise.

SNR, bandwidth, and channel capacity of a communication channel are connected by the Shannon–Hartley theorem.

Definition

Signal-to-noise ratio is defined as the ratio of the power of a signal (meaningful input) to the power of background noise (meaningless or unwanted input):

where  is average power. Both signal and noise power must be measured at the same or equivalent points in a system, and within the same system bandwidth.

Depending on whether the signal is a constant () or a random variable (), the signal-to-noise ratio for random noise  becomes:

where E refers to the expected value, i.e. in this case the mean square of ,
or"
What is the interpretation of supersymmetry in stochastic supersymmetric theory?,Supersymmetry is a type of hydromagnetic dynamo that arises when the magnetic field becomes strong enough to affect the fluid motions.,Supersymmetry is a measure of the amplitude of the dynamo in the induction equation of the kinematic approximation.,Supersymmetry is a measure of the strength of the magnetic field in the induction equation of the kinematic dynamo.,Supersymmetry is a property of deterministic chaos that arises from the continuity of the flow in the model's phase space.,"Supersymmetry is an intrinsic property of all stochastic differential equations, and it preserves continuity in the model's phase space via continuous time flows.",E,"Itô interpretation and supersymmetric theory of SDEs 

In the supersymmetric theory of SDEs, stochastic evolution is defined via stochastic evolution operator (SEO) acting on differential forms of the phase space. The Itô-Stratonovich dilemma takes the form of the ambiguity of the operator ordering that arises on the way from the path integral to the operator representation of stochastic evolution. The Itô interpretation corresponds to the operator ordering convention that all the momentum operators act after all the position operators. The SEO can be made unique by supplying it with its most natural mathematical definition of the pullback induced by the noise-configuration-dependent SDE-defined diffeomorphisms and averaged over the noise configurations. This disambiguation leads to the Stratonovich interpretation of SDEs that can be turned into the Itô interpretation by a specific shift of the flow vector field of the SDE.

See also"
"What is the purpose of expressing a map's scale as a ratio, such as 1:10,000?","To indicate the use of south-up orientation, as used in Ancient Africa and some maps in Brazil today.","To indicate the orientation of the map, such as whether the 0° meridian is at the top or bottom of the page.","To indicate the projection used to create the map, such as Buckminster Fuller's Dymaxion projection.","To indicate the arrangement of the map, such as the world map of Gott, Vanderbei, and Goldberg arranged as a pair of disks back-to-back.",To indicate the relationship between the size of the map and the size of the area being represented.,E,"Terminology

Representation of scale
Map scales may be expressed in words (a lexical scale), as a ratio, or as a fraction. Examples are:
'one centimetre to one hundred metres'    or    1:10,000   or    1/10,000
'one inch to one mile'    or    1:63,360    or    1/63,360
'one centimetre to one thousand kilometres'   or   1:100,000,000    or    1/100,000,000.  (The ratio would usually be abbreviated to 1:100M)

Bar scale vs. lexical scale
In addition to the above many maps carry one or more (graphical) bar scales. For example, some modern British maps have three bar scales, one each for kilometres, miles and nautical miles.

A lexical scale in a language known to the user may be easier to visualise than a ratio: if the scale is an inch to two miles and the map user can see two villages that are about two inches apart on the map, then it is easy to work out that the villages are about four miles apart on the ground."
What is the main sequence in astronomy?,The main sequence is a type of galaxy that contains a large number of stars.,The main sequence is a type of black hole that is formed from the collapse of a massive star.,The main sequence is a continuous and distinctive band of stars that appears on plots of stellar color versus brightness. Stars on this band are known as main-sequence stars or dwarf stars.,The main sequence is a group of planets that orbit around a star in a solar system.,The main sequence is a type of nebula that is formed from the explosion of a supernova.,C,"In astronomy, the main sequence is a continuous and distinctive band of stars that appears on plots of stellar color versus brightness. These color-magnitude plots are known as Hertzsprung–Russell diagrams after their co-developers, Ejnar Hertzsprung and Henry Norris Russell. Stars on this band are known as main-sequence stars or dwarf stars. These are the most numerous true stars in the universe, and include the Sun."
"Who proposed the concept of ""maximal acceleration""?",Max Planck,Niels Bohr,Eduardo R. Caianiello,Hideki Yukawa,Albert Einstein,C,"Origin of the concept
In the 17th century, British scientist Robert Hooke presented the idea of an object accelerating inside a planet in a letter to Isaac Newton. A gravity train project was seriously presented to the French Academy of Sciences in the 19th century. The same idea was proposed, without calculation, by Lewis Carroll in 1893 in Sylvie and Bruno Concluded. The idea was rediscovered in the 1960s when physicist Paul Cooper published a paper in the American Journal of Physics suggesting that gravity trains be considered for a future transportation project.

Mathematical considerations
Under the assumption of a spherical planet with uniform density, and ignoring relativistic effects as well as friction, a gravity train has the following properties:

 The duration of a trip depends only on the density of the planet and the gravitational constant, but not on the diameter of the planet.
 The maximum speed is reached at the middle point of the trajectory."
What is indirect photophoresis?,"Indirect photophoresis is a phenomenon that occurs when particles absorb incident light uniformly, creating a temperature gradient within the particle, and causing a migration of particles in a random direction.","Indirect photophoresis is a phenomenon that occurs when particles absorb incident light only on the irradiated side, creating a temperature gradient within the particle, and causing a migration of particles in the same direction as the surface temperature gradient.","Indirect photophoresis is a phenomenon that occurs when particles absorb incident light uniformly, creating a temperature gradient within the particle, and causing a migration of particles in the same direction as the surface temperature gradient.","Indirect photophoresis is a phenomenon that occurs when particles absorb incident light only on the irradiated side, creating a temperature gradient within the particle, and causing a migration of particles in a direction opposite to the surface temperature gradient.","Indirect photophoresis is a phenomenon that occurs when particles absorb incident light uniformly, creating a temperature gradient within the particle, and causing a migration of particles in a direction opposite to the surface temperature gradient.",D,"A photophore is a glandular organ that appears as luminous spots on various marine animals, including fish and cephalopods. The organ can be simple, or as complex as the human eye; equipped with lenses, shutters, color filters and reflectors, however unlike an eye it is optimized to produce light, not absorb it. The bioluminescence can variously be produced from compounds during the digestion of prey, from specialized mitochondrial cells in the organism called photocytes (""light producing"" cells), or, similarly, associated with symbiotic bacteria in the organism that are cultured.

The character of photophores is important in the identification of deep sea fishes. Photophores on fish are used for attracting food or for camouflage from predators by counter-illumination."
What does Earnshaw's theorem state?,A collection of point charges can be maintained in a stable stationary equilibrium configuration solely by the gravitational interaction of the charges.,A collection of point charges can be maintained in a stable stationary equilibrium configuration solely by the electrostatic interaction of the charges.,"A collection of point charges can be maintained in a stable stationary equilibrium configuration solely by the magnetic interaction of the charges, if the magnets are hard.",A collection of point charges cannot be maintained in a stable stationary equilibrium configuration solely by the electrostatic interaction of the charges.,A collection of point charges can be maintained in a stable stationary equilibrium configuration solely by the magnetic interaction of the charges.,D,"Earnshaw's theorem

The idea of particle instability in an electrostatic field originated with Samuel Earnshaw in 1839 and was formalized by James Clerk Maxwell in 1874 who gave it the title ""Earnshaw's theorem"" and proved it with the Laplace equation. Earnshaw's theorem explains why a system of electrons is not stable and was invoked by Niels Bohr in his atom model of 1913 when criticizing J. J. Thomson's atom. 

Earnshaw's theorem holds that a charged particle suspended in an electrostatic field is unstable, because the forces of attraction and repulsion vary at an equal rate that is proportional to the inverse square law and remain in balance wherever a particle moves. Since the forces remain in balance, there is no inequality to provide a restoring force; and the particle remains unstable and can freely move without restriction."
What is radiosity in radiometry?,"Radiosity is the radiant flux entering a surface per unit area, including emitted, reflected, and transmitted radiation.","Radiosity is the radiant flux entering a surface per unit area, including absorbed, reflected, and transmitted radiation.","Radiosity is the radiant flux leaving a surface per unit area, including absorbed, reflected, and transmitted radiation.","Radiosity is the radiant flux leaving a surface per unit area, including emitted, reflected, and transmitted radiation.","Radiosity is the radiant flux leaving a surface per unit volume, including emitted, reflected, and transmitted radiation.",D,"In radiometry, radiosity is the radiant flux leaving (emitted, reflected and transmitted by) a surface per unit area, and spectral radiosity is the radiosity of a surface per unit frequency or wavelength, depending on whether the spectrum is taken as a function of frequency or of wavelength. The SI unit of radiosity is the watt per square metre (), while that of spectral radiosity in frequency is the watt per square metre per hertz (W·m−2·Hz−1) and that of spectral radiosity in wavelength is the watt per square metre per metre (W·m−3)—commonly the watt per square metre per nanometre (). The CGS unit erg per square centimeter per second () is often used in astronomy. Radiosity is often called  in branches of physics other than radiometry, but in radiometry this usage leads to confusion with radiant intensity.

Mathematical definitions

Radiosity
Radiosity of a surface, denoted Je (""e"" for ""energetic"", to avoid confusion with photometric quantities), is defined as"
What is a virtual particle?,A particle that is not affected by the strong force.,A particle that is not affected by the weak force.,A particle that is created in a laboratory for experimental purposes.,A particle that is not directly observable but is inferred from its effects on measurable particles.,A particle that is directly observable and can be measured in experiments.,D,"In particle physics, this inequality permits a qualitative understanding of virtual particles, which carry momentum. The exchange of virtual particles with real particles is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons are also responsible for the electrostatic interaction between electric charges (which results in Coulomb's law), for spontaneous radiative decay of excited atomic and nuclear states, for the Casimir force, for the Van der Waals force and some other observable phenomena.

Energy transfer"
"Who proposed the principle of ""complexity from noise"" and when was it first introduced?",Ilya Prigogine in 1979,Henri Atlan in 1972,Democritus and Lucretius in ancient times,None of the above.,René Descartes in 1637,B,"The term self-organized criticality was first introduced in Bak, Tang and Wiesenfeld's 1987 paper, which clearly linked together those factors: a simple cellular automaton was shown to produce several characteristic features observed in natural complexity (fractal geometry, pink (1/f) noise and power laws) in a way that could be linked to critical-point phenomena. Crucially, however, the paper emphasized that the complexity observed emerged in a robust manner that did not depend on finely tuned details of the system: variable parameters in the model could be changed widely without affecting the emergence of critical behavior: hence, self-organized criticality. Thus, the key result of BTW's paper was its discovery of a mechanism by which the emergence of complexity from simple local interactions could be spontaneous—and therefore plausible as a source of natural complexity—rather than something that was only possible in artificial situations in which control parameters are tuned to"
What is the order parameter that breaks the electromagnetic gauge symmetry in superconductors?,None of the above.,A thin cylindrical plastic rod.,A condensed-matter collective field ψ.,The cosmic microwave background.,A component of the Higgs field.,C,"In superconductors, there is a condensed-matter collective field ψ, which acts as the order parameter breaking the electromagnetic gauge symmetry.
 Take a thin cylindrical plastic rod and push both ends together. Before buckling, the system is symmetric under rotation, and so visibly cylindrically symmetric. But after buckling, it looks different, and asymmetric. Nevertheless, features of the cylindrical symmetry are still there: ignoring friction, it would take no force to freely spin the rod around, displacing the ground state in time, and amounting to an oscillation of vanishing frequency, unlike the radial oscillations in the direction of the buckle. This spinning mode is effectively the requisite Nambu–Goldstone boson."
What is the reason for the sun appearing slightly yellowish when viewed from Earth?,The sun appears yellowish due to a reflection of the Earth's atmosphere.,"The longer wavelengths of light, such as red and yellow, are not scattered away and are directly visible when looking towards the sun.","The sun appears yellowish due to the scattering of all colors of light, mainly blue and green, in the Earth's atmosphere.","The sun emits a yellow light due to its own spectrum, which is visible when viewed from Earth.","The atmosphere absorbs the shorter wavelengths of light, such as blue and red, leaving only the longer wavelengths of light, such as green and yellow, visible when looking towards the sun.",B,"The Sun emits light across the visible spectrum, so its color is white, with a CIE color-space index near (0.3, 0.3), when viewed from space or when the Sun is high in the sky. The Solar radiance per wavelength peaks in the green portion of the spectrum when viewed from space. When the Sun is low in the sky, atmospheric scattering renders the Sun yellow, red, orange, or magenta. Despite its typical whiteness, most people mentally picture the Sun as yellow; the reasons for this are the subject of debate.
The Sun is a G2V star, with G2 indicating its surface temperature of approximately 5,778 K (5,505 °C, 9,941 °F), and V that it, like most stars, is a main-sequence star. The average luminance of the Sun is about 1.88 giga candela per square metre, but as viewed through Earth's atmosphere, this is lowered to about 1.44 Gcd/m2. However, the luminance is not constant across the disk of the Sun, due to limb darkening.

Composition"
What is the Landau-Lifshitz-Gilbert equation used for in physics?,"The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a liquid, and is commonly used in micromagnetics to model the effects of a magnetic field on ferromagnetic materials.","The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a solid, and is commonly used in astrophysics to model the effects of a magnetic field on celestial bodies.","The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a solid, and is commonly used in micromagnetics to model the effects of a magnetic field on ferromagnetic materials.","The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a solid, and is commonly used in macro-magnetics to model the effects of a magnetic field on ferromagnetic materials.","The Landau-Lifshitz-Gilbert equation is a differential equation used to describe the precessional motion of magnetization M in a liquid, and is commonly used in macro-magnetics to model the effects of a magnetic field on ferromagnetic materials.",C,"In physics, the Landau–Lifshitz–Gilbert equation, named for Lev Landau, Evgeny Lifshitz, and T. L. Gilbert, is a name used for a differential equation describing the precessional motion of  magnetization  in a solid. It is a modification by Gilbert of the original equation of Landau and Lifshitz.

The various forms of the equation are commonly used in micromagnetics to model the effects of a magnetic field on ferromagnetic materials. In particular it can be used to model the time domain behavior of magnetic elements due to a magnetic field. An additional term was added to the equation to describe the effect of spin polarized current on magnets.

Landau–Lifshitz equation"
What is spatial dispersion?,"Spatial dispersion is a phenomenon in the physics of continuous media where material parameters such as permittivity or conductivity have dependence on time. It represents memory effects in systems, commonly seen in optics and electronics.",Spatial dispersion is a phenomenon in the physics of continuous media where material parameters such as permittivity or conductivity have dependence on time. It represents spreading effects and is usually significant only at microscopic length scales.,"Spatial dispersion is a phenomenon in the physics of continuous media where material parameters such as permittivity or conductivity have no dependence on wavevector. It represents memory effects in systems, commonly seen in optics and electronics.",Spatial dispersion is a phenomenon in the physics of continuous media where material parameters such as permittivity or conductivity have dependence on wavevector. It represents spreading effects and is usually significant only at microscopic length scales.,"Spatial dispersion is a phenomenon in the physics of continuous media where material parameters such as permittivity or conductivity have dependence on wavevector. It represents memory effects in systems, commonly seen in optics and electronics.",D,"Spatial dispersion 

In electromagnetics and optics, the term dispersion generally refers to aforementioned temporal or frequency dispersion. Spatial dispersion refers to the non-local response of the medium to the space; this can be reworded as the wavevector dependence of the permittivity. For an exemplary anisotropic medium, the spatial relation between electric and electric displacement field can be expressed as a convolution:"
What are the constituents of cold dark matter?,"They are unknown, but possibilities include large objects like MACHOs or new particles such as WIMPs and axions.",They are known to be black holes and Preon stars.,They are only MACHOs.,They are clusters of brown dwarfs.,They are new particles such as RAMBOs.,A,"Composition 
Dark matter is detected through its gravitational interactions with ordinary matter and radiation. As such, it is very difficult to determine what the constituents of cold dark matter are. The candidates fall roughly into three categories:

 Axions, very light particles with a specific type of self-interaction that makes them a suitable CDM candidate. Axions have the theoretical advantage that their existence solves the strong CP problem in quantum chromodynamics, but axion particles have only been theorized and never detected. Axions are an example of a more general category of particle called a WISP (weakly interacting ""slender"" or ""slim"" particle), which are the low-mass counterparts of WIMPs."
What is the mechanism of FTIR?,"The mechanism of FTIR is called ray optics, which is a good analog to visualize quantum tunneling.","The mechanism of FTIR is called scattering, which is a good analog to visualize quantum tunneling.","The mechanism of FTIR is called frustrated TIR, which is a good analog to visualize quantum tunneling.","The mechanism of FTIR is called evanescent-wave coupling, which is a good analog to visualize quantum tunneling.","The mechanism of FTIR is called total internal reflection microscopy, which is a good analog to visualize quantum tunneling.",D,"Nano-FTIR (nanoscale Fourier transform infrared spectroscopy, also known as scattering-type scanning near-field optical spectroscopy) is a scanning probe technique that can be considered as a combination of two techniques: Fourier transform infrared spectroscopy (FTIR) and scattering-type scanning near-field optical microscopy (s-SNOM). As s-SNOM, nano-FTIR is based on atomic-force microscopy (AFM), where a sharp tip is illuminated by an external light source and the tip-scattered light (typically back-scattered) is detected as a function of tip position. A typical nano-FTIR setup thus consists of an atomic force microscope, a broadband infrared light source used for tip illumination, and a Michelson interferometer acting as Fourier transform spectrometer. In nano-FTIR, the sample stage is placed in one of the interferometer arms, which allows for recording both amplitude and phase of the detected light (unlike conventional FTIR that normally does not yield phase information). Scanning"
What is the origin of the permanent moment in paramagnetism?,The permanent moment is generally due to the spin of unpaired electrons in atomic or molecular electron orbitals.,The permanent moment is due to the alignment of dipoles perpendicular to the applied field.,"The permanent moment is due to the torque provided on the magnetic moments by an applied field, which tries to align the dipoles perpendicular to the applied field.",The permanent moment is due to the quantum-mechanical properties of spin and angular momentum.,The permanent moment is due to the interaction of dipoles with one another and are randomly oriented in the absence of an external field due to thermal agitation.,A,"Permanent magnetism is caused by the alignment of magnetic moments and induced magnetism is created when disordered magnetic moments are forced to align in an applied magnetic field. For example, the ordered magnetic moments (ferromagnetic, Figure 1) change and become disordered (paramagnetic, Figure 2) at the Curie temperature. Higher temperatures make magnets weaker, as spontaneous magnetism only occurs below the Curie temperature. Magnetic susceptibility above the Curie temperature can be calculated from the Curie–Weiss law, which is derived from Curie's law.

In analogy to ferromagnetic and paramagnetic materials, the Curie temperature can also be used to describe the phase transition between ferroelectricity and paraelectricity. In this context, the order parameter is the electric polarization that goes from a finite value to zero when the temperature is increased above the Curie temperature.

Magnetic moments"
What is the reason that Newton's second law cannot be used to calculate the development of a physical system in quantum mechanics?,"The existence of particle spin, which is linear momentum that can be described by the cumulative effect of point-like motions in space.","The existence of particle spin, which is angular momentum that is always equal to zero.","The existence of particle spin, which is linear momentum that cannot be described by the cumulative effect of point-like motions in space.","The existence of particle spin, which is angular momentum that cannot be described by the cumulative effect of point-like motions in space.","The existence of particle spin, which is angular momentum that can be described by the cumulative effect of point-like motions in space.",D,"Conceptually, the Schrödinger equation is the quantum counterpart of Newton's second law in classical mechanics. Given a set of known initial conditions, Newton's second law makes a mathematical prediction as to what path a given physical system will take over time. The Schrödinger equation gives the evolution over time of a wave function, the quantum-mechanical characterization of an isolated physical system. The equation can be derived from the fact that the time-evolution operator must be unitary, and must therefore be generated by the exponential of a self-adjoint operator, which is the quantum Hamiltonian."
"What is the butterfly effect, as defined by Lorenz in his book ""The Essence of Chaos""?","The butterfly effect is the phenomenon that a small change in the initial conditions of a dynamical system can cause subsequent states to differ greatly from the states that would have followed without the alteration, as defined by Einstein in his book ""The Theory of Relativity.""","The butterfly effect is the phenomenon that a large change in the initial conditions of a dynamical system has no effect on subsequent states, as defined by Lorenz in his book ""The Essence of Chaos.""","The butterfly effect is the phenomenon that a small change in the initial conditions of a dynamical system can cause significant differences in subsequent states, as defined by Lorenz in his book ""The Essence of Chaos.""","The butterfly effect is the phenomenon that a small change in the initial conditions of a dynamical system has no effect on subsequent states, as defined by Lorenz in his book ""The Essence of Chaos.""","The butterfly effect is the phenomenon that a large change in the initial conditions of a dynamical system can cause significant differences in subsequent states, as defined by Lorenz in his book ""The Essence of Chaos.""",C,"The Lorenz system is a system of ordinary differential equations first studied by mathematician and meteorologist Edward Lorenz. It is notable for having chaotic solutions for certain parameter values and initial conditions. In particular, the Lorenz attractor is a set of chaotic solutions of the Lorenz system. In popular media the ""butterfly effect"" stems from the real-world implications of the Lorenz attractor, i.e. that in any physical system, in the absence of perfect knowledge of the initial conditions (even the minuscule disturbance of the air due to a butterfly flapping its wings), our ability to predict its future course will always fail. This underscores that physical systems can be completely deterministic and yet still be inherently unpredictable even in the absence of quantum effects. The shape of the Lorenz attractor itself, when plotted graphically, may also be seen to resemble a butterfly."
What is the role of CYCLOIDEA genes in the evolution of bilateral symmetry?,CYCLOIDEA genes are responsible for the selection of symmetry in the evolution of animals.,"CYCLOIDEA genes are responsible for the evolution of specialized pollinators in plants, which in turn led to the transition of radially symmetrical flowers to bilaterally symmetrical flowers.","CYCLOIDEA genes are responsible for the expression of dorsal petals in Antirrhinum majus, which control their size and shape.","CYCLOIDEA genes are responsible for the expression of transcription factors that control the expression of other genes, allowing their expression to influence developmental pathways relating to symmetry.",CYCLOIDEA genes are responsible for mutations that cause a reversion to radial symmetry.,D,"Another example is that of Linaria vulgaris, which has two kinds of flower symmetries-radial and bilateral. These symmetries are due to epigenetic changes in just one gene called CYCLOIDEA.

Arabidopsis thaliana has a gene called AGAMOUS that plays an important role in defining how many petals and sepals and other organs are generated. Mutations in this gene give rise to the floral meristem obtaining an indeterminate fate, and proliferation of floral organs in double-flowered forms of roses, carnations and morning glory. These phenotypes have been selected by horticulturists for their increased number of petals. Several studies on diverse plants like petunia, tomato, Impatiens, maize, etc. have suggested that the enormous diversity of flowers is a result of small changes in genes controlling their development."
What is the required excess quark per billion quark-antiquark pairs in the early universe in order to provide all the observed matter in the universe?,One,Five,Three,Two,Four,A,"Because of the extremely high energies involved, quark-antiquark pairs are produced by pair production and thus QGP is a roughly equal mixture of quarks and antiquarks of various flavors, with only a slight excess of quarks. This property is not a general feature of conventional plasmas, which may be too cool for pair production (see however pair instability supernova)."
"What is the meaning of the term ""horror vacui""?",The quantified extension of volume in empty space.,The commonly held view that nature abhorred a vacuum.,The medieval thought experiment into the idea of a vacuum.,The success of Descartes' namesake coordinate system.,The spatial-corporeal component of Descartes' metaphysics.,B,"In ophthalmology, horror fusionis is a condition in which the eyes have an unsteady deviation, with the extraocular muscles performing spasm-like movements that continuously shift the eyes away from the position in which they would be directed to the same point in space, giving rise to diplopia. Even when the double vision images are made to nearly overlap using optical means such as prisms, the irregular movements prevent binocular fusion. The name horror fusionis (Latin phrase literally meaning ""fear of fusion"") arises from the notion that the brain is, or at least appears to be, actively preventing binocular fusion.

The condition is an extreme type of binocular fusion deficiency."
What is the Droste effect?,The Droste effect is a type of optical illusion that creates the appearance of a three-dimensional image within a two-dimensional picture.,"The Droste effect is a type of packaging design used by a variety of products, named after a Dutch brand of cocoa, with an image designed by Jan Misset in 1904.","The Droste effect is a type of painting technique used by Dutch artist M. C. Escher in his 1956 lithograph Print Gallery, which portrays a gallery that depicts itself.","The Droste effect is a recursive image effect in which a picture appears within itself in a place where a similar picture would realistically be expected to appear. This creates a loop that can continue as far as the image's resolution allows, and is named after a Dutch brand of cocoa.",The Droste effect is a type of recursive algorithm used in computer programming to create self-referential images.,D,
What is water hammer?,Water hammer is a type of water turbine used in hydroelectric generating stations to generate electricity.,Water hammer is a type of air trap or standpipe used to dampen the sound of moving water in plumbing systems.,Water hammer is a type of plumbing tool used to break pipelines and absorb the potentially damaging forces caused by moving water.,Water hammer is a type of water pump used to increase the pressure of water in pipelines.,"Water hammer is a loud banging noise resembling a hammering sound that occurs when moving water is suddenly stopped, causing a rise in pressure and resulting shock wave.",E,"Many plumbers, firefighters, and  steamfitters are aware of this phenomenon, which is called  ""water hammer"". A several-ounce ""slug"" of water passing through a steam line at high velocity and striking a 90-degree elbow can instantly fracture a fitting that is otherwise capable of handling several times the normal static pressure. It can then be understood that a few hundred, or even a few thousand pounds of water moving at the same velocity inside a boiler shell can easily blow out a tube sheet, collapse a firebox, even toss the entire boiler a surprising distance through reaction as the water exits the boiler, like the recoil of a heavy cannon firing a ball."
What is the reason for the stochastic nature of all observed resistance-switching processes?,"The free-energy barriers for the transition {i} → {j} are not high enough, and the memory device can switch without having to do anything.","The device is subjected to random thermal fluctuations, which trigger the switching event, but it is impossible to predict when it will occur.","The memory device is found to be in a distinct resistance state {j}, and there exists no physical one-to-one relationship between its present state and its foregoing voltage history.","The device is subjected to biases below the threshold value, which still allows for a finite probability of switching, but it is possible to predict when it will occur.","The external bias is set to a value above a certain threshold value, which reduces the free-energy barrier for the transition {i} → {j} to zero.",B,"A ""resistance switching"" event can simply be enforced by setting the external bias to a value above a certain threshold value. This is the trivial case, i.e., the free-energy barrier for the transition {i} → {j} is reduced to zero. In case one applies biases below the threshold value, there is still a finite probability that the device will switch in course of time (triggered by a random thermal fluctuation), but – as one is dealing with probabilistic processes – it is impossible to predict when the switching event will occur. That is the basic reason for the stochastic nature of all observed resistance-switching (ReRAM) processes. If the free-energy barriers are not high enough, the memory device can even switch without having to do anything."
What is the Einstein@Home project?,The Einstein@Home project is a project that aims to detect signals from supernovae or binary black holes. It takes data from LIGO and GEO and sends it out in little pieces to thousands of volunteers for parallel analysis on their home computers.,The Einstein@Home project is a project that aims to detect signals from supernovae or binary black holes. It takes data from SETI and GEO and sends it out in little pieces to thousands of volunteers for parallel analysis on their home computers.,The Einstein@Home project is a distributed computing project that aims to detect simple gravitational waves with constant frequency. It takes data from LIGO and GEO and sends it out in little pieces to thousands of volunteers for parallel analysis on their home computers.,The Einstein@Home project is a project that aims to detect simple gravitational waves with constant frequency. It takes data from LIGO and GEO and sends it out in large pieces to thousands of volunteers for parallel analysis on their home computers.,The Einstein@Home project is a project that aims to detect simple gravitational waves with constant frequency. It takes data from SETI and GEO and sends it out in little pieces to thousands of volunteers for parallel analysis on their home computers.,C,"Since March 2009, part of the Einstein@Home computing power is used to analyze PALFA data. The Einstein@Home algorithm is particularly sensitive to radio pulsars in tight binary systems (as short as 11 minutes), with a phase-space coverage that is complementary to that of the PRESTO pipeline. To date, it has re-detected 123 previously known radio pulsars as well as several previously unknown pulsars."
What happens to an initially inhomogeneous physical system that is isolated by a thermodynamic operation?,It will change its internal state only if it is composed of a single subsystem and has internal walls.,It will change its internal state only if it is composed of several subsystems separated from each other by walls.,It will remain in its initial state indefinitely.,It will generally change its internal state over time.,It will change its internal state only if it is composed of a single subsystem.,D,"An isolated physical system may be inhomogeneous, or may be composed of several subsystems separated from each other by walls. If an initially inhomogeneous physical system, without internal walls, is isolated by a thermodynamic operation, it will in general over time change its internal state. Or if it is composed of several subsystems separated from each other by walls, it may change its state after a thermodynamic operation that changes its walls. Such changes may include change of temperature or spatial distribution of temperature, by changing the state of constituent materials. A rod of iron, initially prepared to be hot at one end and cold at the other, when isolated, will change so that its temperature becomes uniform all along its length; during the process, the rod is not in thermal equilibrium until its temperature is uniform. In a system prepared as a block of ice floating in a bath of hot water, and then isolated, the ice can melt; during the melting, the system is not in"
"What is the concept of simultaneity in Einstein's book, Relativity?","Simultaneity is relative, meaning that two events that appear simultaneous to an observer in a particular inertial reference frame need not be judged as simultaneous by a second observer in a different inertial frame of reference.","Simultaneity is relative, meaning that two events that appear simultaneous to an observer in a particular inertial reference frame will always be judged as simultaneous by a second observer in a different inertial frame of reference.","Simultaneity is absolute, meaning that two events that appear simultaneous to an observer in a particular inertial reference frame will always be judged as simultaneous by a second observer in a different inertial frame of reference.",Simultaneity is a concept that applies only to Newtonian theories and not to relativistic theories.,Simultaneity is a concept that applies only to relativistic theories and not to Newtonian theories.,A,"In physics, the relativity of simultaneity is the concept that distant simultaneity – whether two spatially separated events occur at the same time – is not absolute, but depends on the observer's reference frame.

Description
According to Einstein's special theory of relativity, it is impossible to say in an absolute sense that two distinct events occur at the same time if those events are separated in space. If one reference frame assigns precisely the same time to two events that are at different points in space, a reference frame that is moving relative to the first will generally assign different times to the two events (the only exception being when motion is exactly perpendicular to the line connecting the locations of both events)."
What is the Josephson effect?,"The Josephson effect is a phenomenon exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum Φ0 = h/(2e), where h is the Planck constant.","The Josephson effect is a phenomenon exploited by magnetic devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum Φ0 = h/(2e), where h is the magnetic constant.","The Josephson effect is a phenomenon exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the electric flux quantum Φ0 = h/(2e), where h is the Planck constant.","The Josephson effect is a phenomenon exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum Φ0 = e/(2h), where h is the Planck constant.","The Josephson effect is a phenomenon exploited by magnetic devices such as SQUIDs. It is used in the most accurate available measurements of the electric flux quantum Φ0 = h/(2e), where h is the magnetic constant.",A,"The inverse of the flux quantum, , is called the Josephson constant, and is denoted J. It is the constant of proportionality of the Josephson effect, relating the potential difference across a Josephson junction to the frequency of the irradiation. The Josephson effect is very widely used to provide a standard for high-precision measurements of potential difference, which (from 1990 to 2019) were related to a fixed, conventional value of the Josephson constant, denoted J-90.  With the 2019 redefinition of SI base units, the Josephson constant has an exact value of J = , which replaces the conventional value J-90.

Introduction 
The following physical equations use SI units. In CGS units, a factor of  would appear."
What is the SI unit of the physical quantity m/Q?,Meter per second,Pascal per second,Kilogram per coulomb,Newton per meter,Joule per second,C,"Exceptions 

There are non-classical effects that derive from quantum mechanics, such as the Stern–Gerlach effect that can diverge the path of ions of identical m/Q.

Symbols and units 
The IUPAC recommended symbol for mass and charge are m and Q, respectively, however using a lowercase q for charge is also very common. Charge is a scalar property, meaning that it can be either positive (+) or negative (−). The Coulomb (C) is the SI unit of charge; however, other units can be used, such as expressing charge in terms of the elementary charge (e). The SI unit of the physical quantity m/Q is kilogram per coulomb.

Mass spectrometry and m/z"
How many crystallographic point groups are there in three-dimensional space?,7,32,14,5,27,B,"In the classification of crystals, each point group defines a so-called (geometric) crystal class. There are infinitely many three-dimensional point groups. However, the crystallographic restriction on the general point groups results in there being only 32 crystallographic point groups. These 32 point groups are one-and-the-same as the 32 types of morphological (external) crystalline symmetries derived in 1830 by Johann Friedrich Christian Hessel from a consideration of observed crystal forms. 

The point group of a crystal determines, among other things, the directional variation of physical properties that arise from its structure, including optical properties such as birefringency, or electro-optical features such as the Pockels effect. For a periodic crystal (as opposed to a quasicrystal), the group must maintain the three-dimensional translational symmetry that defines crystallinity.

Notation"
What is the Liouville density?,The Liouville density is a probability distribution that specifies the probability of finding a particle at a certain position in phase space for a collection of particles.,The Liouville density is a quasiprobability distribution that plays an analogous role to the probability distribution for a quantum particle.,The Liouville density is a bounded probability distribution that is a convenient indicator of quantum-mechanical interference.,The Liouville density is a probability distribution that takes on negative values for states which have no classical model.,The Liouville density is a probability distribution that satisfies all the properties of a conventional probability distribution for a quantum particle.,A,"The density operator (density matrix)  is split by means of a projection operator

into two parts 
, 
where . The projection operator  selects the aforementioned relevant part from the density operator, for which an equation of motion is to be derived.

The Liouville – von Neumann equation can thus be represented as

The second line is formally solved as

By plugging the solution into the first equation, we obtain the Nakajima–Zwanzig equation:

Under the assumption that the inhomogeneous term vanishes and using

 as well as
 
we obtain the final form

See also 

 Redfield equation

Notes

References"
What are the four qualitative levels of crystallinity described by geologists?,"Holocrystalline, hypocrystalline, hypercrystalline, and holohyaline","Holocrystalline, hypocrystalline, hypohyaline, and holohyaline","Holocrystalline, hypohyaline, hypercrystalline, and holohyaline","Holocrystalline, hypocrystalline, hypercrystalline, and hyperhyaline","Holocrystalline, hypocrystalline, hypohyaline, and hyperhyaline",B,"Crystallinity can be measured using x-ray crystallography, but calorimetric techniques are also commonly used.

Rock crystallinity 
Geologists describe four qualitative levels of crystallinity:
 holocrystalline rocks are completely crystalline;
 hypocrystalline rocks are partially crystalline, with crystals embedded in an amorphous or glassy matrix;
 hypohyaline rocks are partially glassy;
 holohyaline rocks (such as obsidian) are completely glassy.

References
Oxford dictionary of science, 1999, .

Crystals
Physical quantities
Phases of matter"
What is an order parameter?,An order parameter is a measure of the temperature of a physical system.,An order parameter is a measure of the gravitational force in a physical system.,An order parameter is a measure of the magnetic field strength in a physical system.,An order parameter is a measure of the degree of symmetry breaking in a physical system.,An order parameter is a measure of the rotational symmetry in a physical system.,D,"An example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.

From a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.

Some phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition."
What is the significance of the discovery of the Crab pulsar?,The discovery of the Crab pulsar confirmed the black hole model of pulsars.,The discovery of the Crab pulsar confirmed the rotating neutron star model of pulsars.,The discovery of the Crab pulsar confirmed the white dwarf model of pulsars.,The discovery of the Crab pulsar disproved the rotating neutron star model of pulsars.,The discovery of the Crab pulsar confirmed the red giant model of pulsars.,B,"The Crab Pulsar (PSR B0531+21) is a relatively young neutron star. The star is the central star in the Crab Nebula, a remnant of the supernova SN 1054, which was widely observed on Earth in the year 1054. Discovered in 1968, the pulsar was the first to be connected with a supernova remnant."
What is the De Haas-Van Alphen effect?,The measurement of the electronic properties of a material using several experimental techniques.,The complex number quantity that describes AC susceptibility and AC permeability.,"The oscillation of the differential susceptibility as a function of 1/H in metals under strong magnetic fields, which relates the period of the susceptibility with the Fermi surface of the material.",The analogue non-linear relation between magnetization and magnetic field in antiferromagnetic materials.,The measurement of magnetic susceptibility in response to an AC magnetic field.,C,"The De Haas–Van Alphen effect, often abbreviated to DHVA, is a quantum mechanical effect in which the magnetic susceptibility of a pure metal crystal oscillates as the intensity of the magnetic field B is increased. It can be used to determine the Fermi surface of a material. Other quantities also oscillate, such as the electrical resistivity (Shubnikov–de Haas effect), specific heat, and sound attenuation and speed.  It is named after Wander Johannes de Haas and his student Pieter M. van Alphen. The DHVA effect comes from the orbital motion of itinerant electrons in the material. An equivalent phenomenon at low magnetic fields is known as Landau diamagnetism.

Description 
The differential magnetic susceptibility of a material is defined as"
"What is a ""coffee ring"" in physics?",A type of coffee that is made by boiling coffee grounds in water.,"A pattern left by a particle-laden liquid after it is spilled, named for the characteristic ring-like deposit along the perimeter of a spill of coffee or red wine.",A type of coffee that is made by mixing instant coffee with hot water.,A type of coffee that is made by pouring hot water over coffee grounds in a filter.,"A pattern left by a particle-laden liquid after it evaporates, named for the characteristic ring-like deposit along the perimeter of a spill of coffee or red wine.",E,"In physics, a ""coffee ring"" is a pattern left by a puddle of particle-laden liquid after it evaporates. The phenomenon is named for the characteristic ring-like deposit along the perimeter of a spill of coffee. It is also commonly seen after spilling red wine. The mechanism behind the formation of these and similar rings is known as the coffee ring effect or in some instances, the coffee stain effect, or simply ring stain.

Flow mechanism
The coffee-ring pattern originates from the capillary flow induced by the evaporation of the drop: liquid evaporating from the edge is replenished by liquid from the interior. The resulting edgeward flow can carry nearly all the dispersed material to the edge. As a function of time, this process exhibits a ""rush-hour"" effect, that is, a rapid acceleration of the edgeward flow at the final stage of the drying process."
What is the significance of probability amplitudes in quantum mechanics?,Probability amplitudes are used to determine the mass of particles in quantum mechanics.,Probability amplitudes have no significance in quantum mechanics.,Probability amplitudes are used to determine the velocity of particles in quantum mechanics.,"Probability amplitudes act as the equivalent of conventional probabilities in classical mechanics, with many analogous laws.","Probability amplitudes act as the equivalent of conventional probabilities in quantum mechanics, with many analogous laws.",E,"Quantum mechanics ascribes a special significance to the wave packet; it is interpreted as a probability amplitude, its norm squared describing the probability density that a particle or particles in a particular state will be measured to have a given position or momentum. The wave equation is in this case the Schrödinger equation, and through its application it is possible to deduce the time evolution of a quantum mechanical system, similar to the process of the Hamiltonian formalism in classical mechanics. The dispersive character of solutions of the Schrödinger equation has played an important role in rejecting Schrödinger's original interpretation, and accepting the Born rule."
What is the relationship between the amplitude of a sound wave and its loudness?,The amplitude of a sound wave is related to its loudness.,The amplitude of a sound wave is directly proportional to its frequency.,The amplitude of a sound wave is not related to its loudness.,The amplitude of a sound wave is not related to its frequency.,The amplitude of a sound wave is inversely related to its loudness.,A,"In experimental sciences, noise can refer to any random fluctuations of data that hinders perception of a signal.

Measurement 

Sound is measured based on the amplitude and frequency of a sound wave. Amplitude measures how forceful the wave is. The energy in a sound wave is measured in decibels (dB), the measure of loudness, or intensity of a sound; this measurement describes the amplitude of a sound wave. Decibels are expressed in a logarithmic scale. On the other hand, pitch describes the frequency of a sound and is measured in hertz (Hz).

The main instrument to measure sounds in the air is the Sound Level Meter. There are many different varieties of instruments that are used to measure noise - Noise Dosimeters are often used in occupational environments, noise monitors are used to measure environmental noise and noise pollution,  and recently smartphone-based sound level meter applications (apps) are being used to crowdsource and map recreational and community noise."
What are coherent turbulent structures?,"Coherent turbulent structures are the most elementary components of complex multi-scale and chaotic motions in turbulent flows, which do not have temporal coherence and persist in their form for long enough periods that the methods of time-averaged statistics can be applied.","Coherent turbulent structures are the most elementary components of complex multi-scale and chaotic motions in turbulent flows, which have temporal coherence and persist in their form for very short periods that the methods of time-averaged statistics cannot be applied.","Coherent turbulent structures are more elementary components of complex multi-scale and chaotic motions in turbulent flows, which have temporal coherence and persist in their form for long enough periods that the methods of time-averaged statistics can be applied.","Coherent turbulent structures are the most complex and chaotic motions in turbulent flows, which have temporal coherence and persist in their form for long enough periods that the methods of time-averaged statistics can be applied.","Coherent turbulent structures are the most complex and chaotic motions in turbulent flows, which do not have temporal coherence and persist in their form for very short periods that the methods of time-averaged statistics cannot be applied.",C,"Turbulence 
ADCPs with pulse-to-pulse coherent processing can estimate the velocity with the precision required to resolve small scale motion. As a consequence, it is possible to estimate turbulent parameters from properly configured ADCPs.  A typical approach is to fit the along beam velocity to the Kolmogorov structure configuration and thereby estimate the dissipation rate.  The application of ADCPs to turbulence measurement is possible from stationary deployments but can also be done from moving underwater structures like gliders or from subsurface buoys."
What is the main factor that determines the occurrence of each type of supernova?,The star's distance from Earth,The star's age,The star's temperature,The star's luminosity,The progenitor star's metallicity,E,"Although pair-instability supernovae are core collapse supernovae with spectra and light curves similar to type II-P, the nature after core collapse is more like that of a giant type Ia with runaway fusion of carbon, oxygen, and silicon. The total energy released by the highest-mass events is comparable to other core collapse supernovae but neutrino production is thought to be very low, hence the kinetic and electromagnetic energy released is very high. The cores of these stars are much larger than any white dwarf and the amount of radioactive nickel and other heavy elements ejected from their cores can be orders of magnitude higher, with consequently high visual luminosity.

Progenitor

The supernova classification type is closely tied to the type of star at the time of the collapse. The occurrence of each type of supernova depends dramatically on the metallicity, and hence the age of the host galaxy."
What is the Erlangen program?,"The Erlangen program is a method of characterizing geometries based on statistics and probability, published by Felix Klein in 1872 as Vergleichende Betrachtungen über neuere geometrische Forschungen.","The Erlangen program is a method of characterizing geometries based on group theory and projective geometry, published by Felix Klein in 1872 as Vergleichende Betrachtungen über neuere geometrische Forschungen.","The Erlangen program is a method of characterizing geometries based on algebra and trigonometry, published by Felix Klein in 1872 as Vergleichende Betrachtungen über neuere geometrische Forschungen.","The Erlangen program is a method of characterizing geometries based on geometry and topology, published by Felix Klein in 1872 as Vergleichende Betrachtungen über neuere geometrische Forschungen.","The Erlangen program is a method of characterizing geometries based on calculus and differential equations, published by Felix Klein in 1872 as Vergleichende Betrachtungen über neuere geometrische Forschungen.",B,"Lisp Flavored Erlang (LFE) is a functional, concurrent, garbage collected, general-purpose programming language and Lisp dialect built on Core Erlang and the Erlang virtual machine (BEAM). LFE builds on Erlang to provide a Lisp syntax for writing distributed, fault-tolerant, soft real-time, non-stop applications. LFE also extends Erlang to support metaprogramming with Lisp macros and an improved developer experience with a feature-rich read–eval–print loop (REPL). LFE is actively supported on all recent releases of Erlang; the oldest version of Erlang supported is R14.

History

Initial release 
Initial work on LFE began in 2007, when Robert Virding started creating a prototype of Lisp running on Erlang. This work was focused primarily on parsing and exploring what an implementation might look like. No version control system was being used at the time, so tracking exact initial dates is somewhat problematic."
What is emissivity?,Emissivity is a measure of how well a surface resists deformation under stress.,Emissivity is a measure of how well a surface conducts heat.,Emissivity is a measure of how well a surface absorbs and emits thermal radiation.,Emissivity is a measure of how well a surface reflects visible light.,Emissivity is a measure of how well a surface absorbs and emits sound waves.,C,"Emissivity
Emissivity is a term that is often misunderstood and misused. It represents a material's ability to emit thermal radiation and is an optical property of matter.

Each material has a different emissivity, which may vary by temperature and infrared wavelength. For example, clean metal surfaces have emissivity that decreases at longer wavelengths; many dielectric materials, such as quartz (SiO2), sapphire (Al2O3), calcium fluoride (CaF2), etc. have emissivity that increases at longer wavelength; simple oxides, such as iron oxide (Fe2O3) display relatively flat emissivity in the infrared spectrum.

A material's emissivity can range from a theoretical 0.00 (completely not-emitting) to an equally theoretical 1.00 (completely emitting). An example of a substance with low emissivity would be silver, with an emissivity coefficient of .02. An example of a substance with high emissivity would be asphalt, with an emissivity coefficient of .98."
Who was the first person to describe the pulmonary circulation system?,Galen,Avicenna,Hippocrates,Aristotle,Ibn al-Nafis,E,"Michael Servetus was the first European to describe the function of pulmonary circulation, although his achievement was not widely recognized at the time, for a few reasons. He firstly described it in the ""Manuscript of Paris"" (near 1546), but this work was never published. And later he published  this description, but in a theological treatise, Christianismi Restitutio, not in a book on medicine. Only three copies of the book survived but these remained hidden for decades, the rest were burned  shortly after its publication in 1553 because of persecution of Servetus by religious authorities.

Better known discovery of pulmonary circulation was by Vesalius's successor at Padua, Realdo Colombo, in 1559."
What is the fate of a carbocation formed in crystalline naphthalene?,"The carbocation remains positively charged, trapped in the solid.","The carbocation undergoes spontaneous bond breaking, yielding a carbon-helium ion.","The carbocation forms a bond with helium, becoming a stable compound.","The carbocation undergoes decay, forming a negatively charged ion.","The carbocation gains an electron from surrounding molecules, becoming an electrically neutral radical.",E,"Radical formation

In a condensed phase, the carbocation can also gain an electron from surrounding molecules, thus becoming an electrically neutral radical.  For example, in crystalline naphthalene, a molecule with tritium substituted for hydrogen in the 1 (or 2) position will be turned by decay into a cation with a positive charge at that position.  That charge will however be quickly neutralized by an electron transported through the lattice, turning the molecule into the 1-naphthyl (or 2-naphthyl) radical; which are stable, trapped in the solid, below ."
What is the main focus of the Environmental Science Center at Qatar University?,"Environmental studies, with a main focus on marine science, atmospheric and political sciences.","Environmental studies, with a main focus on marine science, atmospheric and physical sciences.","Environmental studies, with a main focus on marine science, atmospheric and social sciences.","Environmental studies, with a main focus on marine science, atmospheric and biological sciences.","Environmental studies, with a main focus on space science, atmospheric and biological sciences.",D,
What is the purpose of obtaining surgical resection specimens?,"To remove an entire diseased area or organ for definitive surgical treatment of a disease, with pathological analysis of the specimen used to confirm the diagnosis.",To perform visual and microscopic tests on tissue samples using automated analysers and cultures.,To work in close collaboration with medical technologists and hospital administrations.,To administer a variety of tests of the biophysical properties of tissue samples.,To obtain bodily fluids such as blood and urine for laboratory analysis of disease diagnosis.,A,"Obtaining samples 
Since cytology deals with tissues, which are composed of cells, samples of tissues must be obtained in order to analyze the cells. There are several ways of obtaining a sample, the first is through dissecting a corpse, with a sample of tissue taken during an autopsy. The second is performing an aspirate (bone marrow, cerebrospinal fluid, etc.). To perform an aspirate in liquid tissues, a needle is inserted inside the body and a sample is extracted. Another common method is surgery, with a piece being removed during the procedure for later analysis. Finally, another common method is biopsy. In a biopsy, a needle is inserted into the skin and a solid sample of tissue is obtained."
What is the function of mammary glands in mammals?,Mammary glands produce milk to feed the young.,Mammary glands help mammals draw air into the lungs.,Mammary glands help mammals breathe with lungs.,Mammary glands excrete nitrogenous waste as urea.,Mammary glands separate oxygenated and deoxygenated blood in the mammalian heart.,A,"A mammary gland is an exocrine gland in humans and other mammals that produces milk to feed young offspring. Mammals get their name from the Latin word mamma, ""breast"". The mammary glands are arranged in organs such as the breasts in primates (for example, humans and chimpanzees), the udder in ruminants (for example, cows, goats, sheep, and deer), and the dugs of other animals (for example, dogs and cats). Lactorrhea, the occasional production of milk by the glands, can occur in any mammal, but in most mammals, lactation, the production of enough milk for nursing, occurs only in phenotypic females who have gestated in recent months or years. It is directed by hormonal guidance from sex steroids. In a few mammalian species, male lactation can occur. With humans, male lactation can occur only under specific circumstances."
What is the relationship between interstellar and cometary chemistry?,"Cometary chemistry is responsible for the formation of interstellar molecules, but there is no direct connection between the two.","Interstellar and cometary chemistry are the same thing, just with different names.","There is a possible connection between interstellar and cometary chemistry, as indicated by the similarity between interstellar and cometary ices and the analysis of organics from comet samples returned by the Stardust mission.","There is no relationship between interstellar and cometary chemistry, as they are two completely different phenomena.","Interstellar chemistry is responsible for the formation of comets, but there is no direct connection between the two.",C,
What is the reason for recycling rare metals according to the United Nations?,"The demand for rare metals will quickly exceed the consumed tonnage in 2013, but recycling rare metals with a worldwide production higher than 100 000 t/year is a good way to conserve natural resources and energy.","The demand for rare metals will decrease in 2013, and recycling rare metals with a worldwide production lower than 100 000 t/year is a good way to conserve natural resources and energy.","The demand for rare metals will quickly exceed the consumed tonnage in 2013, but recycling rare metals with a worldwide production higher than 100 000 t/year is not a good way to conserve natural resources and energy.","The demand for rare metals will quickly exceed the consumed tonnage in 2013, but recycling rare metals with a worldwide production lower than 100 000 t/year is not a good way to conserve natural resources and energy.","The demand for rare metals will quickly exceed the consumed tonnage in 2013, and recycling rare metals with a worldwide production lower than 100 000 t/year is urgent and priority should be placed on it in order to conserve natural resources and energy.",E,
What is radiometric dating?,"Radiometric dating is a method of measuring geological time using geological sedimentation, discovered in the early 20th century.","Radiometric dating is a method of measuring geological time using radioactive decay, discovered in the early 20th century.","Radiometric dating is a method of measuring geological time using the position of rocks, discovered in the early 20th century.","Radiometric dating is a method of measuring geological time using the age of fossils, discovered in the early 20th century.","Radiometric dating is a method of measuring geological time using the cooling of the earth, discovered in the early 20th century.",B,"Radiometric dating measures the steady decay of radioactive elements in an object to determine its age. It is used to calculate dates for the older part of the planet's geological record. The theory is very complicated but, in essence, the radioactive elements within an object decay to form isotopes of each chemical element. Isotopes are atoms of the element that differ in mass but share the same general properties. Geologists are most interested in the decay of isotopes carbon-14 (into nitrogen-14) and potassium-40 (into argon-40). Carbon-14 aka radiocarbon dating works for organic materials that are less than about 50,000 years old. For older periods, the potassium-argon dating process is more accurate."
What is the role of methane in Fischer-Tropsch processes?,Methane is partially converted to carbon monoxide for utilization in Fischer-Tropsch processes.,Methane is used as a catalyst in Fischer-Tropsch processes.,Methane is not used in Fischer-Tropsch processes.,Methane is fully converted to carbon monoxide for utilization in Fischer-Tropsch processes.,Methane is a byproduct of Fischer-Tropsch processes.,A,"In order to obtain the mixture of CO and H2 required for the Fischer–Tropsch process, methane (main component of natural gas) may be subjected to partial oxidation which yields a raw synthesis gas mixture of mostly carbon dioxide, carbon monoxide, hydrogen gas (and sometimes water and nitrogen). The ratio of carbon monoxide to hydrogen in the raw synthesis gas mixture can be adjusted e.g. using the water gas shift reaction. Removing impurities, particularly nitrogen, carbon dioxide and water, from the raw synthesis gas mixture yields pure synthesis gas (syngas).

The pure syngas is routed into the Fischer–Tropsch process, where the syngas reacts over an iron or cobalt catalyst to produce synthetic hydrocarbons, including alcohols."
What is a phageome?,"A community of viruses and their metagenomes localized in a particular environment, similar to a microbiome.","A community of bacteria and their metagenomes localized in a particular environment, similar to a microbiome.","A community of bacteriophages and their metagenomes localized in a particular environment, similar to a microbiome.","A community of fungi and their metagenomes localized in a particular environment, similar to a microbiome.","A community of archaea and their metagenomes localized in a particular environment, similar to a microbiome.",C,"A bacteriophage (), also known informally as a phage (), is a virus that infects and replicates within bacteria and archaea. The term was derived from ""bacteria"" and the Greek φαγεῖν (), meaning ""to devour"". Bacteriophages are composed of proteins that encapsulate a DNA or RNA genome, and may have structures that are either simple or elaborate. Their genomes may encode as few as four genes (e.g. MS2) and as many as hundreds of genes. Phages replicate within the bacterium following the injection of their genome into its cytoplasm."
What is organography?,Organography is the study of the stem and root of plants.,Organography is the scientific description of the structure and function of the organs of living things.,"Organography is the study of the development of organs from the ""growing points"" or apical meristems.",Organography is the study of the commonality of development between foliage leaves and floral leaves.,Organography is the study of the relationship between different organs and different functions in plants.,B,"organs. Tomoelastography, on the other hand, is a radiological imaging method that allows estimation of quantitative mechanical parameters of all organs and structures in the field of view. Moreover, tomoelastography does not rely on a single, specific imaging modality. While it has been introduced and is mostly performed using magnetic resonance elastography (MRE), tomoelastography can be extended to other imaging techniques as well."
What is the definition of anatomy?,Anatomy is the rarely used term that refers to the superstructure of polymers such as fiber formation or to larger composite assemblies.,Anatomy is a branch of morphology that deals with the structure of organisms.,"Anatomy is the study of the effects of external factors upon the morphology of organisms under experimental conditions, such as the effect of genetic mutation.","Anatomy is the analysis of the patterns of the locus of structures within the body plan of an organism, and forms the basis of taxonomical categorization.",Anatomy is the study of the relationship between the structure and function of morphological features.,B,Anatomy
What is a trophic level in an ecological pyramid?,A group of organisms that acquire most of their energy from the level above them in the pyramid.,A group of organisms that acquire most of their energy from the abiotic sources in the ecosystem.,A group of organisms that acquire most of their energy from the level below them in the pyramid.,A group of organisms that acquire most of their energy from the same level in the pyramid.,A group of organisms that do not acquire any energy from the ecosystem.,C,"An ecological pyramid (also trophic pyramid, Eltonian pyramid, energy pyramid, or sometimes food pyramid) is a graphical representation designed to show the biomass or bioproductivity at each trophic level in a given ecosystem.

A pyramid of energy shows how much energy is retained in the form of new biomass at each trophic level, while a pyramid of biomass shows how much biomass (the amount of living or organic matter present in an organism) is present in the organisms. There is also a pyramid of numbers representing the number of individual organisms at each trophic level. Pyramids of energy are normally upright, but other pyramids can be inverted or take other shapes.

Ecological pyramids begin with producers on the bottom (such as plants) and proceed through the various trophic levels (such as herbivores that eat plants, then carnivores that eat flesh, then omnivores that eat both plants and flesh, and so on).  The highest level is the top of the food chain."
What is a crossover experiment?,An experiment that involves crossing over two different types of materials to create a new material.,"A type of experiment used to distinguish between different mechanisms proposed for a chemical reaction, such as intermolecular vs. intramolecular mechanisms.",An experiment that involves crossing over two different types of organisms to create a hybrid.,An experiment that involves crossing over two different types of cells to create a new cell.,An experiment that involves crossing over two different chemicals to create a new substance.,B,"Crossover interference is the term used to refer to the non-random placement of crossovers with respect to each other during meiosis. The term is attributed to Hermann Joseph Muller, who observed that one crossover ""interferes with the coincident occurrence of another crossing over in the same pair of chromosomes, and I have accordingly termed this phenomenon ‘interference’."""
What is the role of IL-10 in the formation of Tr1 cells and tolerogenic DCs?,"IL-10 inhibits the formation of Tr1 cells and tolerogenic DCs, which are dependent on TGF-β and Tregs. Tr1 cells produce low levels of IL-10 and TGF-β, while tolerogenic DCs produce TGF-β that is important for Tr1 formation.","IL-10 induces the formation of Tr1 cells and tolerogenic DCs, which are dependent on IL-10 and TGF-β, but differ from Tregs by lacking expression of Foxp3. Tr1 cells produce high levels of IL-10 and TGF-β, while tolerogenic DCs produce IL-10 that is important for Tr1 formation.",IL-10 has no role in the formation of Tr1 cells and tolerogenic DCs. TGF-β and Tregs are the only factors involved in the formation of Tr1 cells and tolerogenic DCs.,"IL-10 induces the formation of Tr1 cells and tolerogenic DCs, which are dependent on IL-10 and Tregs, but differ from Tregs by expressing Foxp3. Tr1 cells produce low levels of IL-10 and TGF-β, while tolerogenic DCs produce IL-10 that is important for Tr1 formation.","IL-10 induces the formation of Tregs, which are dependent on TGF-β and Foxp3. Tr1 cells and tolerogenic DCs are not involved in this process.",B,"Mechanisms of suppression 
 Cytokines mediated
Tr1 cells secrete large amount of suppressing cytokines IL-10 and TGF-β. IL-10 directly inhibits T cells by blocking its production of IL-2, IFNy and GM-CSF and have tolerogenic effect on B cells and support differentiation of other regulatory T cells.  IL-10 indirectly downregulates MHC II molecules and co-stimulatory molecules on antigen-presenting cells (APC) and force them to upregulate tolerogenic molecules such as ILT-3, ILT-4 and HLA-G. 
 Cell to cell contact:
Type 1 regulatory T cells poses inhibitory receptor CTLA-4 through which they exert suppressor function.
 Metabolic disruption:
Tr1 cells can express ectoenzymes CD39 and CD73 and are suspected of generating adenosine which suppresses effector T cell proliferation and their cytokine production in vitro.
 Cytolitic activity:"
"What is the reason behind the designation of Class L dwarfs, and what is their color and composition?","Class L dwarfs are hotter than M stars and are designated L because L is the remaining letter alphabetically closest to M. They are bright blue in color and are brightest in ultraviolet. Their atmosphere is hot enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs.","Class L dwarfs are cooler than M stars and are designated L because L is the remaining letter alphabetically closest to M. They are dark red in color and are brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs.","Class L dwarfs are hotter than M stars and are designated L because L is the next letter alphabetically after M. They are dark red in color and are brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs.","Class L dwarfs are cooler than M stars and are designated L because L is the next letter alphabetically after M. They are bright yellow in color and are brightest in visible light. Their atmosphere is hot enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs.","Class L dwarfs are cooler than M stars and are designated L because L is the remaining letter alphabetically closest to M. They are bright green in color and are brightest in visible light. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra. Some of these objects have masses small enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs.",B,"the T spectral class for objects exhibiting H- and K-band CH4 absorption. , 355 T dwarfs are known. NIR classification schemes for T dwarfs have recently been developed by Adam Burgasser and Tom Geballe. Theory suggests that L dwarfs are a mixture of very-low-mass stars and sub-stellar objects (brown dwarfs), whereas the T dwarf class is composed entirely of brown dwarfs. Because of the absorption of sodium and potassium in the green part of the spectrum of T dwarfs, the actual appearance of T dwarfs to human visual perception is estimated to be not brown, but magenta. T-class brown dwarfs, such as WISE 0316+4307, have been detected more than 100 light-years from the Sun."
What was Isaac Newton's explanation for rectilinear propagation of light?,"Isaac Newton rejected the wave theory of light and proposed that light consists of corpuscles that are subject to a force acting parallel to the interface. In this model, the critical angle was the angle of incidence at which the normal velocity of the approaching corpuscle was just enough to reach the far side of the force field; at more oblique incidence, the corpuscle would be turned back.","Isaac Newton rejected the wave theory of light and proposed that light consists of corpuscles that are subject to a force acting perpendicular to the interface. In this model, the critical angle was the angle of incidence at which the normal velocity of the approaching corpuscle was just enough to reach the near side of the force field; at more oblique incidence, the corpuscle would be turned back.","Isaac Newton accepted the wave theory of light and proposed that light consists of transverse waves that are subject to a force acting perpendicular to the interface. In this model, the critical angle was the angle of incidence at which the normal velocity of the approaching wave was just enough to reach the far side of the force field; at more oblique incidence, the wave would be turned back.","Isaac Newton rejected the wave theory of light and proposed that light consists of corpuscles that are subject to a force acting perpendicular to the interface. In this model, the critical angle was the angle of incidence at which the normal velocity of the approaching corpuscle was just enough to reach the far side of the force field; at more oblique incidence, the corpuscle would be turned back.","Isaac Newton accepted the wave theory of light and proposed that light consists of longitudinal waves that are subject to a force acting perpendicular to the interface. In this model, the critical angle was the angle of incidence at which the normal velocity of the approaching wave was just enough to reach the far side of the force field; at more oblique incidence, the wave would be turned back.",D,"Isaac Newton rejected the wave explanation of rectilinear propagation, believing that if light consisted of waves, it would ""bend and spread every way"" into the shadows. His corpuscular theory of light explained rectilinear propagation more simply, and it accounted for the ordinary laws of refraction and reflection, including TIR, on the hypothesis that the corpuscles of light were subject to a force acting perpendicular to the interface. In this model, for dense-to-rare incidence, the force was an attraction back towards the denser medium, and the critical angle was the angle of incidence at which the normal velocity of the approaching corpuscle was just enough to reach the far side of the force field; at more oblique incidence, the corpuscle would be turned back. Newton gave what amounts to a formula for the critical angle, albeit in words: ""as the Sines are which measure the Refraction, so is the Sine of Incidence at which the total Reflexion begins, to the Radius of the Circle""."
What is the relationship between chemical potential and quarks/antiquarks?,"Chemical potential, represented by μ, is a measure of the imbalance between quarks and antiquarks in a system. Higher μ indicates a stronger bias favoring quarks over antiquarks.","Chemical potential, represented by μ, is a measure of the balance between quarks and antiquarks in a system. Higher μ indicates an equal number of quarks and antiquarks.","Chemical potential, represented by μ, is a measure of the imbalance between quarks and antiquarks in a system. Higher μ indicates a stronger bias favoring antiquarks over quarks.","Chemical potential, represented by μ, is a measure of the density of antiquarks in a system. Higher μ indicates a higher density of antiquarks.","Chemical potential, represented by μ, is a measure of the density of quarks in a system. Higher μ indicates a higher density of quarks.",A,"observed in experiments—quarks and antiquarks are always found in groups of three (baryons), or bound in quark–antiquark pairs (mesons). Likewise, there is no experimental evidence that there are any significant concentrations of antimatter in the observable universe."
What is the American Petroleum Institute (API) gravity?,API gravity is a measure of how heavy or light a petroleum liquid is compared to water. It is an inverse measure of a petroleum liquid's density relative to that of water and is graduated in degrees on a hydrometer instrument.,API gravity is a measure of the viscosity of a petroleum liquid. It is an inverse measure of a petroleum liquid's density relative to that of water and is graduated in degrees on a hydrometer instrument.,API gravity is a measure of the temperature at which a petroleum liquid freezes. It is an inverse measure of a petroleum liquid's density relative to that of water and is graduated in degrees on a hydrometer instrument.,API gravity is a measure of how much petroleum liquid is present in a given volume of water. It is an inverse measure of a petroleum liquid's density relative to that of water and is graduated in degrees on a hydrometer instrument.,API gravity is a measure of the acidity or alkalinity of a petroleum liquid. It is an inverse measure of a petroleum liquid's density relative to that of water and is graduated in degrees on a hydrometer instrument.,A,"The American Petroleum Institute gravity, or API gravity, is a measure of how heavy or light a petroleum liquid is compared to water: if its API gravity is greater than 10, it is lighter and floats on water; if less than 10, it is heavier and sinks.

API gravity is thus an inverse measure of a petroleum liquid's density relative to that of water (also known as specific gravity). It is used to compare densities of petroleum liquids. For example, if one petroleum liquid is less dense than another, it has a greater API gravity. Although API gravity is mathematically a dimensionless quantity (see the formula below), it is referred to as being in 'degrees'. API gravity is graduated in degrees on a hydrometer instrument. API gravity values of most petroleum liquids  fall between 10 and 70 degrees."
What are the two main factors that cause resistance in a metal?,"The amount of resistance in a metal is mainly caused by the temperature and the pressure applied to the metal. Higher temperatures cause bigger vibrations, and pressure causes the metal to become more compact, leading to more resistance.","The amount of resistance in a metal is mainly caused by the temperature and the purity of the metal. Higher temperatures cause bigger vibrations, and a mixture of different ions acts as an irregularity.","The amount of resistance in a metal is mainly caused by the temperature and the thickness of the metal. Higher temperatures cause bigger vibrations, and thicker metals have more irregularities, leading to more resistance.","The amount of resistance in a metal is mainly caused by the purity of the metal and the amount of pressure applied to the metal. A mixture of different ions acts as an irregularity, and pressure causes the metal to become more compact, leading to more resistance.","The amount of resistance in a metal is mainly caused by the purity of the metal and the thickness of the metal. A mixture of different ions acts as an irregularity, and thicker metals have more irregularities, leading to more resistance.",B,"Most metals have electrical resistance. In simpler models (non quantum mechanical models) this can be explained by replacing electrons and the crystal lattice by a wave-like structure. When the electron wave travels through the lattice, the waves interfere, which causes resistance. The more regular the lattice is, the less disturbance happens and thus the less resistance. The amount of resistance is thus mainly caused by two factors. First, it is caused by the temperature and thus amount of vibration of the crystal lattice. Higher temperatures cause bigger vibrations, which act as irregularities in the lattice. Second, the purity of the metal is relevant as a mixture of different ions is also an irregularity. The small decrease in conductivity on melting of pure metals is due to the loss of long range crystalline order. The short range order remains and strong correlation between positions of ions results in coherence between waves diffracted by adjacent ions."
What is the significance of the redshift-distance relationship in determining the expansion history of the universe?,"Observations of the redshift-distance relationship can be used to determine the expansion history of the universe and the matter and energy content, especially for galaxies whose light has been travelling to us for much shorter times.","Observations of the redshift-distance relationship can be used to determine the age of the universe and the matter and energy content, especially for nearby galaxies whose light has been travelling to us for much shorter times.","Observations of the redshift-distance relationship can be used to determine the expansion history of the universe and the matter and energy content, especially for nearby galaxies whose light has been travelling to us for much longer times.","Observations of the redshift-distance relationship can be used to determine the age of the universe and the matter and energy content, especially for distant galaxies whose light has been travelling to us for much shorter times.","Observations of the redshift-distance relationship can be used to determine the expansion history of the universe and the matter and energy content, especially for distant galaxies whose light has been travelling to us for much longer times.",E,"The Hubble law's linear relationship between distance and redshift assumes that the rate of expansion of the universe is constant. However, when the universe was much younger, the expansion rate, and thus the Hubble ""constant"", was larger than it is today. For more distant galaxies, then, whose light has been travelling to us for much longer times, the approximation of constant expansion rate fails, and the Hubble law becomes a non-linear integral relationship and dependent on the history of the expansion rate since the emission of the light from the galaxy in question. Observations of the redshift-distance relationship can be used, then, to determine the expansion history of the universe and thus the matter and energy content."
What is the Evans balance?,"The Evans balance is a system used to measure the change in weight of a sample when an electromagnet is turned on, which is proportional to the susceptibility.",The Evans balance is a system used to measure the dependence of the NMR frequency of a liquid sample on its shape or orientation to determine its susceptibility.,The Evans balance is a system used to measure the magnetic field distortion around a sample immersed in water inside an MR scanner to determine its susceptibility.,The Evans balance is a system used to measure the susceptibility of a sample by measuring the force change on a strong compact magnet upon insertion of the sample.,"The Evans balance is a system used to measure the magnetic susceptibility of most crystals, which is not a scalar quantity.",D,"An Evans balance, also known as a Johnson-Matthey balance (after the most prolific producer of the Evans balance) is a device for measuring magnetic susceptibility. Magnetic susceptibility is  related to the force experienced by a substance in a magnetic field. Various practical devices are available for the measurement of susceptibility, which differ in the shape of the magnetic field and the way the force is measured.

In the Gouy balance there is a homogeneous field in the central region between two (flat) poles of a permanent magnet, or an electromagnet. The sample, in the form of a powder in a cylindrical tube, is suspended in such a way the one end lies in the centre of the field and the other is effectively outside the magnetic field. The force is measured by an analytical balance

The Evans balance employs a similar sample configuration, but measures the force on the magnet.

Mechanism"
What is the definition of dimension in mathematics?,"The dimension of an object is the number of independent parameters or coordinates needed to define the position of a point constrained to be on the object, and is an extrinsic property of the object, dependent on the dimension of the space in which it is embedded.","The dimension of an object is the number of degrees of freedom of a point that moves on this object, and is an extrinsic property of the object, dependent on the dimension of the space in which it is embedded.","The dimension of an object is the number of independent parameters or coordinates needed to define the position of a point constrained to be on the object, and is an intrinsic property of the object, independent of the dimension of the space in which it is embedded.","The dimension of an object is the number of directions in which a point can move on the object, and is an extrinsic property of the object, dependent on the dimension of the space in which it is embedded.","The dimension of an object is the number of directions in which a point can move on the object, and is an intrinsic property of the object, independent of the dimension of the space in which it is embedded.",C,"The concept of dimension is not restricted to physical objects. s frequently occur in mathematics and the sciences. They may be parameter spaces or configuration spaces such as in Lagrangian or Hamiltonian mechanics; these are abstract spaces, independent of the physical space in which we live.

In mathematics 

In mathematics, the dimension of an object is, roughly speaking, the number of degrees of freedom of a point that moves on this object. In other words, the dimension is the number of independent parameters or coordinates that are needed for defining the position of a point that is constrained to be on the object. For example, the dimension of a point is zero; the dimension of a line is one, as a point can move on a line in only one direction (or its opposite); the dimension of a plane is two, etc."
What is accelerator-based light-ion fusion?,"Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions. This method is relatively easy to implement and can be done in an efficient manner, requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer. Fusion can be observed with as little as 10 kV between the electrodes.","Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce heavy-ion fusion reactions. This method is relatively difficult to implement and requires a complex system of vacuum tubes, electrodes, and transformers. Fusion can be observed with as little as 10 kV between the electrodes.","Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions. This method is relatively difficult to implement and requires a complex system of vacuum tubes, electrodes, and transformers. Fusion can be observed with as little as 100 kV between the electrodes.","Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce heavy-ion fusion reactions. This method is relatively easy to implement and can be done in an efficient manner, requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer. Fusion can be observed with as little as 100 kV between the electrodes.","Accelerator-based light-ion fusion is a technique that uses particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fission reactions. This method is relatively easy to implement and can be done in an efficient manner, requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer. Fission can be observed with as little as 10 kV between the electrodes.",A,"Accelerator-based fusion is not practical because the reactor cross section is tiny; most of the particles in the accelerator will scatter off the fuel, not fuse with it. These scatterings cause the particles to lose energy to the point where they can no longer undergo fusion. The energy put into these particles is thus lost, and it is easy to demonstrate this is much more energy than the resulting fusion reactions can release."
What is the interstellar medium (ISM)?,"The matter and radiation that exist in the space between the star systems in a galaxy, including gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills interstellar space and blends smoothly into the surrounding intergalactic space.","The matter and radiation that exist in the space between stars in a galaxy, including gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills interstellar space and blends smoothly into the surrounding interplanetary space.","The matter and radiation that exist in the space between galaxies, including gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills intergalactic space and blends smoothly into the surrounding interstellar space.","The matter and radiation that exist in the space between planets in a solar system, including gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills interplanetary space and blends smoothly into the surrounding interstellar space.","The matter and radiation that exist within a star, including gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills the star and blends smoothly into the surrounding interstellar space.",A,"In astronomy, the interstellar medium (ISM) is the matter and radiation that exist in the space between the star systems in a galaxy. This matter includes gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills interstellar space and blends smoothly into the surrounding intergalactic space. The energy that occupies the same volume, in the form of electromagnetic radiation, is the interstellar radiation field.

The interstellar medium is composed of multiple phases distinguished by whether matter is ionic, atomic, or molecular, and the temperature and density of the matter. The interstellar medium is composed, primarily, of hydrogen, followed by helium with trace amounts of carbon, oxygen, and nitrogen comparatively to hydrogen. The thermal pressures of these phases are in rough equilibrium with one another. Magnetic fields and turbulent motions also provide pressure in the ISM, and are typically more important, dynamically, than the thermal pressure is."
What is the significance of the change in slope of the pinched hysteresis curves in ReRAM and other forms of two-terminal resistance memory?,"The change in slope of the pinched hysteresis curves demonstrates switching between different resistance states, which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory.","The change in slope of the pinched hysteresis curves indicates the presence of a Type-II non-crossing curve, which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory.","The change in slope of the pinched hysteresis curves demonstrates the presence of a memristor, which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory.","The change in slope of the pinched hysteresis curves demonstrates the presence of a memristive network, which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory.","The change in slope of the pinched hysteresis curves indicates the presence of a linear resistor, which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory.",A,"Pinched hysteresis

One of the resulting properties of memristors and memristive systems is the existence of a pinched hysteresis effect. For a current-controlled memristive system, the input u(t) is the current i(t), the output y(t) is the voltage v(t), and the slope of the curve represents the electrical resistance. The change in slope of the pinched hysteresis curves demonstrates switching between different resistance states which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory. At high frequencies, memristive theory predicts the pinched hysteresis effect will degenerate, resulting in a straight line representative of a linear resistor. It has been proven that some types of non-crossing pinched hysteresis curves (denoted Type-II) cannot be described by memristors.

Extended systems"
What is geometric quantization in mathematical physics?,Geometric quantization is a mathematical approach to defining a classical theory corresponding to a given quantum theory. It attempts to carry out quantization in such a way that certain analogies between the quantum theory and the classical theory are lost.,Geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization in such a way that certain analogies between the classical theory and the quantum theory are lost.,Geometric quantization is a mathematical approach to defining a classical theory corresponding to a given quantum theory. It attempts to carry out quantization in such a way that certain analogies between the quantum theory and the classical theory remain manifest.,Geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization in such a way that certain analogies between the classical theory and the quantum theory are not important.,Geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization in such a way that certain analogies between the classical theory and the quantum theory remain manifest.,E,"Geometric quantization

In contrast to the theory of deformation quantization described above, geometric quantization seeks to construct an actual Hilbert space and operators on it. Starting with a symplectic manifold , one first constructs a prequantum Hilbert space consisting of the space of square-integrable sections of an appropriate line bundle over . On this space, one can map all classical observables to operators on the prequantum Hilbert space, with the commutator corresponding exactly to the Poisson bracket. The prequantum Hilbert space, however, is clearly too big to describe the quantization of ."
What is the definition of an improper rotation?,"An improper rotation is the combination of a rotation about an axis and reflection in a plane perpendicular to that axis, or inversion about a point on the axis. The order of the rotation and reflection does not matter, and the symmetry elements for improper rotation are the rotation axis, and either the mirror plane, the inversion point, or both, and a third plane.","An improper rotation is the combination of a rotation about an axis and reflection in a plane perpendicular to that axis, or inversion about a point on the axis. The order of the rotation and reflection does not matter, and the symmetry elements for improper rotation are the rotation axis, and either the mirror plane, the inversion point, or both.","An improper rotation is the combination of a rotation about an axis and reflection in a plane parallel to that axis, or inversion about a point on the axis. The order of the rotation and reflection does not matter, and the symmetry elements for improper rotation are the rotation axis, and either the mirror plane, the inversion point, or neither.","An improper rotation is the combination of a rotation about an axis and reflection in a plane perpendicular to that axis, or inversion about a point on the axis. The order of the rotation and reflection does not matter, and the symmetry elements for improper rotation are the rotation axis, and either the mirror plane, the inversion point, or neither.","An improper rotation is the combination of a rotation about an axis and reflection in a plane parallel to that axis, or inversion about a point on the axis. The order of the rotation and reflection does not matter, and the symmetry elements for improper rotation are the rotation axis, and either the mirror plane, the inversion point, or both.",B,"Definitions and representations

In Euclidean geometry

A motion of a Euclidean space is the same as its isometry: it leaves the distance between any two points unchanged after the transformation. But a (proper) rotation also has to preserve the orientation structure. The ""improper rotation"" term refers to isometries that reverse (flip) the orientation. In the language of group theory the distinction is expressed as direct vs indirect isometries in the Euclidean group, where the former comprise the identity component. Any direct Euclidean motion can be represented as a composition of a rotation about the fixed point and a translation."
"What is power density in the context of energy systems, and how does it differ between renewable and non-renewable energy sources?","Power density is a measure of the amount of power obtained per unit of Earth surface area used by a specific energy system, including all supporting infrastructure, manufacturing, mining of fuel (if applicable) and decommissioning. Both renewable and non-renewable energy sources have similar power density, which means that the same amount of power can be obtained from power plants occupying similar areas.","Power density is a measure of the amount of power obtained per unit of Earth surface area used by a specific energy system, including all supporting infrastructure, manufacturing, mining of fuel (if applicable) and decommissioning. Fossil fuels and nuclear power have high power density, which means large power can be drawn from power plants occupying relatively small areas. Renewable energy sources have power density at least three orders of magnitude smaller and, for the same energy output, they need to occupy accordingly larger areas.","Power density is a measure of the amount of power obtained per unit of Earth surface area used by a specific energy system, including all supporting infrastructure, manufacturing, mining of fuel (if applicable) and decommissioning. Renewable energy sources have higher power density than non-renewable energy sources, which means that they can produce more power from power plants occupying smaller areas.","Power density is a measure of the amount of power obtained per unit of Earth surface area used by a specific energy system, including all supporting infrastructure, manufacturing, mining of fuel (if applicable) and decommissioning. Fossil fuels and nuclear power have low power density, which means that they need to occupy larger areas to produce the same amount of power as renewable energy sources.","Power density is a measure of the amount of power obtained per unit of Earth surface area used by a specific energy system, including all supporting infrastructure, manufacturing, mining of fuel (if applicable) and decommissioning. Both renewable and non-renewable energy sources have low power density, which means that they need to occupy larger areas to produce the same amount of power.",B,"Power density is the amount of power (time rate of energy transfer) per unit volume.

In energy transformers including batteries, fuel cells, motors, power supply units etc., power density refers to a volume, where it is often called volume power density, expressed as W/m3.

In reciprocating internal combustion engines, power density (power per swept volume or brake horsepower per cubic centimeter) is an important metric, based on the internal capacity of the engine, not its external size.

Examples

See also
Surface power density, energy per unit of area
Energy density, energy per unit volume
 Specific energy, energy per unit mass
Power-to-weight ratio/specific power, power per unit mass
Specific absorption rate (SAR)

References

Power (physics)"
What is Modified Newtonian Dynamics (MOND)?,MOND is a theory that explains the behavior of light in the presence of strong gravitational fields. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.,MOND is a hypothesis that proposes a modification of Einstein's theory of general relativity to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.,MOND is a hypothesis that proposes a modification of Newton's law of universal gravitation to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.,MOND is a hypothesis that proposes a modification of Coulomb's law to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.,MOND is a theory that explains the behavior of subatomic particles in the presence of strong magnetic fields. It is an alternative to the hypothesis of dark energy in terms of explaining why subatomic particles do not appear to obey the currently understood laws of physics.,C,"MOND 

Modified Newtonian Dynamics (MOND) is a relatively modern proposal to explain the galaxy rotation problem based on a variation of Newton's Second Law of Dynamics at low accelerations. This would produce a large-scale variation of Newton's universal theory of gravity. A modification of Newton's theory would also imply a modification of general relativistic cosmology in as much as Newtonian cosmology is the limit of Friedman cosmology. While almost all astrophysicists today reject MOND in favor of dark matter, a small number of researchers continue to enhance it, recently incorporating Brans–Dicke theories into treatments that attempt to account for cosmological observations."
What is linear frame dragging?,Linear frame dragging is the effect of the general principle of relativity applied to the mass of a body when other masses are placed nearby. It is a tiny effect that is difficult to confirm experimentally and often omitted from articles on frame-dragging.,"Linear frame dragging is the effect of the general principle of relativity applied to rotational momentum, which is a large effect that is easily confirmed experimentally and often discussed in articles on frame-dragging.","Linear frame dragging is the effect of the general principle of relativity applied to rotational momentum, which is similarly inevitable to the linear effect. It is a tiny effect that is difficult to confirm experimentally and often omitted from articles on frame-dragging.","Linear frame dragging is the effect of the general principle of relativity applied to linear momentum, which is similarly inevitable to the rotational effect. It is a tiny effect that is difficult to confirm experimentally and often omitted from articles on frame-dragging.","Linear frame dragging is the effect of the general principle of relativity applied to linear momentum, which is a large effect that is easily confirmed experimentally and often discussed in articles on frame-dragging.",D,"Linear frame dragging is the similarly inevitable result of the general principle of relativity, applied to linear momentum. Although it arguably has equal theoretical legitimacy to the ""rotational"" effect, the difficulty of obtaining an experimental verification of the effect means that it receives much less discussion and is often omitted from articles on frame-dragging (but see Einstein, 1921).

Static mass increase is a third effect noted by Einstein in the same paper. The effect is an increase in inertia of a body when other masses are placed nearby. While not strictly a frame dragging effect (the term frame dragging is not used by Einstein), it is demonstrated by Einstein that it derives from the same equation of general relativity. It is also a tiny effect that is difficult to confirm experimentally."
What is explicit symmetry breaking in theoretical physics?,"Explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion that do not respect the symmetry, always in situations where these symmetry-breaking terms are large, so that the symmetry is not respected by the theory.","Explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion that do not respect the symmetry, usually in situations where these symmetry-breaking terms are small, so that the symmetry is approximately respected by the theory.","Explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion that respect the symmetry, always in situations where these symmetry-breaking terms are small, so that the symmetry is approximately respected by the theory.","Explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion that respect the symmetry, always in situations where these symmetry-breaking terms are large, so that the symmetry is not respected by the theory.","Explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion that respect the symmetry, usually in situations where these symmetry-breaking terms are large, so that the symmetry is not respected by the theory.",B,"In the absence of explicit breaking, spontaneous symmetry breaking would engender massless Nambu–Goldstone bosons for the  exact spontaneously broken chiral symmetries. The chiral symmetries discussed, however, are only approximate symmetries in nature, given their small explicit breaking.

The explicit symmetry breaking occurs at a smaller energy scale.  The properties of these pseudo-Goldstone bosons can normally be calculated using chiral perturbation theory, expanding around the exactly symmetric theory in terms of the explicit symmetry-breaking parameters. In particular, the computed mass must be small,

Three-quark model

For three light quarks, the up quark, down quark, and strange quark, the flavor-chiral symmetries extending those discussed above also decompose, to Gell-Mann's"
What is the role of the Higgs boson in the Standard Model?,The Higgs boson is responsible for giving mass to the photon and gluon in the Standard Model.,The Higgs boson has no role in the Standard Model.,The Higgs boson is responsible for giving mass to all the elementary particles in the Standard Model.,"The Higgs boson is responsible for giving mass to all the elementary particles, except the photon and gluon, in the Standard Model.",The Higgs boson is responsible for giving mass to all the composite particles in the Standard Model.,D,"Background

The Higgs boson

The Higgs boson, sometimes called the Higgs particle, is an elementary particle in the Standard Model of particle physics produced by the quantum excitation of the Higgs field, one of the fields in particle physics theory. In the Standard Model, the Higgs particle is a massive scalar boson with zero spin, even (positive)  parity, no electric charge, and no colour charge, that couples to (interacts with) mass. It is also very unstable, decaying into other particles almost immediately.

Experimental requirements
Like other massive particles (e.g. the top quark and W and Z bosons), Higgs bosons decay to other particles almost immediately, long before they can be observed directly. However, the Standard Model precisely predicts the possible modes of decay and their probabilities. This allows the creation and decay of a Higgs boson to be shown by careful examination of the decay products of collisions."
What is Lorentz symmetry or Lorentz invariance in relativistic physics?,Lorentz symmetry or Lorentz invariance is a property of the underlying spacetime manifold that describes the feature of nature that says experimental results are independent of the orientation or the boost velocity of the laboratory through space.,"Lorentz symmetry or Lorentz invariance is a measure of the curvature of spacetime caused by the presence of massive objects, which describes the feature of nature that says experimental results are independent of the orientation or the boost velocity of the laboratory through space.","Lorentz symmetry or Lorentz invariance is a physical quantity that transforms under a given representation of the Lorentz group, built out of scalars, four-vectors, four-tensors, and spinors.","Lorentz symmetry or Lorentz invariance is a measure of the time dilation and length contraction effects predicted by special relativity, which states that the laws of physics stay the same for all observers that are moving with respect to one another within an inertial frame.",Lorentz symmetry or Lorentz invariance is an equivalence of observation or observational symmetry due to special relativity implying that the laws of physics stay the same for all observers that are moving with respect to one another within an inertial frame.,E,"Relativistic

Lorentz invariance
Newtonian physics assumes that absolute time and space exist outside of any observer; this gives rise to Galilean invariance. It also results in a prediction that the speed of light can vary from one reference frame to another. This is contrary to observation. In the special theory of relativity, Einstein keeps the postulate that the equations of motion do not depend on the reference frame, but assumes that the speed of light  is invariant. As a result, position and time in two reference frames are related by the Lorentz transformation instead of the Galilean transformation.

Consider, for example, one reference frame moving relative to another at velocity  in the  direction. The Galilean transformation gives the coordinates of the moving frame as

while the Lorentz transformation gives

where  is the Lorentz factor:"
What is the significance of Baryon Acoustic Oscillations (BAOs) in the study of the universe?,"BAOs establish a preferred length scale for baryons, which can be used to detect a subtle preference for pairs of galaxies to be separated by 147 Mpc, compared to those separated by 130-160 Mpc.",BAOs help to determine the average temperature of the Universe by measuring the temperature of the cosmic microwave background radiation.,BAOs provide a way to measure the time it takes for a signal to reach its destination compared to the time it takes for background noise to dissipate.,BAOs can be used to make a two-dimensional map of the galaxy distribution in the Universe.,BAOs are used to measure the speed of light in the Universe.,A,"In cosmology, baryon acoustic oscillations (BAO) are fluctuations in the density of the visible baryonic matter (normal matter) of the universe, caused by acoustic density waves in the primordial plasma of the early universe.  In the same way that supernovae provide a ""standard candle"" for astronomical observations, BAO matter clustering provides a ""standard ruler"" for length scale in cosmology."
What can be inferred about the electronic entropy of insulators and metals based on their densities of states at the Fermi level?,"Insulators and metals have zero density of states at the Fermi level, and therefore, their density of states-based electronic entropy is essentially zero.","Insulators have zero density of states at the Fermi level, and therefore, their density of states-based electronic entropy is essentially zero. Metals have non-zero density of states at the Fermi level, and thus, their electronic entropy should be proportional to the temperature and density of states at the Fermi level.","Insulators have non-zero density of states at the Fermi level, and therefore, their density of states-based electronic entropy is proportional to the temperature and density of states at the Fermi level. Metals have zero density of states at the Fermi level, and thus, their electronic entropy is essentially zero.","Insulators and metals have varying densities of states at the Fermi level, and thus, their electronic entropy may or may not be proportional to the temperature and density of states at the Fermi level.","Insulators and metals have non-zero density of states at the Fermi level, and thus, their electronic entropy should be proportional to the temperature and density of states at the Fermi level.",B,"where  is the density of states (number of levels per unit energy) at the Fermi level. Several other approximations can be made, but they all indicate that the electronic entropy should, to first order, be proportional to the temperature and the density of states at the Fermi level. As the density of states at the Fermi level varies widely between systems, this approximation is a reasonable heuristic for inferring when it may be necessary to include electronic entropy in the thermodynamic description of a system; only systems with large densities of states at the Fermi level should exhibit non-negligible electronic entropy (where large may be approximately defined as ).

Application to different materials classes 
Insulators have zero density of states at the Fermi level due to their band gaps. Thus, the density of states-based electronic entropy is essentially zero in these systems."
What are permutation-inversion groups?,Permutation-inversion groups are groups of symmetry operations that are energetically feasible inversions of identical nuclei or rotation with respect to the center of mass.,"Permutation-inversion groups are groups of symmetry operations that are energetically feasible inversions of identical nuclei or rotation with respect to the center of mass, or a combination of both.",Permutation-inversion groups are groups of symmetry operations that are energetically feasible rotations of the entire molecule about the C3 axis.,Permutation-inversion groups are groups of symmetry operations that are energetically feasible inversions of the entire molecule about the C3 axis.,"Permutation-inversion groups are groups of symmetry operations that are energetically feasible permutations of identical nuclei or inversion with respect to the center of mass, or a combination of both.",E,"In mathematics, a permutation group is a group G whose elements are permutations of a given set M and whose group operation is the composition of permutations in G (which are thought of as bijective functions from the set M to itself). The group of all permutations of a set M is the symmetric group of M, often written as Sym(M). The term permutation group thus means a subgroup of the symmetric group. If  then Sym(M) is usually denoted by Sn, and may be called the symmetric group on n letters.

By Cayley's theorem, every group is isomorphic to some permutation group.

The way in which the elements of a permutation group permute the elements of the set is called its group action. Group actions have applications in the study of symmetries, combinatorics and many other branches of mathematics, physics and chemistry.

Basic properties and terminology"
What is the relationship between dielectric loss and the transparency of a material?,"Dielectric loss in a material can cause refraction, which can decrease the material's transparency at higher frequencies.","Dielectric loss in a material can cause absorption, which can reduce the material's transparency at higher frequencies.","Dielectric loss in a material can cause reflection, which can increase the material's transparency at higher frequencies.",Dielectric loss in a material has no effect on the material's transparency at any frequency.,"Dielectric loss in a material can cause scattering, which can increase the material's transparency at higher frequencies.",B,"Dielectric loss and non-zero DC conductivity in materials cause absorption. Good dielectric materials such as glass have extremely low DC conductivity, and at low frequencies the dielectric loss is also negligible, resulting in almost no absorption. However, at higher frequencies (such as visible light), dielectric loss may increase absorption significantly, reducing the material's transparency to these frequencies.

The real, n, and imaginary, κ, parts of the complex refractive index are related through the Kramers–Kronig relations. In 1986 A.R. Forouhi and I. Bloomer deduced an equation describing κ as a function of photon energy, E, applicable to amorphous materials. Forouhi and Bloomer then applied the Kramers–Kronig relation to derive the corresponding equation for n as a function of E. The same formalism was applied to crystalline materials by Forouhi and Bloomer in 1988."
What is the purpose of measuring the Larmor precession fields at about 100 microtesla with highly sensitive superconducting quantum interference devices (SQUIDs) in ultra-low field MRI?,To measure the magnetization in the same direction as the static magnetic field in T1 relaxation.,"To create a T1-weighted image that is useful for assessing the cerebral cortex, identifying fatty tissue, and characterizing focal liver lesions.","To obtain sufficient signal quality in the microtesla-to-millitesla range, where MRI has been demonstrated recently.",To measure the independent relaxation processes of T1 and T2 in each tissue after excitation.,To change the repetition time (TR) and obtain morphological information in post-contrast imaging.,C,"A SQUID (superconducting quantum interference device) is a very sensitive magnetometer used to measure extremely subtle magnetic fields, based on superconducting loops containing Josephson junctions.

SQUIDs are sensitive enough to measure fields as low as 5×10−14 T with a few days of averaged measurements. Their noise levels are as low as 3 fT·Hz−. For comparison, a typical refrigerator magnet produces 0.01 tesla (10−2 T), and some processes in animals produce very small magnetic fields between 10−9 T and 10−6 T. SERF atomic magnetometers, invented in the early 2000s are potentially more sensitive and do not require cryogenic refrigeration but are orders of magnitude larger in size (~1 cm3) and must be operated in a near-zero magnetic field.

History and design"
What is the difference between illuminance and luminance?,"Illuminance is the amount of light absorbed by a surface per unit area, while luminance is the amount of light reflected by a surface per unit area.","Illuminance is the amount of light falling on a surface per unit area, while luminance is the amount of light emitted by a source per unit area.","Illuminance is the amount of light concentrated into a smaller area, while luminance is the amount of light filling a larger solid angle.","Illuminance is the amount of light emitted by a source per unit area, while luminance is the amount of light falling on a surface per unit area.","Illuminance is the amount of light reflected by a surface per unit area, while luminance is the amount of light absorbed by a surface per unit area.",B,"Luminance is invariant in geometric optics. This means that for an ideal optical system, the luminance at the output is the same as the input luminance. 

For real, passive optical systems, the output luminance is at most equal to the input. As an example, if one uses a lens to form an image that is smaller than the source object, the luminous power is concentrated into a smaller area, meaning that the illuminance is higher at the image. The light at the image plane, however, fills a larger solid angle so the luminance comes out to be the same assuming there is no loss at the lens. The image can never be ""brighter"" than the source.

Health effects

Retinal damage can occur when the eye is exposed to high luminance. Damage can occur because of local heating of the retina. Photochemical effects can also cause damage, especially at short wavelengths."
What is a magnetic monopole in particle physics?,A hypothetical elementary particle that is an isolated electric charge with both positive and negative poles.,A hypothetical elementary particle that is an isolated magnet with no magnetic poles.,"A hypothetical elementary particle that is an isolated electric charge with only one electric pole, either a positive pole or a negative pole.",A hypothetical elementary particle that is an isolated magnet with both north and south poles.,"A hypothetical elementary particle that is an isolated magnet with only one magnetic pole, either a north pole or a south pole.",E,"In physics, a dyon is a hypothetical particle in 4-dimensional theories with both electric and magnetic charges. A dyon with a zero electric charge is usually referred to as a magnetic monopole. Many grand unified theories predict the existence of both magnetic monopoles and dyons.

Dyons were first proposed by Julian Schwinger in 1969 as a phenomenological alternative to
quarks.  He extended the Dirac quantization condition to the dyon and used the model to predict the existence of a particle with the properties of the J/ψ meson prior to its discovery in 1974."
What is the difference between redshift due to the expansion of the universe and Doppler redshift?,"Redshift due to the expansion of the universe depends on the rate of change of a(t) at the times of emission or absorption, while Doppler redshift depends on the increase of a(t) in the whole period from emission to absorption.","Redshift due to the expansion of the universe depends on the local velocity of the object emitting the light, while Doppler redshift depends on the cosmological model chosen to describe the expansion of the universe.",There is no difference between redshift due to the expansion of the universe and Doppler redshift.,"Redshift due to the expansion of the universe depends on the cosmological model chosen to describe the expansion of the universe, while Doppler redshift depends on the local velocity of the object emitting the light.","Redshift due to the expansion of the universe depends on the increase of a(t) in the whole period from emission to absorption, while Doppler redshift depends on the rate of change of a(t) at the times of emission or absorption.",D,"The redshifts of galaxies include both a component related to recessional velocity from expansion of the universe, and a component related to peculiar motion (Doppler shift). The redshift due to expansion of the universe depends upon the recessional velocity in a fashion determined by the cosmological model chosen to describe the expansion of the universe, which is very different from how Doppler redshift depends upon local velocity. Describing the cosmological expansion origin of redshift, cosmologist Edward Robert Harrison said, ""Light leaves a galaxy, which is stationary in its local region of space, and is eventually received by observers who are stationary in their own local region of space. Between the galaxy and the observer, light travels through vast regions of expanding space. As a result, all wavelengths of the light are stretched by the expansion of space. It is as simple as that..."" Steven Weinberg clarified, ""The increase of wavelength from emission to absorption of"
What is the relationship between Coordinated Universal Time (UTC) and Universal Time (UT1)?,UTC and Universal Time (UT1) are identical time scales that are used interchangeably in science and engineering.,"UTC is a time scale that is completely independent of Universal Time (UT1). UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the ""leap second"".","UTC is an atomic time scale designed to approximate Universal Time (UT1), but it differs from UT1 by a non-integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the ""leap second"".","UTC is an atomic time scale designed to approximate Universal Time (UT1), but it differs from UT1 by an integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the ""leap second"".",UTC is a time scale that is based on the irregularities in Earth's rotation and is completely independent of Universal Time (UT1).,D,"Universal Time (UT) is a time standard based on Earth's rotation. There are several versions of Universal Time, which differ by up to a few seconds. The most commonly used are Coordinated Universal Time (UTC) and UT1 (see ). All of these versions of UT, except for UTC, are based on Earth's rotation relative to distant celestial objects (stars and quasars), but with a scaling factor and other adjustments to make them closer to solar time. UTC is based on International Atomic Time, with leap seconds added to keep it within 0.9 second of UT1."
What is the reason for heating metals to a temperature just above the upper critical temperature?,"To prevent the grains of solution from growing too large, which decreases mechanical properties such as toughness, shear strength, and tensile strength.","To increase the size of the grains of solution, which enhances mechanical properties such as toughness, shear strength, and tensile strength.","To prevent the grains of solution from growing too large, which enhances mechanical properties such as toughness, shear strength, and tensile strength.","To prevent the grains of solution from growing too small, which enhances mechanical properties such as toughness, shear strength, and tensile strength.","To increase the size of the grains of solution, which decreases mechanical properties such as toughness, shear strength, and tensile strength.",C,"The upper critical solution temperature (UCST) or upper consolute temperature is the critical temperature above which the components of a mixture are miscible in all proportions. The word upper indicates that the UCST is an upper bound to a temperature range of partial miscibility, or miscibility for certain compositions only. For example, hexane-nitrobenzene mixtures have a UCST of , so that these two substances are miscible in all proportions above  but not at lower temperatures. Examples at higher temperatures are the aniline-water system at  (at pressures high enough for liquid water to exist at that temperature), and the lead-zinc system at  (a temperature where both metals are liquid).

A solid state example is the palladium-hydrogen system which has a solid solution phase (H2 in Pd) in equilibrium with a hydride phase (PdHn) below the UCST at 300 °C. Above this temperature there is a single solid solution phase."
What is the cause of the observed change in the periods of moons orbiting a distant planet when measured from Earth?,"The difference in the size of the planet's moons when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun.","The difference in the speed of light when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun.","The difference in distance travelled by light from the planet (or its moon) to Earth when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun.","The difference in the atmospheric conditions of the planet when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun.","The difference in the gravitational pull of the planet on its moons when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun.",C,"The gravitational attraction between Earth and the Moon causes tides on Earth. The same effect on the Moon has led to its tidal locking: its rotation period is the same as the time it takes to orbit Earth. As a result, it always presents the same face to the planet. As the Moon orbits Earth, different parts of its face are illuminated by the Sun, leading to the lunar phases. Due to their tidal interaction, the Moon recedes from Earth at the rate of approximately . Over millions of years, these tiny modifications—and the lengthening of Earth's day by about 23 µs/yr—add up to significant changes. During the Ediacaran period, for example, (approximately ) there were 400±7 days in a year, with each day lasting 21.9±0.4 hours."
What is the origin of the radio emission observed from supernova remnants?,The radio emission from supernova remnants originates from the rebound of gas falling inward during the supernova explosion. This emission is a form of non-thermal emission called synchrotron emission.,The radio emission from supernova remnants originates from high-velocity electrons oscillating within magnetic fields. This emission is a form of non-thermal emission called synchrotron emission.,The radio emission from supernova remnants originates from the fusion of hydrogen and helium in the core of the star. This emission is a form of non-thermal emission called synchrotron emission.,The radio emission from supernova remnants originates from the expansion of the shell of gas during the supernova explosion. This emission is a form of thermal emission called synchrotron emission.,The radio emission from supernova remnants originates from the ionized gas present in the remnants. This emission is a form of thermal emission called synchrotron emission.,B,"Supernova remnants

A supernova occurs when a high-mass star reaches the end of its life.  When nuclear fusion in the core of the star stops, the star collapses.  The gas falling inward either rebounds or gets so strongly heated that it expands outwards from the core, thus causing the star to explode. The expanding shell of gas forms a supernova remnant, a special diffuse nebula. Although much of the optical and X-ray emission from supernova remnants originates from ionized gas, a great amount of the radio emission is a form of non-thermal emission called synchrotron emission. This emission originates from high-velocity electrons oscillating within magnetic fields.

Notable examples

Ant Nebula
Barnard's Loop
Boomerang Nebula
Cat's Eye Nebula
Crab Nebula
Eagle Nebula
Eskimo Nebula
Carina Nebula
Fox Fur Nebula
Helix Nebula
Horsehead Nebula
Engraved Hourglass Nebula
Lagoon Nebula
Orion Nebula
Pelican Nebula
Red Square Nebula
Ring Nebula
Rosette Nebula
Tarantula Nebula"
What is the relationship between the Hamiltonians and eigenstates in supersymmetric quantum mechanics?,"For every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with the same energy.","For every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with a higher energy.","For every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with a different spin.","For every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with a different energy.","For every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with a lower energy.",A,"SUSY quantum mechanics involves pairs of Hamiltonians which share a particular mathematical relationship, which are called partner Hamiltonians. (The potential energy terms which occur in the Hamiltonians are then known as partner potentials.) An introductory theorem shows that for every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with the same energy. This fact can be exploited to deduce many properties of the eigenstate spectrum. It is analogous to the original description of SUSY, which referred to bosons and fermions. We can imagine a ""bosonic Hamiltonian"", whose eigenstates are the various bosons of our theory. The SUSY partner of this Hamiltonian would be ""fermionic"", and its eigenstates would be the theory's fermions. Each boson would have a fermionic partner of equal energy.

In finance 

In 2021, supersymmetric quantum mechanics was applied to option pricing and the analysis of markets in finance, and to financial networks."
What is the proposed name for the field that is responsible for cosmic inflation and the metric expansion of space?,Inflaton,Quanta,Scalar,Metric,Conformal cyclic cosmology,A,"No field responsible for cosmic inflation has been discovered. However such a field, if found in the future, would be scalar. The first similar scalar field proven to exist was only discovered in 2012–2013 and is still being researched. So it is not seen as problematic that a field responsible for cosmic inflation and the metric expansion of space has not yet been discovered.

The proposed field and its quanta (the subatomic particles related to it) have been named inflaton. If this field did not exist, scientists would have to propose a different explanation for all the observations that strongly suggest a metric expansion of space has occurred, and is still occurring much more slowly today.

Overview of metrics and comoving coordinates
 
To understand the metric expansion of the universe, it is helpful to discuss briefly what a metric is, and how metric expansion works."
Which of the following statements accurately describes the characteristics of gravitational waves?,"Gravitational waves have an amplitude denoted by h, which represents the size of the wave. The amplitude varies with time according to Newton's quadrupole formula. Gravitational waves also have a frequency denoted by f, which is the frequency of the wave's oscillation, and a wavelength denoted by λ, which is the distance between points of minimum stretch or squeeze.","Gravitational waves have an amplitude denoted by λ, which represents the distance between points of maximum stretch or squeeze. The amplitude varies with time according to Einstein's quadrupole formula. Gravitational waves also have a frequency denoted by h, which is the size of the wave, and a wavelength denoted by f, which is the frequency of the wave's oscillation.","Gravitational waves have an amplitude denoted by h, which represents the size of the wave. The amplitude varies with time according to Einstein's quadrupole formula. Gravitational waves also have a frequency denoted by f, which is the frequency of the wave's oscillation, and a wavelength denoted by λ, which is the distance between points of maximum stretch or squeeze.","Gravitational waves have an amplitude denoted by f, which represents the frequency of the wave's oscillation. The amplitude varies with time according to Einstein's quadrupole formula. Gravitational waves also have a frequency denoted by h, which is the size of the wave, and a wavelength denoted by λ, which is the distance between points of maximum stretch or squeeze.","Gravitational waves have an amplitude denoted by f, which represents the frequency of the wave's oscillation. The amplitude varies with time according to Newton's quadrupole formula. Gravitational waves also have a frequency denoted by h, which is the size of the wave, and a wavelength denoted by λ, which is the distance between points of minimum stretch or squeeze.",C,"manner.  If the wave happens to be a polarized gravitational plane wave, he will see circular images alternately squeezed horizontally while expanded vertically, and squeezed vertically while expanded horizontally.  This directly exhibits the characteristic effect of a gravitational wave in general relativity on light."
What is the difference between the coevolution of myrmecophytes and the mutualistic symbiosis of mycorrhiza?,"Myrmecophytes coevolve with ants, providing them with a home and sometimes food, while the ants defend the plant from herbivores and competing plants. On the other hand, mycorrhiza is a mutualistic symbiosis between plants and fungi, where the fungi help the plants gain water and mineral nutrients from the soil, while the plant gives the fungi carbohydrates manufactured in photosynthesis.","Myrmecophytes coevolve with ants, providing them with food, while the ants defend the plant from herbivores and competing plants. On the other hand, mycorrhiza is a mutualistic symbiosis between plants and fungi, where the fungi help the plants gain water and mineral nutrients from the soil, while the plant gives the fungi water and mineral nutrients.","Myrmecophytes coevolve with butterflies, providing them with a home and sometimes food, while the butterflies defend the plant from herbivores and competing plants. On the other hand, mycorrhiza is a mutualistic symbiosis between plants and birds, where the birds help the plants gain water and mineral nutrients from the soil, while the plant gives the birds carbohydrates manufactured in photosynthesis.","Myrmecophytes coevolve with birds, providing them with a home and sometimes food, while the birds defend the plant from herbivores and competing plants. On the other hand, mycorrhiza is a mutualistic symbiosis between plants and bacteria, where the bacteria help the plants gain water and mineral nutrients from the soil, while the plant gives the bacteria carbohydrates manufactured in photosynthesis.","Myrmecophytes coevolve with bees, providing them with a home and sometimes food, while the bees defend the plant from herbivores and competing plants. On the other hand, mycorrhiza is a mutualistic symbiosis between plants and insects, where the insects help the plants gain water and mineral nutrients from the soil, while the plant gives the insects carbohydrates manufactured in photosynthesis.",A,"A mycorrhiza (from Greek μύκης , ""fungus"", and ῥίζα , ""root""; pl. mycorrhizae, mycorrhiza or mycorrhizas) is a mutual symbiotic association between a fungus and a plant. The term mycorrhiza refers to the role of the fungus in the plant's rhizosphere, its root system. Mycorrhizae play important roles in plant nutrition, soil biology, and soil chemistry.

In a mycorrhizal association, the fungus colonizes the host plant's root tissues, either intracellularly as in arbuscular mycorrhizal fungi (AMF or AM), or extracellularly as in ectomycorrhizal fungi. The association is sometimes mutualistic. In particular species or in particular circumstances, mycorrhizae may have a parasitic association with host plants."
What is the Kelvin-Helmholtz instability and how does it affect Earth's magnetosphere?,"The Kelvin-Helmholtz instability is a phenomenon that occurs when large swirls of plasma travel along the edge of the magnetosphere at a different velocity from the magnetosphere, causing the plasma to slip past. This results in magnetic reconnection, and as the magnetic field lines break and reconnect, solar wind particles are able to enter the magnetosphere.",The Kelvin-Helmholtz instability is a phenomenon that occurs when the magnetosphere is compared to a sieve because it allows solar wind particles to enter.,"The Kelvin-Helmholtz instability is a phenomenon that occurs when Earth's bow shock is about 17 kilometers (11 mi) thick and located about 90,000 kilometers (56,000 mi) from Earth.","The Kelvin-Helmholtz instability is a phenomenon that occurs when the magnetic field extends in the magnetotail on Earth's nightside, which lengthwise exceeds 6,300,000 kilometers (3,900,000 mi).","The Kelvin-Helmholtz instability is a phenomenon that occurs when the magnetosphere is compressed by the solar wind to a distance of approximately 65,000 kilometers (40,000 mi) on the dayside of Earth. This results in the magnetopause existing at a distance of several hundred kilometers above Earth's surface.",A,"The research characterised variances in formation of the interplanetary magnetic field (IMF) largely influenced by Kelvin–Helmholtz instability (which occur at the interface of two fluids) as a result of differences in thickness and numerous other characteristics of the boundary layer. Experts believe that this was the first occasion that the appearance of Kelvin–Helmholtz waves at the magnetopause had been displayed at high latitude downward orientation of the IMF. These waves are being seen in unforeseen places under solar wind conditions that were formerly believed to be undesired for their generation. These discoveries show how Earth's magnetosphere can be penetrated by solar particles under specific IMF circumstances. The findings are also relevant to studies of magnetospheric progressions around other planetary bodies. This study suggests that Kelvin–Helmholtz waves can be a somewhat common, and possibly constant, instrument for the entrance of solar wind into terrestrial"
What is the significance of the high degree of fatty-acyl disorder in the thylakoid membranes of plants?,The high degree of fatty-acyl disorder in the thylakoid membranes of plants is responsible for the low fluidity of membrane lipid fatty-acyl chains in the gel phase.,The high degree of fatty-acyl disorder in the thylakoid membranes of plants is responsible for the exposure of chloroplast thylakoid membranes to cold environmental temperatures.,The high degree of fatty-acyl disorder in the thylakoid membranes of plants allows for innate fluidity even at relatively low temperatures.,The high degree of fatty-acyl disorder in the thylakoid membranes of plants allows for a gel-to-liquid crystalline phase transition temperature to be determined by many techniques.,"The high degree of fatty-acyl disorder in the thylakoid membranes of plants restricts the movement of membrane proteins, thus hindering their physiological role.",C,"In biological membranes, gel to liquid crystalline phase transitions play a critical role in physiological functioning of biomembranes. In gel phase, due to low fluidity of membrane lipid fatty-acyl chains, membrane proteins have restricted movement and thus are restrained in exercise of their physiological role. Plants depend critically on photosynthesis by chloroplast thylakoid membranes which are exposed cold environmental temperatures. Thylakoid membranes retain innate fluidity even at relatively low temperatures because of high degree of fatty-acyl disorder allowed by their high content of linolenic acid, 18-carbon chain with 3-double bonds. Gel-to-liquid crystalline phase transition temperature of biological membranes can be determined by many techniques including calorimetry, fluorescence, spin label electron paramagnetic resonance and NMR by recording measurements of the concerned parameter by at series of sample temperatures. A simple method for its determination from 13-C"
What is the explanation for the effective supersymmetry in quark-diquark models?,"Two different color charges close together appear as the corresponding anti-color under coarse resolution, which makes a diquark cluster viewed with coarse resolution effectively appear as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves like a meson.","Two different color charges close together appear as the corresponding color under coarse resolution, which makes a diquark cluster viewed with coarse resolution effectively appear as a quark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves like a baryon.","Two different color charges close together appear as the corresponding color under fine resolution, which makes a diquark cluster viewed with fine resolution effectively appear as a quark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves like a baryon.","Two different color charges close together appear as the corresponding anti-color under fine resolution, which makes a diquark cluster viewed with fine resolution effectively appear as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves like a meson.","Two different color charges close together appear as the corresponding anti-color under any resolution, which makes a diquark cluster viewed with any resolution effectively appear as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves like a meson.",A,"In particle physics, a diquark, or diquark correlation/clustering, is a hypothetical state of two quarks grouped inside a baryon (that consists of three quarks) (Lichtenberg 1982). Corresponding models of baryons are referred to as quark–diquark models. The diquark is often treated as a single subatomic particle with which the third quark interacts via the strong interaction. The existence of diquarks inside the nucleons is a disputed issue, but it helps to explain some nucleon properties and to reproduce experimental data sensitive to the nucleon structure. Diquark–antidiquark pairs have also been advanced for anomalous particles such as the X(3872).

Formation 
The forces between the two quarks in a diquark is attractive when both the colors and spins are antisymmetric. When both quarks are correlated in this way they tend to form a very low energy configuration. This low energy configuration has become known as a diquark."
What is the relationship between the complete electromagnetic Hamiltonian of a molecule and the parity operation?,"The complete electromagnetic Hamiltonian of any molecule is invariant to the parity operation, and its eigenvalues cannot be given the parity symmetry label + or -.","The complete electromagnetic Hamiltonian of any molecule is dependent on the parity operation, and its eigenvalues can be given the parity symmetry label even or odd, respectively.","The complete electromagnetic Hamiltonian of any molecule is dependent on the parity operation, and its eigenvalues can be given the parity symmetry label + or - depending on whether they are even or odd, respectively.","The complete electromagnetic Hamiltonian of any molecule is invariant to the parity operation, and its eigenvalues can be given the parity symmetry label + or - depending on whether they are even or odd, respectively.","The complete electromagnetic Hamiltonian of any molecule does not involve the parity operation, and its eigenvalues cannot be given the parity symmetry label + or -.",D,
What is the difference between active and passive transport in cells?,Active transport and passive transport both require energy input from the cell to function.,"Passive transport is powered by the arithmetic sum of osmosis and an electric field, while active transport requires energy input from the cell.","Passive transport requires energy input from the cell, while active transport is powered by the arithmetic sum of osmosis and an electric field.",Active transport and passive transport are both powered by the arithmetic sum of osmosis and an electric field.,"Active transport is powered by the arithmetic sum of osmosis and an electric field, while passive transport requires energy input from the cell.",B,"Physiological processes
There are different ways through which cells can transport substances across the cell membrane. The two main pathways are passive transport and active transport. Passive transport is more direct and does not require the use of the cell's energy. It relies on an area that maintains a high-to-low concentration gradient. Active transport uses adenosine triphosphate (ATP) to transport a substance that moves against its concentration gradient.

Movement of proteins 
The pathway for proteins to move in cells starts at the ER. Lipids and proteins are synthesized in the ER, and carbohydrates are added to make glycoproteins. Glycoproteins undergo further synthesis in the Golgi apparatus, becoming glycolipids. Both glycoproteins and glycolipids are transported into vesicles to the plasma membrane. The cell releases secretory proteins known as exocytosis.

Transport of ions"
What is the Heisenberg uncertainty principle and how does it relate to angular momentum in quantum mechanics?,"The Heisenberg uncertainty principle states that the axis of rotation of a quantum particle is undefined, and that quantum particles possess a type of non-orbital angular momentum called ""spin"". This is because angular momentum, like other quantities in quantum mechanics, is expressed as a tensorial operator in relativistic quantum mechanics.","The Heisenberg uncertainty principle states that the total angular momentum of a system of particles is equal to the sum of the individual particle angular momenta, and that the centre of mass is for the system. This is because angular momentum, like other quantities in quantum mechanics, is expressed as an operator with quantized eigenvalues.","The Heisenberg uncertainty principle states that the total angular momentum of a system of particles is subject to quantization, and that the individual particle angular momenta are expressed as operators. This is because angular momentum, like other quantities in quantum mechanics, is subject to the Heisenberg uncertainty principle.","The Heisenberg uncertainty principle states that the axis of rotation of a quantum particle is undefined, and that at any given time, only one projection of angular momentum can be measured with definite precision, while the other two remain uncertain. This is because angular momentum, like other quantities in quantum mechanics, is subject to quantization and expressed as an operator with quantized eigenvalues.","The Heisenberg uncertainty principle states that at any given time, only one projection of angular momentum can be measured with definite precision, while the other two remain uncertain. This is because angular momentum, like other quantities in quantum mechanics, is expressed as an operator with quantized eigenvalues.",E,"De Broglie was awarded the Nobel Prize for Physics in 1929 for his hypothesis. Thomson and Davisson shared the Nobel Prize for Physics in 1937 for their experimental work.

Heisenberg's uncertainty principle

In his work on formulating quantum mechanics, Werner Heisenberg postulated his uncertainty principle, which states:

where
 here indicates standard deviation, a measure of spread or uncertainty;
 and  are a particle's position and linear momentum respectively.
 is the reduced Planck's constant (Planck's constant divided by 2).

Heisenberg originally explained this as a consequence of the process of measuring: Measuring position accurately would disturb momentum and vice versa, offering an example (the ""gamma-ray microscope"") that depended crucially on the de Broglie hypothesis. The thought is now, however, that this only partly explains the phenomenon, but that the uncertainty also exists in the particle itself, even before the measurement is made."
What is the difference between natural convection and forced convection?,"Natural convection and forced convection are the same phenomenon, where a fluid is forced to flow over the surface by an internal source such as fans, stirring, and pumps, causing the fluid to be less dense and displaced.",Natural convection and forced convection are two different phenomena that do not relate to each other.,"Natural convection occurs when a fluid is in contact with a hot surface, causing the fluid to be less dense and displaced, while forced convection is when a fluid is forced to flow over the surface by an internal source such as fans, stirring, and pumps.","Natural convection is when a fluid is forced to flow over the surface by an internal source such as fans, stirring, and pumps, while forced convection occurs when a fluid is in contact with a hot surface, causing the fluid to be less dense and displaced.","Natural convection and forced convection are the same phenomenon, where a fluid is in contact with a hot surface, causing the fluid to be less dense and displaced, and then forced to flow over the surface by an internal source such as fans, stirring, and pumps.",C,"Convection can be ""forced"" by movement of a fluid by means other than buoyancy forces (for example, a water pump in an automobile engine). Thermal expansion of fluids may also force convection. In other cases, natural buoyancy forces alone are entirely responsible for fluid motion when the fluid is heated, and this process is called ""natural convection"". An example is the draft in a chimney or around any fire. In natural convection, an increase in temperature produces a reduction in density, which in turn causes fluid motion due to pressures and forces when fluids of different densities are affected by gravity (or any g-force). For example, when water is heated on a stove, hot water from the bottom of the pan is displaced (or forced up) by the colder denser liquid, which falls. After heating has stopped, mixing and conduction from this natural convection eventually result in a nearly homogeneous density, and even temperature. Without the presence of gravity (or conditions that cause a"
What is magnetic susceptibility?,"Magnetic susceptibility is a measure of how much a material will absorb magnetization in an applied magnetic field. It is the ratio of magnetization to the applied magnetizing field intensity, allowing for a simple classification of most materials' responses to an applied magnetic field.","Magnetic susceptibility is a measure of how much a material will become magnetized in an applied magnetic field. It is the ratio of magnetization to the applied magnetizing field intensity, allowing for a simple classification of most materials' responses to an applied magnetic field.","Magnetic susceptibility is a measure of how much a material will resist magnetization in an applied magnetic field. It is the ratio of magnetization to the applied magnetizing field intensity, allowing for a simple classification of most materials' responses to an applied magnetic field.","Magnetic susceptibility is a measure of how much a material will conduct magnetization in an applied magnetic field. It is the ratio of magnetization to the applied magnetizing field intensity, allowing for a simple classification of most materials' responses to an applied magnetic field.","Magnetic susceptibility is a measure of how much a material will reflect magnetization in an applied magnetic field. It is the ratio of magnetization to the applied magnetizing field intensity, allowing for a simple classification of most materials' responses to an applied magnetic field.",B,"In electromagnetism, the magnetic susceptibility (Latin: , ""receptive""; denoted ) is a measure of how much a material will become magnetized in an applied magnetic field. It is the ratio of magnetization  (magnetic moment per unit volume) to the applied magnetizing field intensity . This allows a simple classification, into two categories, of most materials' responses to an applied magnetic field: an alignment with the magnetic field, , called paramagnetism, or an alignment against the field, , called diamagnetism."
"What is a transient condensation cloud, also known as a Wilson cloud?","A visible cloud of smoke that forms when a nuclear weapon or a large amount of a conventional explosive is detonated in humid air, due to the burning of materials in the explosion.","A visible cloud of microscopic water droplets that forms when a nuclear weapon or a large amount of a conventional explosive is detonated in humid air, due to a temporary cooling of the air caused by a rarefaction of the air surrounding the explosion.","A visible cloud of microscopic water droplets that forms when a nuclear weapon or a large amount of a conventional explosive is detonated in dry air, due to a temporary cooling of the air caused by a rarefaction of the air surrounding the explosion.","A visible cloud of gas that forms when a nuclear weapon or a large amount of a conventional explosive is detonated in humid air, due to the release of gases from the explosion.","A visible cloud of smoke that forms when a nuclear weapon or a large amount of a conventional explosive is detonated in dry air, due to the burning of materials in the explosion.",B,"A transient condensation cloud, also called a Wilson cloud, is observable surrounding large explosions in humid air.

When a nuclear weapon or a large amount of a conventional explosive is detonated in sufficiently humid air, the ""negative phase"" of the shock wave causes a rarefaction of the air surrounding the explosion, but not contained within it. This rarefaction results in a temporary cooling of that air, which causes a condensation of some of the water vapor contained in it. When the pressure and the temperature return to normal, the Wilson cloud dissipates.

Mechanism"
What is a uniform tiling in the hyperbolic plane?,A uniform tiling in the hyperbolic plane is a tessellation of the hyperbolic plane with irregular polygons as faces. These are not vertex-transitive and isogonal.,A uniform tiling in the hyperbolic plane is a tessellation of the hyperbolic plane with regular polygons as faces. These are not vertex-transitive and isogonal.,A uniform tiling in the hyperbolic plane is a tessellation of the hyperbolic plane with irregular polygons as faces. These are vertex-transitive and isogonal.,"A uniform tiling in the hyperbolic plane is an edge-to-edge filling of the hyperbolic plane, with regular polygons as faces. These are vertex-transitive and isogonal.","A uniform tiling in the hyperbolic plane is an edge-to-edge filling of the hyperbolic plane, with irregular polygons as faces. These are vertex-transitive and isogonal.",D,"In hyperbolic geometry, a uniform hyperbolic tiling (or regular, quasiregular or semiregular hyperbolic tiling) is an edge-to-edge filling of the hyperbolic plane which has regular polygons as faces and is vertex-transitive (transitive on its vertices, isogonal, i.e. there is an isometry mapping any vertex onto any other). It follows that all vertices are congruent, and the tiling has a high degree of rotational and translational symmetry.

Uniform tilings can be identified by their vertex configuration, a sequence of numbers representing the number of sides of the polygons around each vertex. For example, 7.7.7 represents the heptagonal tiling which has 3 heptagons around each vertex. It is also regular since all the polygons are the same size, so it can also be given the Schläfli symbol {7,3}."
What is the relation between the three moment theorem and the bending moments at three successive supports of a continuous beam?,The three moment theorem expresses the relation between the deflection of two points on a beam relative to the point of intersection between tangent at those two points and the vertical through the first point.,"The three moment theorem is used to calculate the maximum allowable bending moment of a beam, which is determined by the weight distribution of each segment of the beam.","The three moment theorem describes the relationship between bending moments at three successive supports of a continuous beam, subject to a loading on two adjacent spans with or without settlement of the supports.","The three moment theorem is used to calculate the weight distribution of each segment of a beam, which is required to apply Mohr's theorem.","The three moment theorem is used to derive the change in slope of a deflection curve between two points of a beam, which is equal to the area of the M/EI diagram between those two points.",C,"In civil engineering and structural analysis Clapeyron's theorem of three moments is a relationship among the bending moments at three consecutive supports of a horizontal beam.

Let A,B,C be the three consecutive points of support, and denote by- l the length of AB and  the length of BC, by w and  the weight per unit of length in these segments.  Then the bending moments  at the three points are related by:

This equation can also be written as 

where a1 is the area on the bending moment diagram due to vertical loads on AB, a2 is the area due to loads on BC, x1 is the distance from A to the centroid of the bending moment diagram of beam AB, x2 is the distance from C to the centroid of the area of the bending moment diagram of beam BC.

The second equation is more general as it does not require that the weight of each segment be distributed uniformly.

Derivation of three moments equations 
Mohr's theorem can be used to derive the three moment theorem  (TMT)."
"What is the throttling process, and why is it important?","The throttling process is a steady flow of a fluid through a flow resistance, such as a valve or porous plug, and is responsible for the pressure increase in domestic refrigerators. This process is important because it is at the heart of the refrigeration cycle.","The throttling process is a steady adiabatic flow of a fluid through a flow resistance, such as a valve or porous plug, and is responsible for the temperature drop in domestic refrigerators. This process is important because it is at the heart of the refrigeration cycle.","The throttling process is a steady adiabatic flow of a fluid through a flow resistance, such as a valve or porous plug, and is responsible for the pressure drop in domestic refrigerators. This process is important because it is at the heart of the refrigeration cycle.","The throttling process is a steady flow of a fluid through a flow resistance, such as a valve or porous plug, and is responsible for the temperature increase in domestic refrigerators. This process is important because it is at the heart of the refrigeration cycle.","The throttling process is a steady adiabatic flow of a fluid through a flow resistance, such as a valve or porous plug, and is responsible for the temperature drop in domestic refrigerators. This process is not important because it is not used in the refrigeration cycle.",B,"Throttling can be used to actively limit a user's upload and download rates on programs such as video streaming, BitTorrent protocols and other file sharing applications, as well as even out the usage of the total bandwidth supplied across all users on the network. Bandwidth throttling is also often used in Internet applications, in order to spread a load over a wider network to reduce local network congestion, or over a number of servers to avoid overloading individual ones, and so reduce their risk of the system crashing, and gain additional revenue by giving users an incentive to use more expensive tiered pricing schemes, where bandwidth is not throttled."
What happens to excess base metal as a solution cools from the upper transformation temperature towards an insoluble state?,"The excess base metal will often solidify, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.","The excess base metal will often crystallize-out, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.","The excess base metal will often dissolve, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.","The excess base metal will often liquefy, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.","The excess base metal will often evaporate, becoming the proeutectoid until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.",B,"The base metal iron of the iron-carbon alloy known as steel, undergoes a change in the arrangement (allotropy) of the atoms of its crystal matrix at a certain temperature (usually between  and , depending on carbon content). This allows the smaller carbon atoms to enter the interstices of the iron crystal. When this diffusion happens, the carbon atoms are said to be in solution in the iron, forming a particular single, homogeneous, crystalline phase called austenite. If the steel is cooled slowly, the carbon can diffuse out of the iron and it will gradually revert to its low temperature allotrope. During slow cooling, the carbon atoms will no longer be as soluble with the iron, and will be forced to precipitate out of solution, nucleating into a more concentrated form of iron carbide (Fe3C) in the spaces between the pure iron crystals. The steel then becomes heterogeneous, as it is formed of two phases, the iron-carbon phase called cementite (or carbide), and pure iron ferrite. Such a"
"What is the relationship between mass, force, and acceleration, according to Sir Isaac Newton's laws of motion?","Mass is a property that determines the weight of an object. According to Newton's laws of motion and the formula F = ma, an object with a mass of one kilogram accelerates at one meter per second per second when acted upon by a force of one newton.","Mass is an inertial property that determines an object's tendency to remain at constant velocity unless acted upon by an outside force. According to Newton's laws of motion and the formula F = ma, an object with a mass of one kilogram accelerates at ten meters per second per second when acted upon by a force of one newton.","Mass is an inertial property that determines an object's tendency to remain at constant velocity unless acted upon by an outside force. According to Newton's laws of motion and the formula F = ma, an object with a mass of one kilogram accelerates at ten meters per second per second when acted upon by a force of ten newtons.","Mass is an inertial property that determines an object's tendency to remain at constant velocity unless acted upon by an outside force. According to Newton's laws of motion and the formula F = ma, an object with a mass of one kilogram accelerates at one meter per second per second when acted upon by a force of one newton.","Mass is a property that determines the size of an object. According to Newton's laws of motion and the formula F = ma, an object with a mass of one kilogram accelerates at one meter per second per second when acted upon by a force of ten newtons.",D,"Second law
Newton's second law describes a simple relationship between the acceleration of an object with mass , and the net force  acting on that object:

The net force and the object's acceleration are both vectors, and they point in the same direction. This version of the law applies to an object with a fixed mass . This relationship says that the net force applied to a body produces a proportional acceleration. It also means that if a body is accelerating, then a net force is being applied to it.

The law is also commonly stated in terms of the object's momentum , since  and . So, Newton's second law is also written as:

Some textbooks use Newton's second law as a definition of force, but this has been disparaged in other textbooks.

Variable-mass systems"
What did Arthur Eddington discover about two of Einstein's types of gravitational waves?,"Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could only be made to propagate at the speed of gravity by choosing appropriate coordinates.","Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could only be made to propagate at the speed of sound by choosing appropriate coordinates.","Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could be made to propagate at any speed by choosing appropriate coordinates.","Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could not be made to propagate at any speed by choosing appropriate coordinates.","Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could only be made to propagate at the speed of light by choosing appropriate coordinates.",C,"1916 – Albert Einstein shows that the field equations of general relativity admit wavelike solutions
 1918 – Josef Lense and Hans Thirring find the gravitomagnetic precession of gyroscopes in the equations of general relativity
 1919 – Arthur Eddington leads a solar eclipse expedition which claims to detect gravitational deflection of light by the Sun
 1921 – Theodor Kaluza demonstrates that a five-dimensional version of Einstein's equations unifies gravitation and electromagnetism
 1937 – Fritz Zwicky states that galaxies could act as gravitational lenses
 1937 – Albert Einstein, Leopold Infeld, and Banesh Hoffmann show that the geodesic equations of general relativity can be deduced from its field equations"
Who is the dean of the University of Central Florida College of Engineering and Computer Science?,John Doe,Michael Georgiopoulos,Jane Smith,Robert Johnson,David Brown,B,"The University of Central Florida College of Engineering and Computer Science is an academic college of the University of Central Florida located in Orlando, Florida, United States. The college offers degrees in engineering, computer science and management systems, and houses UCF's Department of Electrical Engineering and Computer Science. The dean of the college is Michael Georgiopoulos, Ph.D.UCF is listed as a university with ""very high research activity"" by The Carnegie Foundation for the Advancement of Teaching. With an enrollment of over 7,500 undergraduate and graduate students as of Fall 2012, the college is one of the premier engineering schools in the United States. The college is recognized by U.S. News & World Report as one of the nation's best Engineering schools, and as one of the world's best in the ARWU rankings. The university has made noted research contributions to modeling and simulation, digital media, and engineering and computer science.

History
The College of Engineering and Computer Science was one of the four original academic colleges when UCF began classes in 1968 as Florida Technological University."
What degrees does the college offer?,Engineering and computer science,Business and economics,Psychology and sociology,Mathematics and statistics,Arts and humanities,A,"The University of Central Florida College of Engineering and Computer Science is an academic college of the University of Central Florida located in Orlando, Florida, United States. The college offers degrees in engineering, computer science and management systems, and houses UCF's Department of Electrical Engineering and Computer Science. The dean of the college is Michael Georgiopoulos, Ph.D.UCF is listed as a university with ""very high research activity"" by The Carnegie Foundation for the Advancement of Teaching. With an enrollment of over 7,500 undergraduate and graduate students as of Fall 2012, the college is one of the premier engineering schools in the United States. The college is recognized by U.S. News & World Report as one of the nation's best Engineering schools, and as one of the world's best in the ARWU rankings. The university has made noted research contributions to modeling and simulation, digital media, and engineering and computer science.

History
The College of Engineering and Computer Science was one of the four original academic colleges when UCF began classes in 1968 as Florida Technological University."
How many undergraduate and graduate students were enrolled in the college as of Fall 2012?,"Over 5,000","Over 10,000","Over 7,500","Over 3,000","Over 15,000",C,"The University of Central Florida College of Engineering and Computer Science is an academic college of the University of Central Florida located in Orlando, Florida, United States. The college offers degrees in engineering, computer science and management systems, and houses UCF's Department of Electrical Engineering and Computer Science. The dean of the college is Michael Georgiopoulos, Ph.D.UCF is listed as a university with ""very high research activity"" by The Carnegie Foundation for the Advancement of Teaching. With an enrollment of over 7,500 undergraduate and graduate students as of Fall 2012, the college is one of the premier engineering schools in the United States. The college is recognized by U.S. News & World Report as one of the nation's best Engineering schools, and as one of the world's best in the ARWU rankings. The university has made noted research contributions to modeling and simulation, digital media, and engineering and computer science.

History
The College of Engineering and Computer Science was one of the four original academic colleges when UCF began classes in 1968 as Florida Technological University."
What is the University of Central Florida listed as by The Carnegie Foundation for the Advancement of Teaching?,A liberal arts college,A research university,An Ivy League institution,A community college,A vocational school,B,"The University of Central Florida College of Engineering and Computer Science is an academic college of the University of Central Florida located in Orlando, Florida, United States. The college offers degrees in engineering, computer science and management systems, and houses UCF's Department of Electrical Engineering and Computer Science. The dean of the college is Michael Georgiopoulos, Ph.D.UCF is listed as a university with ""very high research activity"" by The Carnegie Foundation for the Advancement of Teaching. With an enrollment of over 7,500 undergraduate and graduate students as of Fall 2012, the college is one of the premier engineering schools in the United States. The college is recognized by U.S. News & World Report as one of the nation's best Engineering schools, and as one of the world's best in the ARWU rankings. The university has made noted research contributions to modeling and simulation, digital media, and engineering and computer science.

History
The College of Engineering and Computer Science was one of the four original academic colleges when UCF began classes in 1968 as Florida Technological University."
What are some of the research contributions made by the university?,Architecture and design,Medicine and healthcare,Environmental science,Modeling and simulation,Linguistics and literature,D,"The University of Central Florida College of Engineering and Computer Science is an academic college of the University of Central Florida located in Orlando, Florida, United States. The college offers degrees in engineering, computer science and management systems, and houses UCF's Department of Electrical Engineering and Computer Science. The dean of the college is Michael Georgiopoulos, Ph.D.UCF is listed as a university with ""very high research activity"" by The Carnegie Foundation for the Advancement of Teaching. With an enrollment of over 7,500 undergraduate and graduate students as of Fall 2012, the college is one of the premier engineering schools in the United States. The college is recognized by U.S. News & World Report as one of the nation's best Engineering schools, and as one of the world's best in the ARWU rankings. The university has made noted research contributions to modeling and simulation, digital media, and engineering and computer science.

History
The College of Engineering and Computer Science was one of the four original academic colleges when UCF began classes in 1968 as Florida Technological University."
What is the definition of physical mathematics?,The study of mathematical physics,The study of mathematical representations of physical phenomena,The study of mathematical theories in relation to physical applications,The study of mathematical models in the field of physics,The study of mathematics applied to physical sciences,C,"The subject of physical mathematics is concerned with physically motivated mathematics and is considered by some as a subfield of mathematical physics.
According to Margaret Osler the simple machines of Hero of Alexandria and the ray tracing of Alhazen did not refer to causality or forces. Accordingly these early expressions of kinematics and optics do not rise to the level of mathematical physics as practiced by Galileo and Newton.
The details of physical units and their manipulation were addressed by Alexander Macfarlane in Physical Arithmetic in 1885. The science of kinematics created a need for mathematical representation of motion and has found expression with complex numbers, quaternions, and linear algebra.
At Cambridge University the Mathematical Tripos tested students on their knowledge of ""mixed mathematics"". ""... [N]ew books which appeared  in the mid-eighteenth century offered a systematic introduction to the fundamental  operations of the fluxional calculus and showed how it could be applied to a wide range of mathematical and physical problems. ... The strongly problem-oriented presentation in the treatises ..."
Who is considered to have laid the foundations of mathematical physics?,Hero of Alexandria,Alhazen,Galileo,Newton,Margaret Osler,C,"The subject of physical mathematics is concerned with physically motivated mathematics and is considered by some as a subfield of mathematical physics.
According to Margaret Osler the simple machines of Hero of Alexandria and the ray tracing of Alhazen did not refer to causality or forces. Accordingly these early expressions of kinematics and optics do not rise to the level of mathematical physics as practiced by Galileo and Newton.
The details of physical units and their manipulation were addressed by Alexander Macfarlane in Physical Arithmetic in 1885. The science of kinematics created a need for mathematical representation of motion and has found expression with complex numbers, quaternions, and linear algebra.
At Cambridge University the Mathematical Tripos tested students on their knowledge of ""mixed mathematics"". ""... [N]ew books which appeared  in the mid-eighteenth century offered a systematic introduction to the fundamental  operations of the fluxional calculus and showed how it could be applied to a wide range of mathematical and physical problems. ... The strongly problem-oriented presentation in the treatises ..."
Which mathematician addressed the details of physical units and their manipulation?,Margaret Osler,Hero of Alexandria,Alhazen,Alexander Macfarlane,Galileo,D,"The subject of physical mathematics is concerned with physically motivated mathematics and is considered by some as a subfield of mathematical physics.
According to Margaret Osler the simple machines of Hero of Alexandria and the ray tracing of Alhazen did not refer to causality or forces. Accordingly these early expressions of kinematics and optics do not rise to the level of mathematical physics as practiced by Galileo and Newton.
The details of physical units and their manipulation were addressed by Alexander Macfarlane in Physical Arithmetic in 1885. The science of kinematics created a need for mathematical representation of motion and has found expression with complex numbers, quaternions, and linear algebra.
At Cambridge University the Mathematical Tripos tested students on their knowledge of ""mixed mathematics"". ""... [N]ew books which appeared  in the mid-eighteenth century offered a systematic introduction to the fundamental  operations of the fluxional calculus and showed how it could be applied to a wide range of mathematical and physical problems. ... The strongly problem-oriented presentation in the treatises ..."
What mathematical representation is commonly used in kinematics?,Complex numbers,Quaternions,Linear algebra,All of the above,None of the above,D,"The subject of physical mathematics is concerned with physically motivated mathematics and is considered by some as a subfield of mathematical physics.
According to Margaret Osler the simple machines of Hero of Alexandria and the ray tracing of Alhazen did not refer to causality or forces. Accordingly these early expressions of kinematics and optics do not rise to the level of mathematical physics as practiced by Galileo and Newton.
The details of physical units and their manipulation were addressed by Alexander Macfarlane in Physical Arithmetic in 1885. The science of kinematics created a need for mathematical representation of motion and has found expression with complex numbers, quaternions, and linear algebra.
At Cambridge University the Mathematical Tripos tested students on their knowledge of ""mixed mathematics"". ""... [N]ew books which appeared  in the mid-eighteenth century offered a systematic introduction to the fundamental  operations of the fluxional calculus and showed how it could be applied to a wide range of mathematical and physical problems. ... The strongly problem-oriented presentation in the treatises ..."
What did the Mathematical Tripos test students on at Cambridge University?,Their knowledge of mixed mathematics,Their knowledge of physical arithmetic,Their knowledge of mathematical physics,Their knowledge of fluxional calculus,Their knowledge of mathematical and physical problems,A,"The subject of physical mathematics is concerned with physically motivated mathematics and is considered by some as a subfield of mathematical physics.
According to Margaret Osler the simple machines of Hero of Alexandria and the ray tracing of Alhazen did not refer to causality or forces. Accordingly these early expressions of kinematics and optics do not rise to the level of mathematical physics as practiced by Galileo and Newton.
The details of physical units and their manipulation were addressed by Alexander Macfarlane in Physical Arithmetic in 1885. The science of kinematics created a need for mathematical representation of motion and has found expression with complex numbers, quaternions, and linear algebra.
At Cambridge University the Mathematical Tripos tested students on their knowledge of ""mixed mathematics"". ""... [N]ew books which appeared  in the mid-eighteenth century offered a systematic introduction to the fundamental  operations of the fluxional calculus and showed how it could be applied to a wide range of mathematical and physical problems. ... The strongly problem-oriented presentation in the treatises ..."
What is classical mechanics?,A model of the physics of forces acting upon bodies,The study of the object and properties of an object in motion,"The study of changes in temperature, pressure, and volume on physical systems","The study of the behavior of solids, gases, and fluids",The study of the motion of particles and general systems of particles,A,"Physics is a scientific discipline that seeks to construct and experimentally test theories of the physical universe. These theories vary in their scope and can be organized into several distinct branches, which are outlined in this article.

Classical mechanics
Classical mechanics is a model of the physics of forces acting upon bodies; includes sub-fields to describe the behaviors of solids, gases, and fluids. It is often referred to as ""Newtonian mechanics"" after Isaac Newton and his laws of motion. It also includes the classical approach as given by Hamiltonian and Lagrange methods. It deals with the motion of particles and the general system of particles.
There are many branches of classical mechanics, such as: statics, dynamics, kinematics, continuum mechanics (which includes fluid mechanics), statistical mechanics, etc.

Mechanics: A branch of physics in which we study the object and properties of an object in form of a motion under the action of the force.

Thermodynamics and statistical mechanics
The first chapter of The Feynman Lectures on Physics is about the existence of atoms, which Feynman considered to be the most compact statement of physics, from which science could easily result even if all other knowledge was lost. By modeling matter as collections of hard spheres, it is possible to describe the kinetic theory of gases, upon which classical thermodynamics is based.
Thermodynamics studies the effects of changes in temperature, pressure, and volume on physical systems on the macroscopic scale, and the transfer of energy as heat. Historically, thermodynamics developed out of the desire to increase the efficiency of early steam engines.The starting point for most thermodynamic considerations is the laws of thermodynamics, which postulate that energy can be exchanged between physical systems as heat or work."
Who is often referred to as the father of classical mechanics?,Isaac Newton,Albert Einstein,Galileo Galilei,Leonhard Euler,Johannes Kepler,A,"Physics is a scientific discipline that seeks to construct and experimentally test theories of the physical universe. These theories vary in their scope and can be organized into several distinct branches, which are outlined in this article.

Classical mechanics
Classical mechanics is a model of the physics of forces acting upon bodies; includes sub-fields to describe the behaviors of solids, gases, and fluids. It is often referred to as ""Newtonian mechanics"" after Isaac Newton and his laws of motion. It also includes the classical approach as given by Hamiltonian and Lagrange methods. It deals with the motion of particles and the general system of particles.
There are many branches of classical mechanics, such as: statics, dynamics, kinematics, continuum mechanics (which includes fluid mechanics), statistical mechanics, etc.

Mechanics: A branch of physics in which we study the object and properties of an object in form of a motion under the action of the force.

Thermodynamics and statistical mechanics
The first chapter of The Feynman Lectures on Physics is about the existence of atoms, which Feynman considered to be the most compact statement of physics, from which science could easily result even if all other knowledge was lost. By modeling matter as collections of hard spheres, it is possible to describe the kinetic theory of gases, upon which classical thermodynamics is based.
Thermodynamics studies the effects of changes in temperature, pressure, and volume on physical systems on the macroscopic scale, and the transfer of energy as heat. Historically, thermodynamics developed out of the desire to increase the efficiency of early steam engines.The starting point for most thermodynamic considerations is the laws of thermodynamics, which postulate that energy can be exchanged between physical systems as heat or work."
What is thermodynamics?,The study of the physics of forces acting upon bodies,The study of the object and properties of an object in motion,"The study of changes in temperature, pressure, and volume on physical systems","The study of the behavior of solids, gases, and fluids",The study of the motion of particles and general systems of particles,C,"Physics is a scientific discipline that seeks to construct and experimentally test theories of the physical universe. These theories vary in their scope and can be organized into several distinct branches, which are outlined in this article.

Classical mechanics
Classical mechanics is a model of the physics of forces acting upon bodies; includes sub-fields to describe the behaviors of solids, gases, and fluids. It is often referred to as ""Newtonian mechanics"" after Isaac Newton and his laws of motion. It also includes the classical approach as given by Hamiltonian and Lagrange methods. It deals with the motion of particles and the general system of particles.
There are many branches of classical mechanics, such as: statics, dynamics, kinematics, continuum mechanics (which includes fluid mechanics), statistical mechanics, etc.

Mechanics: A branch of physics in which we study the object and properties of an object in form of a motion under the action of the force.

Thermodynamics and statistical mechanics
The first chapter of The Feynman Lectures on Physics is about the existence of atoms, which Feynman considered to be the most compact statement of physics, from which science could easily result even if all other knowledge was lost. By modeling matter as collections of hard spheres, it is possible to describe the kinetic theory of gases, upon which classical thermodynamics is based.
Thermodynamics studies the effects of changes in temperature, pressure, and volume on physical systems on the macroscopic scale, and the transfer of energy as heat. Historically, thermodynamics developed out of the desire to increase the efficiency of early steam engines.The starting point for most thermodynamic considerations is the laws of thermodynamics, which postulate that energy can be exchanged between physical systems as heat or work."
What is the starting point for most thermodynamic considerations?,The laws of thermodynamics,The laws of classical mechanics,The laws of quantum mechanics,The laws of relativity,The laws of statistical mechanics,A,"Physics is a scientific discipline that seeks to construct and experimentally test theories of the physical universe. These theories vary in their scope and can be organized into several distinct branches, which are outlined in this article.

Classical mechanics
Classical mechanics is a model of the physics of forces acting upon bodies; includes sub-fields to describe the behaviors of solids, gases, and fluids. It is often referred to as ""Newtonian mechanics"" after Isaac Newton and his laws of motion. It also includes the classical approach as given by Hamiltonian and Lagrange methods. It deals with the motion of particles and the general system of particles.
There are many branches of classical mechanics, such as: statics, dynamics, kinematics, continuum mechanics (which includes fluid mechanics), statistical mechanics, etc.

Mechanics: A branch of physics in which we study the object and properties of an object in form of a motion under the action of the force.

Thermodynamics and statistical mechanics
The first chapter of The Feynman Lectures on Physics is about the existence of atoms, which Feynman considered to be the most compact statement of physics, from which science could easily result even if all other knowledge was lost. By modeling matter as collections of hard spheres, it is possible to describe the kinetic theory of gases, upon which classical thermodynamics is based.
Thermodynamics studies the effects of changes in temperature, pressure, and volume on physical systems on the macroscopic scale, and the transfer of energy as heat. Historically, thermodynamics developed out of the desire to increase the efficiency of early steam engines.The starting point for most thermodynamic considerations is the laws of thermodynamics, which postulate that energy can be exchanged between physical systems as heat or work."
What did Feynman consider to be the most compact statement of physics?,Classical mechanics,Thermodynamics,The existence of atoms,The laws of motion,The laws of thermodynamics,C,"Physics is a scientific discipline that seeks to construct and experimentally test theories of the physical universe. These theories vary in their scope and can be organized into several distinct branches, which are outlined in this article.

Classical mechanics
Classical mechanics is a model of the physics of forces acting upon bodies; includes sub-fields to describe the behaviors of solids, gases, and fluids. It is often referred to as ""Newtonian mechanics"" after Isaac Newton and his laws of motion. It also includes the classical approach as given by Hamiltonian and Lagrange methods. It deals with the motion of particles and the general system of particles.
There are many branches of classical mechanics, such as: statics, dynamics, kinematics, continuum mechanics (which includes fluid mechanics), statistical mechanics, etc.

Mechanics: A branch of physics in which we study the object and properties of an object in form of a motion under the action of the force.

Thermodynamics and statistical mechanics
The first chapter of The Feynman Lectures on Physics is about the existence of atoms, which Feynman considered to be the most compact statement of physics, from which science could easily result even if all other knowledge was lost. By modeling matter as collections of hard spheres, it is possible to describe the kinetic theory of gases, upon which classical thermodynamics is based.
Thermodynamics studies the effects of changes in temperature, pressure, and volume on physical systems on the macroscopic scale, and the transfer of energy as heat. Historically, thermodynamics developed out of the desire to increase the efficiency of early steam engines.The starting point for most thermodynamic considerations is the laws of thermodynamics, which postulate that energy can be exchanged between physical systems as heat or work."
What are symmetries in quantum mechanics?,Symmetries describe features of spacetime that are changed under some transformation.,Symmetries describe features of particles that are unchanged under some transformation.,Symmetries describe features of particles that are changed under some transformation.,Symmetries describe features of spacetime that are unchanged under some transformation.,Symmetries describe features of particles that are irrelevant under some transformation.,D,"Symmetries in quantum mechanics describe features of spacetime and particles which are unchanged under some transformation, in the context of quantum mechanics, relativistic quantum mechanics and quantum field theory, and with applications in the mathematical formulation of the standard model and condensed matter physics. In general, symmetry in physics, invariance, and conservation laws, are fundamentally important constraints for formulating physical theories and models. In practice, they are powerful methods for solving problems and predicting what can happen. While conservation laws do not always give the answer to the problem directly, they form the correct constraints and the first steps to solving a multitude of problems.
This article outlines the connection between the classical form of continuous symmetries as well as their quantum operators, and relates them to the Lie groups, and relativistic transformations in the Lorentz group and Poincaré group.

Notation
The notational conventions used in this article are as follows. Boldface indicates vectors, four vectors, matrices, and vectorial operators, while quantum states use bra–ket notation. Wide hats are for operators, narrow hats are for unit vectors (including their components in tensor index notation). The summation convention on the repeated tensor indices is used, unless stated otherwise."
"What are symmetry, invariance, and conservation laws in physics?",They are fundamental constraints for formulating physical theories and models.,They are powerful methods for solving problems and predicting outcomes.,They are direct answers to all physical problems.,They are redundant in physics and can be ignored.,They are mathematical concepts with no practical use in physics.,A,"Symmetries in quantum mechanics describe features of spacetime and particles which are unchanged under some transformation, in the context of quantum mechanics, relativistic quantum mechanics and quantum field theory, and with applications in the mathematical formulation of the standard model and condensed matter physics. In general, symmetry in physics, invariance, and conservation laws, are fundamentally important constraints for formulating physical theories and models. In practice, they are powerful methods for solving problems and predicting what can happen. While conservation laws do not always give the answer to the problem directly, they form the correct constraints and the first steps to solving a multitude of problems.
This article outlines the connection between the classical form of continuous symmetries as well as their quantum operators, and relates them to the Lie groups, and relativistic transformations in the Lorentz group and Poincaré group.

Notation
The notational conventions used in this article are as follows. Boldface indicates vectors, four vectors, matrices, and vectorial operators, while quantum states use bra–ket notation. Wide hats are for operators, narrow hats are for unit vectors (including their components in tensor index notation). The summation convention on the repeated tensor indices is used, unless stated otherwise."
How do conservation laws contribute to solving problems in physics?,They directly provide the answer to the problem.,They are the correct constraints and the first steps to solving a multitude of problems.,They are irrelevant in the formulation of physical theories and models.,"They are only useful in classical physics, not quantum mechanics.",They hinder problem-solving in physics.,B,"Symmetries in quantum mechanics describe features of spacetime and particles which are unchanged under some transformation, in the context of quantum mechanics, relativistic quantum mechanics and quantum field theory, and with applications in the mathematical formulation of the standard model and condensed matter physics. In general, symmetry in physics, invariance, and conservation laws, are fundamentally important constraints for formulating physical theories and models. In practice, they are powerful methods for solving problems and predicting what can happen. While conservation laws do not always give the answer to the problem directly, they form the correct constraints and the first steps to solving a multitude of problems.
This article outlines the connection between the classical form of continuous symmetries as well as their quantum operators, and relates them to the Lie groups, and relativistic transformations in the Lorentz group and Poincaré group.

Notation
The notational conventions used in this article are as follows. Boldface indicates vectors, four vectors, matrices, and vectorial operators, while quantum states use bra–ket notation. Wide hats are for operators, narrow hats are for unit vectors (including their components in tensor index notation). The summation convention on the repeated tensor indices is used, unless stated otherwise."
What do Lie groups and relativistic transformations relate to in quantum mechanics?,They relate to the mathematical formulation of the standard model.,They relate to the condensed matter physics.,They relate to the classical form of continuous symmetries.,They relate to the quantum operators of symmetries.,They relate to the mathematical formulation of quantum field theory.,D,"Symmetries in quantum mechanics describe features of spacetime and particles which are unchanged under some transformation, in the context of quantum mechanics, relativistic quantum mechanics and quantum field theory, and with applications in the mathematical formulation of the standard model and condensed matter physics. In general, symmetry in physics, invariance, and conservation laws, are fundamentally important constraints for formulating physical theories and models. In practice, they are powerful methods for solving problems and predicting what can happen. While conservation laws do not always give the answer to the problem directly, they form the correct constraints and the first steps to solving a multitude of problems.
This article outlines the connection between the classical form of continuous symmetries as well as their quantum operators, and relates them to the Lie groups, and relativistic transformations in the Lorentz group and Poincaré group.

Notation
The notational conventions used in this article are as follows. Boldface indicates vectors, four vectors, matrices, and vectorial operators, while quantum states use bra–ket notation. Wide hats are for operators, narrow hats are for unit vectors (including their components in tensor index notation). The summation convention on the repeated tensor indices is used, unless stated otherwise."
What do wide hats and narrow hats represent in notation?,Wide hats represent unit vectors and narrow hats represent operators.,Wide hats represent matrices and narrow hats represent vectors.,Wide hats represent vectors and narrow hats represent unit vectors.,Wide hats represent operators and narrow hats represent unit vectors.,Wide hats represent unit vectors and narrow hats represent matrices.,D,"Symmetries in quantum mechanics describe features of spacetime and particles which are unchanged under some transformation, in the context of quantum mechanics, relativistic quantum mechanics and quantum field theory, and with applications in the mathematical formulation of the standard model and condensed matter physics. In general, symmetry in physics, invariance, and conservation laws, are fundamentally important constraints for formulating physical theories and models. In practice, they are powerful methods for solving problems and predicting what can happen. While conservation laws do not always give the answer to the problem directly, they form the correct constraints and the first steps to solving a multitude of problems.
This article outlines the connection between the classical form of continuous symmetries as well as their quantum operators, and relates them to the Lie groups, and relativistic transformations in the Lorentz group and Poincaré group.

Notation
The notational conventions used in this article are as follows. Boldface indicates vectors, four vectors, matrices, and vectorial operators, while quantum states use bra–ket notation. Wide hats are for operators, narrow hats are for unit vectors (including their components in tensor index notation). The summation convention on the repeated tensor indices is used, unless stated otherwise."
When was the Wisconsin Alumni Research Foundation founded?,1925,1935,1945,1955,1965,A,"The Wisconsin Alumni Research Foundation is the independent nonprofit technology transfer organization serving the University of Wisconsin–Madison and Morgridge Institute for Research. It provides significant research support, granting tens of millions of dollars to the university each year and contributing to the university's ""margin of excellence"".

History
WARF was founded in 1925 to manage a discovery by Harry Steenbock, who invented the process for using ultraviolet radiation to add vitamin D to milk and other foods. Rather than leaving the invention unpatented—then the standard practice for university inventions—Steenbock used $300 of his own money to file for a patent. He received commercial interest from Quaker Oats but declined the company's initial offer.
Instead, Steenbock sought a way to protect discoveries made by UW-Madison faculty, ensure use of the ideas for public benefit and bring any financial gains back to the university. His concept gained support from Harry L. Russell, dean of the College of Agriculture, and Charles Sumner Slichter, dean of the Graduate School.
Slichter began soliciting the interest and financial support of wealthy UW-Madison alumni acquaintances in Chicago and New York. He gained a substantial sum in verbal pledges from a number of alumni, nine of whom would eventually contribute $100 each.
The UW Board of Regents approved the plan on June 22, 1925, and the organization's charter was filed with the Secretary of State of Wisconsin on November 14 that same year."
Who invented the process for using ultraviolet radiation to add vitamin D to milk and other foods?,Harry Steenbock,Harry L. Russell,Charles Sumner Slichter,Quaker Oats,UW-Madison,A,"The Wisconsin Alumni Research Foundation is the independent nonprofit technology transfer organization serving the University of Wisconsin–Madison and Morgridge Institute for Research. It provides significant research support, granting tens of millions of dollars to the university each year and contributing to the university's ""margin of excellence"".

History
WARF was founded in 1925 to manage a discovery by Harry Steenbock, who invented the process for using ultraviolet radiation to add vitamin D to milk and other foods. Rather than leaving the invention unpatented—then the standard practice for university inventions—Steenbock used $300 of his own money to file for a patent. He received commercial interest from Quaker Oats but declined the company's initial offer.
Instead, Steenbock sought a way to protect discoveries made by UW-Madison faculty, ensure use of the ideas for public benefit and bring any financial gains back to the university. His concept gained support from Harry L. Russell, dean of the College of Agriculture, and Charles Sumner Slichter, dean of the Graduate School.
Slichter began soliciting the interest and financial support of wealthy UW-Madison alumni acquaintances in Chicago and New York. He gained a substantial sum in verbal pledges from a number of alumni, nine of whom would eventually contribute $100 each.
The UW Board of Regents approved the plan on June 22, 1925, and the organization's charter was filed with the Secretary of State of Wisconsin on November 14 that same year."
How did Harry Steenbock initially protect his invention?,By leaving it unpatented,By seeking financial support from alumni,By filing for a patent,By declining Quaker Oats offer,By gaining commercial interest,C,"The Wisconsin Alumni Research Foundation is the independent nonprofit technology transfer organization serving the University of Wisconsin–Madison and Morgridge Institute for Research. It provides significant research support, granting tens of millions of dollars to the university each year and contributing to the university's ""margin of excellence"".

History
WARF was founded in 1925 to manage a discovery by Harry Steenbock, who invented the process for using ultraviolet radiation to add vitamin D to milk and other foods. Rather than leaving the invention unpatented—then the standard practice for university inventions—Steenbock used $300 of his own money to file for a patent. He received commercial interest from Quaker Oats but declined the company's initial offer.
Instead, Steenbock sought a way to protect discoveries made by UW-Madison faculty, ensure use of the ideas for public benefit and bring any financial gains back to the university. His concept gained support from Harry L. Russell, dean of the College of Agriculture, and Charles Sumner Slichter, dean of the Graduate School.
Slichter began soliciting the interest and financial support of wealthy UW-Madison alumni acquaintances in Chicago and New York. He gained a substantial sum in verbal pledges from a number of alumni, nine of whom would eventually contribute $100 each.
The UW Board of Regents approved the plan on June 22, 1925, and the organization's charter was filed with the Secretary of State of Wisconsin on November 14 that same year."
Who supported Harry Steenbock's concept of protecting discoveries made by UW-Madison faculty?,Harry Steenbock,Harry L. Russell,Charles Sumner Slichter,Quaker Oats,UW-Madison,C,"The Wisconsin Alumni Research Foundation is the independent nonprofit technology transfer organization serving the University of Wisconsin–Madison and Morgridge Institute for Research. It provides significant research support, granting tens of millions of dollars to the university each year and contributing to the university's ""margin of excellence"".

History
WARF was founded in 1925 to manage a discovery by Harry Steenbock, who invented the process for using ultraviolet radiation to add vitamin D to milk and other foods. Rather than leaving the invention unpatented—then the standard practice for university inventions—Steenbock used $300 of his own money to file for a patent. He received commercial interest from Quaker Oats but declined the company's initial offer.
Instead, Steenbock sought a way to protect discoveries made by UW-Madison faculty, ensure use of the ideas for public benefit and bring any financial gains back to the university. His concept gained support from Harry L. Russell, dean of the College of Agriculture, and Charles Sumner Slichter, dean of the Graduate School.
Slichter began soliciting the interest and financial support of wealthy UW-Madison alumni acquaintances in Chicago and New York. He gained a substantial sum in verbal pledges from a number of alumni, nine of whom would eventually contribute $100 each.
The UW Board of Regents approved the plan on June 22, 1925, and the organization's charter was filed with the Secretary of State of Wisconsin on November 14 that same year."
When was the organization's charter filed with the Secretary of State of Wisconsin?,"June 22, 1925","November 14, 1925","November 22, 1925","June 14, 1925","November 14, 1935",B,"The Wisconsin Alumni Research Foundation is the independent nonprofit technology transfer organization serving the University of Wisconsin–Madison and Morgridge Institute for Research. It provides significant research support, granting tens of millions of dollars to the university each year and contributing to the university's ""margin of excellence"".

History
WARF was founded in 1925 to manage a discovery by Harry Steenbock, who invented the process for using ultraviolet radiation to add vitamin D to milk and other foods. Rather than leaving the invention unpatented—then the standard practice for university inventions—Steenbock used $300 of his own money to file for a patent. He received commercial interest from Quaker Oats but declined the company's initial offer.
Instead, Steenbock sought a way to protect discoveries made by UW-Madison faculty, ensure use of the ideas for public benefit and bring any financial gains back to the university. His concept gained support from Harry L. Russell, dean of the College of Agriculture, and Charles Sumner Slichter, dean of the Graduate School.
Slichter began soliciting the interest and financial support of wealthy UW-Madison alumni acquaintances in Chicago and New York. He gained a substantial sum in verbal pledges from a number of alumni, nine of whom would eventually contribute $100 each.
The UW Board of Regents approved the plan on June 22, 1925, and the organization's charter was filed with the Secretary of State of Wisconsin on November 14 that same year."
What is the relationship between technology and society?,Technology and society are completely independent of each other.,Technology and society have no influence on each other.,Technology and society are inter-dependent and co-influence each other.,Technology and society only depend on each other.,Technology and society have no impact on each other.,C,"Technology society and life or technology and culture refers to the inter-dependency, co-dependence, co-influence, and co-production of technology and society upon one another. Evidence for this synergy has been found since humanity first started using simple tools. The inter-relationship has continued as modern technologies such as the printing press and computers have helped shape society. The first scientific approach to this relationship occurred with the development of tektology, the ""science of organization"", in early twentieth century Imperial Russia. In modern academia, the interdisciplinary study of the mutual impacts of science, technology, and society, is called science and technology studies.
The simplest form of technology is the development and use of basic tools. The prehistoric discovery of how to control fire and the later Neolithic Revolution increased the available sources of food, and the invention of the wheel helped humans to travel in and control their environment. Developments in historic times have lessened physical barriers to communication and allowed humans to interact freely on a global scale, such as the printing press, telephone, and Internet.
Technology has developed advanced economies, such as the modern  global economy, and has led to the rise of a leisure class."
When did the first scientific approach to the relationship between technology and society occur?,In modern times,In ancient times,In the 20th century,In prehistoric times,In medieval times,C,"Technology society and life or technology and culture refers to the inter-dependency, co-dependence, co-influence, and co-production of technology and society upon one another. Evidence for this synergy has been found since humanity first started using simple tools. The inter-relationship has continued as modern technologies such as the printing press and computers have helped shape society. The first scientific approach to this relationship occurred with the development of tektology, the ""science of organization"", in early twentieth century Imperial Russia. In modern academia, the interdisciplinary study of the mutual impacts of science, technology, and society, is called science and technology studies.
The simplest form of technology is the development and use of basic tools. The prehistoric discovery of how to control fire and the later Neolithic Revolution increased the available sources of food, and the invention of the wheel helped humans to travel in and control their environment. Developments in historic times have lessened physical barriers to communication and allowed humans to interact freely on a global scale, such as the printing press, telephone, and Internet.
Technology has developed advanced economies, such as the modern  global economy, and has led to the rise of a leisure class."
"What is the study of the mutual impacts of science, technology, and society called?",History of Science,Technology and Society Studies,Sociology of Technology,Science and Technology Studies,Cultural Anthropology,D,"Technology society and life or technology and culture refers to the inter-dependency, co-dependence, co-influence, and co-production of technology and society upon one another. Evidence for this synergy has been found since humanity first started using simple tools. The inter-relationship has continued as modern technologies such as the printing press and computers have helped shape society. The first scientific approach to this relationship occurred with the development of tektology, the ""science of organization"", in early twentieth century Imperial Russia. In modern academia, the interdisciplinary study of the mutual impacts of science, technology, and society, is called science and technology studies.
The simplest form of technology is the development and use of basic tools. The prehistoric discovery of how to control fire and the later Neolithic Revolution increased the available sources of food, and the invention of the wheel helped humans to travel in and control their environment. Developments in historic times have lessened physical barriers to communication and allowed humans to interact freely on a global scale, such as the printing press, telephone, and Internet.
Technology has developed advanced economies, such as the modern  global economy, and has led to the rise of a leisure class."
What is the simplest form of technology?,Computers,The Internet,Basic tools,Telephones,Printing press,C,"Technology society and life or technology and culture refers to the inter-dependency, co-dependence, co-influence, and co-production of technology and society upon one another. Evidence for this synergy has been found since humanity first started using simple tools. The inter-relationship has continued as modern technologies such as the printing press and computers have helped shape society. The first scientific approach to this relationship occurred with the development of tektology, the ""science of organization"", in early twentieth century Imperial Russia. In modern academia, the interdisciplinary study of the mutual impacts of science, technology, and society, is called science and technology studies.
The simplest form of technology is the development and use of basic tools. The prehistoric discovery of how to control fire and the later Neolithic Revolution increased the available sources of food, and the invention of the wheel helped humans to travel in and control their environment. Developments in historic times have lessened physical barriers to communication and allowed humans to interact freely on a global scale, such as the printing press, telephone, and Internet.
Technology has developed advanced economies, such as the modern  global economy, and has led to the rise of a leisure class."
What did the development of technology lead to?,The decline of economies,The rise of a leisure class,The disappearance of physical barriers,The reduction of global interaction,The decrease in food sources,B,"Technology society and life or technology and culture refers to the inter-dependency, co-dependence, co-influence, and co-production of technology and society upon one another. Evidence for this synergy has been found since humanity first started using simple tools. The inter-relationship has continued as modern technologies such as the printing press and computers have helped shape society. The first scientific approach to this relationship occurred with the development of tektology, the ""science of organization"", in early twentieth century Imperial Russia. In modern academia, the interdisciplinary study of the mutual impacts of science, technology, and society, is called science and technology studies.
The simplest form of technology is the development and use of basic tools. The prehistoric discovery of how to control fire and the later Neolithic Revolution increased the available sources of food, and the invention of the wheel helped humans to travel in and control their environment. Developments in historic times have lessened physical barriers to communication and allowed humans to interact freely on a global scale, such as the printing press, telephone, and Internet.
Technology has developed advanced economies, such as the modern  global economy, and has led to the rise of a leisure class."
What are the two major mathematical reviewing databases that the MSC is based on?,Mathematical Research and Zentralblatt MATH,Mathematical Reviews and Zentralblatt MATH,Mathematical Research and Mathematical Reviews,Zentralblatt MATH and Mathematics Subject Classification,Mathematics Reviews and Mathematics Subject Classification,B,"The Mathematics Subject Classification (MSC) is an alphanumerical classification scheme that has collaboratively been produced by staff of, and based on the coverage of, the two major mathematical reviewing databases, Mathematical Reviews and Zentralblatt MATH. The MSC is used by many mathematics journals, which ask authors of research papers and expository articles to list subject codes from the Mathematics Subject Classification in their papers. The current version is MSC2020.

Structure
The MSC is a hierarchical scheme, with three levels of structure. A classification can be two, three or five digits long, depending on how many levels of the classification scheme are used.
The first level is represented by a two-digit number, the second by a letter, and the third by another two-digit number. For example:

53 is the classification for differential geometry
53A is the classification for classical differential geometry
53A45 is the classification for vector and tensor analysis

First level
At the top level, 64 mathematical disciplines are labeled with a unique two-digit number. In addition to the typical areas of mathematical research, there are top-level categories for ""History and Biography"", ""Mathematics Education"", and for the overlap with different sciences. Physics (i.e."
How many levels does the MSC hierarchical scheme have?,One,Two,Three,Four,Five,C,"The Mathematics Subject Classification (MSC) is an alphanumerical classification scheme that has collaboratively been produced by staff of, and based on the coverage of, the two major mathematical reviewing databases, Mathematical Reviews and Zentralblatt MATH. The MSC is used by many mathematics journals, which ask authors of research papers and expository articles to list subject codes from the Mathematics Subject Classification in their papers. The current version is MSC2020.

Structure
The MSC is a hierarchical scheme, with three levels of structure. A classification can be two, three or five digits long, depending on how many levels of the classification scheme are used.
The first level is represented by a two-digit number, the second by a letter, and the third by another two-digit number. For example:

53 is the classification for differential geometry
53A is the classification for classical differential geometry
53A45 is the classification for vector and tensor analysis

First level
At the top level, 64 mathematical disciplines are labeled with a unique two-digit number. In addition to the typical areas of mathematical research, there are top-level categories for ""History and Biography"", ""Mathematics Education"", and for the overlap with different sciences. Physics (i.e."
What is the classification for classical differential geometry in the MSC?,53,53A,53A45,2,45,B,"The Mathematics Subject Classification (MSC) is an alphanumerical classification scheme that has collaboratively been produced by staff of, and based on the coverage of, the two major mathematical reviewing databases, Mathematical Reviews and Zentralblatt MATH. The MSC is used by many mathematics journals, which ask authors of research papers and expository articles to list subject codes from the Mathematics Subject Classification in their papers. The current version is MSC2020.

Structure
The MSC is a hierarchical scheme, with three levels of structure. A classification can be two, three or five digits long, depending on how many levels of the classification scheme are used.
The first level is represented by a two-digit number, the second by a letter, and the third by another two-digit number. For example:

53 is the classification for differential geometry
53A is the classification for classical differential geometry
53A45 is the classification for vector and tensor analysis

First level
At the top level, 64 mathematical disciplines are labeled with a unique two-digit number. In addition to the typical areas of mathematical research, there are top-level categories for ""History and Biography"", ""Mathematics Education"", and for the overlap with different sciences. Physics (i.e."
How many mathematical disciplines are labeled at the top level of the MSC?,64,53,2,45,5,A,"The Mathematics Subject Classification (MSC) is an alphanumerical classification scheme that has collaboratively been produced by staff of, and based on the coverage of, the two major mathematical reviewing databases, Mathematical Reviews and Zentralblatt MATH. The MSC is used by many mathematics journals, which ask authors of research papers and expository articles to list subject codes from the Mathematics Subject Classification in their papers. The current version is MSC2020.

Structure
The MSC is a hierarchical scheme, with three levels of structure. A classification can be two, three or five digits long, depending on how many levels of the classification scheme are used.
The first level is represented by a two-digit number, the second by a letter, and the third by another two-digit number. For example:

53 is the classification for differential geometry
53A is the classification for classical differential geometry
53A45 is the classification for vector and tensor analysis

First level
At the top level, 64 mathematical disciplines are labeled with a unique two-digit number. In addition to the typical areas of mathematical research, there are top-level categories for ""History and Biography"", ""Mathematics Education"", and for the overlap with different sciences. Physics (i.e."
Which top-level category covers the overlap of mathematics with other sciences?,History and Biography,Mathematics Education,Physics,Mathematical Research,Mathematical Reviews,C,"The Mathematics Subject Classification (MSC) is an alphanumerical classification scheme that has collaboratively been produced by staff of, and based on the coverage of, the two major mathematical reviewing databases, Mathematical Reviews and Zentralblatt MATH. The MSC is used by many mathematics journals, which ask authors of research papers and expository articles to list subject codes from the Mathematics Subject Classification in their papers. The current version is MSC2020.

Structure
The MSC is a hierarchical scheme, with three levels of structure. A classification can be two, three or five digits long, depending on how many levels of the classification scheme are used.
The first level is represented by a two-digit number, the second by a letter, and the third by another two-digit number. For example:

53 is the classification for differential geometry
53A is the classification for classical differential geometry
53A45 is the classification for vector and tensor analysis

First level
At the top level, 64 mathematical disciplines are labeled with a unique two-digit number. In addition to the typical areas of mathematical research, there are top-level categories for ""History and Biography"", ""Mathematics Education"", and for the overlap with different sciences. Physics (i.e."
What is the main purpose of statistical mechanics?,To explain the behavior of microscopic entities,To study the properties of matter in aggregate,To develop models of probability distribution,To understand thermodynamic equilibrium,To analyze chemical reactions and flows of particles,B,"In physics, statistical mechanics is a mathematical framework that applies statistical methods and probability theory to large assemblies of microscopic entities. It does not assume or postulate any natural laws, but explains the macroscopic behavior of nature from the behavior of such ensembles.
Sometimes called statistical physics or statistical thermodynamics, its applications include many problems in the fields of physics, biology, chemistry, and neuroscience. Its main purpose is to clarify the properties of matter in aggregate, in terms of physical laws governing atomic motion.Statistical mechanics arose out of the development of classical thermodynamics, a field for which it was successful in explaining macroscopic physical properties—such as temperature, pressure, and heat capacity—in terms of microscopic parameters that fluctuate about average values and are characterized by probability distributions.
The founding of the field of statistical mechanics is generally credited to three physicists:

Ludwig Boltzmann, who developed the fundamental interpretation of entropy in terms of a collection of microstates
James Clerk Maxwell, who developed models of probability distribution of such states
Josiah Willard Gibbs, who coined the name of the field in 1884While classical thermodynamics is primarily concerned with thermodynamic equilibrium, statistical mechanics has been applied in non-equilibrium statistical mechanics to the issues of microscopically modeling the speed of irreversible processes that are driven by imbalances. Examples of such processes include chemical reactions and flows of particles and heat. The fluctuation–dissipation theorem is the basic knowledge obtained from applying non-equilibrium statistical mechanics to study the simplest non-equilibrium situation of a steady state current flow in a system of many particles.

History
In 1738, Swiss physicist and mathematician Daniel Bernoulli published Hydrodynamica which laid the basis for the kinetic theory of gases. In this work, Bernoulli posited the argument, still used to this day, that gases consist of great numbers of molecules moving in all directions, that their impact on a surface causes the gas pressure that we feel, and that what we experience as heat is simply the kinetic energy of their motion.In 1859, after reading a paper on the diffusion of molecules by Rudolf Clausius, Scottish physicist James Clerk Maxwell formulated the Maxwell distribution of molecular velocities, which gave the proportion of molecules having a certain velocity in a specific range. This was the first-ever statistical law in physics."
Who developed the fundamental interpretation of entropy?,Ludwig Boltzmann,James Clerk Maxwell,Josiah Willard Gibbs,Rudolf Clausius,Daniel Bernoulli,A,"In physics, statistical mechanics is a mathematical framework that applies statistical methods and probability theory to large assemblies of microscopic entities. It does not assume or postulate any natural laws, but explains the macroscopic behavior of nature from the behavior of such ensembles.
Sometimes called statistical physics or statistical thermodynamics, its applications include many problems in the fields of physics, biology, chemistry, and neuroscience. Its main purpose is to clarify the properties of matter in aggregate, in terms of physical laws governing atomic motion.Statistical mechanics arose out of the development of classical thermodynamics, a field for which it was successful in explaining macroscopic physical properties—such as temperature, pressure, and heat capacity—in terms of microscopic parameters that fluctuate about average values and are characterized by probability distributions.
The founding of the field of statistical mechanics is generally credited to three physicists:

Ludwig Boltzmann, who developed the fundamental interpretation of entropy in terms of a collection of microstates
James Clerk Maxwell, who developed models of probability distribution of such states
Josiah Willard Gibbs, who coined the name of the field in 1884While classical thermodynamics is primarily concerned with thermodynamic equilibrium, statistical mechanics has been applied in non-equilibrium statistical mechanics to the issues of microscopically modeling the speed of irreversible processes that are driven by imbalances. Examples of such processes include chemical reactions and flows of particles and heat. The fluctuation–dissipation theorem is the basic knowledge obtained from applying non-equilibrium statistical mechanics to study the simplest non-equilibrium situation of a steady state current flow in a system of many particles.

History
In 1738, Swiss physicist and mathematician Daniel Bernoulli published Hydrodynamica which laid the basis for the kinetic theory of gases. In this work, Bernoulli posited the argument, still used to this day, that gases consist of great numbers of molecules moving in all directions, that their impact on a surface causes the gas pressure that we feel, and that what we experience as heat is simply the kinetic energy of their motion.In 1859, after reading a paper on the diffusion of molecules by Rudolf Clausius, Scottish physicist James Clerk Maxwell formulated the Maxwell distribution of molecular velocities, which gave the proportion of molecules having a certain velocity in a specific range. This was the first-ever statistical law in physics."
Which physicist formulated the Maxwell distribution of molecular velocities?,Ludwig Boltzmann,James Clerk Maxwell,Josiah Willard Gibbs,Rudolf Clausius,Daniel Bernoulli,B,"In physics, statistical mechanics is a mathematical framework that applies statistical methods and probability theory to large assemblies of microscopic entities. It does not assume or postulate any natural laws, but explains the macroscopic behavior of nature from the behavior of such ensembles.
Sometimes called statistical physics or statistical thermodynamics, its applications include many problems in the fields of physics, biology, chemistry, and neuroscience. Its main purpose is to clarify the properties of matter in aggregate, in terms of physical laws governing atomic motion.Statistical mechanics arose out of the development of classical thermodynamics, a field for which it was successful in explaining macroscopic physical properties—such as temperature, pressure, and heat capacity—in terms of microscopic parameters that fluctuate about average values and are characterized by probability distributions.
The founding of the field of statistical mechanics is generally credited to three physicists:

Ludwig Boltzmann, who developed the fundamental interpretation of entropy in terms of a collection of microstates
James Clerk Maxwell, who developed models of probability distribution of such states
Josiah Willard Gibbs, who coined the name of the field in 1884While classical thermodynamics is primarily concerned with thermodynamic equilibrium, statistical mechanics has been applied in non-equilibrium statistical mechanics to the issues of microscopically modeling the speed of irreversible processes that are driven by imbalances. Examples of such processes include chemical reactions and flows of particles and heat. The fluctuation–dissipation theorem is the basic knowledge obtained from applying non-equilibrium statistical mechanics to study the simplest non-equilibrium situation of a steady state current flow in a system of many particles.

History
In 1738, Swiss physicist and mathematician Daniel Bernoulli published Hydrodynamica which laid the basis for the kinetic theory of gases. In this work, Bernoulli posited the argument, still used to this day, that gases consist of great numbers of molecules moving in all directions, that their impact on a surface causes the gas pressure that we feel, and that what we experience as heat is simply the kinetic energy of their motion.In 1859, after reading a paper on the diffusion of molecules by Rudolf Clausius, Scottish physicist James Clerk Maxwell formulated the Maxwell distribution of molecular velocities, which gave the proportion of molecules having a certain velocity in a specific range. This was the first-ever statistical law in physics."
What is the name of the field coined by Josiah Willard Gibbs in 1884?,Statistical mechanics,Statistical physics,Statistical thermodynamics,Quantum mechanics,Classical thermodynamics,A,"In physics, statistical mechanics is a mathematical framework that applies statistical methods and probability theory to large assemblies of microscopic entities. It does not assume or postulate any natural laws, but explains the macroscopic behavior of nature from the behavior of such ensembles.
Sometimes called statistical physics or statistical thermodynamics, its applications include many problems in the fields of physics, biology, chemistry, and neuroscience. Its main purpose is to clarify the properties of matter in aggregate, in terms of physical laws governing atomic motion.Statistical mechanics arose out of the development of classical thermodynamics, a field for which it was successful in explaining macroscopic physical properties—such as temperature, pressure, and heat capacity—in terms of microscopic parameters that fluctuate about average values and are characterized by probability distributions.
The founding of the field of statistical mechanics is generally credited to three physicists:

Ludwig Boltzmann, who developed the fundamental interpretation of entropy in terms of a collection of microstates
James Clerk Maxwell, who developed models of probability distribution of such states
Josiah Willard Gibbs, who coined the name of the field in 1884While classical thermodynamics is primarily concerned with thermodynamic equilibrium, statistical mechanics has been applied in non-equilibrium statistical mechanics to the issues of microscopically modeling the speed of irreversible processes that are driven by imbalances. Examples of such processes include chemical reactions and flows of particles and heat. The fluctuation–dissipation theorem is the basic knowledge obtained from applying non-equilibrium statistical mechanics to study the simplest non-equilibrium situation of a steady state current flow in a system of many particles.

History
In 1738, Swiss physicist and mathematician Daniel Bernoulli published Hydrodynamica which laid the basis for the kinetic theory of gases. In this work, Bernoulli posited the argument, still used to this day, that gases consist of great numbers of molecules moving in all directions, that their impact on a surface causes the gas pressure that we feel, and that what we experience as heat is simply the kinetic energy of their motion.In 1859, after reading a paper on the diffusion of molecules by Rudolf Clausius, Scottish physicist James Clerk Maxwell formulated the Maxwell distribution of molecular velocities, which gave the proportion of molecules having a certain velocity in a specific range. This was the first-ever statistical law in physics."
What did Ludwig Boltzmann develop in terms of entropy?,A collection of microstates,Models of probability distribution,The foundations of statistical mechanics,The kinetic theory of gases,The Maxwell distribution of molecular velocities,A,"In physics, statistical mechanics is a mathematical framework that applies statistical methods and probability theory to large assemblies of microscopic entities. It does not assume or postulate any natural laws, but explains the macroscopic behavior of nature from the behavior of such ensembles.
Sometimes called statistical physics or statistical thermodynamics, its applications include many problems in the fields of physics, biology, chemistry, and neuroscience. Its main purpose is to clarify the properties of matter in aggregate, in terms of physical laws governing atomic motion.Statistical mechanics arose out of the development of classical thermodynamics, a field for which it was successful in explaining macroscopic physical properties—such as temperature, pressure, and heat capacity—in terms of microscopic parameters that fluctuate about average values and are characterized by probability distributions.
The founding of the field of statistical mechanics is generally credited to three physicists:

Ludwig Boltzmann, who developed the fundamental interpretation of entropy in terms of a collection of microstates
James Clerk Maxwell, who developed models of probability distribution of such states
Josiah Willard Gibbs, who coined the name of the field in 1884While classical thermodynamics is primarily concerned with thermodynamic equilibrium, statistical mechanics has been applied in non-equilibrium statistical mechanics to the issues of microscopically modeling the speed of irreversible processes that are driven by imbalances. Examples of such processes include chemical reactions and flows of particles and heat. The fluctuation–dissipation theorem is the basic knowledge obtained from applying non-equilibrium statistical mechanics to study the simplest non-equilibrium situation of a steady state current flow in a system of many particles.

History
In 1738, Swiss physicist and mathematician Daniel Bernoulli published Hydrodynamica which laid the basis for the kinetic theory of gases. In this work, Bernoulli posited the argument, still used to this day, that gases consist of great numbers of molecules moving in all directions, that their impact on a surface causes the gas pressure that we feel, and that what we experience as heat is simply the kinetic energy of their motion.In 1859, after reading a paper on the diffusion of molecules by Rudolf Clausius, Scottish physicist James Clerk Maxwell formulated the Maxwell distribution of molecular velocities, which gave the proportion of molecules having a certain velocity in a specific range. This was the first-ever statistical law in physics."
What is intelligent automation?,A combination of AI and RPA used to replace workers,A technology used to build self-driving cars,A concept identified as one of the top technology trends of 2020,A software that improves efficiency in digital business processes,A technology used to automate the workflow behind distributing Covid-19 vaccines,A,"Intelligent automation, or alternately intelligent process automation, is a software term that refers to a combination of artificial intelligence (AI) and robotic process automation (RPA). Companies use intelligent automation to cut costs by using artificial-intelligence-powered robotic software to replace workers who handle repetitive tasks. The term is similar to hyperautomation, a concept identified by research group Gartner as being one of the top technology trends of 2020.

Technology
Intelligent automation applies the assembly line concept of breaking tasks into repetitive steps to digital business processes.  Rather than having humans do each step, intelligent automation replaces each step with an intelligent software robot or bot, improving efficiency.

Applications
The technology is used to process unstructured content.  Common applications include self-driving cars, self-checkouts at grocery stores, smart home assistants, and appliances. Businesses can apply data and machine learning to build predictive analytics that react to consumer behavior changes, or to implement RPA to improve manufacturing floor operations.The technology has also been used to automate the workflow behind distributing Covid-19 vaccines.  Data provided by hospital systems’ electronic health records can be processed to identify and educate patients, and schedule vaccinations.Intelligent Automation can provide real-time insights on profitability and efficiency."
How does intelligent automation improve efficiency in business processes?,By applying the assembly line concept,By using machine learning to build predictive analytics,By replacing humans with software robots,By automating the workflow behind distributing Covid-19 vaccines,By providing real-time insights on profitability and efficiency,C,"Intelligent automation, or alternately intelligent process automation, is a software term that refers to a combination of artificial intelligence (AI) and robotic process automation (RPA). Companies use intelligent automation to cut costs by using artificial-intelligence-powered robotic software to replace workers who handle repetitive tasks. The term is similar to hyperautomation, a concept identified by research group Gartner as being one of the top technology trends of 2020.

Technology
Intelligent automation applies the assembly line concept of breaking tasks into repetitive steps to digital business processes.  Rather than having humans do each step, intelligent automation replaces each step with an intelligent software robot or bot, improving efficiency.

Applications
The technology is used to process unstructured content.  Common applications include self-driving cars, self-checkouts at grocery stores, smart home assistants, and appliances. Businesses can apply data and machine learning to build predictive analytics that react to consumer behavior changes, or to implement RPA to improve manufacturing floor operations.The technology has also been used to automate the workflow behind distributing Covid-19 vaccines.  Data provided by hospital systems’ electronic health records can be processed to identify and educate patients, and schedule vaccinations.Intelligent Automation can provide real-time insights on profitability and efficiency."
What are some common applications of intelligent automation?,Self-driving cars and smart home assistants,Self-checkouts at grocery stores and appliances,Implementing RPA to improve manufacturing floor operations,Distributing Covid-19 vaccines,All of the above,E,"Intelligent automation, or alternately intelligent process automation, is a software term that refers to a combination of artificial intelligence (AI) and robotic process automation (RPA). Companies use intelligent automation to cut costs by using artificial-intelligence-powered robotic software to replace workers who handle repetitive tasks. The term is similar to hyperautomation, a concept identified by research group Gartner as being one of the top technology trends of 2020.

Technology
Intelligent automation applies the assembly line concept of breaking tasks into repetitive steps to digital business processes.  Rather than having humans do each step, intelligent automation replaces each step with an intelligent software robot or bot, improving efficiency.

Applications
The technology is used to process unstructured content.  Common applications include self-driving cars, self-checkouts at grocery stores, smart home assistants, and appliances. Businesses can apply data and machine learning to build predictive analytics that react to consumer behavior changes, or to implement RPA to improve manufacturing floor operations.The technology has also been used to automate the workflow behind distributing Covid-19 vaccines.  Data provided by hospital systems’ electronic health records can be processed to identify and educate patients, and schedule vaccinations.Intelligent Automation can provide real-time insights on profitability and efficiency."
How is data and machine learning used in intelligent automation?,To automate the workflow behind distributing Covid-19 vaccines,To improve manufacturing floor operations,To build predictive analytics that react to consumer behavior changes,To provide real-time insights on profitability and efficiency,None of the above,C,"Intelligent automation, or alternately intelligent process automation, is a software term that refers to a combination of artificial intelligence (AI) and robotic process automation (RPA). Companies use intelligent automation to cut costs by using artificial-intelligence-powered robotic software to replace workers who handle repetitive tasks. The term is similar to hyperautomation, a concept identified by research group Gartner as being one of the top technology trends of 2020.

Technology
Intelligent automation applies the assembly line concept of breaking tasks into repetitive steps to digital business processes.  Rather than having humans do each step, intelligent automation replaces each step with an intelligent software robot or bot, improving efficiency.

Applications
The technology is used to process unstructured content.  Common applications include self-driving cars, self-checkouts at grocery stores, smart home assistants, and appliances. Businesses can apply data and machine learning to build predictive analytics that react to consumer behavior changes, or to implement RPA to improve manufacturing floor operations.The technology has also been used to automate the workflow behind distributing Covid-19 vaccines.  Data provided by hospital systems’ electronic health records can be processed to identify and educate patients, and schedule vaccinations.Intelligent Automation can provide real-time insights on profitability and efficiency."
What can intelligent automation provide insights on?,Consumer behavior changes,Profitability and efficiency,Manufacturing floor operations,Covid-19 vaccine distribution,None of the above,B,"Intelligent automation, or alternately intelligent process automation, is a software term that refers to a combination of artificial intelligence (AI) and robotic process automation (RPA). Companies use intelligent automation to cut costs by using artificial-intelligence-powered robotic software to replace workers who handle repetitive tasks. The term is similar to hyperautomation, a concept identified by research group Gartner as being one of the top technology trends of 2020.

Technology
Intelligent automation applies the assembly line concept of breaking tasks into repetitive steps to digital business processes.  Rather than having humans do each step, intelligent automation replaces each step with an intelligent software robot or bot, improving efficiency.

Applications
The technology is used to process unstructured content.  Common applications include self-driving cars, self-checkouts at grocery stores, smart home assistants, and appliances. Businesses can apply data and machine learning to build predictive analytics that react to consumer behavior changes, or to implement RPA to improve manufacturing floor operations.The technology has also been used to automate the workflow behind distributing Covid-19 vaccines.  Data provided by hospital systems’ electronic health records can be processed to identify and educate patients, and schedule vaccinations.Intelligent Automation can provide real-time insights on profitability and efficiency."
What is the purpose of prescriptive analytics?,To analyze past performance,To suggest decision options for the future,To understand the reasons behind past success or failure,To predict future events,To enable an enterprise to consider the best course of action,E,"Prescriptive analytics is a form of business analytics which suggests decision options for how to take advantage of a future opportunity or mitigate a future risk, and shows the implication of each decision option. It enables an enterprise to consider ""the best course of action to take"" in the light of information derived from descriptive and predictive analytics.

Overview
Prescriptive analytics is the third and final phase of business analytics, which also includes descriptive and predictive analytics. Referred to as the ""final frontier of analytic capabilities"", prescriptive analytics entails the application of mathematical and computational sciences and suggests decision options for how to take advantage of the results of descriptive and predictive phases. 
The first stage of business analytics is descriptive analytics, which still accounts for the majority of all business analytics today. Descriptive analytics looks at past performance and understands that performance by mining historical data to look for the reasons behind past success or failure. Most management reporting – such as sales, marketing, operations, and finance – uses this type of post-mortem analysis.

The next phase is predictive analytics. Predictive analytics answers the question of what is likely to happen."
Which phase of business analytics is prescriptive analytics?,The first phase,The second phase,The third and final phase,The post-mortem analysis phase,The predictive analytics phase,C,"Prescriptive analytics is a form of business analytics which suggests decision options for how to take advantage of a future opportunity or mitigate a future risk, and shows the implication of each decision option. It enables an enterprise to consider ""the best course of action to take"" in the light of information derived from descriptive and predictive analytics.

Overview
Prescriptive analytics is the third and final phase of business analytics, which also includes descriptive and predictive analytics. Referred to as the ""final frontier of analytic capabilities"", prescriptive analytics entails the application of mathematical and computational sciences and suggests decision options for how to take advantage of the results of descriptive and predictive phases. 
The first stage of business analytics is descriptive analytics, which still accounts for the majority of all business analytics today. Descriptive analytics looks at past performance and understands that performance by mining historical data to look for the reasons behind past success or failure. Most management reporting – such as sales, marketing, operations, and finance – uses this type of post-mortem analysis.

The next phase is predictive analytics. Predictive analytics answers the question of what is likely to happen."
What does descriptive analytics focus on?,Analyzing past performance,Predicting future events,Suggesting decision options for the future,Understanding the reasons behind past success or failure,Enabling an enterprise to consider the best course of action,A,"Prescriptive analytics is a form of business analytics which suggests decision options for how to take advantage of a future opportunity or mitigate a future risk, and shows the implication of each decision option. It enables an enterprise to consider ""the best course of action to take"" in the light of information derived from descriptive and predictive analytics.

Overview
Prescriptive analytics is the third and final phase of business analytics, which also includes descriptive and predictive analytics. Referred to as the ""final frontier of analytic capabilities"", prescriptive analytics entails the application of mathematical and computational sciences and suggests decision options for how to take advantage of the results of descriptive and predictive phases. 
The first stage of business analytics is descriptive analytics, which still accounts for the majority of all business analytics today. Descriptive analytics looks at past performance and understands that performance by mining historical data to look for the reasons behind past success or failure. Most management reporting – such as sales, marketing, operations, and finance – uses this type of post-mortem analysis.

The next phase is predictive analytics. Predictive analytics answers the question of what is likely to happen."
What is the main purpose of predictive analytics?,To analyze past performance,To suggest decision options for the future,To understand the reasons behind past success or failure,To predict future events,To enable an enterprise to consider the best course of action,D,"Prescriptive analytics is a form of business analytics which suggests decision options for how to take advantage of a future opportunity or mitigate a future risk, and shows the implication of each decision option. It enables an enterprise to consider ""the best course of action to take"" in the light of information derived from descriptive and predictive analytics.

Overview
Prescriptive analytics is the third and final phase of business analytics, which also includes descriptive and predictive analytics. Referred to as the ""final frontier of analytic capabilities"", prescriptive analytics entails the application of mathematical and computational sciences and suggests decision options for how to take advantage of the results of descriptive and predictive phases. 
The first stage of business analytics is descriptive analytics, which still accounts for the majority of all business analytics today. Descriptive analytics looks at past performance and understands that performance by mining historical data to look for the reasons behind past success or failure. Most management reporting – such as sales, marketing, operations, and finance – uses this type of post-mortem analysis.

The next phase is predictive analytics. Predictive analytics answers the question of what is likely to happen."
Which phase of business analytics looks for the reasons behind past success or failure?,Prescriptive analytics,Descriptive analytics,Predictive analytics,Post-mortem analysis,"Sales, marketing, operations, and finance",B,"Prescriptive analytics is a form of business analytics which suggests decision options for how to take advantage of a future opportunity or mitigate a future risk, and shows the implication of each decision option. It enables an enterprise to consider ""the best course of action to take"" in the light of information derived from descriptive and predictive analytics.

Overview
Prescriptive analytics is the third and final phase of business analytics, which also includes descriptive and predictive analytics. Referred to as the ""final frontier of analytic capabilities"", prescriptive analytics entails the application of mathematical and computational sciences and suggests decision options for how to take advantage of the results of descriptive and predictive phases. 
The first stage of business analytics is descriptive analytics, which still accounts for the majority of all business analytics today. Descriptive analytics looks at past performance and understands that performance by mining historical data to look for the reasons behind past success or failure. Most management reporting – such as sales, marketing, operations, and finance – uses this type of post-mortem analysis.

The next phase is predictive analytics. Predictive analytics answers the question of what is likely to happen."
What is Diophantine geometry?,The study of equations in algebraic geometry,The study of number theory,The study of calculus,The study of differential equations,The study of topology,A,"In mathematics, Diophantine geometry is the study of Diophantine equations by means of powerful methods in algebraic geometry. By the 20th century it became clear for some mathematicians that methods of algebraic geometry are ideal tools to study these equations. Diophantine geometry is part of the broader field of arithmetic geometry.
Four theorems in Diophantine geometry which are of fundamental importance include:
Mordell–Weil theorem
Roth's theorem
Siegel's theorem
Faltings's theorem

Background
Serge Lang published a book Diophantine Geometry in the area in 1962, and by this book he coined the term ""Diophantine Geometry"". The traditional arrangement of material on Diophantine equations was by degree and number of variables, as in Mordell's Diophantine Equations (1969). Mordell's book starts with a remark on homogeneous equations f = 0 over the rational field, attributed to C. F. Gauss, that non-zero solutions in integers (even primitive lattice points) exist if non-zero rational solutions do, and notes a caveat of L."
"Who coined the term ""Diophantine Geometry""?",Serge Lang,Mordell,C. F. Gauss,L.,Roth,A,"In mathematics, Diophantine geometry is the study of Diophantine equations by means of powerful methods in algebraic geometry. By the 20th century it became clear for some mathematicians that methods of algebraic geometry are ideal tools to study these equations. Diophantine geometry is part of the broader field of arithmetic geometry.
Four theorems in Diophantine geometry which are of fundamental importance include:
Mordell–Weil theorem
Roth's theorem
Siegel's theorem
Faltings's theorem

Background
Serge Lang published a book Diophantine Geometry in the area in 1962, and by this book he coined the term ""Diophantine Geometry"". The traditional arrangement of material on Diophantine equations was by degree and number of variables, as in Mordell's Diophantine Equations (1969). Mordell's book starts with a remark on homogeneous equations f = 0 over the rational field, attributed to C. F. Gauss, that non-zero solutions in integers (even primitive lattice points) exist if non-zero rational solutions do, and notes a caveat of L."
What are the four theorems in Diophantine geometry of fundamental importance?,"Mordell–Weil theorem, Roth's theorem, Siegel's theorem, Faltings's theorem","Gauss's theorem, Lang's theorem, Mordell's theorem, Roth's theorem","Riemann's theorem, Euler's theorem, Fermat's theorem, Pythagorean theorem","Einstein's theorem, Newton's theorem, Pythagorean theorem, Pascal's theorem","Siegel's theorem, Fermat's theorem, Riemann's theorem, Faltings's theorem",A,"In mathematics, Diophantine geometry is the study of Diophantine equations by means of powerful methods in algebraic geometry. By the 20th century it became clear for some mathematicians that methods of algebraic geometry are ideal tools to study these equations. Diophantine geometry is part of the broader field of arithmetic geometry.
Four theorems in Diophantine geometry which are of fundamental importance include:
Mordell–Weil theorem
Roth's theorem
Siegel's theorem
Faltings's theorem

Background
Serge Lang published a book Diophantine Geometry in the area in 1962, and by this book he coined the term ""Diophantine Geometry"". The traditional arrangement of material on Diophantine equations was by degree and number of variables, as in Mordell's Diophantine Equations (1969). Mordell's book starts with a remark on homogeneous equations f = 0 over the rational field, attributed to C. F. Gauss, that non-zero solutions in integers (even primitive lattice points) exist if non-zero rational solutions do, and notes a caveat of L."
What field is Diophantine geometry part of?,Algebraic geometry,Number theory,Calculus,Differential equations,Topology,A,"In mathematics, Diophantine geometry is the study of Diophantine equations by means of powerful methods in algebraic geometry. By the 20th century it became clear for some mathematicians that methods of algebraic geometry are ideal tools to study these equations. Diophantine geometry is part of the broader field of arithmetic geometry.
Four theorems in Diophantine geometry which are of fundamental importance include:
Mordell–Weil theorem
Roth's theorem
Siegel's theorem
Faltings's theorem

Background
Serge Lang published a book Diophantine Geometry in the area in 1962, and by this book he coined the term ""Diophantine Geometry"". The traditional arrangement of material on Diophantine equations was by degree and number of variables, as in Mordell's Diophantine Equations (1969). Mordell's book starts with a remark on homogeneous equations f = 0 over the rational field, attributed to C. F. Gauss, that non-zero solutions in integers (even primitive lattice points) exist if non-zero rational solutions do, and notes a caveat of L."
What book did Serge Lang publish in 1962?,Mordell's Diophantine Equations,Diophantine Geometry,Gauss's Diophantine Equations,Lang's Arithmetic Geometry,Roth's Theorem,B,"In mathematics, Diophantine geometry is the study of Diophantine equations by means of powerful methods in algebraic geometry. By the 20th century it became clear for some mathematicians that methods of algebraic geometry are ideal tools to study these equations. Diophantine geometry is part of the broader field of arithmetic geometry.
Four theorems in Diophantine geometry which are of fundamental importance include:
Mordell–Weil theorem
Roth's theorem
Siegel's theorem
Faltings's theorem

Background
Serge Lang published a book Diophantine Geometry in the area in 1962, and by this book he coined the term ""Diophantine Geometry"". The traditional arrangement of material on Diophantine equations was by degree and number of variables, as in Mordell's Diophantine Equations (1969). Mordell's book starts with a remark on homogeneous equations f = 0 over the rational field, attributed to C. F. Gauss, that non-zero solutions in integers (even primitive lattice points) exist if non-zero rational solutions do, and notes a caveat of L."
Where is Envigo headquartered?,"Los Angeles, California","Indianapolis, Indiana","New York City, New York","Chicago, Illinois","Houston, Texas",B,"Envigo (en-VEE-go) is a privately held contract research organization and laboratory animal sourcer  that provides live animals and related products and services to pharmaceutical and biotechnology industries, government, academia and other life science organizations engaged in animal testing. The company breeds and sells research animals – which are referred to in the industry as ""research models""– including rodents (mice, rats, hamsters and guinea pigs), rabbits, beagles and non-human primates. Envigo is headquartered in Indianapolis, Indiana and employs more than 1,200 people at 30+ locations across North America, Europe and the Middle East.Many of Envigo's breeding sites have been accredited by the Association for Assessment and Accreditation of Laboratory Animal Care (AAALAC) International Council including: Horst, the Netherlands (March 29, 2016), Indianapolis, IN, Livermore, CA, Boyertown and Denver, PA, St Louis, MO and others. However, the U.S. Department of Agriculture (USDA) found multiple violations of the Animal Welfare Act at the company's Cumberland, Virginia dog-breading facility and the U.S. Department of Justice filed a complaint against Envigo for the violations in federal court. In a settlement reached in July 2022, the company agreed to no longer engage in activities at the site that require an Animal Welfare Act license and to relinquish the over 4000 beagles at the facility to the Humane Society of the United States.

History
Envigo was created in September 2015 when Huntingdon Life Sciences, Harlan Laboratories and three subsidiaries (GFA, NDA Analytics and LSR Associates) merged."
How many people does Envigo employ?,500 people,800 people,"1,200 people","1,500 people","2,000 people",C,"Envigo (en-VEE-go) is a privately held contract research organization and laboratory animal sourcer  that provides live animals and related products and services to pharmaceutical and biotechnology industries, government, academia and other life science organizations engaged in animal testing. The company breeds and sells research animals – which are referred to in the industry as ""research models""– including rodents (mice, rats, hamsters and guinea pigs), rabbits, beagles and non-human primates. Envigo is headquartered in Indianapolis, Indiana and employs more than 1,200 people at 30+ locations across North America, Europe and the Middle East.Many of Envigo's breeding sites have been accredited by the Association for Assessment and Accreditation of Laboratory Animal Care (AAALAC) International Council including: Horst, the Netherlands (March 29, 2016), Indianapolis, IN, Livermore, CA, Boyertown and Denver, PA, St Louis, MO and others. However, the U.S. Department of Agriculture (USDA) found multiple violations of the Animal Welfare Act at the company's Cumberland, Virginia dog-breading facility and the U.S. Department of Justice filed a complaint against Envigo for the violations in federal court. In a settlement reached in July 2022, the company agreed to no longer engage in activities at the site that require an Animal Welfare Act license and to relinquish the over 4000 beagles at the facility to the Humane Society of the United States.

History
Envigo was created in September 2015 when Huntingdon Life Sciences, Harlan Laboratories and three subsidiaries (GFA, NDA Analytics and LSR Associates) merged."
Which organization accredited many of Envigo's breeding sites?,National Institutes of Health (NIH),Association for Assessment and Accreditation of Laboratory Animal Care (AAALAC),Food and Drug Administration (FDA),World Health Organization (WHO),Environmental Protection Agency (EPA),B,"Envigo (en-VEE-go) is a privately held contract research organization and laboratory animal sourcer  that provides live animals and related products and services to pharmaceutical and biotechnology industries, government, academia and other life science organizations engaged in animal testing. The company breeds and sells research animals – which are referred to in the industry as ""research models""– including rodents (mice, rats, hamsters and guinea pigs), rabbits, beagles and non-human primates. Envigo is headquartered in Indianapolis, Indiana and employs more than 1,200 people at 30+ locations across North America, Europe and the Middle East.Many of Envigo's breeding sites have been accredited by the Association for Assessment and Accreditation of Laboratory Animal Care (AAALAC) International Council including: Horst, the Netherlands (March 29, 2016), Indianapolis, IN, Livermore, CA, Boyertown and Denver, PA, St Louis, MO and others. However, the U.S. Department of Agriculture (USDA) found multiple violations of the Animal Welfare Act at the company's Cumberland, Virginia dog-breading facility and the U.S. Department of Justice filed a complaint against Envigo for the violations in federal court. In a settlement reached in July 2022, the company agreed to no longer engage in activities at the site that require an Animal Welfare Act license and to relinquish the over 4000 beagles at the facility to the Humane Society of the United States.

History
Envigo was created in September 2015 when Huntingdon Life Sciences, Harlan Laboratories and three subsidiaries (GFA, NDA Analytics and LSR Associates) merged."
Which facility of Envigo had multiple violations of the Animal Welfare Act?,"Horst, the Netherlands","Livermore, CA","Cumberland, Virginia","St Louis, MO","Indianapolis, IN",C,"Envigo (en-VEE-go) is a privately held contract research organization and laboratory animal sourcer  that provides live animals and related products and services to pharmaceutical and biotechnology industries, government, academia and other life science organizations engaged in animal testing. The company breeds and sells research animals – which are referred to in the industry as ""research models""– including rodents (mice, rats, hamsters and guinea pigs), rabbits, beagles and non-human primates. Envigo is headquartered in Indianapolis, Indiana and employs more than 1,200 people at 30+ locations across North America, Europe and the Middle East.Many of Envigo's breeding sites have been accredited by the Association for Assessment and Accreditation of Laboratory Animal Care (AAALAC) International Council including: Horst, the Netherlands (March 29, 2016), Indianapolis, IN, Livermore, CA, Boyertown and Denver, PA, St Louis, MO and others. However, the U.S. Department of Agriculture (USDA) found multiple violations of the Animal Welfare Act at the company's Cumberland, Virginia dog-breading facility and the U.S. Department of Justice filed a complaint against Envigo for the violations in federal court. In a settlement reached in July 2022, the company agreed to no longer engage in activities at the site that require an Animal Welfare Act license and to relinquish the over 4000 beagles at the facility to the Humane Society of the United States.

History
Envigo was created in September 2015 when Huntingdon Life Sciences, Harlan Laboratories and three subsidiaries (GFA, NDA Analytics and LSR Associates) merged."
"What happened to the beagles at Envigo's Cumberland, Virginia facility?",They were released into the wild.,They were transferred to another facility.,They were sold to pharmaceutical companies.,They were euthanized.,They were adopted by employees.,B,"Envigo (en-VEE-go) is a privately held contract research organization and laboratory animal sourcer  that provides live animals and related products and services to pharmaceutical and biotechnology industries, government, academia and other life science organizations engaged in animal testing. The company breeds and sells research animals – which are referred to in the industry as ""research models""– including rodents (mice, rats, hamsters and guinea pigs), rabbits, beagles and non-human primates. Envigo is headquartered in Indianapolis, Indiana and employs more than 1,200 people at 30+ locations across North America, Europe and the Middle East.Many of Envigo's breeding sites have been accredited by the Association for Assessment and Accreditation of Laboratory Animal Care (AAALAC) International Council including: Horst, the Netherlands (March 29, 2016), Indianapolis, IN, Livermore, CA, Boyertown and Denver, PA, St Louis, MO and others. However, the U.S. Department of Agriculture (USDA) found multiple violations of the Animal Welfare Act at the company's Cumberland, Virginia dog-breading facility and the U.S. Department of Justice filed a complaint against Envigo for the violations in federal court. In a settlement reached in July 2022, the company agreed to no longer engage in activities at the site that require an Animal Welfare Act license and to relinquish the over 4000 beagles at the facility to the Humane Society of the United States.

History
Envigo was created in September 2015 when Huntingdon Life Sciences, Harlan Laboratories and three subsidiaries (GFA, NDA Analytics and LSR Associates) merged."
What is the purpose of Integrated information theory (IIT)?,To explain why some physical systems are conscious,To describe the causal properties of physical systems,To test the consciousness of other animals,To determine the degree of consciousness in physical systems,To examine the conscious experience of the whole Universe,A,"Integrated information theory (IIT) attempts to provide a framework capable of explaining why some physical systems (such as human brains) are conscious, why they feel the particular way they do in particular states (e.g. why our visual field appears extended when we gaze out at the night sky), and what it would take for other physical systems to be conscious (Are other animals conscious? Might the whole Universe be?). In principle, once the theory is mature and has been tested extensively in controlled conditions, the IIT framework may be capable of providing a concrete inference about whether any physical system is conscious, to what degree it is conscious, and what particular experience it is having. In IIT, a system's consciousness (what it is like subjectively) is conjectured to be identical to its causal properties (what it is like objectively). Therefore it should be possible to account for the conscious experience of a physical system by unfolding its complete causal powers (see Central identity).IIT was proposed by neuroscientist Giulio Tononi in 2004. The latest version of the theory, labeled IIT 3.0, was published in 2014. However, the theory is still in development, as is evident from the later publications improving on the formalism presented in IIT 3.0.

Overview
Relationship to the ""hard problem of consciousness""
David Chalmers has argued that any attempt to explain consciousness in purely physical terms (i.e."
What is the relationship between a system's consciousness and its causal properties in IIT?,They are unrelated,They are identical,They are inversely proportional,They are dependent on each other,They are partially related,B,"Integrated information theory (IIT) attempts to provide a framework capable of explaining why some physical systems (such as human brains) are conscious, why they feel the particular way they do in particular states (e.g. why our visual field appears extended when we gaze out at the night sky), and what it would take for other physical systems to be conscious (Are other animals conscious? Might the whole Universe be?). In principle, once the theory is mature and has been tested extensively in controlled conditions, the IIT framework may be capable of providing a concrete inference about whether any physical system is conscious, to what degree it is conscious, and what particular experience it is having. In IIT, a system's consciousness (what it is like subjectively) is conjectured to be identical to its causal properties (what it is like objectively). Therefore it should be possible to account for the conscious experience of a physical system by unfolding its complete causal powers (see Central identity).IIT was proposed by neuroscientist Giulio Tononi in 2004. The latest version of the theory, labeled IIT 3.0, was published in 2014. However, the theory is still in development, as is evident from the later publications improving on the formalism presented in IIT 3.0.

Overview
Relationship to the ""hard problem of consciousness""
David Chalmers has argued that any attempt to explain consciousness in purely physical terms (i.e."
Who proposed Integrated information theory (IIT)?,Giulio Tononi,David Chalmers,Neuroscientist,IIT 3.0,IIT framework,A,"Integrated information theory (IIT) attempts to provide a framework capable of explaining why some physical systems (such as human brains) are conscious, why they feel the particular way they do in particular states (e.g. why our visual field appears extended when we gaze out at the night sky), and what it would take for other physical systems to be conscious (Are other animals conscious? Might the whole Universe be?). In principle, once the theory is mature and has been tested extensively in controlled conditions, the IIT framework may be capable of providing a concrete inference about whether any physical system is conscious, to what degree it is conscious, and what particular experience it is having. In IIT, a system's consciousness (what it is like subjectively) is conjectured to be identical to its causal properties (what it is like objectively). Therefore it should be possible to account for the conscious experience of a physical system by unfolding its complete causal powers (see Central identity).IIT was proposed by neuroscientist Giulio Tononi in 2004. The latest version of the theory, labeled IIT 3.0, was published in 2014. However, the theory is still in development, as is evident from the later publications improving on the formalism presented in IIT 3.0.

Overview
Relationship to the ""hard problem of consciousness""
David Chalmers has argued that any attempt to explain consciousness in purely physical terms (i.e."
When was the latest version of IIT (labeled IIT 3.0) published?,2004,2014,2020,The theory is still in development,It was not published,B,"Integrated information theory (IIT) attempts to provide a framework capable of explaining why some physical systems (such as human brains) are conscious, why they feel the particular way they do in particular states (e.g. why our visual field appears extended when we gaze out at the night sky), and what it would take for other physical systems to be conscious (Are other animals conscious? Might the whole Universe be?). In principle, once the theory is mature and has been tested extensively in controlled conditions, the IIT framework may be capable of providing a concrete inference about whether any physical system is conscious, to what degree it is conscious, and what particular experience it is having. In IIT, a system's consciousness (what it is like subjectively) is conjectured to be identical to its causal properties (what it is like objectively). Therefore it should be possible to account for the conscious experience of a physical system by unfolding its complete causal powers (see Central identity).IIT was proposed by neuroscientist Giulio Tononi in 2004. The latest version of the theory, labeled IIT 3.0, was published in 2014. However, the theory is still in development, as is evident from the later publications improving on the formalism presented in IIT 3.0.

Overview
Relationship to the ""hard problem of consciousness""
David Chalmers has argued that any attempt to explain consciousness in purely physical terms (i.e."
What is the current status of Integrated information theory (IIT)?,It is widely accepted and extensively tested,It is a fully mature theory,It is still in development,It has been disproven,It is not applicable to physical systems,C,"Integrated information theory (IIT) attempts to provide a framework capable of explaining why some physical systems (such as human brains) are conscious, why they feel the particular way they do in particular states (e.g. why our visual field appears extended when we gaze out at the night sky), and what it would take for other physical systems to be conscious (Are other animals conscious? Might the whole Universe be?). In principle, once the theory is mature and has been tested extensively in controlled conditions, the IIT framework may be capable of providing a concrete inference about whether any physical system is conscious, to what degree it is conscious, and what particular experience it is having. In IIT, a system's consciousness (what it is like subjectively) is conjectured to be identical to its causal properties (what it is like objectively). Therefore it should be possible to account for the conscious experience of a physical system by unfolding its complete causal powers (see Central identity).IIT was proposed by neuroscientist Giulio Tononi in 2004. The latest version of the theory, labeled IIT 3.0, was published in 2014. However, the theory is still in development, as is evident from the later publications improving on the formalism presented in IIT 3.0.

Overview
Relationship to the ""hard problem of consciousness""
David Chalmers has argued that any attempt to explain consciousness in purely physical terms (i.e."
What are some essential parts of broadcast engineering?,Mechanical engineering and civil engineering,Audio engineering and RF engineering,Chemical engineering and computer engineering,Electrical engineering and computer science,Industrial engineering and software engineering,B,"Broadcast engineering is the field of electrical engineering, and now to some extent computer engineering and information technology, which deals with radio and television broadcasting.  Audio engineering and RF engineering are also essential parts of broadcast engineering, being their own subsets of electrical engineering.
Broadcast engineering involves both the studio and transmitter aspects (the entire airchain), as well as remote broadcasts.  Every station has a broadcast engineer, though one may now serve an entire station group in a city. In small media markets the engineer may work on a contract basis for one or more stations as needed.

Duties
Modern duties of a broadcast engineer include maintaining broadcast automation systems for the studio and automatic transmission systems for the transmitter plant.  There are also important duties regarding radio towers, which must be maintained with proper lighting and painting.  Occasionally a station's engineer must deal with complaints of RF interference, particularly after a station has made changes to its transmission facilities.

Titles
Broadcast engineers may have varying titles depending on their level of expertise and field specialty. Some widely used titles include:

Broadcast design engineer
Broadcast Integration Engineer
Broadcast systems engineer
Broadcast IT engineer
Broadcast IT systems engineer
Broadcast network engineer
Broadcast maintenance engineer
Video broadcast engineer
TV studio broadcast engineer
Outside broadcast engineer
Remote broadcast engineer

Qualifications
Broadcast engineers may need to possess some or all of the following degrees, depending on the broadcast technical environment."
What are the modern duties of a broadcast engineer?,Maintaining radio towers with proper lighting and painting,Designing broadcast automation systems for the studio,Dealing with complaints of RF interference,Working on a contract basis for one or more stations,Managing the entire airchain of a station,C,"Broadcast engineering is the field of electrical engineering, and now to some extent computer engineering and information technology, which deals with radio and television broadcasting.  Audio engineering and RF engineering are also essential parts of broadcast engineering, being their own subsets of electrical engineering.
Broadcast engineering involves both the studio and transmitter aspects (the entire airchain), as well as remote broadcasts.  Every station has a broadcast engineer, though one may now serve an entire station group in a city. In small media markets the engineer may work on a contract basis for one or more stations as needed.

Duties
Modern duties of a broadcast engineer include maintaining broadcast automation systems for the studio and automatic transmission systems for the transmitter plant.  There are also important duties regarding radio towers, which must be maintained with proper lighting and painting.  Occasionally a station's engineer must deal with complaints of RF interference, particularly after a station has made changes to its transmission facilities.

Titles
Broadcast engineers may have varying titles depending on their level of expertise and field specialty. Some widely used titles include:

Broadcast design engineer
Broadcast Integration Engineer
Broadcast systems engineer
Broadcast IT engineer
Broadcast IT systems engineer
Broadcast network engineer
Broadcast maintenance engineer
Video broadcast engineer
TV studio broadcast engineer
Outside broadcast engineer
Remote broadcast engineer

Qualifications
Broadcast engineers may need to possess some or all of the following degrees, depending on the broadcast technical environment."
What are some titles used for broadcast engineers?,Civil engineer and mechanical engineer,Broadcast integration engineer and broadcast maintenance engineer,Broadcast IT engineer and broadcast network engineer,Software engineer and industrial engineer,Broadcast systems engineer and video broadcast engineer,E,"Broadcast engineering is the field of electrical engineering, and now to some extent computer engineering and information technology, which deals with radio and television broadcasting.  Audio engineering and RF engineering are also essential parts of broadcast engineering, being their own subsets of electrical engineering.
Broadcast engineering involves both the studio and transmitter aspects (the entire airchain), as well as remote broadcasts.  Every station has a broadcast engineer, though one may now serve an entire station group in a city. In small media markets the engineer may work on a contract basis for one or more stations as needed.

Duties
Modern duties of a broadcast engineer include maintaining broadcast automation systems for the studio and automatic transmission systems for the transmitter plant.  There are also important duties regarding radio towers, which must be maintained with proper lighting and painting.  Occasionally a station's engineer must deal with complaints of RF interference, particularly after a station has made changes to its transmission facilities.

Titles
Broadcast engineers may have varying titles depending on their level of expertise and field specialty. Some widely used titles include:

Broadcast design engineer
Broadcast Integration Engineer
Broadcast systems engineer
Broadcast IT engineer
Broadcast IT systems engineer
Broadcast network engineer
Broadcast maintenance engineer
Video broadcast engineer
TV studio broadcast engineer
Outside broadcast engineer
Remote broadcast engineer

Qualifications
Broadcast engineers may need to possess some or all of the following degrees, depending on the broadcast technical environment."
What qualifications do broadcast engineers need?,Degrees in mechanical engineering and electrical engineering,Degrees in chemical engineering and computer science,Degrees in audio engineering and RF engineering,Degrees in industrial engineering and software engineering,Degrees in broadcast technical environment,A,"Broadcast engineering is the field of electrical engineering, and now to some extent computer engineering and information technology, which deals with radio and television broadcasting.  Audio engineering and RF engineering are also essential parts of broadcast engineering, being their own subsets of electrical engineering.
Broadcast engineering involves both the studio and transmitter aspects (the entire airchain), as well as remote broadcasts.  Every station has a broadcast engineer, though one may now serve an entire station group in a city. In small media markets the engineer may work on a contract basis for one or more stations as needed.

Duties
Modern duties of a broadcast engineer include maintaining broadcast automation systems for the studio and automatic transmission systems for the transmitter plant.  There are also important duties regarding radio towers, which must be maintained with proper lighting and painting.  Occasionally a station's engineer must deal with complaints of RF interference, particularly after a station has made changes to its transmission facilities.

Titles
Broadcast engineers may have varying titles depending on their level of expertise and field specialty. Some widely used titles include:

Broadcast design engineer
Broadcast Integration Engineer
Broadcast systems engineer
Broadcast IT engineer
Broadcast IT systems engineer
Broadcast network engineer
Broadcast maintenance engineer
Video broadcast engineer
TV studio broadcast engineer
Outside broadcast engineer
Remote broadcast engineer

Qualifications
Broadcast engineers may need to possess some or all of the following degrees, depending on the broadcast technical environment."
What aspects of broadcast engineering does a broadcast engineer deal with?,Only the studio aspect,Only the transmitter aspect,Both the studio and transmitter aspects,Only remote broadcasts,Only radio towers,C,"Broadcast engineering is the field of electrical engineering, and now to some extent computer engineering and information technology, which deals with radio and television broadcasting.  Audio engineering and RF engineering are also essential parts of broadcast engineering, being their own subsets of electrical engineering.
Broadcast engineering involves both the studio and transmitter aspects (the entire airchain), as well as remote broadcasts.  Every station has a broadcast engineer, though one may now serve an entire station group in a city. In small media markets the engineer may work on a contract basis for one or more stations as needed.

Duties
Modern duties of a broadcast engineer include maintaining broadcast automation systems for the studio and automatic transmission systems for the transmitter plant.  There are also important duties regarding radio towers, which must be maintained with proper lighting and painting.  Occasionally a station's engineer must deal with complaints of RF interference, particularly after a station has made changes to its transmission facilities.

Titles
Broadcast engineers may have varying titles depending on their level of expertise and field specialty. Some widely used titles include:

Broadcast design engineer
Broadcast Integration Engineer
Broadcast systems engineer
Broadcast IT engineer
Broadcast IT systems engineer
Broadcast network engineer
Broadcast maintenance engineer
Video broadcast engineer
TV studio broadcast engineer
Outside broadcast engineer
Remote broadcast engineer

Qualifications
Broadcast engineers may need to possess some or all of the following degrees, depending on the broadcast technical environment."
What is dance science?,The study of dance and dancers,The study of music and dance,The study of sports medicine,The study of psychology in dance,The study of dance choreography,A,"Dance science is the scientific study of dance and dancers, as well as the practical application of scientific principles to dance. Its aims are the enhancement of performance, the reduction of injury, and the improvement of well-being and health.

Overview
Dance medicine and science as a field of study developed in the 1970s and '80s out of the field of sports medicine. In the early 1980s, the American Dance Festival (ADF) started including dance medicine courses in their coursework for dancers. When ADF moved to Duke University, physicians from Duke University Hospital became interested in dancers. Then, in 1990, the International Association for Dance Medicine and Science (IADMS) was formed by an international group of dance medicine practitioners, dance educators, dance scientists, and dancers. Membership of IADMS began with 48 members in 1991, and has grown to over 900 members in 35 countries as of 2016.Dance science as an academic discipline has been evolving over the past 20 years. In the United Kingdom, three degrees (at master's level) now exist: one at the University of Bedfordshire, one at the University of Wolverhampton, and one at the Trinity Laban Conservatoire of Music and Dance in London."
What are the aims of dance science?,To enhance performance and reduce injury,To study the history of dance,To improve music in dance performances,To promote different dance styles,To analyze dance costumes,A,"Dance science is the scientific study of dance and dancers, as well as the practical application of scientific principles to dance. Its aims are the enhancement of performance, the reduction of injury, and the improvement of well-being and health.

Overview
Dance medicine and science as a field of study developed in the 1970s and '80s out of the field of sports medicine. In the early 1980s, the American Dance Festival (ADF) started including dance medicine courses in their coursework for dancers. When ADF moved to Duke University, physicians from Duke University Hospital became interested in dancers. Then, in 1990, the International Association for Dance Medicine and Science (IADMS) was formed by an international group of dance medicine practitioners, dance educators, dance scientists, and dancers. Membership of IADMS began with 48 members in 1991, and has grown to over 900 members in 35 countries as of 2016.Dance science as an academic discipline has been evolving over the past 20 years. In the United Kingdom, three degrees (at master's level) now exist: one at the University of Bedfordshire, one at the University of Wolverhampton, and one at the Trinity Laban Conservatoire of Music and Dance in London."
When did dance medicine and science start to develop?,1970s and 1980s,1960s and 1970s,1990s and 2000s,1980s and 1990s,1950s and 1960s,A,"Dance science is the scientific study of dance and dancers, as well as the practical application of scientific principles to dance. Its aims are the enhancement of performance, the reduction of injury, and the improvement of well-being and health.

Overview
Dance medicine and science as a field of study developed in the 1970s and '80s out of the field of sports medicine. In the early 1980s, the American Dance Festival (ADF) started including dance medicine courses in their coursework for dancers. When ADF moved to Duke University, physicians from Duke University Hospital became interested in dancers. Then, in 1990, the International Association for Dance Medicine and Science (IADMS) was formed by an international group of dance medicine practitioners, dance educators, dance scientists, and dancers. Membership of IADMS began with 48 members in 1991, and has grown to over 900 members in 35 countries as of 2016.Dance science as an academic discipline has been evolving over the past 20 years. In the United Kingdom, three degrees (at master's level) now exist: one at the University of Bedfordshire, one at the University of Wolverhampton, and one at the Trinity Laban Conservatoire of Music and Dance in London."
Which organization was formed in 1990 to promote dance medicine and science?,American Dance Festival,International Association for Dance Medicine and Science (IADMS),Duke University Hospital,University of Bedfordshire,Trinity Laban Conservatoire of Music and Dance,B,"Dance science is the scientific study of dance and dancers, as well as the practical application of scientific principles to dance. Its aims are the enhancement of performance, the reduction of injury, and the improvement of well-being and health.

Overview
Dance medicine and science as a field of study developed in the 1970s and '80s out of the field of sports medicine. In the early 1980s, the American Dance Festival (ADF) started including dance medicine courses in their coursework for dancers. When ADF moved to Duke University, physicians from Duke University Hospital became interested in dancers. Then, in 1990, the International Association for Dance Medicine and Science (IADMS) was formed by an international group of dance medicine practitioners, dance educators, dance scientists, and dancers. Membership of IADMS began with 48 members in 1991, and has grown to over 900 members in 35 countries as of 2016.Dance science as an academic discipline has been evolving over the past 20 years. In the United Kingdom, three degrees (at master's level) now exist: one at the University of Bedfordshire, one at the University of Wolverhampton, and one at the Trinity Laban Conservatoire of Music and Dance in London."
How many members does the International Association for Dance Medicine and Science have as of 2016?,48 members,900 members,35 members,500 members,100 members,B,"Dance science is the scientific study of dance and dancers, as well as the practical application of scientific principles to dance. Its aims are the enhancement of performance, the reduction of injury, and the improvement of well-being and health.

Overview
Dance medicine and science as a field of study developed in the 1970s and '80s out of the field of sports medicine. In the early 1980s, the American Dance Festival (ADF) started including dance medicine courses in their coursework for dancers. When ADF moved to Duke University, physicians from Duke University Hospital became interested in dancers. Then, in 1990, the International Association for Dance Medicine and Science (IADMS) was formed by an international group of dance medicine practitioners, dance educators, dance scientists, and dancers. Membership of IADMS began with 48 members in 1991, and has grown to over 900 members in 35 countries as of 2016.Dance science as an academic discipline has been evolving over the past 20 years. In the United Kingdom, three degrees (at master's level) now exist: one at the University of Bedfordshire, one at the University of Wolverhampton, and one at the Trinity Laban Conservatoire of Music and Dance in London."
When was the Institut für Kunststoffverarbeitung in Industrie und Handwerk (IKV) founded?,1940,1950,1960,1970,1980,B,"The Institut für Kunststoffverarbeitung in Industrie und Handwerk (IKV), the Institute for Plastics Processing in Industry and Trade at the Rheinisch-Westfälische Technische Hochschule Aachen, Germany, is a teaching and research institute for the study of plastics technology. It stands for practice-oriented research, innovation and technology transfer. The focus of the IKV is the integrative view of product development in the material, construction and processing sectors, in particular in plastics and rubber. The sponsor is a non-profit association that currently includes around 300 companies from the plastics industry worldwide (as of December 2018) and through which the institute maintains a close connection between industry and science. In addition, the IKV is a member of the Arbeitsgemeinschaft industrieller Forschungsvereinigungen ""Otto von Guericke"" (AiF).The institute was founded in 1950 and, with around 350 employees, has become Europe's largest research and training institute in the field of plastics technology. The first head of the institute was Karl Krekeler, followed in 1959 by A. H."
What is the focus of the IKV?,Product development in the automotive industry,Product development in the electronics industry,Product development in the plastics and rubber industry,Product development in the food industry,Product development in the textile industry,C,"The Institut für Kunststoffverarbeitung in Industrie und Handwerk (IKV), the Institute for Plastics Processing in Industry and Trade at the Rheinisch-Westfälische Technische Hochschule Aachen, Germany, is a teaching and research institute for the study of plastics technology. It stands for practice-oriented research, innovation and technology transfer. The focus of the IKV is the integrative view of product development in the material, construction and processing sectors, in particular in plastics and rubber. The sponsor is a non-profit association that currently includes around 300 companies from the plastics industry worldwide (as of December 2018) and through which the institute maintains a close connection between industry and science. In addition, the IKV is a member of the Arbeitsgemeinschaft industrieller Forschungsvereinigungen ""Otto von Guericke"" (AiF).The institute was founded in 1950 and, with around 350 employees, has become Europe's largest research and training institute in the field of plastics technology. The first head of the institute was Karl Krekeler, followed in 1959 by A. H."
How many companies are currently included in the non-profit association that sponsors the IKV?,100,200,300,400,500,C,"The Institut für Kunststoffverarbeitung in Industrie und Handwerk (IKV), the Institute for Plastics Processing in Industry and Trade at the Rheinisch-Westfälische Technische Hochschule Aachen, Germany, is a teaching and research institute for the study of plastics technology. It stands for practice-oriented research, innovation and technology transfer. The focus of the IKV is the integrative view of product development in the material, construction and processing sectors, in particular in plastics and rubber. The sponsor is a non-profit association that currently includes around 300 companies from the plastics industry worldwide (as of December 2018) and through which the institute maintains a close connection between industry and science. In addition, the IKV is a member of the Arbeitsgemeinschaft industrieller Forschungsvereinigungen ""Otto von Guericke"" (AiF).The institute was founded in 1950 and, with around 350 employees, has become Europe's largest research and training institute in the field of plastics technology. The first head of the institute was Karl Krekeler, followed in 1959 by A. H."
What is the name of the research association that the IKV is a member of?,"Arbeitsgemeinschaft industrieller Forschungsvereinigungen ""Otto von Guericke"" (AiF)",Deutsche Forschungsgemeinschaft (DFG),Fraunhofer-Gesellschaft,Max Planck Society,Leibniz Association,A,"The Institut für Kunststoffverarbeitung in Industrie und Handwerk (IKV), the Institute for Plastics Processing in Industry and Trade at the Rheinisch-Westfälische Technische Hochschule Aachen, Germany, is a teaching and research institute for the study of plastics technology. It stands for practice-oriented research, innovation and technology transfer. The focus of the IKV is the integrative view of product development in the material, construction and processing sectors, in particular in plastics and rubber. The sponsor is a non-profit association that currently includes around 300 companies from the plastics industry worldwide (as of December 2018) and through which the institute maintains a close connection between industry and science. In addition, the IKV is a member of the Arbeitsgemeinschaft industrieller Forschungsvereinigungen ""Otto von Guericke"" (AiF).The institute was founded in 1950 and, with around 350 employees, has become Europe's largest research and training institute in the field of plastics technology. The first head of the institute was Karl Krekeler, followed in 1959 by A. H."
Who was the first head of the institute?,Karl Krekeler,A. H.,Karl Krekeler and A. H.,There is no information about the first head,The first head is still the current head,A,"The Institut für Kunststoffverarbeitung in Industrie und Handwerk (IKV), the Institute for Plastics Processing in Industry and Trade at the Rheinisch-Westfälische Technische Hochschule Aachen, Germany, is a teaching and research institute for the study of plastics technology. It stands for practice-oriented research, innovation and technology transfer. The focus of the IKV is the integrative view of product development in the material, construction and processing sectors, in particular in plastics and rubber. The sponsor is a non-profit association that currently includes around 300 companies from the plastics industry worldwide (as of December 2018) and through which the institute maintains a close connection between industry and science. In addition, the IKV is a member of the Arbeitsgemeinschaft industrieller Forschungsvereinigungen ""Otto von Guericke"" (AiF).The institute was founded in 1950 and, with around 350 employees, has become Europe's largest research and training institute in the field of plastics technology. The first head of the institute was Karl Krekeler, followed in 1959 by A. H."
Who was appointed Principal Medical Officer of the Portuguese Army during the Peninsular War?,Florence Nightingale,James McGrigor,Colonial Surgeon,Superintendents of Hospitals,NHS Scotland,B,"Principal Medical Officer (PMO) is a senior position in the Royal Army Medical Corps, the Royal Canadian Navy, the Army Medical Corps (India), NHS Scotland and the Irish Health Service Executive.The title was formerly used within the British National Health Service and the health services of former colonies.  James McGrigor was appointed Principal Medical Officer of the Portuguese Army during the Peninsular War.  A position of Principal Medical Officer in the Crimea was established from 1854 to 1856 for the duration of the Crimean War.  Florence Nightingale made suggestions as to the duties of the post.The position still existed in England and Wales until the 1974 reorganisation. The title is still used in NHS Scotland.The St John Ambulance in England formerly had a Principal Medical Officer position. In Australia the position was the senior administrator of the Medical Department of each state from 1895, when it was created as part of the Colonial Secretary's Department, replacing the position of Colonial Surgeon, to 1911.  The superintendents of hospitals reported to the Principal Medical Officer."
In which country is the title Principal Medical Officer still used in the health service?,India,Australia,England and Wales,Canada,Ireland,C,"Principal Medical Officer (PMO) is a senior position in the Royal Army Medical Corps, the Royal Canadian Navy, the Army Medical Corps (India), NHS Scotland and the Irish Health Service Executive.The title was formerly used within the British National Health Service and the health services of former colonies.  James McGrigor was appointed Principal Medical Officer of the Portuguese Army during the Peninsular War.  A position of Principal Medical Officer in the Crimea was established from 1854 to 1856 for the duration of the Crimean War.  Florence Nightingale made suggestions as to the duties of the post.The position still existed in England and Wales until the 1974 reorganisation. The title is still used in NHS Scotland.The St John Ambulance in England formerly had a Principal Medical Officer position. In Australia the position was the senior administrator of the Medical Department of each state from 1895, when it was created as part of the Colonial Secretary's Department, replacing the position of Colonial Surgeon, to 1911.  The superintendents of hospitals reported to the Principal Medical Officer."
During which war was the position of Principal Medical Officer in the Crimea established?,Crimean War,Peninsular War,Colonial Surgeon,NHS Scotland,St John Ambulance,A,"Principal Medical Officer (PMO) is a senior position in the Royal Army Medical Corps, the Royal Canadian Navy, the Army Medical Corps (India), NHS Scotland and the Irish Health Service Executive.The title was formerly used within the British National Health Service and the health services of former colonies.  James McGrigor was appointed Principal Medical Officer of the Portuguese Army during the Peninsular War.  A position of Principal Medical Officer in the Crimea was established from 1854 to 1856 for the duration of the Crimean War.  Florence Nightingale made suggestions as to the duties of the post.The position still existed in England and Wales until the 1974 reorganisation. The title is still used in NHS Scotland.The St John Ambulance in England formerly had a Principal Medical Officer position. In Australia the position was the senior administrator of the Medical Department of each state from 1895, when it was created as part of the Colonial Secretary's Department, replacing the position of Colonial Surgeon, to 1911.  The superintendents of hospitals reported to the Principal Medical Officer."
Who made suggestions as to the duties of the Principal Medical Officer in the Crimea?,James McGrigor,Florence Nightingale,Principal Medical Officer,Superintendents of Hospitals,Royal Army Medical Corps,B,"Principal Medical Officer (PMO) is a senior position in the Royal Army Medical Corps, the Royal Canadian Navy, the Army Medical Corps (India), NHS Scotland and the Irish Health Service Executive.The title was formerly used within the British National Health Service and the health services of former colonies.  James McGrigor was appointed Principal Medical Officer of the Portuguese Army during the Peninsular War.  A position of Principal Medical Officer in the Crimea was established from 1854 to 1856 for the duration of the Crimean War.  Florence Nightingale made suggestions as to the duties of the post.The position still existed in England and Wales until the 1974 reorganisation. The title is still used in NHS Scotland.The St John Ambulance in England formerly had a Principal Medical Officer position. In Australia the position was the senior administrator of the Medical Department of each state from 1895, when it was created as part of the Colonial Secretary's Department, replacing the position of Colonial Surgeon, to 1911.  The superintendents of hospitals reported to the Principal Medical Officer."
Who was the senior administrator of the Medical Department in Australia from 1895 to 1911?,Principal Medical Officer,Colonial Surgeon,NHS Scotland,Superintendents of Hospitals,Royal Canadian Navy,A,"Principal Medical Officer (PMO) is a senior position in the Royal Army Medical Corps, the Royal Canadian Navy, the Army Medical Corps (India), NHS Scotland and the Irish Health Service Executive.The title was formerly used within the British National Health Service and the health services of former colonies.  James McGrigor was appointed Principal Medical Officer of the Portuguese Army during the Peninsular War.  A position of Principal Medical Officer in the Crimea was established from 1854 to 1856 for the duration of the Crimean War.  Florence Nightingale made suggestions as to the duties of the post.The position still existed in England and Wales until the 1974 reorganisation. The title is still used in NHS Scotland.The St John Ambulance in England formerly had a Principal Medical Officer position. In Australia the position was the senior administrator of the Medical Department of each state from 1895, when it was created as part of the Colonial Secretary's Department, replacing the position of Colonial Surgeon, to 1911.  The superintendents of hospitals reported to the Principal Medical Officer."
What is the focus of ECS Electrochemistry Letters (EEL)?,Physics,Chemistry,Electrochemical science and technology,Biology,Geology,C,"ECS Electrochemistry Letters (EEL) is a monthly peer-reviewed scientific journal covering electrochemical science and technology. It was established in 2012 and is published by the Electrochemical Society. EEL ceased publication at the end of 2015. The editors-in-chief were Robert Savinell (Case Western Reserve University), Gerald S. Frankel (Ohio State University), Thomas F. Fuller (Georgia Tech), Charles L. Hussey (University of Mississippi), Shelley D."
When was ECS Electrochemistry Letters (EEL) established?,2010,2012,2014,2016,2018,B,"ECS Electrochemistry Letters (EEL) is a monthly peer-reviewed scientific journal covering electrochemical science and technology. It was established in 2012 and is published by the Electrochemical Society. EEL ceased publication at the end of 2015. The editors-in-chief were Robert Savinell (Case Western Reserve University), Gerald S. Frankel (Ohio State University), Thomas F. Fuller (Georgia Tech), Charles L. Hussey (University of Mississippi), Shelley D."
Which organization publishes ECS Electrochemistry Letters (EEL)?,American Chemical Society,Electrochemical Society,International Union of Pure and Applied Chemistry,Royal Society of Chemistry,Institute of Electrical and Electronics Engineers,B,"ECS Electrochemistry Letters (EEL) is a monthly peer-reviewed scientific journal covering electrochemical science and technology. It was established in 2012 and is published by the Electrochemical Society. EEL ceased publication at the end of 2015. The editors-in-chief were Robert Savinell (Case Western Reserve University), Gerald S. Frankel (Ohio State University), Thomas F. Fuller (Georgia Tech), Charles L. Hussey (University of Mississippi), Shelley D."
Who was one of the editors-in-chief of ECS Electrochemistry Letters (EEL)?,Robert Savinell,Gerald S. Frankel,Thomas F. Fuller,Charles L. Hussey,Shelley D.,A,"ECS Electrochemistry Letters (EEL) is a monthly peer-reviewed scientific journal covering electrochemical science and technology. It was established in 2012 and is published by the Electrochemical Society. EEL ceased publication at the end of 2015. The editors-in-chief were Robert Savinell (Case Western Reserve University), Gerald S. Frankel (Ohio State University), Thomas F. Fuller (Georgia Tech), Charles L. Hussey (University of Mississippi), Shelley D."
When did ECS Electrochemistry Letters (EEL) cease publication?,End of 2012,End of 2013,End of 2014,End of 2015,End of 2016,D,"ECS Electrochemistry Letters (EEL) is a monthly peer-reviewed scientific journal covering electrochemical science and technology. It was established in 2012 and is published by the Electrochemical Society. EEL ceased publication at the end of 2015. The editors-in-chief were Robert Savinell (Case Western Reserve University), Gerald S. Frankel (Ohio State University), Thomas F. Fuller (Georgia Tech), Charles L. Hussey (University of Mississippi), Shelley D."
What are the reactances of synchronous machines specified in?,Amps,Volts,Ohms,Watts,Joules,C,"The reactances of synchronous machines comprise a set of characteristic constants used in the theory of synchronous machines. Technically, these constants are specified in units of the electrical reactance (ohms), although they are typically expressed in the per-unit system and thus dimensionless. Since for practically all (except for the tiniest) machines the resistance of the coils is negligibly small in comparison to the reactance, the latter can be used instead of (complex) electrical impedance, simplifying the calculations.

Two reactions theory
The air gap of the machines with a salient pole rotor is quite different along the pole axis (so called direct axis) and in the orthogonal direction (so called quadrature axis). Andre Blondel in 1899 proposed in his paper ""Empirical Theory of Synchronous Generators"" the two reactions theory that divided the armature magnetomotive force (MMF) into two components: the direct axis component and the quadrature axis component. The direct axis component is aligned with the magnetic axis of the rotor, while the quadrature (or transverse) axis component is perpendicular to the direct axis. The relative strengths of these two components depend on the design of the machine and the operating conditions. Since the equations naturally split into direct and quadrature components, many reactances come in pairs, one for the direct axis (with the index d), one for the quadrature axis (with the index q)."
What did Andre Blondel propose in his paper in 1899?,The theory of electrical impedance,The theory of electromagnetic induction,The two reactions theory,The theory of synchronous generators,The theory of electrical resistance,C,"The reactances of synchronous machines comprise a set of characteristic constants used in the theory of synchronous machines. Technically, these constants are specified in units of the electrical reactance (ohms), although they are typically expressed in the per-unit system and thus dimensionless. Since for practically all (except for the tiniest) machines the resistance of the coils is negligibly small in comparison to the reactance, the latter can be used instead of (complex) electrical impedance, simplifying the calculations.

Two reactions theory
The air gap of the machines with a salient pole rotor is quite different along the pole axis (so called direct axis) and in the orthogonal direction (so called quadrature axis). Andre Blondel in 1899 proposed in his paper ""Empirical Theory of Synchronous Generators"" the two reactions theory that divided the armature magnetomotive force (MMF) into two components: the direct axis component and the quadrature axis component. The direct axis component is aligned with the magnetic axis of the rotor, while the quadrature (or transverse) axis component is perpendicular to the direct axis. The relative strengths of these two components depend on the design of the machine and the operating conditions. Since the equations naturally split into direct and quadrature components, many reactances come in pairs, one for the direct axis (with the index d), one for the quadrature axis (with the index q)."
What are the two components of the armature magnetomotive force (MMF) according to the two reactions theory?,Direct axis and quadrangle axis,Vertical axis and horizontal axis,Parallel axis and perpendicular axis,Direct axis and transverse axis,Longitudinal axis and latitudinal axis,D,"The reactances of synchronous machines comprise a set of characteristic constants used in the theory of synchronous machines. Technically, these constants are specified in units of the electrical reactance (ohms), although they are typically expressed in the per-unit system and thus dimensionless. Since for practically all (except for the tiniest) machines the resistance of the coils is negligibly small in comparison to the reactance, the latter can be used instead of (complex) electrical impedance, simplifying the calculations.

Two reactions theory
The air gap of the machines with a salient pole rotor is quite different along the pole axis (so called direct axis) and in the orthogonal direction (so called quadrature axis). Andre Blondel in 1899 proposed in his paper ""Empirical Theory of Synchronous Generators"" the two reactions theory that divided the armature magnetomotive force (MMF) into two components: the direct axis component and the quadrature axis component. The direct axis component is aligned with the magnetic axis of the rotor, while the quadrature (or transverse) axis component is perpendicular to the direct axis. The relative strengths of these two components depend on the design of the machine and the operating conditions. Since the equations naturally split into direct and quadrature components, many reactances come in pairs, one for the direct axis (with the index d), one for the quadrature axis (with the index q)."
Which axis is the direct axis component aligned with?,Pole axis,Orthogonal direction,Magnetic axis of the rotor,Quadrature axis,Armature magnetomotive force axis,C,"The reactances of synchronous machines comprise a set of characteristic constants used in the theory of synchronous machines. Technically, these constants are specified in units of the electrical reactance (ohms), although they are typically expressed in the per-unit system and thus dimensionless. Since for practically all (except for the tiniest) machines the resistance of the coils is negligibly small in comparison to the reactance, the latter can be used instead of (complex) electrical impedance, simplifying the calculations.

Two reactions theory
The air gap of the machines with a salient pole rotor is quite different along the pole axis (so called direct axis) and in the orthogonal direction (so called quadrature axis). Andre Blondel in 1899 proposed in his paper ""Empirical Theory of Synchronous Generators"" the two reactions theory that divided the armature magnetomotive force (MMF) into two components: the direct axis component and the quadrature axis component. The direct axis component is aligned with the magnetic axis of the rotor, while the quadrature (or transverse) axis component is perpendicular to the direct axis. The relative strengths of these two components depend on the design of the machine and the operating conditions. Since the equations naturally split into direct and quadrature components, many reactances come in pairs, one for the direct axis (with the index d), one for the quadrature axis (with the index q)."
What do the relative strengths of the direct and quadrature components depend on?,The size of the machine,The operating conditions,The design of the machine,Both the design of the machine and the operating conditions,None of the above,D,"The reactances of synchronous machines comprise a set of characteristic constants used in the theory of synchronous machines. Technically, these constants are specified in units of the electrical reactance (ohms), although they are typically expressed in the per-unit system and thus dimensionless. Since for practically all (except for the tiniest) machines the resistance of the coils is negligibly small in comparison to the reactance, the latter can be used instead of (complex) electrical impedance, simplifying the calculations.

Two reactions theory
The air gap of the machines with a salient pole rotor is quite different along the pole axis (so called direct axis) and in the orthogonal direction (so called quadrature axis). Andre Blondel in 1899 proposed in his paper ""Empirical Theory of Synchronous Generators"" the two reactions theory that divided the armature magnetomotive force (MMF) into two components: the direct axis component and the quadrature axis component. The direct axis component is aligned with the magnetic axis of the rotor, while the quadrature (or transverse) axis component is perpendicular to the direct axis. The relative strengths of these two components depend on the design of the machine and the operating conditions. Since the equations naturally split into direct and quadrature components, many reactances come in pairs, one for the direct axis (with the index d), one for the quadrature axis (with the index q)."
What is the difference between chemistry and physics?,Chemistry focuses on matter while physics focuses on energy.,Chemistry studies matter at a molecular level while physics studies matter at a subatomic level.,Chemistry is concerned with the behavior of matter at a large scale while physics is concerned with the behavior of matter at a small scale.,Chemistry and physics have the same scope and approach but different professional roles.,Chemistry and physics are interchangeable and can be used interchangeably.,B,"Chemistry and physics are branches of science that both study matter. The difference between the two lies in their scope and approach. Chemists and physicists are trained differently, and they have different professional roles, even when working in a team. The division between chemistry and physics becomes diffused at the interface of the two branches, notably in fields such as physical chemistry, chemical physics, quantum mechanics, nuclear physics/chemistry, materials science, spectroscopy, solid state physics, solid-state chemistry, crystallography, and nanotechnology.

Scope
Physics and chemistry may overlap when the system under study involves matter composed of electrons and nuclei made of protons and neutrons. On the other hand, chemistry is not usually concerned with other forms of matter such as quarks, mu and tau leptons and dark matter.
Although fundamental laws that govern the behaviour of matter apply to both in chemistry and physics, the disciplines of physics and chemistry are distinct in focus:
Physics is concerned with nature from a huge scale (the entire universe) down to a very small scale (subatomic particles). All physical phenomena that are measurable follow some behaviour that is in accordance with the most basic principles studied in physics. Physics is involved with the fundamental principles of physical phenomena and the basic forces of nature, and also gives insight into the aspects of space and time."
What are some fields that combine the two branches of chemistry and physics?,Organic chemistry and astrophysics,Thermochemistry and quantum mechanics,Physical chemistry and chemical physics,Analytical chemistry and particle physics,Inorganic chemistry and classical mechanics,C,"Chemistry and physics are branches of science that both study matter. The difference between the two lies in their scope and approach. Chemists and physicists are trained differently, and they have different professional roles, even when working in a team. The division between chemistry and physics becomes diffused at the interface of the two branches, notably in fields such as physical chemistry, chemical physics, quantum mechanics, nuclear physics/chemistry, materials science, spectroscopy, solid state physics, solid-state chemistry, crystallography, and nanotechnology.

Scope
Physics and chemistry may overlap when the system under study involves matter composed of electrons and nuclei made of protons and neutrons. On the other hand, chemistry is not usually concerned with other forms of matter such as quarks, mu and tau leptons and dark matter.
Although fundamental laws that govern the behaviour of matter apply to both in chemistry and physics, the disciplines of physics and chemistry are distinct in focus:
Physics is concerned with nature from a huge scale (the entire universe) down to a very small scale (subatomic particles). All physical phenomena that are measurable follow some behaviour that is in accordance with the most basic principles studied in physics. Physics is involved with the fundamental principles of physical phenomena and the basic forces of nature, and also gives insight into the aspects of space and time."
What is the scope of physics?,Physics only studies matter at a subatomic level.,Physics is concerned with the behavior of matter at a large scale.,Physics studies the fundamental principles of physical phenomena and the basic forces of nature.,Physics focuses on the structure and function of molecules.,Physics is concerned with the behavior of matter at a molecular level.,C,"Chemistry and physics are branches of science that both study matter. The difference between the two lies in their scope and approach. Chemists and physicists are trained differently, and they have different professional roles, even when working in a team. The division between chemistry and physics becomes diffused at the interface of the two branches, notably in fields such as physical chemistry, chemical physics, quantum mechanics, nuclear physics/chemistry, materials science, spectroscopy, solid state physics, solid-state chemistry, crystallography, and nanotechnology.

Scope
Physics and chemistry may overlap when the system under study involves matter composed of electrons and nuclei made of protons and neutrons. On the other hand, chemistry is not usually concerned with other forms of matter such as quarks, mu and tau leptons and dark matter.
Although fundamental laws that govern the behaviour of matter apply to both in chemistry and physics, the disciplines of physics and chemistry are distinct in focus:
Physics is concerned with nature from a huge scale (the entire universe) down to a very small scale (subatomic particles). All physical phenomena that are measurable follow some behaviour that is in accordance with the most basic principles studied in physics. Physics is involved with the fundamental principles of physical phenomena and the basic forces of nature, and also gives insight into the aspects of space and time."
What are some examples of matter that chemistry is not usually concerned with?,Protons and neutrons,Subatomic particles,Quarks,Mu and tau leptons,Dark matter,C,"Chemistry and physics are branches of science that both study matter. The difference between the two lies in their scope and approach. Chemists and physicists are trained differently, and they have different professional roles, even when working in a team. The division between chemistry and physics becomes diffused at the interface of the two branches, notably in fields such as physical chemistry, chemical physics, quantum mechanics, nuclear physics/chemistry, materials science, spectroscopy, solid state physics, solid-state chemistry, crystallography, and nanotechnology.

Scope
Physics and chemistry may overlap when the system under study involves matter composed of electrons and nuclei made of protons and neutrons. On the other hand, chemistry is not usually concerned with other forms of matter such as quarks, mu and tau leptons and dark matter.
Although fundamental laws that govern the behaviour of matter apply to both in chemistry and physics, the disciplines of physics and chemistry are distinct in focus:
Physics is concerned with nature from a huge scale (the entire universe) down to a very small scale (subatomic particles). All physical phenomena that are measurable follow some behaviour that is in accordance with the most basic principles studied in physics. Physics is involved with the fundamental principles of physical phenomena and the basic forces of nature, and also gives insight into the aspects of space and time."
What insights does physics provide?,Insights into the behavior of matter at a large scale,Insights into the behavior of matter at a molecular level,Insights into the fundamental principles of physical phenomena and the basic forces of nature,Insights into the structure and function of molecules,Insights into the behavior of matter at a small scale,C,"Chemistry and physics are branches of science that both study matter. The difference between the two lies in their scope and approach. Chemists and physicists are trained differently, and they have different professional roles, even when working in a team. The division between chemistry and physics becomes diffused at the interface of the two branches, notably in fields such as physical chemistry, chemical physics, quantum mechanics, nuclear physics/chemistry, materials science, spectroscopy, solid state physics, solid-state chemistry, crystallography, and nanotechnology.

Scope
Physics and chemistry may overlap when the system under study involves matter composed of electrons and nuclei made of protons and neutrons. On the other hand, chemistry is not usually concerned with other forms of matter such as quarks, mu and tau leptons and dark matter.
Although fundamental laws that govern the behaviour of matter apply to both in chemistry and physics, the disciplines of physics and chemistry are distinct in focus:
Physics is concerned with nature from a huge scale (the entire universe) down to a very small scale (subatomic particles). All physical phenomena that are measurable follow some behaviour that is in accordance with the most basic principles studied in physics. Physics is involved with the fundamental principles of physical phenomena and the basic forces of nature, and also gives insight into the aspects of space and time."
What is the main benefit of high-energy X-rays?,They can be used for X-ray crystallography,They can penetrate deep into materials,They can generate beams for cancer therapy,They can be used in materials science,They can be used for electron diffraction,B,"High-energy X-rays or HEX-rays are very hard X-rays, with typical energies of 80–1000 keV (1 MeV), about one order of magnitude higher than conventional X-rays used for X-ray crystallography (and well into gamma-ray energies over 120 keV). They are produced at modern synchrotron radiation sources such as the beamlines ID15 and BM18 at the European Synchrotron Radiation Facility (ESRF). The main benefit is the deep penetration into matter which makes them a probe for thick samples in physics and materials science and permits an in-air sample environment and operation. Scattering angles are small and diffraction directed forward allows for simple detector setups.
High energy (megavolt) X-rays are also used in cancer therapy, using beams generated by linear accelerators to suppress tumors.

Advantages
High-energy X-rays (HEX-rays) between 100 and 300 keV bear unique advantage over conventional hard X-rays, which lie in the range of 5–20 keV  They can be listed as follows:

High penetration into materials due to a strongly reduced photo absorption cross section. The photo-absorption strongly depends on the atomic number of the material and the X-ray energy. Several centimeter thick volumes can be accessed in steel and millimeters in lead containing samples.
No radiation damage of the sample, which can pin incommensurations or destroy the chemical compound to be analyzed.
The Ewald sphere has a curvature ten times smaller than in the low energy case and allows whole regions to be mapped in a reciprocal lattice, similar to electron diffraction.
Access to diffuse scattering. This is absorption and not extinction limited  at low energies while volume enhancement takes place at high energies."
What is one advantage of high-energy X-rays over conventional hard X-rays?,They can penetrate several centimeters in steel,They can cause radiation damage to the sample,They have a smaller Ewald sphere curvature,They can access to diffuse scattering,They are absorption limited at low energies,A,"High-energy X-rays or HEX-rays are very hard X-rays, with typical energies of 80–1000 keV (1 MeV), about one order of magnitude higher than conventional X-rays used for X-ray crystallography (and well into gamma-ray energies over 120 keV). They are produced at modern synchrotron radiation sources such as the beamlines ID15 and BM18 at the European Synchrotron Radiation Facility (ESRF). The main benefit is the deep penetration into matter which makes them a probe for thick samples in physics and materials science and permits an in-air sample environment and operation. Scattering angles are small and diffraction directed forward allows for simple detector setups.
High energy (megavolt) X-rays are also used in cancer therapy, using beams generated by linear accelerators to suppress tumors.

Advantages
High-energy X-rays (HEX-rays) between 100 and 300 keV bear unique advantage over conventional hard X-rays, which lie in the range of 5–20 keV  They can be listed as follows:

High penetration into materials due to a strongly reduced photo absorption cross section. The photo-absorption strongly depends on the atomic number of the material and the X-ray energy. Several centimeter thick volumes can be accessed in steel and millimeters in lead containing samples.
No radiation damage of the sample, which can pin incommensurations or destroy the chemical compound to be analyzed.
The Ewald sphere has a curvature ten times smaller than in the low energy case and allows whole regions to be mapped in a reciprocal lattice, similar to electron diffraction.
Access to diffuse scattering. This is absorption and not extinction limited  at low energies while volume enhancement takes place at high energies."
What is the atomic number dependence of photo-absorption in high-energy X-rays?,It is strongly reduced,It is strongly increased,It has no effect on photo-absorption,It is absorption limited,It is extinction limited,A,"High-energy X-rays or HEX-rays are very hard X-rays, with typical energies of 80–1000 keV (1 MeV), about one order of magnitude higher than conventional X-rays used for X-ray crystallography (and well into gamma-ray energies over 120 keV). They are produced at modern synchrotron radiation sources such as the beamlines ID15 and BM18 at the European Synchrotron Radiation Facility (ESRF). The main benefit is the deep penetration into matter which makes them a probe for thick samples in physics and materials science and permits an in-air sample environment and operation. Scattering angles are small and diffraction directed forward allows for simple detector setups.
High energy (megavolt) X-rays are also used in cancer therapy, using beams generated by linear accelerators to suppress tumors.

Advantages
High-energy X-rays (HEX-rays) between 100 and 300 keV bear unique advantage over conventional hard X-rays, which lie in the range of 5–20 keV  They can be listed as follows:

High penetration into materials due to a strongly reduced photo absorption cross section. The photo-absorption strongly depends on the atomic number of the material and the X-ray energy. Several centimeter thick volumes can be accessed in steel and millimeters in lead containing samples.
No radiation damage of the sample, which can pin incommensurations or destroy the chemical compound to be analyzed.
The Ewald sphere has a curvature ten times smaller than in the low energy case and allows whole regions to be mapped in a reciprocal lattice, similar to electron diffraction.
Access to diffuse scattering. This is absorption and not extinction limited  at low energies while volume enhancement takes place at high energies."
What is a characteristic of the Ewald sphere in high-energy X-rays?,It is ten times larger than in low energy cases,It allows mapping of whole regions in a reciprocal lattice,It is absorption limited at high energies,It is equal to the low energy case,It is limited to electron diffraction,B,"High-energy X-rays or HEX-rays are very hard X-rays, with typical energies of 80–1000 keV (1 MeV), about one order of magnitude higher than conventional X-rays used for X-ray crystallography (and well into gamma-ray energies over 120 keV). They are produced at modern synchrotron radiation sources such as the beamlines ID15 and BM18 at the European Synchrotron Radiation Facility (ESRF). The main benefit is the deep penetration into matter which makes them a probe for thick samples in physics and materials science and permits an in-air sample environment and operation. Scattering angles are small and diffraction directed forward allows for simple detector setups.
High energy (megavolt) X-rays are also used in cancer therapy, using beams generated by linear accelerators to suppress tumors.

Advantages
High-energy X-rays (HEX-rays) between 100 and 300 keV bear unique advantage over conventional hard X-rays, which lie in the range of 5–20 keV  They can be listed as follows:

High penetration into materials due to a strongly reduced photo absorption cross section. The photo-absorption strongly depends on the atomic number of the material and the X-ray energy. Several centimeter thick volumes can be accessed in steel and millimeters in lead containing samples.
No radiation damage of the sample, which can pin incommensurations or destroy the chemical compound to be analyzed.
The Ewald sphere has a curvature ten times smaller than in the low energy case and allows whole regions to be mapped in a reciprocal lattice, similar to electron diffraction.
Access to diffuse scattering. This is absorption and not extinction limited  at low energies while volume enhancement takes place at high energies."
What is the volume enhancement in diffuse scattering at high-energy X-rays?,It is absorption limited,It is extinction limited,It is similar to electron diffraction,It is volume enhancement,It is absorption and not extinction limited,E,"High-energy X-rays or HEX-rays are very hard X-rays, with typical energies of 80–1000 keV (1 MeV), about one order of magnitude higher than conventional X-rays used for X-ray crystallography (and well into gamma-ray energies over 120 keV). They are produced at modern synchrotron radiation sources such as the beamlines ID15 and BM18 at the European Synchrotron Radiation Facility (ESRF). The main benefit is the deep penetration into matter which makes them a probe for thick samples in physics and materials science and permits an in-air sample environment and operation. Scattering angles are small and diffraction directed forward allows for simple detector setups.
High energy (megavolt) X-rays are also used in cancer therapy, using beams generated by linear accelerators to suppress tumors.

Advantages
High-energy X-rays (HEX-rays) between 100 and 300 keV bear unique advantage over conventional hard X-rays, which lie in the range of 5–20 keV  They can be listed as follows:

High penetration into materials due to a strongly reduced photo absorption cross section. The photo-absorption strongly depends on the atomic number of the material and the X-ray energy. Several centimeter thick volumes can be accessed in steel and millimeters in lead containing samples.
No radiation damage of the sample, which can pin incommensurations or destroy the chemical compound to be analyzed.
The Ewald sphere has a curvature ten times smaller than in the low energy case and allows whole regions to be mapped in a reciprocal lattice, similar to electron diffraction.
Access to diffuse scattering. This is absorption and not extinction limited  at low energies while volume enhancement takes place at high energies."
What is solarization?,A phenomenon where a material undergoes a temporary change in color after being subjected to high-energy electromagnetic radiation,A process where glass turns blue after long-term solar exposure in the desert,A technique used in clinical imaging to degrade the accuracy of the diagnosis,A mechanism involved in the breakdown of plastics within the environment,A process where glass turns amber or green when subjected to X-radiation,A,"Solarization refers to a phenomenon in physics where a material undergoes a temporary change in color after being subjected to high-energy electromagnetic radiation, such as ultraviolet light or X-rays. Clear glass and many plastics will turn amber, green or other colors when subjected to X-radiation, and glass may turn blue after long-term solar exposure in the desert. It is believed that solarization is caused by the formation of internal defects, called color centers, which selectively absorb portions of the visible light spectrum. In glass, color center absorption can often be reversed by heating the glass to high temperatures (a process called thermal bleaching) to restore the glass to its initial transparent state. Solarization may also permanently degrade a material's physical or mechanical properties, and is one of the mechanisms involved in the breakdown of plastics within the environment.

Examples
In the field of clinical imaging, with sufficient exposure, solarization of certain screen-film systems can occur which obscures details within the X-ray image and degrades the accuracy of the diagnosis. Even though degradation can occur this was found to be a rare phenomenon.

See also
Photodegradation."
What is the cause of solarization?,Formation of internal defects called color centers,Exposure to ultraviolet light,Exposure to X-rays,Thermal bleaching,Long-term solar exposure,A,"Solarization refers to a phenomenon in physics where a material undergoes a temporary change in color after being subjected to high-energy electromagnetic radiation, such as ultraviolet light or X-rays. Clear glass and many plastics will turn amber, green or other colors when subjected to X-radiation, and glass may turn blue after long-term solar exposure in the desert. It is believed that solarization is caused by the formation of internal defects, called color centers, which selectively absorb portions of the visible light spectrum. In glass, color center absorption can often be reversed by heating the glass to high temperatures (a process called thermal bleaching) to restore the glass to its initial transparent state. Solarization may also permanently degrade a material's physical or mechanical properties, and is one of the mechanisms involved in the breakdown of plastics within the environment.

Examples
In the field of clinical imaging, with sufficient exposure, solarization of certain screen-film systems can occur which obscures details within the X-ray image and degrades the accuracy of the diagnosis. Even though degradation can occur this was found to be a rare phenomenon.

See also
Photodegradation."
What can reverse color center absorption in glass?,Exposure to X-rays,Heating the glass to high temperatures,Long-term solar exposure,Formation of color centers,Photodegradation,B,"Solarization refers to a phenomenon in physics where a material undergoes a temporary change in color after being subjected to high-energy electromagnetic radiation, such as ultraviolet light or X-rays. Clear glass and many plastics will turn amber, green or other colors when subjected to X-radiation, and glass may turn blue after long-term solar exposure in the desert. It is believed that solarization is caused by the formation of internal defects, called color centers, which selectively absorb portions of the visible light spectrum. In glass, color center absorption can often be reversed by heating the glass to high temperatures (a process called thermal bleaching) to restore the glass to its initial transparent state. Solarization may also permanently degrade a material's physical or mechanical properties, and is one of the mechanisms involved in the breakdown of plastics within the environment.

Examples
In the field of clinical imaging, with sufficient exposure, solarization of certain screen-film systems can occur which obscures details within the X-ray image and degrades the accuracy of the diagnosis. Even though degradation can occur this was found to be a rare phenomenon.

See also
Photodegradation."
What is the effect of solarization on clinical imaging?,Enhances the accuracy of the diagnosis,Obscures details within the X-ray image,Improves the clarity of the image,Has no effect on the diagnosis,Breaks down the plastics within the environment,B,"Solarization refers to a phenomenon in physics where a material undergoes a temporary change in color after being subjected to high-energy electromagnetic radiation, such as ultraviolet light or X-rays. Clear glass and many plastics will turn amber, green or other colors when subjected to X-radiation, and glass may turn blue after long-term solar exposure in the desert. It is believed that solarization is caused by the formation of internal defects, called color centers, which selectively absorb portions of the visible light spectrum. In glass, color center absorption can often be reversed by heating the glass to high temperatures (a process called thermal bleaching) to restore the glass to its initial transparent state. Solarization may also permanently degrade a material's physical or mechanical properties, and is one of the mechanisms involved in the breakdown of plastics within the environment.

Examples
In the field of clinical imaging, with sufficient exposure, solarization of certain screen-film systems can occur which obscures details within the X-ray image and degrades the accuracy of the diagnosis. Even though degradation can occur this was found to be a rare phenomenon.

See also
Photodegradation."
What is solarization in the context of plastics?,A process where plastics turn amber or green when subjected to X-radiation,A process where plastics turn blue after long-term solar exposure in the desert,A technique used in clinical imaging to degrade the accuracy of the diagnosis,A mechanism involved in the breakdown of plastics within the environment,A phenomenon where plastics undergo a temporary change in color after being subjected to high-energy electromagnetic radiation,D,"Solarization refers to a phenomenon in physics where a material undergoes a temporary change in color after being subjected to high-energy electromagnetic radiation, such as ultraviolet light or X-rays. Clear glass and many plastics will turn amber, green or other colors when subjected to X-radiation, and glass may turn blue after long-term solar exposure in the desert. It is believed that solarization is caused by the formation of internal defects, called color centers, which selectively absorb portions of the visible light spectrum. In glass, color center absorption can often be reversed by heating the glass to high temperatures (a process called thermal bleaching) to restore the glass to its initial transparent state. Solarization may also permanently degrade a material's physical or mechanical properties, and is one of the mechanisms involved in the breakdown of plastics within the environment.

Examples
In the field of clinical imaging, with sufficient exposure, solarization of certain screen-film systems can occur which obscures details within the X-ray image and degrades the accuracy of the diagnosis. Even though degradation can occur this was found to be a rare phenomenon.

See also
Photodegradation."
What is the definition of range management according to Holechek et al.?,The study of rangelands and sustainable land management practices,The manipulation of rangeland components for the benefit of society,The conservation and sustainable management of arid lands,The optimization of goods and services provided by rangelands,The study of grazing practices and rangeland vegetation for livestock,B,"Rangeland management (also range management, range science, or arid-land management) is a natural science that centers around the study of rangelands and the ""conservation and sustainable management [of Arid-Lands] for the benefit of current societies and future generations"". Range management is defined by Holechek et al. as the ""manipulation of rangeland components to obtain optimum combination of goods and services for society on a sustained basis"".

History
The earliest form of Rangeland Management is not formally deemed part of the natural science studied today, although its roots can be traced to nomadic grazing practices of the neolithic agricultural revolution when humans domesticated plants and animals under pressures from population growth and environmental change. Humans might even have altered the environment in times preceding the  Neolithic through hunting of large-game, whereby large losses of grazing herbivores could have resulted in altered ecological states; meaning humans have been inadvertently managing land throughout prehistory.Rangeland management was developed in the United States in response to rangeland deterioration and in some cases, denudation, due to overgrazing and other misuse of arid lands as demonstrated by the 20th century ""Dust Bowl"" and described in Hardin's 1968 ""Tragedy of the Commons"". Historically, the discipline focused on the manipulation of grazing and the proper use of rangeland vegetation for livestock.

Modern application
Global
Range management's focus has been expanded to include the host of ecosystem services that rangelands provide to humans world-wide. Key management components seek to optimize such goods and services through the protection and enhancement of soils, riparian zones, watersheds, and vegetation complexes, sustainably improving outputs of consumable range products such as red meat, wildlife, water, wood, fiber, leather, energy resource extraction, and outdoor recreation, as well as maintaining a focus on the manipulation of grazing activities of large herbivores to maintain or improve animal and plant production.Pastoralism has become a contemporary anthropological and ecological study as it faces many threats including fragmentation of land, conversion of rangeland into urban development, lack of grazing movement, impending threats on global diversity, damage to species with large terrain, decreases in shared public goods, decreased biological movements, threats of a ""tragedy of enclosures"", limitation of key resources, reduced biomass and invasive plant species growth. Interest in contemporary pastoralist cultures like the Maasai has continued to increase, especially because the traditional syncreticly-adaptive ability of pastoralists could promise lessons in collaborative and adaptive management for contemporary pastoralist societies threatened by globalization as well as for contemporary non-pastoralist societies that are managing livestock on rangelands.

United States of America
The United States Society for Range Management is ""the professional society dedicated to supporting persons who work with rangelands and have a commitment to their sustainable use""."
What is the historical significance of rangeland management?,It originated from nomadic grazing practices during the neolithic revolution,It was developed in response to rangeland deterioration and overgrazing,"It was first described in the 1968 ""Tragedy of the Commons""",It focuses on the manipulation of grazing and vegetation for livestock,It has been practiced throughout prehistory by hunting large game,B,"Rangeland management (also range management, range science, or arid-land management) is a natural science that centers around the study of rangelands and the ""conservation and sustainable management [of Arid-Lands] for the benefit of current societies and future generations"". Range management is defined by Holechek et al. as the ""manipulation of rangeland components to obtain optimum combination of goods and services for society on a sustained basis"".

History
The earliest form of Rangeland Management is not formally deemed part of the natural science studied today, although its roots can be traced to nomadic grazing practices of the neolithic agricultural revolution when humans domesticated plants and animals under pressures from population growth and environmental change. Humans might even have altered the environment in times preceding the  Neolithic through hunting of large-game, whereby large losses of grazing herbivores could have resulted in altered ecological states; meaning humans have been inadvertently managing land throughout prehistory.Rangeland management was developed in the United States in response to rangeland deterioration and in some cases, denudation, due to overgrazing and other misuse of arid lands as demonstrated by the 20th century ""Dust Bowl"" and described in Hardin's 1968 ""Tragedy of the Commons"". Historically, the discipline focused on the manipulation of grazing and the proper use of rangeland vegetation for livestock.

Modern application
Global
Range management's focus has been expanded to include the host of ecosystem services that rangelands provide to humans world-wide. Key management components seek to optimize such goods and services through the protection and enhancement of soils, riparian zones, watersheds, and vegetation complexes, sustainably improving outputs of consumable range products such as red meat, wildlife, water, wood, fiber, leather, energy resource extraction, and outdoor recreation, as well as maintaining a focus on the manipulation of grazing activities of large herbivores to maintain or improve animal and plant production.Pastoralism has become a contemporary anthropological and ecological study as it faces many threats including fragmentation of land, conversion of rangeland into urban development, lack of grazing movement, impending threats on global diversity, damage to species with large terrain, decreases in shared public goods, decreased biological movements, threats of a ""tragedy of enclosures"", limitation of key resources, reduced biomass and invasive plant species growth. Interest in contemporary pastoralist cultures like the Maasai has continued to increase, especially because the traditional syncreticly-adaptive ability of pastoralists could promise lessons in collaborative and adaptive management for contemporary pastoralist societies threatened by globalization as well as for contemporary non-pastoralist societies that are managing livestock on rangelands.

United States of America
The United States Society for Range Management is ""the professional society dedicated to supporting persons who work with rangelands and have a commitment to their sustainable use""."
What are some of the goods and services that rangelands provide?,"Red meat, wildlife, water, wood, fiber, leather, and energy resources","Soil protection, riparian zone enhancement, and vegetation maintenance",Outdoor recreation and sustainable range product outputs,Improvement of animal and plant production through grazing,All of the above,E,"Rangeland management (also range management, range science, or arid-land management) is a natural science that centers around the study of rangelands and the ""conservation and sustainable management [of Arid-Lands] for the benefit of current societies and future generations"". Range management is defined by Holechek et al. as the ""manipulation of rangeland components to obtain optimum combination of goods and services for society on a sustained basis"".

History
The earliest form of Rangeland Management is not formally deemed part of the natural science studied today, although its roots can be traced to nomadic grazing practices of the neolithic agricultural revolution when humans domesticated plants and animals under pressures from population growth and environmental change. Humans might even have altered the environment in times preceding the  Neolithic through hunting of large-game, whereby large losses of grazing herbivores could have resulted in altered ecological states; meaning humans have been inadvertently managing land throughout prehistory.Rangeland management was developed in the United States in response to rangeland deterioration and in some cases, denudation, due to overgrazing and other misuse of arid lands as demonstrated by the 20th century ""Dust Bowl"" and described in Hardin's 1968 ""Tragedy of the Commons"". Historically, the discipline focused on the manipulation of grazing and the proper use of rangeland vegetation for livestock.

Modern application
Global
Range management's focus has been expanded to include the host of ecosystem services that rangelands provide to humans world-wide. Key management components seek to optimize such goods and services through the protection and enhancement of soils, riparian zones, watersheds, and vegetation complexes, sustainably improving outputs of consumable range products such as red meat, wildlife, water, wood, fiber, leather, energy resource extraction, and outdoor recreation, as well as maintaining a focus on the manipulation of grazing activities of large herbivores to maintain or improve animal and plant production.Pastoralism has become a contemporary anthropological and ecological study as it faces many threats including fragmentation of land, conversion of rangeland into urban development, lack of grazing movement, impending threats on global diversity, damage to species with large terrain, decreases in shared public goods, decreased biological movements, threats of a ""tragedy of enclosures"", limitation of key resources, reduced biomass and invasive plant species growth. Interest in contemporary pastoralist cultures like the Maasai has continued to increase, especially because the traditional syncreticly-adaptive ability of pastoralists could promise lessons in collaborative and adaptive management for contemporary pastoralist societies threatened by globalization as well as for contemporary non-pastoralist societies that are managing livestock on rangelands.

United States of America
The United States Society for Range Management is ""the professional society dedicated to supporting persons who work with rangelands and have a commitment to their sustainable use""."
What are some of the threats to contemporary pastoralist cultures?,Fragmentation of land and conversion of rangeland into urban development,Lack of grazing movement and threats to global diversity,Damage to species with large terrain and decreased shared public goods,Decreased biological movements and threats of enclosures,All of the above,E,"Rangeland management (also range management, range science, or arid-land management) is a natural science that centers around the study of rangelands and the ""conservation and sustainable management [of Arid-Lands] for the benefit of current societies and future generations"". Range management is defined by Holechek et al. as the ""manipulation of rangeland components to obtain optimum combination of goods and services for society on a sustained basis"".

History
The earliest form of Rangeland Management is not formally deemed part of the natural science studied today, although its roots can be traced to nomadic grazing practices of the neolithic agricultural revolution when humans domesticated plants and animals under pressures from population growth and environmental change. Humans might even have altered the environment in times preceding the  Neolithic through hunting of large-game, whereby large losses of grazing herbivores could have resulted in altered ecological states; meaning humans have been inadvertently managing land throughout prehistory.Rangeland management was developed in the United States in response to rangeland deterioration and in some cases, denudation, due to overgrazing and other misuse of arid lands as demonstrated by the 20th century ""Dust Bowl"" and described in Hardin's 1968 ""Tragedy of the Commons"". Historically, the discipline focused on the manipulation of grazing and the proper use of rangeland vegetation for livestock.

Modern application
Global
Range management's focus has been expanded to include the host of ecosystem services that rangelands provide to humans world-wide. Key management components seek to optimize such goods and services through the protection and enhancement of soils, riparian zones, watersheds, and vegetation complexes, sustainably improving outputs of consumable range products such as red meat, wildlife, water, wood, fiber, leather, energy resource extraction, and outdoor recreation, as well as maintaining a focus on the manipulation of grazing activities of large herbivores to maintain or improve animal and plant production.Pastoralism has become a contemporary anthropological and ecological study as it faces many threats including fragmentation of land, conversion of rangeland into urban development, lack of grazing movement, impending threats on global diversity, damage to species with large terrain, decreases in shared public goods, decreased biological movements, threats of a ""tragedy of enclosures"", limitation of key resources, reduced biomass and invasive plant species growth. Interest in contemporary pastoralist cultures like the Maasai has continued to increase, especially because the traditional syncreticly-adaptive ability of pastoralists could promise lessons in collaborative and adaptive management for contemporary pastoralist societies threatened by globalization as well as for contemporary non-pastoralist societies that are managing livestock on rangelands.

United States of America
The United States Society for Range Management is ""the professional society dedicated to supporting persons who work with rangelands and have a commitment to their sustainable use""."
What is the mission of the United States Society for Range Management?,To support sustainable use of rangelands and promote collaboration,To study and improve grazing practices for livestock production,"To protect and enhance soils, watersheds, and vegetation complexes",To optimize goods and services provided by rangelands,To educate and provide resources for range management professionals,A,"Rangeland management (also range management, range science, or arid-land management) is a natural science that centers around the study of rangelands and the ""conservation and sustainable management [of Arid-Lands] for the benefit of current societies and future generations"". Range management is defined by Holechek et al. as the ""manipulation of rangeland components to obtain optimum combination of goods and services for society on a sustained basis"".

History
The earliest form of Rangeland Management is not formally deemed part of the natural science studied today, although its roots can be traced to nomadic grazing practices of the neolithic agricultural revolution when humans domesticated plants and animals under pressures from population growth and environmental change. Humans might even have altered the environment in times preceding the  Neolithic through hunting of large-game, whereby large losses of grazing herbivores could have resulted in altered ecological states; meaning humans have been inadvertently managing land throughout prehistory.Rangeland management was developed in the United States in response to rangeland deterioration and in some cases, denudation, due to overgrazing and other misuse of arid lands as demonstrated by the 20th century ""Dust Bowl"" and described in Hardin's 1968 ""Tragedy of the Commons"". Historically, the discipline focused on the manipulation of grazing and the proper use of rangeland vegetation for livestock.

Modern application
Global
Range management's focus has been expanded to include the host of ecosystem services that rangelands provide to humans world-wide. Key management components seek to optimize such goods and services through the protection and enhancement of soils, riparian zones, watersheds, and vegetation complexes, sustainably improving outputs of consumable range products such as red meat, wildlife, water, wood, fiber, leather, energy resource extraction, and outdoor recreation, as well as maintaining a focus on the manipulation of grazing activities of large herbivores to maintain or improve animal and plant production.Pastoralism has become a contemporary anthropological and ecological study as it faces many threats including fragmentation of land, conversion of rangeland into urban development, lack of grazing movement, impending threats on global diversity, damage to species with large terrain, decreases in shared public goods, decreased biological movements, threats of a ""tragedy of enclosures"", limitation of key resources, reduced biomass and invasive plant species growth. Interest in contemporary pastoralist cultures like the Maasai has continued to increase, especially because the traditional syncreticly-adaptive ability of pastoralists could promise lessons in collaborative and adaptive management for contemporary pastoralist societies threatened by globalization as well as for contemporary non-pastoralist societies that are managing livestock on rangelands.

United States of America
The United States Society for Range Management is ""the professional society dedicated to supporting persons who work with rangelands and have a commitment to their sustainable use""."
What is textile manufacturing?,The process of converting fabric into useful goods,"The process of converting fibre into yarn, then yarn into fabric",The process of dyeing or printing fabric,The process of producing cotton clothing and accessories,The process of making various industrial products,B,"Textile manufacturing (or textile engineering) is a major industry. It is largely based on the conversion of fibre into yarn, then yarn into fabric. These are then dyed or printed, fabricated into cloth which is then converted into useful goods such as clothing, household items, upholstery and various industrial products.Different types of fibres are used to produce yarn. Cotton remains the most widely used and common natural fiber making up 90% of all-natural fibers used in the textile industry. People often use cotton clothing and accessories because of comfort, not limited to different weathers. There are many variable processes available at the spinning and fabric-forming stages coupled with the complexities of the finishing and colouration processes to the production of a wide range of products.

History
Textile manufacturing in the modern era is an evolved form of the art and craft industries. Until the 18th and 19th centuries, the textile industry was a household work."
What is the most widely used natural fiber in the textile industry?,Silk,Wool,Cotton,Linen,Hemp,C,"Textile manufacturing (or textile engineering) is a major industry. It is largely based on the conversion of fibre into yarn, then yarn into fabric. These are then dyed or printed, fabricated into cloth which is then converted into useful goods such as clothing, household items, upholstery and various industrial products.Different types of fibres are used to produce yarn. Cotton remains the most widely used and common natural fiber making up 90% of all-natural fibers used in the textile industry. People often use cotton clothing and accessories because of comfort, not limited to different weathers. There are many variable processes available at the spinning and fabric-forming stages coupled with the complexities of the finishing and colouration processes to the production of a wide range of products.

History
Textile manufacturing in the modern era is an evolved form of the art and craft industries. Until the 18th and 19th centuries, the textile industry was a household work."
What are some examples of useful goods produced by textile manufacturing?,Electronics,Automobiles,Furniture,Food,Clothing,E,"Textile manufacturing (or textile engineering) is a major industry. It is largely based on the conversion of fibre into yarn, then yarn into fabric. These are then dyed or printed, fabricated into cloth which is then converted into useful goods such as clothing, household items, upholstery and various industrial products.Different types of fibres are used to produce yarn. Cotton remains the most widely used and common natural fiber making up 90% of all-natural fibers used in the textile industry. People often use cotton clothing and accessories because of comfort, not limited to different weathers. There are many variable processes available at the spinning and fabric-forming stages coupled with the complexities of the finishing and colouration processes to the production of a wide range of products.

History
Textile manufacturing in the modern era is an evolved form of the art and craft industries. Until the 18th and 19th centuries, the textile industry was a household work."
What was the textile industry like before the 18th and 19th centuries?,It was an industrial process,It was a household work,It was non-existent,It was a form of art and craft,It was focused on producing industrial products,B,"Textile manufacturing (or textile engineering) is a major industry. It is largely based on the conversion of fibre into yarn, then yarn into fabric. These are then dyed or printed, fabricated into cloth which is then converted into useful goods such as clothing, household items, upholstery and various industrial products.Different types of fibres are used to produce yarn. Cotton remains the most widely used and common natural fiber making up 90% of all-natural fibers used in the textile industry. People often use cotton clothing and accessories because of comfort, not limited to different weathers. There are many variable processes available at the spinning and fabric-forming stages coupled with the complexities of the finishing and colouration processes to the production of a wide range of products.

History
Textile manufacturing in the modern era is an evolved form of the art and craft industries. Until the 18th and 19th centuries, the textile industry was a household work."
What are the different stages involved in textile manufacturing?,Spinning and fabric-forming,Dyeing and printing,Finishing and colouration,Production of cotton clothing and accessories,Fabrication into cloth,A,"Textile manufacturing (or textile engineering) is a major industry. It is largely based on the conversion of fibre into yarn, then yarn into fabric. These are then dyed or printed, fabricated into cloth which is then converted into useful goods such as clothing, household items, upholstery and various industrial products.Different types of fibres are used to produce yarn. Cotton remains the most widely used and common natural fiber making up 90% of all-natural fibers used in the textile industry. People often use cotton clothing and accessories because of comfort, not limited to different weathers. There are many variable processes available at the spinning and fabric-forming stages coupled with the complexities of the finishing and colouration processes to the production of a wide range of products.

History
Textile manufacturing in the modern era is an evolved form of the art and craft industries. Until the 18th and 19th centuries, the textile industry was a household work."
What is a ghost surgery?,A surgery that is performed by unqualified surgeons,A surgery that is performed by licensed surgeons,A surgery that is performed by medical students,A surgery that is performed by experienced surgeons,A surgery that is performed by salesmen,A,"A ghost surgery is a surgery in which the person who performs the operation, the ""ghost surgeon"" or ""ghost doctor"", is not the surgeon that was hired for and is credited with the operation. The ghost doctor substitutes the hired surgeon while the patient is unconscious from anesthesia. Ghost doctors are often unlicensed and unqualified to perform the operations they are hired for. Ghost surgery is widely considered to be unethical.Today, ghost surgery is particularly common in the South Korean cosmetic surgery industry, in which it is ""rampant.""

History
The term ""ghost surgery"" was originally used to refer to qualified surgeons performing operations in the place of the unqualified physicians credited with the operation. By the late 1970s, following multiple highly publicized cases of medical equipment salesmen participating in surgery, it came to refer to multiple kinds of scenarios in which one individual substitutes the officially credited surgeon in an operation.The Lifflander Report, a 1978 investigation commissioned by Speaker of the State Assembly of New York Stanley Steingut, found that 50-95% of surgeries in teaching hospitals were performed by residents under supervision, rather than by the lead surgeon. The report concluded that this generally does not decrease the quality of care, and is necessary for medical students to gain experience. However, patients typically do not understand and are not properly notified of the extent to which persons besides the lead surgeon will participate in the surgery.Ghost surgery has continued as a practice into the present day; most reports of ghost surgery in the United States involve associates or residents performing operations to a greater extent than the patient consented to.In South Korea, ghost surgery boomed in the 2010s with the increase in demand for plastic surgery, following the government's promotion of medical tourism."
When did ghost surgery start to refer to scenarios where one individual substitutes the officially credited surgeon?,In the 1970s,In the 1980s,In the 1960s,In the 1950s,In the 1990s,A,"A ghost surgery is a surgery in which the person who performs the operation, the ""ghost surgeon"" or ""ghost doctor"", is not the surgeon that was hired for and is credited with the operation. The ghost doctor substitutes the hired surgeon while the patient is unconscious from anesthesia. Ghost doctors are often unlicensed and unqualified to perform the operations they are hired for. Ghost surgery is widely considered to be unethical.Today, ghost surgery is particularly common in the South Korean cosmetic surgery industry, in which it is ""rampant.""

History
The term ""ghost surgery"" was originally used to refer to qualified surgeons performing operations in the place of the unqualified physicians credited with the operation. By the late 1970s, following multiple highly publicized cases of medical equipment salesmen participating in surgery, it came to refer to multiple kinds of scenarios in which one individual substitutes the officially credited surgeon in an operation.The Lifflander Report, a 1978 investigation commissioned by Speaker of the State Assembly of New York Stanley Steingut, found that 50-95% of surgeries in teaching hospitals were performed by residents under supervision, rather than by the lead surgeon. The report concluded that this generally does not decrease the quality of care, and is necessary for medical students to gain experience. However, patients typically do not understand and are not properly notified of the extent to which persons besides the lead surgeon will participate in the surgery.Ghost surgery has continued as a practice into the present day; most reports of ghost surgery in the United States involve associates or residents performing operations to a greater extent than the patient consented to.In South Korea, ghost surgery boomed in the 2010s with the increase in demand for plastic surgery, following the government's promotion of medical tourism."
"According to the Lifflander Report, what percentage of surgeries in teaching hospitals were performed by residents under supervision?",5-50%,10-30%,50-95%,70-90%,30-70%,C,"A ghost surgery is a surgery in which the person who performs the operation, the ""ghost surgeon"" or ""ghost doctor"", is not the surgeon that was hired for and is credited with the operation. The ghost doctor substitutes the hired surgeon while the patient is unconscious from anesthesia. Ghost doctors are often unlicensed and unqualified to perform the operations they are hired for. Ghost surgery is widely considered to be unethical.Today, ghost surgery is particularly common in the South Korean cosmetic surgery industry, in which it is ""rampant.""

History
The term ""ghost surgery"" was originally used to refer to qualified surgeons performing operations in the place of the unqualified physicians credited with the operation. By the late 1970s, following multiple highly publicized cases of medical equipment salesmen participating in surgery, it came to refer to multiple kinds of scenarios in which one individual substitutes the officially credited surgeon in an operation.The Lifflander Report, a 1978 investigation commissioned by Speaker of the State Assembly of New York Stanley Steingut, found that 50-95% of surgeries in teaching hospitals were performed by residents under supervision, rather than by the lead surgeon. The report concluded that this generally does not decrease the quality of care, and is necessary for medical students to gain experience. However, patients typically do not understand and are not properly notified of the extent to which persons besides the lead surgeon will participate in the surgery.Ghost surgery has continued as a practice into the present day; most reports of ghost surgery in the United States involve associates or residents performing operations to a greater extent than the patient consented to.In South Korea, ghost surgery boomed in the 2010s with the increase in demand for plastic surgery, following the government's promotion of medical tourism."
Why is ghost surgery considered unethical?,Because it is performed by qualified surgeons,Because it decreases the quality of care,Because it is necessary for medical students to gain experience,Because patients are not properly notified of the extent of participation of other individuals,Because it is performed by unqualified surgeons,D,"A ghost surgery is a surgery in which the person who performs the operation, the ""ghost surgeon"" or ""ghost doctor"", is not the surgeon that was hired for and is credited with the operation. The ghost doctor substitutes the hired surgeon while the patient is unconscious from anesthesia. Ghost doctors are often unlicensed and unqualified to perform the operations they are hired for. Ghost surgery is widely considered to be unethical.Today, ghost surgery is particularly common in the South Korean cosmetic surgery industry, in which it is ""rampant.""

History
The term ""ghost surgery"" was originally used to refer to qualified surgeons performing operations in the place of the unqualified physicians credited with the operation. By the late 1970s, following multiple highly publicized cases of medical equipment salesmen participating in surgery, it came to refer to multiple kinds of scenarios in which one individual substitutes the officially credited surgeon in an operation.The Lifflander Report, a 1978 investigation commissioned by Speaker of the State Assembly of New York Stanley Steingut, found that 50-95% of surgeries in teaching hospitals were performed by residents under supervision, rather than by the lead surgeon. The report concluded that this generally does not decrease the quality of care, and is necessary for medical students to gain experience. However, patients typically do not understand and are not properly notified of the extent to which persons besides the lead surgeon will participate in the surgery.Ghost surgery has continued as a practice into the present day; most reports of ghost surgery in the United States involve associates or residents performing operations to a greater extent than the patient consented to.In South Korea, ghost surgery boomed in the 2010s with the increase in demand for plastic surgery, following the government's promotion of medical tourism."
In which country is ghost surgery particularly common in the cosmetic surgery industry?,United States,France,South Korea,Japan,China,C,"A ghost surgery is a surgery in which the person who performs the operation, the ""ghost surgeon"" or ""ghost doctor"", is not the surgeon that was hired for and is credited with the operation. The ghost doctor substitutes the hired surgeon while the patient is unconscious from anesthesia. Ghost doctors are often unlicensed and unqualified to perform the operations they are hired for. Ghost surgery is widely considered to be unethical.Today, ghost surgery is particularly common in the South Korean cosmetic surgery industry, in which it is ""rampant.""

History
The term ""ghost surgery"" was originally used to refer to qualified surgeons performing operations in the place of the unqualified physicians credited with the operation. By the late 1970s, following multiple highly publicized cases of medical equipment salesmen participating in surgery, it came to refer to multiple kinds of scenarios in which one individual substitutes the officially credited surgeon in an operation.The Lifflander Report, a 1978 investigation commissioned by Speaker of the State Assembly of New York Stanley Steingut, found that 50-95% of surgeries in teaching hospitals were performed by residents under supervision, rather than by the lead surgeon. The report concluded that this generally does not decrease the quality of care, and is necessary for medical students to gain experience. However, patients typically do not understand and are not properly notified of the extent to which persons besides the lead surgeon will participate in the surgery.Ghost surgery has continued as a practice into the present day; most reports of ghost surgery in the United States involve associates or residents performing operations to a greater extent than the patient consented to.In South Korea, ghost surgery boomed in the 2010s with the increase in demand for plastic surgery, following the government's promotion of medical tourism."
What is the purpose of Terrestrial Time (TT)?,To measure time for astronomical observations made from Earth,To define the rotation of Earth,To synchronize atomic clocks worldwide,To calculate the positions of celestial bodies in the solar system,To measure time for space missions beyond Earth,A,"Terrestrial Time (TT) is a modern astronomical time standard defined by the International Astronomical Union, primarily for time-measurements of astronomical observations made from the surface of Earth.
For example, the Astronomical Almanac uses TT for its tables of positions (ephemerides) of the Sun, Moon and planets as seen from Earth. In this role, TT continues Terrestrial Dynamical Time (TDT or TD), which succeeded ephemeris time (ET). TT shares the original purpose for which ET was designed, to be free of the irregularities in the rotation of Earth.
The unit of TT is the SI second, the definition of which is based currently on the caesium atomic clock, but TT is not itself defined by atomic clocks.  It is a theoretical ideal, and real clocks can only approximate it.
TT is distinct from the time scale often used as a basis for civil purposes, Coordinated Universal Time (UTC).  TT is indirectly the basis of UTC, via International Atomic Time (TAI).  Because of the historical difference between TAI and ET when TT was introduced, TT is approximately 32.184 s ahead of TAI.

History
A definition of a terrestrial time standard was adopted by the International Astronomical Union (IAU) in 1976 at its XVI General Assembly and later named Terrestrial Dynamical Time (TDT). It was the counterpart to Barycentric Dynamical Time (TDB), which was a time standard for Solar system ephemerides, to be based on a dynamical time scale."
Which time scale is often used for civil purposes?,Terrestrial Time (TT),Coordinated Universal Time (UTC),International Atomic Time (TAI),Terrestrial Dynamical Time (TDT),Barycentric Dynamical Time (TDB),B,"Terrestrial Time (TT) is a modern astronomical time standard defined by the International Astronomical Union, primarily for time-measurements of astronomical observations made from the surface of Earth.
For example, the Astronomical Almanac uses TT for its tables of positions (ephemerides) of the Sun, Moon and planets as seen from Earth. In this role, TT continues Terrestrial Dynamical Time (TDT or TD), which succeeded ephemeris time (ET). TT shares the original purpose for which ET was designed, to be free of the irregularities in the rotation of Earth.
The unit of TT is the SI second, the definition of which is based currently on the caesium atomic clock, but TT is not itself defined by atomic clocks.  It is a theoretical ideal, and real clocks can only approximate it.
TT is distinct from the time scale often used as a basis for civil purposes, Coordinated Universal Time (UTC).  TT is indirectly the basis of UTC, via International Atomic Time (TAI).  Because of the historical difference between TAI and ET when TT was introduced, TT is approximately 32.184 s ahead of TAI.

History
A definition of a terrestrial time standard was adopted by the International Astronomical Union (IAU) in 1976 at its XVI General Assembly and later named Terrestrial Dynamical Time (TDT). It was the counterpart to Barycentric Dynamical Time (TDB), which was a time standard for Solar system ephemerides, to be based on a dynamical time scale."
What is the relationship between Terrestrial Time (TT) and International Atomic Time (TAI)?,TT is ahead of TAI by approximately 32.184 seconds,TT is synchronized with TAI,TT is behind TAI by approximately 32.184 seconds,TT is equal to TAI,TT is ahead of TAI by approximately 1 second,A,"Terrestrial Time (TT) is a modern astronomical time standard defined by the International Astronomical Union, primarily for time-measurements of astronomical observations made from the surface of Earth.
For example, the Astronomical Almanac uses TT for its tables of positions (ephemerides) of the Sun, Moon and planets as seen from Earth. In this role, TT continues Terrestrial Dynamical Time (TDT or TD), which succeeded ephemeris time (ET). TT shares the original purpose for which ET was designed, to be free of the irregularities in the rotation of Earth.
The unit of TT is the SI second, the definition of which is based currently on the caesium atomic clock, but TT is not itself defined by atomic clocks.  It is a theoretical ideal, and real clocks can only approximate it.
TT is distinct from the time scale often used as a basis for civil purposes, Coordinated Universal Time (UTC).  TT is indirectly the basis of UTC, via International Atomic Time (TAI).  Because of the historical difference between TAI and ET when TT was introduced, TT is approximately 32.184 s ahead of TAI.

History
A definition of a terrestrial time standard was adopted by the International Astronomical Union (IAU) in 1976 at its XVI General Assembly and later named Terrestrial Dynamical Time (TDT). It was the counterpart to Barycentric Dynamical Time (TDB), which was a time standard for Solar system ephemerides, to be based on a dynamical time scale."
When was the terrestrial time standard adopted by the International Astronomical Union (IAU)?,XVI General Assembly,XV General Assembly,XIV General Assembly,XIII General Assembly,XII General Assembly,A,"Terrestrial Time (TT) is a modern astronomical time standard defined by the International Astronomical Union, primarily for time-measurements of astronomical observations made from the surface of Earth.
For example, the Astronomical Almanac uses TT for its tables of positions (ephemerides) of the Sun, Moon and planets as seen from Earth. In this role, TT continues Terrestrial Dynamical Time (TDT or TD), which succeeded ephemeris time (ET). TT shares the original purpose for which ET was designed, to be free of the irregularities in the rotation of Earth.
The unit of TT is the SI second, the definition of which is based currently on the caesium atomic clock, but TT is not itself defined by atomic clocks.  It is a theoretical ideal, and real clocks can only approximate it.
TT is distinct from the time scale often used as a basis for civil purposes, Coordinated Universal Time (UTC).  TT is indirectly the basis of UTC, via International Atomic Time (TAI).  Because of the historical difference between TAI and ET when TT was introduced, TT is approximately 32.184 s ahead of TAI.

History
A definition of a terrestrial time standard was adopted by the International Astronomical Union (IAU) in 1976 at its XVI General Assembly and later named Terrestrial Dynamical Time (TDT). It was the counterpart to Barycentric Dynamical Time (TDB), which was a time standard for Solar system ephemerides, to be based on a dynamical time scale."
What is the unit of Terrestrial Time (TT)?,Second,Minute,Hour,Day,Year,A,"Terrestrial Time (TT) is a modern astronomical time standard defined by the International Astronomical Union, primarily for time-measurements of astronomical observations made from the surface of Earth.
For example, the Astronomical Almanac uses TT for its tables of positions (ephemerides) of the Sun, Moon and planets as seen from Earth. In this role, TT continues Terrestrial Dynamical Time (TDT or TD), which succeeded ephemeris time (ET). TT shares the original purpose for which ET was designed, to be free of the irregularities in the rotation of Earth.
The unit of TT is the SI second, the definition of which is based currently on the caesium atomic clock, but TT is not itself defined by atomic clocks.  It is a theoretical ideal, and real clocks can only approximate it.
TT is distinct from the time scale often used as a basis for civil purposes, Coordinated Universal Time (UTC).  TT is indirectly the basis of UTC, via International Atomic Time (TAI).  Because of the historical difference between TAI and ET when TT was introduced, TT is approximately 32.184 s ahead of TAI.

History
A definition of a terrestrial time standard was adopted by the International Astronomical Union (IAU) in 1976 at its XVI General Assembly and later named Terrestrial Dynamical Time (TDT). It was the counterpart to Barycentric Dynamical Time (TDB), which was a time standard for Solar system ephemerides, to be based on a dynamical time scale."
What is the objective of industrial and production engineering?,To increase company profits,To automate all production processes,To improve efficiency and reduce cost,To develop new manufacturing technologies,To manage complex organizations,C,"Industrial and production engineering (IPE) is an interdisciplinary engineering discipline that includes manufacturing technology, engineering sciences, management science, and optimization of complex processes, systems, or organizations. It is concerned with the understanding and application of engineering procedures in manufacturing processes and production methods. Industrial engineering dates back all the way to the industrial revolution, initiated in 1700s by Sir Adam Smith, Henry Ford, Eli Whitney, Frank Gilbreth and Lilian Gilbreth, Henry Gantt, F.W. Taylor, etc. After the 1970s, industrial and production engineering developed worldwide and started to widely use automation and robotics. Industrial and production engineering includes three areas: Mechanical engineering (where the production engineering comes from), industrial engineering, and management science. 
The objective is to improve efficiency, drive up effectiveness of manufacturing, quality control, and to reduce cost while making their products more attractive and marketable."
Who were some of the pioneers in industrial engineering?,Sir Isaac Newton,Thomas Edison,Nikola Tesla,Henry Ford,Albert Einstein,D,"Industrial and production engineering (IPE) is an interdisciplinary engineering discipline that includes manufacturing technology, engineering sciences, management science, and optimization of complex processes, systems, or organizations. It is concerned with the understanding and application of engineering procedures in manufacturing processes and production methods. Industrial engineering dates back all the way to the industrial revolution, initiated in 1700s by Sir Adam Smith, Henry Ford, Eli Whitney, Frank Gilbreth and Lilian Gilbreth, Henry Gantt, F.W. Taylor, etc. After the 1970s, industrial and production engineering developed worldwide and started to widely use automation and robotics. Industrial and production engineering includes three areas: Mechanical engineering (where the production engineering comes from), industrial engineering, and management science. 
The objective is to improve efficiency, drive up effectiveness of manufacturing, quality control, and to reduce cost while making their products more attractive and marketable."
What are the three areas included in industrial and production engineering?,"Mechanical engineering, civil engineering, and electrical engineering","Computer science, chemical engineering, and materials science","Mechanical engineering, industrial engineering, and management science","Aerospace engineering, biomedical engineering, and environmental engineering","Petroleum engineering, nuclear engineering, and software engineering",C,"Industrial and production engineering (IPE) is an interdisciplinary engineering discipline that includes manufacturing technology, engineering sciences, management science, and optimization of complex processes, systems, or organizations. It is concerned with the understanding and application of engineering procedures in manufacturing processes and production methods. Industrial engineering dates back all the way to the industrial revolution, initiated in 1700s by Sir Adam Smith, Henry Ford, Eli Whitney, Frank Gilbreth and Lilian Gilbreth, Henry Gantt, F.W. Taylor, etc. After the 1970s, industrial and production engineering developed worldwide and started to widely use automation and robotics. Industrial and production engineering includes three areas: Mechanical engineering (where the production engineering comes from), industrial engineering, and management science. 
The objective is to improve efficiency, drive up effectiveness of manufacturing, quality control, and to reduce cost while making their products more attractive and marketable."
When did industrial and production engineering start to widely use automation and robotics?,In the 1700s during the industrial revolution,In the 1970s,In the 1800s with the development of the assembly line,In the 1950s with the invention of the computer,In the 2000s with the rise of artificial intelligence,B,"Industrial and production engineering (IPE) is an interdisciplinary engineering discipline that includes manufacturing technology, engineering sciences, management science, and optimization of complex processes, systems, or organizations. It is concerned with the understanding and application of engineering procedures in manufacturing processes and production methods. Industrial engineering dates back all the way to the industrial revolution, initiated in 1700s by Sir Adam Smith, Henry Ford, Eli Whitney, Frank Gilbreth and Lilian Gilbreth, Henry Gantt, F.W. Taylor, etc. After the 1970s, industrial and production engineering developed worldwide and started to widely use automation and robotics. Industrial and production engineering includes three areas: Mechanical engineering (where the production engineering comes from), industrial engineering, and management science. 
The objective is to improve efficiency, drive up effectiveness of manufacturing, quality control, and to reduce cost while making their products more attractive and marketable."
What disciplines are included in industrial and production engineering?,Only mechanical engineering,Only industrial engineering,"Mechanical engineering, industrial engineering, and management science",Civil engineering and electrical engineering,Computer science and materials science,C,"Industrial and production engineering (IPE) is an interdisciplinary engineering discipline that includes manufacturing technology, engineering sciences, management science, and optimization of complex processes, systems, or organizations. It is concerned with the understanding and application of engineering procedures in manufacturing processes and production methods. Industrial engineering dates back all the way to the industrial revolution, initiated in 1700s by Sir Adam Smith, Henry Ford, Eli Whitney, Frank Gilbreth and Lilian Gilbreth, Henry Gantt, F.W. Taylor, etc. After the 1970s, industrial and production engineering developed worldwide and started to widely use automation and robotics. Industrial and production engineering includes three areas: Mechanical engineering (where the production engineering comes from), industrial engineering, and management science. 
The objective is to improve efficiency, drive up effectiveness of manufacturing, quality control, and to reduce cost while making their products more attractive and marketable."
What is the purpose of Particle-induced X-ray emission or proton-induced X-ray emission (PIXE)?,To determine the age of a material or sample,To determine the distribution of trace elements in a wide range of samples,To detect some light elements,To study the interactions between ions and atoms,To measure the wavelengths in the x-ray part of the electromagnetic spectrum,B,"Particle-induced X-ray emission or proton-induced X-ray emission (PIXE) is a technique used for determining the elemental composition of a material or a sample. When a material is exposed to an ion beam, atomic interactions occur that give off EM radiation of wavelengths in the x-ray part of the electromagnetic spectrum specific to an element. PIXE is a powerful yet non-destructive elemental analysis technique now used routinely by geologists, archaeologists, art conservators and others to help answer questions of provenance, dating and authenticity.
The technique was first proposed in 1970 by Sven Johansson of Lund University, Sweden, and developed over the next few years with his colleagues Roland Akselsson and Thomas B Johansson.Recent extensions of PIXE using tightly focused beams (down to 1 μm) gives the additional capability of microscopic analysis. This technique, called microPIXE, can be used to determine the distribution of trace elements in a wide range of samples. A related technique, particle-induced gamma-ray emission (PIGE) can be used to detect some light elements.

Theory
Three types of spectra can be collected from a PIXE experiment:

X-ray emission spectrum.
Rutherford backscattering spectrum.
Proton transmission spectrum.

X-ray emission
Quantum theory states that orbiting electrons of an atom must occupy discrete energy levels in order to be stable. Bombardment with ions of sufficient energy (usually MeV protons) produced by an ion accelerator, will cause inner shell ionization of atoms in a specimen. Outer shell electrons drop down to replace inner shell vacancies, however only certain transitions are allowed."
Who first proposed the technique of PIXE?,Sven Johansson,Roland Akselsson,Thomas B Johansson,Sven Johansson and Roland Akselsson,"Sven Johansson, Roland Akselsson and Thomas B Johansson",E,"Particle-induced X-ray emission or proton-induced X-ray emission (PIXE) is a technique used for determining the elemental composition of a material or a sample. When a material is exposed to an ion beam, atomic interactions occur that give off EM radiation of wavelengths in the x-ray part of the electromagnetic spectrum specific to an element. PIXE is a powerful yet non-destructive elemental analysis technique now used routinely by geologists, archaeologists, art conservators and others to help answer questions of provenance, dating and authenticity.
The technique was first proposed in 1970 by Sven Johansson of Lund University, Sweden, and developed over the next few years with his colleagues Roland Akselsson and Thomas B Johansson.Recent extensions of PIXE using tightly focused beams (down to 1 μm) gives the additional capability of microscopic analysis. This technique, called microPIXE, can be used to determine the distribution of trace elements in a wide range of samples. A related technique, particle-induced gamma-ray emission (PIGE) can be used to detect some light elements.

Theory
Three types of spectra can be collected from a PIXE experiment:

X-ray emission spectrum.
Rutherford backscattering spectrum.
Proton transmission spectrum.

X-ray emission
Quantum theory states that orbiting electrons of an atom must occupy discrete energy levels in order to be stable. Bombardment with ions of sufficient energy (usually MeV protons) produced by an ion accelerator, will cause inner shell ionization of atoms in a specimen. Outer shell electrons drop down to replace inner shell vacancies, however only certain transitions are allowed."
What additional capability does microPIXE offer compared to regular PIXE?,Ability to determine the age of a material or sample,Ability to detect some light elements,Ability to measure the wavelengths in the x-ray part of the electromagnetic spectrum,Ability to determine the distribution of trace elements in a wide range of samples,Ability to perform microscopic analysis,E,"Particle-induced X-ray emission or proton-induced X-ray emission (PIXE) is a technique used for determining the elemental composition of a material or a sample. When a material is exposed to an ion beam, atomic interactions occur that give off EM radiation of wavelengths in the x-ray part of the electromagnetic spectrum specific to an element. PIXE is a powerful yet non-destructive elemental analysis technique now used routinely by geologists, archaeologists, art conservators and others to help answer questions of provenance, dating and authenticity.
The technique was first proposed in 1970 by Sven Johansson of Lund University, Sweden, and developed over the next few years with his colleagues Roland Akselsson and Thomas B Johansson.Recent extensions of PIXE using tightly focused beams (down to 1 μm) gives the additional capability of microscopic analysis. This technique, called microPIXE, can be used to determine the distribution of trace elements in a wide range of samples. A related technique, particle-induced gamma-ray emission (PIGE) can be used to detect some light elements.

Theory
Three types of spectra can be collected from a PIXE experiment:

X-ray emission spectrum.
Rutherford backscattering spectrum.
Proton transmission spectrum.

X-ray emission
Quantum theory states that orbiting electrons of an atom must occupy discrete energy levels in order to be stable. Bombardment with ions of sufficient energy (usually MeV protons) produced by an ion accelerator, will cause inner shell ionization of atoms in a specimen. Outer shell electrons drop down to replace inner shell vacancies, however only certain transitions are allowed."
What are the three types of spectra that can be collected from a PIXE experiment?,"X-ray emission spectrum, Rutherford backscattering spectrum, Proton transmission spectrum","X-ray emission spectrum, Electron emission spectrum, Proton transmission spectrum","X-ray emission spectrum, Rutherford backscattering spectrum, Electron emission spectrum","Rutherford backscattering spectrum, Proton transmission spectrum, Electron emission spectrum","X-ray emission spectrum, Rutherford backscattering spectrum, Gamma-ray emission spectrum",A,"Particle-induced X-ray emission or proton-induced X-ray emission (PIXE) is a technique used for determining the elemental composition of a material or a sample. When a material is exposed to an ion beam, atomic interactions occur that give off EM radiation of wavelengths in the x-ray part of the electromagnetic spectrum specific to an element. PIXE is a powerful yet non-destructive elemental analysis technique now used routinely by geologists, archaeologists, art conservators and others to help answer questions of provenance, dating and authenticity.
The technique was first proposed in 1970 by Sven Johansson of Lund University, Sweden, and developed over the next few years with his colleagues Roland Akselsson and Thomas B Johansson.Recent extensions of PIXE using tightly focused beams (down to 1 μm) gives the additional capability of microscopic analysis. This technique, called microPIXE, can be used to determine the distribution of trace elements in a wide range of samples. A related technique, particle-induced gamma-ray emission (PIGE) can be used to detect some light elements.

Theory
Three types of spectra can be collected from a PIXE experiment:

X-ray emission spectrum.
Rutherford backscattering spectrum.
Proton transmission spectrum.

X-ray emission
Quantum theory states that orbiting electrons of an atom must occupy discrete energy levels in order to be stable. Bombardment with ions of sufficient energy (usually MeV protons) produced by an ion accelerator, will cause inner shell ionization of atoms in a specimen. Outer shell electrons drop down to replace inner shell vacancies, however only certain transitions are allowed."
What happens to the electrons of an atom in a PIXE experiment?,They become stable,They drop down to replace inner shell vacancies,They are bombarded with ions,They ionize the atoms in a specimen,They produce MeV protons,B,"Particle-induced X-ray emission or proton-induced X-ray emission (PIXE) is a technique used for determining the elemental composition of a material or a sample. When a material is exposed to an ion beam, atomic interactions occur that give off EM radiation of wavelengths in the x-ray part of the electromagnetic spectrum specific to an element. PIXE is a powerful yet non-destructive elemental analysis technique now used routinely by geologists, archaeologists, art conservators and others to help answer questions of provenance, dating and authenticity.
The technique was first proposed in 1970 by Sven Johansson of Lund University, Sweden, and developed over the next few years with his colleagues Roland Akselsson and Thomas B Johansson.Recent extensions of PIXE using tightly focused beams (down to 1 μm) gives the additional capability of microscopic analysis. This technique, called microPIXE, can be used to determine the distribution of trace elements in a wide range of samples. A related technique, particle-induced gamma-ray emission (PIGE) can be used to detect some light elements.

Theory
Three types of spectra can be collected from a PIXE experiment:

X-ray emission spectrum.
Rutherford backscattering spectrum.
Proton transmission spectrum.

X-ray emission
Quantum theory states that orbiting electrons of an atom must occupy discrete energy levels in order to be stable. Bombardment with ions of sufficient energy (usually MeV protons) produced by an ion accelerator, will cause inner shell ionization of atoms in a specimen. Outer shell electrons drop down to replace inner shell vacancies, however only certain transitions are allowed."
When was Pakathon founded?,2013,2014,2015,2016,2017,A,"Pakathon is a Boston-based, registered global non-profit organization with eight global chapters in four countries: Pakistan, the United States, Canada, and Australia. Founded in 2013, Pakathon aims to create jobs through innovation and entrepreneurship by mobilizing the Pakistani diaspora. By working to foster a global network of like-minded individuals, investors and entrepreneurs, Pakathon aims to solve problems in sectors such as education, healthcare, gender equality, energy and law.

Introduction
The organization's flagship event is a weekend-long hackathon hosted in chapter cities where students and professionals develop and pitch their projects to a panel of mentors. Top-performing teams are then invited to present their ideas to a global audience where they compete for funding, mentorship, and additional support, with the aim of advancing their ideas from conception to reality. Two winning teams are then selected: one from Pakistan and another from the international host cities.
In 2015, Pakathon partnered with the Higher Education Commission of Pakistan to expand Pakathon's innovation-centred programming in up to 50 Pakistani universities.

Impact
Pakathon's first winning team conceived the idea for ProCheck, a serialization system for authentic medicines in Pakistan. ProCheck allows customers and patients to verify authentic medicines using their mobile phones via text messaging. In November, 2015 ProCheck partnered with Ferozsons Labs, a leading manufacturer of pharmaceuticals in Pakistan, to serialize 35 million units of medicine using ProCheck's track and trace solution."
How many global chapters does Pakathon have?,Four,Six,Eight,Ten,Twelve,C,"Pakathon is a Boston-based, registered global non-profit organization with eight global chapters in four countries: Pakistan, the United States, Canada, and Australia. Founded in 2013, Pakathon aims to create jobs through innovation and entrepreneurship by mobilizing the Pakistani diaspora. By working to foster a global network of like-minded individuals, investors and entrepreneurs, Pakathon aims to solve problems in sectors such as education, healthcare, gender equality, energy and law.

Introduction
The organization's flagship event is a weekend-long hackathon hosted in chapter cities where students and professionals develop and pitch their projects to a panel of mentors. Top-performing teams are then invited to present their ideas to a global audience where they compete for funding, mentorship, and additional support, with the aim of advancing their ideas from conception to reality. Two winning teams are then selected: one from Pakistan and another from the international host cities.
In 2015, Pakathon partnered with the Higher Education Commission of Pakistan to expand Pakathon's innovation-centred programming in up to 50 Pakistani universities.

Impact
Pakathon's first winning team conceived the idea for ProCheck, a serialization system for authentic medicines in Pakistan. ProCheck allows customers and patients to verify authentic medicines using their mobile phones via text messaging. In November, 2015 ProCheck partnered with Ferozsons Labs, a leading manufacturer of pharmaceuticals in Pakistan, to serialize 35 million units of medicine using ProCheck's track and trace solution."
Which sectors does Pakathon aim to solve problems in?,Technology and finance,Education and healthcare,Transportation and construction,Agriculture and manufacturing,Entertainment and hospitality,B,"Pakathon is a Boston-based, registered global non-profit organization with eight global chapters in four countries: Pakistan, the United States, Canada, and Australia. Founded in 2013, Pakathon aims to create jobs through innovation and entrepreneurship by mobilizing the Pakistani diaspora. By working to foster a global network of like-minded individuals, investors and entrepreneurs, Pakathon aims to solve problems in sectors such as education, healthcare, gender equality, energy and law.

Introduction
The organization's flagship event is a weekend-long hackathon hosted in chapter cities where students and professionals develop and pitch their projects to a panel of mentors. Top-performing teams are then invited to present their ideas to a global audience where they compete for funding, mentorship, and additional support, with the aim of advancing their ideas from conception to reality. Two winning teams are then selected: one from Pakistan and another from the international host cities.
In 2015, Pakathon partnered with the Higher Education Commission of Pakistan to expand Pakathon's innovation-centred programming in up to 50 Pakistani universities.

Impact
Pakathon's first winning team conceived the idea for ProCheck, a serialization system for authentic medicines in Pakistan. ProCheck allows customers and patients to verify authentic medicines using their mobile phones via text messaging. In November, 2015 ProCheck partnered with Ferozsons Labs, a leading manufacturer of pharmaceuticals in Pakistan, to serialize 35 million units of medicine using ProCheck's track and trace solution."
What is Pakathon's flagship event?,Conference,Exhibition,Workshop,Hackathon,Competition,D,"Pakathon is a Boston-based, registered global non-profit organization with eight global chapters in four countries: Pakistan, the United States, Canada, and Australia. Founded in 2013, Pakathon aims to create jobs through innovation and entrepreneurship by mobilizing the Pakistani diaspora. By working to foster a global network of like-minded individuals, investors and entrepreneurs, Pakathon aims to solve problems in sectors such as education, healthcare, gender equality, energy and law.

Introduction
The organization's flagship event is a weekend-long hackathon hosted in chapter cities where students and professionals develop and pitch their projects to a panel of mentors. Top-performing teams are then invited to present their ideas to a global audience where they compete for funding, mentorship, and additional support, with the aim of advancing their ideas from conception to reality. Two winning teams are then selected: one from Pakistan and another from the international host cities.
In 2015, Pakathon partnered with the Higher Education Commission of Pakistan to expand Pakathon's innovation-centred programming in up to 50 Pakistani universities.

Impact
Pakathon's first winning team conceived the idea for ProCheck, a serialization system for authentic medicines in Pakistan. ProCheck allows customers and patients to verify authentic medicines using their mobile phones via text messaging. In November, 2015 ProCheck partnered with Ferozsons Labs, a leading manufacturer of pharmaceuticals in Pakistan, to serialize 35 million units of medicine using ProCheck's track and trace solution."
What is the name of the innovation conceived by Pakathon's first winning team?,ProCheck,MediTrack,PharmaVerify,AuthentiCode,Track-Med,A,"Pakathon is a Boston-based, registered global non-profit organization with eight global chapters in four countries: Pakistan, the United States, Canada, and Australia. Founded in 2013, Pakathon aims to create jobs through innovation and entrepreneurship by mobilizing the Pakistani diaspora. By working to foster a global network of like-minded individuals, investors and entrepreneurs, Pakathon aims to solve problems in sectors such as education, healthcare, gender equality, energy and law.

Introduction
The organization's flagship event is a weekend-long hackathon hosted in chapter cities where students and professionals develop and pitch their projects to a panel of mentors. Top-performing teams are then invited to present their ideas to a global audience where they compete for funding, mentorship, and additional support, with the aim of advancing their ideas from conception to reality. Two winning teams are then selected: one from Pakistan and another from the international host cities.
In 2015, Pakathon partnered with the Higher Education Commission of Pakistan to expand Pakathon's innovation-centred programming in up to 50 Pakistani universities.

Impact
Pakathon's first winning team conceived the idea for ProCheck, a serialization system for authentic medicines in Pakistan. ProCheck allows customers and patients to verify authentic medicines using their mobile phones via text messaging. In November, 2015 ProCheck partnered with Ferozsons Labs, a leading manufacturer of pharmaceuticals in Pakistan, to serialize 35 million units of medicine using ProCheck's track and trace solution."
What is the main purpose of FASTRAD?,To calculate radiation effects on electronics,To design mechanical systems for space,To analyze high energy physics experiments,To optimize medical devices,To model nuclear experiments,A,"FASTRAD is a tool dedicated to the calculation of radiation effects (Dose and Displacement Damage) on electronics. The tool includes a 3d modelling interface with all the capabilities required for the representation of any system.  Application areas include: high energy physics and nuclear experiments, medical, accelerator and space physics studies. The software is used by radiation engineers around the world.

History
FASTRAD is a radiation tool dedicated to the analysis and design of radiation sensitive systems.  The project was created in 1999 and has constantly been improved since.  This software can be applied to any radiation related field.
The radiation hardness assurance of satellite manufacturers has been continuously improved over the last decade.  The optimization of space systems in terms of either mechanical design to increase the ratio power/mass, or miniaturization of electronic devices tends to increase the sensitivity of those systems to the space radiation environment."
When was FASTRAD created?,1999,2009,1989,1979,2019,A,"FASTRAD is a tool dedicated to the calculation of radiation effects (Dose and Displacement Damage) on electronics. The tool includes a 3d modelling interface with all the capabilities required for the representation of any system.  Application areas include: high energy physics and nuclear experiments, medical, accelerator and space physics studies. The software is used by radiation engineers around the world.

History
FASTRAD is a radiation tool dedicated to the analysis and design of radiation sensitive systems.  The project was created in 1999 and has constantly been improved since.  This software can be applied to any radiation related field.
The radiation hardness assurance of satellite manufacturers has been continuously improved over the last decade.  The optimization of space systems in terms of either mechanical design to increase the ratio power/mass, or miniaturization of electronic devices tends to increase the sensitivity of those systems to the space radiation environment."
What is one application area of FASTRAD?,Aerospace engineering,Chemical analysis,Digital marketing,Radiation therapy,Quantum computing,D,"FASTRAD is a tool dedicated to the calculation of radiation effects (Dose and Displacement Damage) on electronics. The tool includes a 3d modelling interface with all the capabilities required for the representation of any system.  Application areas include: high energy physics and nuclear experiments, medical, accelerator and space physics studies. The software is used by radiation engineers around the world.

History
FASTRAD is a radiation tool dedicated to the analysis and design of radiation sensitive systems.  The project was created in 1999 and has constantly been improved since.  This software can be applied to any radiation related field.
The radiation hardness assurance of satellite manufacturers has been continuously improved over the last decade.  The optimization of space systems in terms of either mechanical design to increase the ratio power/mass, or miniaturization of electronic devices tends to increase the sensitivity of those systems to the space radiation environment."
What has been continuously improved in satellite manufacturing?,Battery life,Communication systems,Radiation hardness assurance,Solar panel efficiency,Mechanical design,C,"FASTRAD is a tool dedicated to the calculation of radiation effects (Dose and Displacement Damage) on electronics. The tool includes a 3d modelling interface with all the capabilities required for the representation of any system.  Application areas include: high energy physics and nuclear experiments, medical, accelerator and space physics studies. The software is used by radiation engineers around the world.

History
FASTRAD is a radiation tool dedicated to the analysis and design of radiation sensitive systems.  The project was created in 1999 and has constantly been improved since.  This software can be applied to any radiation related field.
The radiation hardness assurance of satellite manufacturers has been continuously improved over the last decade.  The optimization of space systems in terms of either mechanical design to increase the ratio power/mass, or miniaturization of electronic devices tends to increase the sensitivity of those systems to the space radiation environment."
What tends to increase the sensitivity of space systems to radiation?,Increasing power/mass ratio,Decreasing the size of electronic devices,Improving mechanical design,Optimizing radiation shielding,Enhancing communication systems,B,"FASTRAD is a tool dedicated to the calculation of radiation effects (Dose and Displacement Damage) on electronics. The tool includes a 3d modelling interface with all the capabilities required for the representation of any system.  Application areas include: high energy physics and nuclear experiments, medical, accelerator and space physics studies. The software is used by radiation engineers around the world.

History
FASTRAD is a radiation tool dedicated to the analysis and design of radiation sensitive systems.  The project was created in 1999 and has constantly been improved since.  This software can be applied to any radiation related field.
The radiation hardness assurance of satellite manufacturers has been continuously improved over the last decade.  The optimization of space systems in terms of either mechanical design to increase the ratio power/mass, or miniaturization of electronic devices tends to increase the sensitivity of those systems to the space radiation environment."
When was the Anthropocene Working Group (AWG) established?,2000,2002,2009,2010,2012,C,"The Anthropocene Working Group (AWG) is an interdisciplinary research group dedicated to the study of the Anthropocene as a geological time unit. It was established in 2009 as part of the Subcommission on Quaternary Stratigraphy (SQS), a constituent body of the International Commission on Stratigraphy (ICS). As of 2021, the research group features 37 members, with the physical geographer Simon Turner as Secretary and the geologist Colin Neil Waters as Chair of the group. The late Nobel Prize-winning Paul Crutzen, who popularized the word 'Anthropocene' in 2000, had also been a member of the group until he died on January 28, 2021. The main goal of the AWG is providing scientific evidence robust enough for the Anthropocene to be formally ratified by the International Union of Geological Sciences (IUGS) as an Epoch within the Geologic time scale.

History
Prior to the establishment of the Anthropocene Working Group in 2009, no research program dedicated to the formalization of the Anthropocene in the geologic time scale existed. The idea of naming the current epoch 'Anthropocene' rather than using its formal time unit, the Holocene, became popular after Paul Crutzen and Eugene Stoermer published in May 2000 an article on the IGBP Global Change Newsletter called ""The 'Anthropocene'."" Later in 2002, Crutzen published a commentary on Nature titled ""Geology of Mankind"" where he further stressed the idea ""to assign the term ‘Anthropocene’ to the present, in many ways human-dominated, geological epoch, supplementing the Holocene,"" with starting date in the late 18th century (at the onset of the Industrial Revolution). Soon after Paul Crutzen published his influential articles, a debate over the beginning of the Anthropocene took place between supporters of the Early Anthropocene Hypothesis, a thesis originally promoted in 2003 by the palaeoclimatologist William Ruddiman dating the beginning of the Anthropocene as far back as the neolithic revolution, and supporters of more recent starting dates, from  European Colonization of the Americas, to the late 18th century, to the post-WWII Great Acceleration.The discussion over the beginning of the Anthropocene was crucial for the 'stratigraphic turn' that the Anthropocene debate took in the following years."
Who is the Secretary of the AWG?,Paul Crutzen,Simon Turner,Colin Neil Waters,Eugene Stoermer,William Ruddiman,B,"The Anthropocene Working Group (AWG) is an interdisciplinary research group dedicated to the study of the Anthropocene as a geological time unit. It was established in 2009 as part of the Subcommission on Quaternary Stratigraphy (SQS), a constituent body of the International Commission on Stratigraphy (ICS). As of 2021, the research group features 37 members, with the physical geographer Simon Turner as Secretary and the geologist Colin Neil Waters as Chair of the group. The late Nobel Prize-winning Paul Crutzen, who popularized the word 'Anthropocene' in 2000, had also been a member of the group until he died on January 28, 2021. The main goal of the AWG is providing scientific evidence robust enough for the Anthropocene to be formally ratified by the International Union of Geological Sciences (IUGS) as an Epoch within the Geologic time scale.

History
Prior to the establishment of the Anthropocene Working Group in 2009, no research program dedicated to the formalization of the Anthropocene in the geologic time scale existed. The idea of naming the current epoch 'Anthropocene' rather than using its formal time unit, the Holocene, became popular after Paul Crutzen and Eugene Stoermer published in May 2000 an article on the IGBP Global Change Newsletter called ""The 'Anthropocene'."" Later in 2002, Crutzen published a commentary on Nature titled ""Geology of Mankind"" where he further stressed the idea ""to assign the term ‘Anthropocene’ to the present, in many ways human-dominated, geological epoch, supplementing the Holocene,"" with starting date in the late 18th century (at the onset of the Industrial Revolution). Soon after Paul Crutzen published his influential articles, a debate over the beginning of the Anthropocene took place between supporters of the Early Anthropocene Hypothesis, a thesis originally promoted in 2003 by the palaeoclimatologist William Ruddiman dating the beginning of the Anthropocene as far back as the neolithic revolution, and supporters of more recent starting dates, from  European Colonization of the Americas, to the late 18th century, to the post-WWII Great Acceleration.The discussion over the beginning of the Anthropocene was crucial for the 'stratigraphic turn' that the Anthropocene debate took in the following years."
"Who popularized the word ""Anthropocene""?",Paul Crutzen,Simon Turner,Colin Neil Waters,Eugene Stoermer,William Ruddiman,A,"The Anthropocene Working Group (AWG) is an interdisciplinary research group dedicated to the study of the Anthropocene as a geological time unit. It was established in 2009 as part of the Subcommission on Quaternary Stratigraphy (SQS), a constituent body of the International Commission on Stratigraphy (ICS). As of 2021, the research group features 37 members, with the physical geographer Simon Turner as Secretary and the geologist Colin Neil Waters as Chair of the group. The late Nobel Prize-winning Paul Crutzen, who popularized the word 'Anthropocene' in 2000, had also been a member of the group until he died on January 28, 2021. The main goal of the AWG is providing scientific evidence robust enough for the Anthropocene to be formally ratified by the International Union of Geological Sciences (IUGS) as an Epoch within the Geologic time scale.

History
Prior to the establishment of the Anthropocene Working Group in 2009, no research program dedicated to the formalization of the Anthropocene in the geologic time scale existed. The idea of naming the current epoch 'Anthropocene' rather than using its formal time unit, the Holocene, became popular after Paul Crutzen and Eugene Stoermer published in May 2000 an article on the IGBP Global Change Newsletter called ""The 'Anthropocene'."" Later in 2002, Crutzen published a commentary on Nature titled ""Geology of Mankind"" where he further stressed the idea ""to assign the term ‘Anthropocene’ to the present, in many ways human-dominated, geological epoch, supplementing the Holocene,"" with starting date in the late 18th century (at the onset of the Industrial Revolution). Soon after Paul Crutzen published his influential articles, a debate over the beginning of the Anthropocene took place between supporters of the Early Anthropocene Hypothesis, a thesis originally promoted in 2003 by the palaeoclimatologist William Ruddiman dating the beginning of the Anthropocene as far back as the neolithic revolution, and supporters of more recent starting dates, from  European Colonization of the Americas, to the late 18th century, to the post-WWII Great Acceleration.The discussion over the beginning of the Anthropocene was crucial for the 'stratigraphic turn' that the Anthropocene debate took in the following years."
What is the main goal of the AWG?,To study the Holocene,To study the Geologic time scale,To provide scientific evidence for the Anthropocene,To promote the neolithic revolution,To debate the Early Anthropocene Hypothesis,C,"The Anthropocene Working Group (AWG) is an interdisciplinary research group dedicated to the study of the Anthropocene as a geological time unit. It was established in 2009 as part of the Subcommission on Quaternary Stratigraphy (SQS), a constituent body of the International Commission on Stratigraphy (ICS). As of 2021, the research group features 37 members, with the physical geographer Simon Turner as Secretary and the geologist Colin Neil Waters as Chair of the group. The late Nobel Prize-winning Paul Crutzen, who popularized the word 'Anthropocene' in 2000, had also been a member of the group until he died on January 28, 2021. The main goal of the AWG is providing scientific evidence robust enough for the Anthropocene to be formally ratified by the International Union of Geological Sciences (IUGS) as an Epoch within the Geologic time scale.

History
Prior to the establishment of the Anthropocene Working Group in 2009, no research program dedicated to the formalization of the Anthropocene in the geologic time scale existed. The idea of naming the current epoch 'Anthropocene' rather than using its formal time unit, the Holocene, became popular after Paul Crutzen and Eugene Stoermer published in May 2000 an article on the IGBP Global Change Newsletter called ""The 'Anthropocene'."" Later in 2002, Crutzen published a commentary on Nature titled ""Geology of Mankind"" where he further stressed the idea ""to assign the term ‘Anthropocene’ to the present, in many ways human-dominated, geological epoch, supplementing the Holocene,"" with starting date in the late 18th century (at the onset of the Industrial Revolution). Soon after Paul Crutzen published his influential articles, a debate over the beginning of the Anthropocene took place between supporters of the Early Anthropocene Hypothesis, a thesis originally promoted in 2003 by the palaeoclimatologist William Ruddiman dating the beginning of the Anthropocene as far back as the neolithic revolution, and supporters of more recent starting dates, from  European Colonization of the Americas, to the late 18th century, to the post-WWII Great Acceleration.The discussion over the beginning of the Anthropocene was crucial for the 'stratigraphic turn' that the Anthropocene debate took in the following years."
What was the topic of Paul Crutzen's influential article published in 2002?,The Holocene,The neolithic revolution,The Anthropocene,The Industrial Revolution,The European Colonization of the Americas,C,"The Anthropocene Working Group (AWG) is an interdisciplinary research group dedicated to the study of the Anthropocene as a geological time unit. It was established in 2009 as part of the Subcommission on Quaternary Stratigraphy (SQS), a constituent body of the International Commission on Stratigraphy (ICS). As of 2021, the research group features 37 members, with the physical geographer Simon Turner as Secretary and the geologist Colin Neil Waters as Chair of the group. The late Nobel Prize-winning Paul Crutzen, who popularized the word 'Anthropocene' in 2000, had also been a member of the group until he died on January 28, 2021. The main goal of the AWG is providing scientific evidence robust enough for the Anthropocene to be formally ratified by the International Union of Geological Sciences (IUGS) as an Epoch within the Geologic time scale.

History
Prior to the establishment of the Anthropocene Working Group in 2009, no research program dedicated to the formalization of the Anthropocene in the geologic time scale existed. The idea of naming the current epoch 'Anthropocene' rather than using its formal time unit, the Holocene, became popular after Paul Crutzen and Eugene Stoermer published in May 2000 an article on the IGBP Global Change Newsletter called ""The 'Anthropocene'."" Later in 2002, Crutzen published a commentary on Nature titled ""Geology of Mankind"" where he further stressed the idea ""to assign the term ‘Anthropocene’ to the present, in many ways human-dominated, geological epoch, supplementing the Holocene,"" with starting date in the late 18th century (at the onset of the Industrial Revolution). Soon after Paul Crutzen published his influential articles, a debate over the beginning of the Anthropocene took place between supporters of the Early Anthropocene Hypothesis, a thesis originally promoted in 2003 by the palaeoclimatologist William Ruddiman dating the beginning of the Anthropocene as far back as the neolithic revolution, and supporters of more recent starting dates, from  European Colonization of the Americas, to the late 18th century, to the post-WWII Great Acceleration.The discussion over the beginning of the Anthropocene was crucial for the 'stratigraphic turn' that the Anthropocene debate took in the following years."
What is a space frame?,A lightweight structure made of wood,A rigid truss-like structure made of interlocking struts,A flexible structure made of steel cables,A solid structure made of concrete,A hollow structure made of glass panels,B,"In architecture and structural engineering, a space frame or space structure (3D truss) is a rigid, lightweight, truss-like structure constructed from interlocking struts in a geometric pattern. Space frames can be used to span large areas with few interior supports.  Like the truss, a space frame is strong because of the inherent rigidity of the triangle; flexing loads (bending moments) are transmitted as tension and compression loads along the length of each strut.

History
Alexander Graham Bell from 1898 to 1908 developed space frames based on tetrahedral geometry. Bell's interest was primarily in using them to make rigid frames for nautical and aeronautical engineering, with the tetrahedral truss being one of his inventions. Max Mengeringhausen developed the space grid system called MERO (acronym of MEngeringhausen ROhrbauweise) in 1943 in Germany, thus initiating the use of space trusses in architecture. The commonly used method, still in use has individual tubular members connected at node joints (ball shaped) and variations such as the space deck system, octet truss system and cubic system. Stéphane de Chateau in France invented the Tridirectional SDC system (1957), Unibat system (1959), Pyramitec (1960)."
What is the advantage of using space frames?,They are easy to construct,They require many interior supports,They are not strong enough to withstand bending moments,They can span large areas with few interior supports,They are heavy and difficult to transport,D,"In architecture and structural engineering, a space frame or space structure (3D truss) is a rigid, lightweight, truss-like structure constructed from interlocking struts in a geometric pattern. Space frames can be used to span large areas with few interior supports.  Like the truss, a space frame is strong because of the inherent rigidity of the triangle; flexing loads (bending moments) are transmitted as tension and compression loads along the length of each strut.

History
Alexander Graham Bell from 1898 to 1908 developed space frames based on tetrahedral geometry. Bell's interest was primarily in using them to make rigid frames for nautical and aeronautical engineering, with the tetrahedral truss being one of his inventions. Max Mengeringhausen developed the space grid system called MERO (acronym of MEngeringhausen ROhrbauweise) in 1943 in Germany, thus initiating the use of space trusses in architecture. The commonly used method, still in use has individual tubular members connected at node joints (ball shaped) and variations such as the space deck system, octet truss system and cubic system. Stéphane de Chateau in France invented the Tridirectional SDC system (1957), Unibat system (1959), Pyramitec (1960)."
Who developed space frames based on tetrahedral geometry?,Max Mengeringhausen,Alexander Graham Bell,Stéphane de Chateau,Pyramitec,Unibat system,B,"In architecture and structural engineering, a space frame or space structure (3D truss) is a rigid, lightweight, truss-like structure constructed from interlocking struts in a geometric pattern. Space frames can be used to span large areas with few interior supports.  Like the truss, a space frame is strong because of the inherent rigidity of the triangle; flexing loads (bending moments) are transmitted as tension and compression loads along the length of each strut.

History
Alexander Graham Bell from 1898 to 1908 developed space frames based on tetrahedral geometry. Bell's interest was primarily in using them to make rigid frames for nautical and aeronautical engineering, with the tetrahedral truss being one of his inventions. Max Mengeringhausen developed the space grid system called MERO (acronym of MEngeringhausen ROhrbauweise) in 1943 in Germany, thus initiating the use of space trusses in architecture. The commonly used method, still in use has individual tubular members connected at node joints (ball shaped) and variations such as the space deck system, octet truss system and cubic system. Stéphane de Chateau in France invented the Tridirectional SDC system (1957), Unibat system (1959), Pyramitec (1960)."
What is the commonly used method to connect tubular members in a space frame?,Welding,Screwing,Brazing,Bolting,Node joints,E,"In architecture and structural engineering, a space frame or space structure (3D truss) is a rigid, lightweight, truss-like structure constructed from interlocking struts in a geometric pattern. Space frames can be used to span large areas with few interior supports.  Like the truss, a space frame is strong because of the inherent rigidity of the triangle; flexing loads (bending moments) are transmitted as tension and compression loads along the length of each strut.

History
Alexander Graham Bell from 1898 to 1908 developed space frames based on tetrahedral geometry. Bell's interest was primarily in using them to make rigid frames for nautical and aeronautical engineering, with the tetrahedral truss being one of his inventions. Max Mengeringhausen developed the space grid system called MERO (acronym of MEngeringhausen ROhrbauweise) in 1943 in Germany, thus initiating the use of space trusses in architecture. The commonly used method, still in use has individual tubular members connected at node joints (ball shaped) and variations such as the space deck system, octet truss system and cubic system. Stéphane de Chateau in France invented the Tridirectional SDC system (1957), Unibat system (1959), Pyramitec (1960)."
Who invented the Tridirectional SDC system?,Max Mengeringhausen,Alexander Graham Bell,Stéphane de Chateau,Pyramitec,Unibat system,C,"In architecture and structural engineering, a space frame or space structure (3D truss) is a rigid, lightweight, truss-like structure constructed from interlocking struts in a geometric pattern. Space frames can be used to span large areas with few interior supports.  Like the truss, a space frame is strong because of the inherent rigidity of the triangle; flexing loads (bending moments) are transmitted as tension and compression loads along the length of each strut.

History
Alexander Graham Bell from 1898 to 1908 developed space frames based on tetrahedral geometry. Bell's interest was primarily in using them to make rigid frames for nautical and aeronautical engineering, with the tetrahedral truss being one of his inventions. Max Mengeringhausen developed the space grid system called MERO (acronym of MEngeringhausen ROhrbauweise) in 1943 in Germany, thus initiating the use of space trusses in architecture. The commonly used method, still in use has individual tubular members connected at node joints (ball shaped) and variations such as the space deck system, octet truss system and cubic system. Stéphane de Chateau in France invented the Tridirectional SDC system (1957), Unibat system (1959), Pyramitec (1960)."
What is product engineering?,The process of designing and developing a device for sale,The process of manufacturing a product,The process of marketing a product,The process of shipping a product,The process of repairing a product,A,"Product engineering refer to the process of designing and developing a device, assembly, or system such that it be produced as an item for sale through some product manufacturing process. Product engineering usually entails activity dealing with issues of cost, producibility, quality, performance, reliability, serviceability, intended lifespan and user features. These product characteristics are generally all sought in the attempt to make the resulting product attractive to its intended market and a successful contributor to the business of the organization that intends to offer the product to that market. It includes design, development and transitioning to manufacturing of the product. The term encompasses developing the concept of the product and the design and development of its hardware and software components. After the initial design and development is done, transitioning the product to manufacture it in volumes is considered part of product engineering.For example, the engineering of a digital camera would include defining the feature set, design of the optics, the physical, aesthetic and ergonomic design of the packaging, developing the various electrical and mechanical component for the capture, processing and storage of image and developing the software that allows the user to see the pictures, store them in memory and download them to a computer.
Product engineering is an engineering discipline that deals with both design and manufacturing aspects of a product.

Area of responsibility
Product engineers define the yield road map and drive the fulfillment during ramp-up and volume production,
Identify and realize measures for yield improvement, test optimization and product cost-ability methods,
Define qualification plan and perform feasibility analysis.Product engineers are the technical interface between the component development team and the production side (Front End and Back End), especially after the development phase and qualifications when the high volume production is running.
Product engineers improve the product quality and secure the product reliability by balancing the cost of tests and tests coverage that could impact the production fall-off. They support failure analysis request from customers.

Knowledge and skills
The job requires the product engineer to have a very good working knowledge of:

Statistical methods and tools
Manufacturing process
Software, hardware and systems implementation
Product reliability and qualification
Physical analysis methods
Computer-aided design and simulation programs
Specific technology
Strong product Knowledge
Strong analytic work methodology and problem solving skills
Continuous Improvement Knowledge

Tools
A product engineer will use a wide range of tools and software, possibly including:
20/20, AutoCad, CATIA, PTC Creo, Solidworks, Unigraphics, Labview, JMP, DataConductor.

See also
Industrial engineering
Process engineering

References
External links
[1] Application note ""Yield Learning Flow Provides Faster Production Ramp""
[2] Tutorial about yield impact
[3] Custom  Product Engineering."
What are some of the characteristics sought in a product engineering process?,"Cost, quality, and performance","Durability, serviceability, and lifespan",User features and market attractiveness,Reliability and producibility,All of the above,E,"Product engineering refer to the process of designing and developing a device, assembly, or system such that it be produced as an item for sale through some product manufacturing process. Product engineering usually entails activity dealing with issues of cost, producibility, quality, performance, reliability, serviceability, intended lifespan and user features. These product characteristics are generally all sought in the attempt to make the resulting product attractive to its intended market and a successful contributor to the business of the organization that intends to offer the product to that market. It includes design, development and transitioning to manufacturing of the product. The term encompasses developing the concept of the product and the design and development of its hardware and software components. After the initial design and development is done, transitioning the product to manufacture it in volumes is considered part of product engineering.For example, the engineering of a digital camera would include defining the feature set, design of the optics, the physical, aesthetic and ergonomic design of the packaging, developing the various electrical and mechanical component for the capture, processing and storage of image and developing the software that allows the user to see the pictures, store them in memory and download them to a computer.
Product engineering is an engineering discipline that deals with both design and manufacturing aspects of a product.

Area of responsibility
Product engineers define the yield road map and drive the fulfillment during ramp-up and volume production,
Identify and realize measures for yield improvement, test optimization and product cost-ability methods,
Define qualification plan and perform feasibility analysis.Product engineers are the technical interface between the component development team and the production side (Front End and Back End), especially after the development phase and qualifications when the high volume production is running.
Product engineers improve the product quality and secure the product reliability by balancing the cost of tests and tests coverage that could impact the production fall-off. They support failure analysis request from customers.

Knowledge and skills
The job requires the product engineer to have a very good working knowledge of:

Statistical methods and tools
Manufacturing process
Software, hardware and systems implementation
Product reliability and qualification
Physical analysis methods
Computer-aided design and simulation programs
Specific technology
Strong product Knowledge
Strong analytic work methodology and problem solving skills
Continuous Improvement Knowledge

Tools
A product engineer will use a wide range of tools and software, possibly including:
20/20, AutoCad, CATIA, PTC Creo, Solidworks, Unigraphics, Labview, JMP, DataConductor.

See also
Industrial engineering
Process engineering

References
External links
[1] Application note ""Yield Learning Flow Provides Faster Production Ramp""
[2] Tutorial about yield impact
[3] Custom  Product Engineering."
What is included in product engineering?,Concept development and design of hardware and software components,Manufacturing and packaging design,Transitioning to high volume production,All of the above,None of the above,D,"Product engineering refer to the process of designing and developing a device, assembly, or system such that it be produced as an item for sale through some product manufacturing process. Product engineering usually entails activity dealing with issues of cost, producibility, quality, performance, reliability, serviceability, intended lifespan and user features. These product characteristics are generally all sought in the attempt to make the resulting product attractive to its intended market and a successful contributor to the business of the organization that intends to offer the product to that market. It includes design, development and transitioning to manufacturing of the product. The term encompasses developing the concept of the product and the design and development of its hardware and software components. After the initial design and development is done, transitioning the product to manufacture it in volumes is considered part of product engineering.For example, the engineering of a digital camera would include defining the feature set, design of the optics, the physical, aesthetic and ergonomic design of the packaging, developing the various electrical and mechanical component for the capture, processing and storage of image and developing the software that allows the user to see the pictures, store them in memory and download them to a computer.
Product engineering is an engineering discipline that deals with both design and manufacturing aspects of a product.

Area of responsibility
Product engineers define the yield road map and drive the fulfillment during ramp-up and volume production,
Identify and realize measures for yield improvement, test optimization and product cost-ability methods,
Define qualification plan and perform feasibility analysis.Product engineers are the technical interface between the component development team and the production side (Front End and Back End), especially after the development phase and qualifications when the high volume production is running.
Product engineers improve the product quality and secure the product reliability by balancing the cost of tests and tests coverage that could impact the production fall-off. They support failure analysis request from customers.

Knowledge and skills
The job requires the product engineer to have a very good working knowledge of:

Statistical methods and tools
Manufacturing process
Software, hardware and systems implementation
Product reliability and qualification
Physical analysis methods
Computer-aided design and simulation programs
Specific technology
Strong product Knowledge
Strong analytic work methodology and problem solving skills
Continuous Improvement Knowledge

Tools
A product engineer will use a wide range of tools and software, possibly including:
20/20, AutoCad, CATIA, PTC Creo, Solidworks, Unigraphics, Labview, JMP, DataConductor.

See also
Industrial engineering
Process engineering

References
External links
[1] Application note ""Yield Learning Flow Provides Faster Production Ramp""
[2] Tutorial about yield impact
[3] Custom  Product Engineering."
What is the role of a product engineer?,To drive fulfillment during ramp-up and volume production,To improve product quality and reliability,To balance cost of tests and tests coverage,All of the above,None of the above,D,"Product engineering refer to the process of designing and developing a device, assembly, or system such that it be produced as an item for sale through some product manufacturing process. Product engineering usually entails activity dealing with issues of cost, producibility, quality, performance, reliability, serviceability, intended lifespan and user features. These product characteristics are generally all sought in the attempt to make the resulting product attractive to its intended market and a successful contributor to the business of the organization that intends to offer the product to that market. It includes design, development and transitioning to manufacturing of the product. The term encompasses developing the concept of the product and the design and development of its hardware and software components. After the initial design and development is done, transitioning the product to manufacture it in volumes is considered part of product engineering.For example, the engineering of a digital camera would include defining the feature set, design of the optics, the physical, aesthetic and ergonomic design of the packaging, developing the various electrical and mechanical component for the capture, processing and storage of image and developing the software that allows the user to see the pictures, store them in memory and download them to a computer.
Product engineering is an engineering discipline that deals with both design and manufacturing aspects of a product.

Area of responsibility
Product engineers define the yield road map and drive the fulfillment during ramp-up and volume production,
Identify and realize measures for yield improvement, test optimization and product cost-ability methods,
Define qualification plan and perform feasibility analysis.Product engineers are the technical interface between the component development team and the production side (Front End and Back End), especially after the development phase and qualifications when the high volume production is running.
Product engineers improve the product quality and secure the product reliability by balancing the cost of tests and tests coverage that could impact the production fall-off. They support failure analysis request from customers.

Knowledge and skills
The job requires the product engineer to have a very good working knowledge of:

Statistical methods and tools
Manufacturing process
Software, hardware and systems implementation
Product reliability and qualification
Physical analysis methods
Computer-aided design and simulation programs
Specific technology
Strong product Knowledge
Strong analytic work methodology and problem solving skills
Continuous Improvement Knowledge

Tools
A product engineer will use a wide range of tools and software, possibly including:
20/20, AutoCad, CATIA, PTC Creo, Solidworks, Unigraphics, Labview, JMP, DataConductor.

See also
Industrial engineering
Process engineering

References
External links
[1] Application note ""Yield Learning Flow Provides Faster Production Ramp""
[2] Tutorial about yield impact
[3] Custom  Product Engineering."
What tools and software do product engineers use?,20/20 and AutoCad,CATIA and Solidworks,Unigraphics and Labview,JMP and DataConductor,All of the above,E,"Product engineering refer to the process of designing and developing a device, assembly, or system such that it be produced as an item for sale through some product manufacturing process. Product engineering usually entails activity dealing with issues of cost, producibility, quality, performance, reliability, serviceability, intended lifespan and user features. These product characteristics are generally all sought in the attempt to make the resulting product attractive to its intended market and a successful contributor to the business of the organization that intends to offer the product to that market. It includes design, development and transitioning to manufacturing of the product. The term encompasses developing the concept of the product and the design and development of its hardware and software components. After the initial design and development is done, transitioning the product to manufacture it in volumes is considered part of product engineering.For example, the engineering of a digital camera would include defining the feature set, design of the optics, the physical, aesthetic and ergonomic design of the packaging, developing the various electrical and mechanical component for the capture, processing and storage of image and developing the software that allows the user to see the pictures, store them in memory and download them to a computer.
Product engineering is an engineering discipline that deals with both design and manufacturing aspects of a product.

Area of responsibility
Product engineers define the yield road map and drive the fulfillment during ramp-up and volume production,
Identify and realize measures for yield improvement, test optimization and product cost-ability methods,
Define qualification plan and perform feasibility analysis.Product engineers are the technical interface between the component development team and the production side (Front End and Back End), especially after the development phase and qualifications when the high volume production is running.
Product engineers improve the product quality and secure the product reliability by balancing the cost of tests and tests coverage that could impact the production fall-off. They support failure analysis request from customers.

Knowledge and skills
The job requires the product engineer to have a very good working knowledge of:

Statistical methods and tools
Manufacturing process
Software, hardware and systems implementation
Product reliability and qualification
Physical analysis methods
Computer-aided design and simulation programs
Specific technology
Strong product Knowledge
Strong analytic work methodology and problem solving skills
Continuous Improvement Knowledge

Tools
A product engineer will use a wide range of tools and software, possibly including:
20/20, AutoCad, CATIA, PTC Creo, Solidworks, Unigraphics, Labview, JMP, DataConductor.

See also
Industrial engineering
Process engineering

References
External links
[1] Application note ""Yield Learning Flow Provides Faster Production Ramp""
[2] Tutorial about yield impact
[3] Custom  Product Engineering."
What is risk analysis?,A technique used to identify and assess factors that may jeopardize the success of a project,A technique used to analyze revenue and costs in a software company,A technique used to develop preventive measures for business processes,A technique used to manage the risk of specialization in software development,A technique used to forecast future revenue for a company,A,"Risk analysis is a technique used to identify and assess factors that may jeopardize the success of a project or achieving a goal.
This technique also helps to define preventive measures to reduce the probability of these factors from occurring and identify countermeasures to successfully deal with these constraints when they develop to avert possible negative effects on the competitiveness of the company.
One of the more popular methods to perform a risk analysis in the computer field is called facilitated risk analysis process (FRAP).

Facilitated risk analysis process
FRAP analyzes one system, application or segment of business processes at a time.
FRAP assumes that additional efforts to develop precisely quantified risks are not cost-effective because:

such estimates are time-consuming
risk documentation becomes too voluminous for practical use
specific loss estimates are generally not needed to determine if controls are needed.
without assumptions there is little risk analysisAfter identifying and categorizing risks, a team identifies the controls that could mitigate the risk. The decision for what controls are needed lies with the business manager. The team's conclusions as to what risks exist and what controls needed are documented along with a related action plan for control implementation.
Three of the most important risks a software company faces are: unexpected changes in revenue, unexpected changes in costs from those budgeted and the amount of specialization of the software planned. Risks that affect revenues can be: unanticipated competition, privacy, intellectual property right problems, and unit sales that are less than forecast. Unexpected development costs also create the risk that can be in the form of more rework than anticipated, security holes, and privacy invasions.Narrow specialization of software with a large amount of research and development expenditures can lead to both business and technological risks since specialization does not necessarily lead to lower unit costs of software. Combined with the decrease in the potential customer base, specialization risk can be significant for a software firm. After probabilities of scenarios have been calculated with risk analysis, the process of risk management can be applied to help manage the risk.
Methods like applied information economics add to and improve on risk analysis methods by introducing procedures to adjust subjective probabilities, compute the value of additional information and to use the results in part of a larger portfolio management problem.

See also
Benefit risk
Optimism bias
Reference class forecasting
Extreme risk
Risk management
Peren–Clement index

References
Doug Hubbard (1998)."
What is the facilitated risk analysis process (FRAP)?,A method to precisely quantify risks in the computer field,A method to estimate the potential customer base for a software firm,"A method to analyze one system, application, or segment of business processes",A method to develop controls to mitigate risks,A method to calculate the value of additional information in risk analysis,C,"Risk analysis is a technique used to identify and assess factors that may jeopardize the success of a project or achieving a goal.
This technique also helps to define preventive measures to reduce the probability of these factors from occurring and identify countermeasures to successfully deal with these constraints when they develop to avert possible negative effects on the competitiveness of the company.
One of the more popular methods to perform a risk analysis in the computer field is called facilitated risk analysis process (FRAP).

Facilitated risk analysis process
FRAP analyzes one system, application or segment of business processes at a time.
FRAP assumes that additional efforts to develop precisely quantified risks are not cost-effective because:

such estimates are time-consuming
risk documentation becomes too voluminous for practical use
specific loss estimates are generally not needed to determine if controls are needed.
without assumptions there is little risk analysisAfter identifying and categorizing risks, a team identifies the controls that could mitigate the risk. The decision for what controls are needed lies with the business manager. The team's conclusions as to what risks exist and what controls needed are documented along with a related action plan for control implementation.
Three of the most important risks a software company faces are: unexpected changes in revenue, unexpected changes in costs from those budgeted and the amount of specialization of the software planned. Risks that affect revenues can be: unanticipated competition, privacy, intellectual property right problems, and unit sales that are less than forecast. Unexpected development costs also create the risk that can be in the form of more rework than anticipated, security holes, and privacy invasions.Narrow specialization of software with a large amount of research and development expenditures can lead to both business and technological risks since specialization does not necessarily lead to lower unit costs of software. Combined with the decrease in the potential customer base, specialization risk can be significant for a software firm. After probabilities of scenarios have been calculated with risk analysis, the process of risk management can be applied to help manage the risk.
Methods like applied information economics add to and improve on risk analysis methods by introducing procedures to adjust subjective probabilities, compute the value of additional information and to use the results in part of a larger portfolio management problem.

See also
Benefit risk
Optimism bias
Reference class forecasting
Extreme risk
Risk management
Peren–Clement index

References
Doug Hubbard (1998)."
What are some risks that a software company may face?,Unexpected changes in costs from those budgeted,Privacy and intellectual property right problems,Unit sales that are less than forecasted,All of the above,None of the above,D,"Risk analysis is a technique used to identify and assess factors that may jeopardize the success of a project or achieving a goal.
This technique also helps to define preventive measures to reduce the probability of these factors from occurring and identify countermeasures to successfully deal with these constraints when they develop to avert possible negative effects on the competitiveness of the company.
One of the more popular methods to perform a risk analysis in the computer field is called facilitated risk analysis process (FRAP).

Facilitated risk analysis process
FRAP analyzes one system, application or segment of business processes at a time.
FRAP assumes that additional efforts to develop precisely quantified risks are not cost-effective because:

such estimates are time-consuming
risk documentation becomes too voluminous for practical use
specific loss estimates are generally not needed to determine if controls are needed.
without assumptions there is little risk analysisAfter identifying and categorizing risks, a team identifies the controls that could mitigate the risk. The decision for what controls are needed lies with the business manager. The team's conclusions as to what risks exist and what controls needed are documented along with a related action plan for control implementation.
Three of the most important risks a software company faces are: unexpected changes in revenue, unexpected changes in costs from those budgeted and the amount of specialization of the software planned. Risks that affect revenues can be: unanticipated competition, privacy, intellectual property right problems, and unit sales that are less than forecast. Unexpected development costs also create the risk that can be in the form of more rework than anticipated, security holes, and privacy invasions.Narrow specialization of software with a large amount of research and development expenditures can lead to both business and technological risks since specialization does not necessarily lead to lower unit costs of software. Combined with the decrease in the potential customer base, specialization risk can be significant for a software firm. After probabilities of scenarios have been calculated with risk analysis, the process of risk management can be applied to help manage the risk.
Methods like applied information economics add to and improve on risk analysis methods by introducing procedures to adjust subjective probabilities, compute the value of additional information and to use the results in part of a larger portfolio management problem.

See also
Benefit risk
Optimism bias
Reference class forecasting
Extreme risk
Risk management
Peren–Clement index

References
Doug Hubbard (1998)."
What is one risk associated with narrow specialization of software?,Increased unit costs of software,Decreased potential customer base,Both A and B,None of the above,More rework than anticipated,C,"Risk analysis is a technique used to identify and assess factors that may jeopardize the success of a project or achieving a goal.
This technique also helps to define preventive measures to reduce the probability of these factors from occurring and identify countermeasures to successfully deal with these constraints when they develop to avert possible negative effects on the competitiveness of the company.
One of the more popular methods to perform a risk analysis in the computer field is called facilitated risk analysis process (FRAP).

Facilitated risk analysis process
FRAP analyzes one system, application or segment of business processes at a time.
FRAP assumes that additional efforts to develop precisely quantified risks are not cost-effective because:

such estimates are time-consuming
risk documentation becomes too voluminous for practical use
specific loss estimates are generally not needed to determine if controls are needed.
without assumptions there is little risk analysisAfter identifying and categorizing risks, a team identifies the controls that could mitigate the risk. The decision for what controls are needed lies with the business manager. The team's conclusions as to what risks exist and what controls needed are documented along with a related action plan for control implementation.
Three of the most important risks a software company faces are: unexpected changes in revenue, unexpected changes in costs from those budgeted and the amount of specialization of the software planned. Risks that affect revenues can be: unanticipated competition, privacy, intellectual property right problems, and unit sales that are less than forecast. Unexpected development costs also create the risk that can be in the form of more rework than anticipated, security holes, and privacy invasions.Narrow specialization of software with a large amount of research and development expenditures can lead to both business and technological risks since specialization does not necessarily lead to lower unit costs of software. Combined with the decrease in the potential customer base, specialization risk can be significant for a software firm. After probabilities of scenarios have been calculated with risk analysis, the process of risk management can be applied to help manage the risk.
Methods like applied information economics add to and improve on risk analysis methods by introducing procedures to adjust subjective probabilities, compute the value of additional information and to use the results in part of a larger portfolio management problem.

See also
Benefit risk
Optimism bias
Reference class forecasting
Extreme risk
Risk management
Peren–Clement index

References
Doug Hubbard (1998)."
What is one method that improves on risk analysis methods?,Benefit risk,Optimism bias,Reference class forecasting,Extreme risk,Applied information economics,E,"Risk analysis is a technique used to identify and assess factors that may jeopardize the success of a project or achieving a goal.
This technique also helps to define preventive measures to reduce the probability of these factors from occurring and identify countermeasures to successfully deal with these constraints when they develop to avert possible negative effects on the competitiveness of the company.
One of the more popular methods to perform a risk analysis in the computer field is called facilitated risk analysis process (FRAP).

Facilitated risk analysis process
FRAP analyzes one system, application or segment of business processes at a time.
FRAP assumes that additional efforts to develop precisely quantified risks are not cost-effective because:

such estimates are time-consuming
risk documentation becomes too voluminous for practical use
specific loss estimates are generally not needed to determine if controls are needed.
without assumptions there is little risk analysisAfter identifying and categorizing risks, a team identifies the controls that could mitigate the risk. The decision for what controls are needed lies with the business manager. The team's conclusions as to what risks exist and what controls needed are documented along with a related action plan for control implementation.
Three of the most important risks a software company faces are: unexpected changes in revenue, unexpected changes in costs from those budgeted and the amount of specialization of the software planned. Risks that affect revenues can be: unanticipated competition, privacy, intellectual property right problems, and unit sales that are less than forecast. Unexpected development costs also create the risk that can be in the form of more rework than anticipated, security holes, and privacy invasions.Narrow specialization of software with a large amount of research and development expenditures can lead to both business and technological risks since specialization does not necessarily lead to lower unit costs of software. Combined with the decrease in the potential customer base, specialization risk can be significant for a software firm. After probabilities of scenarios have been calculated with risk analysis, the process of risk management can be applied to help manage the risk.
Methods like applied information economics add to and improve on risk analysis methods by introducing procedures to adjust subjective probabilities, compute the value of additional information and to use the results in part of a larger portfolio management problem.

See also
Benefit risk
Optimism bias
Reference class forecasting
Extreme risk
Risk management
Peren–Clement index

References
Doug Hubbard (1998)."
"What is the origin of the word ""algebra""?",Greek,Persian,Arabic,Latin,Chinese,C,"Algebra can essentially be considered as doing computations similar to those of arithmetic but with non-numerical mathematical objects. However, until the 19th century, algebra consisted essentially of the theory of equations. For example, the fundamental theorem of algebra belongs to the theory of equations and is not, nowadays, considered as belonging to algebra (in fact, every proof must use the completeness of the real numbers, which is not an algebraic property).
This article describes the history of the theory of equations, called here ""algebra"", from the origins to the emergence of algebra as a separate area of mathematics.

Etymology
The word ""algebra"" is derived from the Arabic word الجبر al-jabr, and this comes from the treatise written in the year 830 by the medieval Persian mathematician, Muhammad ibn Mūsā al-Khwārizmī, whose Arabic title, Kitāb al-muḫtaṣar fī ḥisāb al-ğabr wa-l-muqābala, can be translated as The Compendious Book on Calculation by Completion and Balancing. The treatise provided for the systematic solution of linear and quadratic equations. According to one history, ""[i]t is not certain just what the terms al-jabr and muqabalah mean, but the usual interpretation is similar to that implied in the previous translation. The word 'al-jabr' presumably meant something like 'restoration' or 'completion' and seems to refer to the transposition of subtracted terms to the other side of an equation; the word 'muqabalah' is said to refer to 'reduction' or 'balancing'—that is, the cancellation of like terms on opposite sides of the equation. Arabic influence in Spain long after the time of al-Khwarizmi is found in Don Quixote, where the word 'algebrista' is used for a bone-setter, that is, a 'restorer'."" The term is used by al-Khwarizmi to describe the operations that he introduced, ""reduction"" and ""balancing"", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation.

Stages of algebra
Algebraic expression
Algebra did not always make use of the symbolism that is now ubiquitous in mathematics; instead, it went through three distinct stages."
What did algebra consist of until the 19th century?,Solving equations,Computing arithmetic operations,Manipulating non-numerical mathematical objects,Exploring geometric properties,Calculating derivatives,A,"Algebra can essentially be considered as doing computations similar to those of arithmetic but with non-numerical mathematical objects. However, until the 19th century, algebra consisted essentially of the theory of equations. For example, the fundamental theorem of algebra belongs to the theory of equations and is not, nowadays, considered as belonging to algebra (in fact, every proof must use the completeness of the real numbers, which is not an algebraic property).
This article describes the history of the theory of equations, called here ""algebra"", from the origins to the emergence of algebra as a separate area of mathematics.

Etymology
The word ""algebra"" is derived from the Arabic word الجبر al-jabr, and this comes from the treatise written in the year 830 by the medieval Persian mathematician, Muhammad ibn Mūsā al-Khwārizmī, whose Arabic title, Kitāb al-muḫtaṣar fī ḥisāb al-ğabr wa-l-muqābala, can be translated as The Compendious Book on Calculation by Completion and Balancing. The treatise provided for the systematic solution of linear and quadratic equations. According to one history, ""[i]t is not certain just what the terms al-jabr and muqabalah mean, but the usual interpretation is similar to that implied in the previous translation. The word 'al-jabr' presumably meant something like 'restoration' or 'completion' and seems to refer to the transposition of subtracted terms to the other side of an equation; the word 'muqabalah' is said to refer to 'reduction' or 'balancing'—that is, the cancellation of like terms on opposite sides of the equation. Arabic influence in Spain long after the time of al-Khwarizmi is found in Don Quixote, where the word 'algebrista' is used for a bone-setter, that is, a 'restorer'."" The term is used by al-Khwarizmi to describe the operations that he introduced, ""reduction"" and ""balancing"", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation.

Stages of algebra
Algebraic expression
Algebra did not always make use of the symbolism that is now ubiquitous in mathematics; instead, it went through three distinct stages."
"What does the word ""al-jabr"" mean?",Restoration,Completion,Reduction,Balancing,All of the above,E,"Algebra can essentially be considered as doing computations similar to those of arithmetic but with non-numerical mathematical objects. However, until the 19th century, algebra consisted essentially of the theory of equations. For example, the fundamental theorem of algebra belongs to the theory of equations and is not, nowadays, considered as belonging to algebra (in fact, every proof must use the completeness of the real numbers, which is not an algebraic property).
This article describes the history of the theory of equations, called here ""algebra"", from the origins to the emergence of algebra as a separate area of mathematics.

Etymology
The word ""algebra"" is derived from the Arabic word الجبر al-jabr, and this comes from the treatise written in the year 830 by the medieval Persian mathematician, Muhammad ibn Mūsā al-Khwārizmī, whose Arabic title, Kitāb al-muḫtaṣar fī ḥisāb al-ğabr wa-l-muqābala, can be translated as The Compendious Book on Calculation by Completion and Balancing. The treatise provided for the systematic solution of linear and quadratic equations. According to one history, ""[i]t is not certain just what the terms al-jabr and muqabalah mean, but the usual interpretation is similar to that implied in the previous translation. The word 'al-jabr' presumably meant something like 'restoration' or 'completion' and seems to refer to the transposition of subtracted terms to the other side of an equation; the word 'muqabalah' is said to refer to 'reduction' or 'balancing'—that is, the cancellation of like terms on opposite sides of the equation. Arabic influence in Spain long after the time of al-Khwarizmi is found in Don Quixote, where the word 'algebrista' is used for a bone-setter, that is, a 'restorer'."" The term is used by al-Khwarizmi to describe the operations that he introduced, ""reduction"" and ""balancing"", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation.

Stages of algebra
Algebraic expression
Algebra did not always make use of the symbolism that is now ubiquitous in mathematics; instead, it went through three distinct stages."
What are the operations described by al-Khwarizmi in his treatise?,Addition and multiplication,Subtraction and division,Reduction and balancing,Transposition and cancellation,Restoration and completion,C,"Algebra can essentially be considered as doing computations similar to those of arithmetic but with non-numerical mathematical objects. However, until the 19th century, algebra consisted essentially of the theory of equations. For example, the fundamental theorem of algebra belongs to the theory of equations and is not, nowadays, considered as belonging to algebra (in fact, every proof must use the completeness of the real numbers, which is not an algebraic property).
This article describes the history of the theory of equations, called here ""algebra"", from the origins to the emergence of algebra as a separate area of mathematics.

Etymology
The word ""algebra"" is derived from the Arabic word الجبر al-jabr, and this comes from the treatise written in the year 830 by the medieval Persian mathematician, Muhammad ibn Mūsā al-Khwārizmī, whose Arabic title, Kitāb al-muḫtaṣar fī ḥisāb al-ğabr wa-l-muqābala, can be translated as The Compendious Book on Calculation by Completion and Balancing. The treatise provided for the systematic solution of linear and quadratic equations. According to one history, ""[i]t is not certain just what the terms al-jabr and muqabalah mean, but the usual interpretation is similar to that implied in the previous translation. The word 'al-jabr' presumably meant something like 'restoration' or 'completion' and seems to refer to the transposition of subtracted terms to the other side of an equation; the word 'muqabalah' is said to refer to 'reduction' or 'balancing'—that is, the cancellation of like terms on opposite sides of the equation. Arabic influence in Spain long after the time of al-Khwarizmi is found in Don Quixote, where the word 'algebrista' is used for a bone-setter, that is, a 'restorer'."" The term is used by al-Khwarizmi to describe the operations that he introduced, ""reduction"" and ""balancing"", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation.

Stages of algebra
Algebraic expression
Algebra did not always make use of the symbolism that is now ubiquitous in mathematics; instead, it went through three distinct stages."
What stages did algebra go through?,"Symbolism, computation, solution","Arithmetic, geometry, equations","Manipulation, reduction, completion","Greek, Persian, Arabic","Addition, subtraction, multiplication",C,"Algebra can essentially be considered as doing computations similar to those of arithmetic but with non-numerical mathematical objects. However, until the 19th century, algebra consisted essentially of the theory of equations. For example, the fundamental theorem of algebra belongs to the theory of equations and is not, nowadays, considered as belonging to algebra (in fact, every proof must use the completeness of the real numbers, which is not an algebraic property).
This article describes the history of the theory of equations, called here ""algebra"", from the origins to the emergence of algebra as a separate area of mathematics.

Etymology
The word ""algebra"" is derived from the Arabic word الجبر al-jabr, and this comes from the treatise written in the year 830 by the medieval Persian mathematician, Muhammad ibn Mūsā al-Khwārizmī, whose Arabic title, Kitāb al-muḫtaṣar fī ḥisāb al-ğabr wa-l-muqābala, can be translated as The Compendious Book on Calculation by Completion and Balancing. The treatise provided for the systematic solution of linear and quadratic equations. According to one history, ""[i]t is not certain just what the terms al-jabr and muqabalah mean, but the usual interpretation is similar to that implied in the previous translation. The word 'al-jabr' presumably meant something like 'restoration' or 'completion' and seems to refer to the transposition of subtracted terms to the other side of an equation; the word 'muqabalah' is said to refer to 'reduction' or 'balancing'—that is, the cancellation of like terms on opposite sides of the equation. Arabic influence in Spain long after the time of al-Khwarizmi is found in Don Quixote, where the word 'algebrista' is used for a bone-setter, that is, a 'restorer'."" The term is used by al-Khwarizmi to describe the operations that he introduced, ""reduction"" and ""balancing"", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation.

Stages of algebra
Algebraic expression
Algebra did not always make use of the symbolism that is now ubiquitous in mathematics; instead, it went through three distinct stages."
What are some of the components that tower climbers work with?,"Satellite dishes, cables, and routers","Coaxial cable, antennas, and radios","Computers, keyboards, and monitors","Microphones, cameras, and lighting equipment","Printers, scanners, and fax machines",B,"A tower climber specialize in maintenance, installation, and decommissioning of cellular and radio tower components such as coaxial cable, antennas, radios, fiber optic cable and broadcast equipment for cell phones, television, radio, etc.

Description
Tower climbers perform routine inspections and tests on broadcasting towers, and may also be called upon to perform repairs and to provide input when plans for new equipment are being developed.
Along with the installers of the tower antennas, other crews climb the towers to perform condition checks of the towers, guy lines and lighting systems. One of the most demanding aspects of many tower climber jobs is hauling materials and tools up to the proper installation height on the tower. In most cases, the climber scales the tower to the desired height and lowers a rope to a ground crew member who then ties it to a needed part on the ground. The tower worker then pulls the piece up and secures the rope before installing the part onto the tower.

Hazards
Tower climbers may be injured or killed by falling objects, structural collapses and equipment failures. Some of the more frequently encountered hazards include falls from great heights; electrical hazards; hazards associated with hoisting personnel and equipment with base-mounted drum hoists; inclement weather; and structural collapse of towers.

United States
Working on cellphone towers is the deadliest job in the United States, according to a trade publication, and it has been claimed that the tower climbing industry experiences 10 times more death casualties than the construction workers. There were 50 - 100 deaths from falls from communication towers between 2003 and 2011. The head of the Occupational Safety and Health Administration (OSHA) once called tower climbing “the most dangerous job in America.”


== References ==."
What are some tasks that tower climbers perform?,Building and painting towers,Testing and repairing broadcasting towers,Installing and maintaining satellite dishes,Programming and troubleshooting routers,Operating and maintaining camera equipment,B,"A tower climber specialize in maintenance, installation, and decommissioning of cellular and radio tower components such as coaxial cable, antennas, radios, fiber optic cable and broadcast equipment for cell phones, television, radio, etc.

Description
Tower climbers perform routine inspections and tests on broadcasting towers, and may also be called upon to perform repairs and to provide input when plans for new equipment are being developed.
Along with the installers of the tower antennas, other crews climb the towers to perform condition checks of the towers, guy lines and lighting systems. One of the most demanding aspects of many tower climber jobs is hauling materials and tools up to the proper installation height on the tower. In most cases, the climber scales the tower to the desired height and lowers a rope to a ground crew member who then ties it to a needed part on the ground. The tower worker then pulls the piece up and secures the rope before installing the part onto the tower.

Hazards
Tower climbers may be injured or killed by falling objects, structural collapses and equipment failures. Some of the more frequently encountered hazards include falls from great heights; electrical hazards; hazards associated with hoisting personnel and equipment with base-mounted drum hoists; inclement weather; and structural collapse of towers.

United States
Working on cellphone towers is the deadliest job in the United States, according to a trade publication, and it has been claimed that the tower climbing industry experiences 10 times more death casualties than the construction workers. There were 50 - 100 deaths from falls from communication towers between 2003 and 2011. The head of the Occupational Safety and Health Administration (OSHA) once called tower climbing “the most dangerous job in America.”


== References ==."
What is one of the most challenging aspects of tower climber jobs?,Scaling the tower to the desired height,Performing routine inspections and tests,Providing input for new equipment plans,Checking the condition of guy lines and lighting systems,Hauling materials and tools up the tower,E,"A tower climber specialize in maintenance, installation, and decommissioning of cellular and radio tower components such as coaxial cable, antennas, radios, fiber optic cable and broadcast equipment for cell phones, television, radio, etc.

Description
Tower climbers perform routine inspections and tests on broadcasting towers, and may also be called upon to perform repairs and to provide input when plans for new equipment are being developed.
Along with the installers of the tower antennas, other crews climb the towers to perform condition checks of the towers, guy lines and lighting systems. One of the most demanding aspects of many tower climber jobs is hauling materials and tools up to the proper installation height on the tower. In most cases, the climber scales the tower to the desired height and lowers a rope to a ground crew member who then ties it to a needed part on the ground. The tower worker then pulls the piece up and secures the rope before installing the part onto the tower.

Hazards
Tower climbers may be injured or killed by falling objects, structural collapses and equipment failures. Some of the more frequently encountered hazards include falls from great heights; electrical hazards; hazards associated with hoisting personnel and equipment with base-mounted drum hoists; inclement weather; and structural collapse of towers.

United States
Working on cellphone towers is the deadliest job in the United States, according to a trade publication, and it has been claimed that the tower climbing industry experiences 10 times more death casualties than the construction workers. There were 50 - 100 deaths from falls from communication towers between 2003 and 2011. The head of the Occupational Safety and Health Administration (OSHA) once called tower climbing “the most dangerous job in America.”


== References ==."
What are some of the hazards tower climbers may encounter?,Falls from great heights and electrical hazards,Chemical exposure and fire hazards,Loud noises and repetitive motion injuries,Exposure to extreme temperatures and dust,Traffic accidents and animal bites,A,"A tower climber specialize in maintenance, installation, and decommissioning of cellular and radio tower components such as coaxial cable, antennas, radios, fiber optic cable and broadcast equipment for cell phones, television, radio, etc.

Description
Tower climbers perform routine inspections and tests on broadcasting towers, and may also be called upon to perform repairs and to provide input when plans for new equipment are being developed.
Along with the installers of the tower antennas, other crews climb the towers to perform condition checks of the towers, guy lines and lighting systems. One of the most demanding aspects of many tower climber jobs is hauling materials and tools up to the proper installation height on the tower. In most cases, the climber scales the tower to the desired height and lowers a rope to a ground crew member who then ties it to a needed part on the ground. The tower worker then pulls the piece up and secures the rope before installing the part onto the tower.

Hazards
Tower climbers may be injured or killed by falling objects, structural collapses and equipment failures. Some of the more frequently encountered hazards include falls from great heights; electrical hazards; hazards associated with hoisting personnel and equipment with base-mounted drum hoists; inclement weather; and structural collapse of towers.

United States
Working on cellphone towers is the deadliest job in the United States, according to a trade publication, and it has been claimed that the tower climbing industry experiences 10 times more death casualties than the construction workers. There were 50 - 100 deaths from falls from communication towers between 2003 and 2011. The head of the Occupational Safety and Health Administration (OSHA) once called tower climbing “the most dangerous job in America.”


== References ==."
"According to a trade publication, what has been claimed about tower climbing as a job?",It is the most physically demanding job in America,It is the highest paying job in the United States,It is the fastest growing job industry in the country,It experiences 10 times more death casualties than construction workers,It has the highest job satisfaction rate among all professions,D,"A tower climber specialize in maintenance, installation, and decommissioning of cellular and radio tower components such as coaxial cable, antennas, radios, fiber optic cable and broadcast equipment for cell phones, television, radio, etc.

Description
Tower climbers perform routine inspections and tests on broadcasting towers, and may also be called upon to perform repairs and to provide input when plans for new equipment are being developed.
Along with the installers of the tower antennas, other crews climb the towers to perform condition checks of the towers, guy lines and lighting systems. One of the most demanding aspects of many tower climber jobs is hauling materials and tools up to the proper installation height on the tower. In most cases, the climber scales the tower to the desired height and lowers a rope to a ground crew member who then ties it to a needed part on the ground. The tower worker then pulls the piece up and secures the rope before installing the part onto the tower.

Hazards
Tower climbers may be injured or killed by falling objects, structural collapses and equipment failures. Some of the more frequently encountered hazards include falls from great heights; electrical hazards; hazards associated with hoisting personnel and equipment with base-mounted drum hoists; inclement weather; and structural collapse of towers.

United States
Working on cellphone towers is the deadliest job in the United States, according to a trade publication, and it has been claimed that the tower climbing industry experiences 10 times more death casualties than the construction workers. There were 50 - 100 deaths from falls from communication towers between 2003 and 2011. The head of the Occupational Safety and Health Administration (OSHA) once called tower climbing “the most dangerous job in America.”


== References ==."
What is the primary material used to make bricks?,Concrete,Clay,Stone,Wood,Metal,B,"A brick is a type of construction material used to build walls, pavements and other elements in masonry construction. Properly, the term brick denotes a unit primarily composed of clay, but is now also used informally to denote units made of other materials or other chemically cured construction blocks. Bricks can be joined using mortar, adhesives or by interlocking. Bricks are usually produced at brickworks in numerous classes, types, materials, and sizes which vary with region, and are produced in bulk quantities.Block is a similar term referring to a rectangular building unit composed of clay or concrete, but is usually larger than a brick. Lightweight bricks (also called lightweight blocks) are made from expanded clay aggregate.
Fired bricks are one of the longest-lasting and strongest building materials, sometimes referred to as artificial stone, and have been used since circa 4000 BC. Air-dried bricks, also known as mud-bricks, have a history older than fired bricks, and have an additional ingredient of a mechanical binder such as straw.
Bricks are laid in courses and numerous patterns known as bonds, collectively known as brickwork, and may be laid in various kinds of mortar to hold the bricks together to make a durable structure.

History
Middle East and South Asia
The earliest bricks were dried mud-bricks, meaning that they were formed from clay-bearing earth or mud and dried (usually in the sun) until they were strong enough for use. The oldest discovered bricks, originally made from shaped mud and dating before 7500 BC, were found at Tell Aswad, in the upper Tigris region and in southeast Anatolia close to Diyarbakir.Mud-brick construction was used at Çatalhöyük, from c."
What are lightweight bricks made from?,Concrete,Clay,Stone,Wood,Expanded clay aggregate,E,"A brick is a type of construction material used to build walls, pavements and other elements in masonry construction. Properly, the term brick denotes a unit primarily composed of clay, but is now also used informally to denote units made of other materials or other chemically cured construction blocks. Bricks can be joined using mortar, adhesives or by interlocking. Bricks are usually produced at brickworks in numerous classes, types, materials, and sizes which vary with region, and are produced in bulk quantities.Block is a similar term referring to a rectangular building unit composed of clay or concrete, but is usually larger than a brick. Lightweight bricks (also called lightweight blocks) are made from expanded clay aggregate.
Fired bricks are one of the longest-lasting and strongest building materials, sometimes referred to as artificial stone, and have been used since circa 4000 BC. Air-dried bricks, also known as mud-bricks, have a history older than fired bricks, and have an additional ingredient of a mechanical binder such as straw.
Bricks are laid in courses and numerous patterns known as bonds, collectively known as brickwork, and may be laid in various kinds of mortar to hold the bricks together to make a durable structure.

History
Middle East and South Asia
The earliest bricks were dried mud-bricks, meaning that they were formed from clay-bearing earth or mud and dried (usually in the sun) until they were strong enough for use. The oldest discovered bricks, originally made from shaped mud and dating before 7500 BC, were found at Tell Aswad, in the upper Tigris region and in southeast Anatolia close to Diyarbakir.Mud-brick construction was used at Çatalhöyük, from c."
Which type of brick is considered one of the strongest building materials?,Fired bricks,Lightweight bricks,Air-dried bricks,Mud-bricks,Artificial stone,A,"A brick is a type of construction material used to build walls, pavements and other elements in masonry construction. Properly, the term brick denotes a unit primarily composed of clay, but is now also used informally to denote units made of other materials or other chemically cured construction blocks. Bricks can be joined using mortar, adhesives or by interlocking. Bricks are usually produced at brickworks in numerous classes, types, materials, and sizes which vary with region, and are produced in bulk quantities.Block is a similar term referring to a rectangular building unit composed of clay or concrete, but is usually larger than a brick. Lightweight bricks (also called lightweight blocks) are made from expanded clay aggregate.
Fired bricks are one of the longest-lasting and strongest building materials, sometimes referred to as artificial stone, and have been used since circa 4000 BC. Air-dried bricks, also known as mud-bricks, have a history older than fired bricks, and have an additional ingredient of a mechanical binder such as straw.
Bricks are laid in courses and numerous patterns known as bonds, collectively known as brickwork, and may be laid in various kinds of mortar to hold the bricks together to make a durable structure.

History
Middle East and South Asia
The earliest bricks were dried mud-bricks, meaning that they were formed from clay-bearing earth or mud and dried (usually in the sun) until they were strong enough for use. The oldest discovered bricks, originally made from shaped mud and dating before 7500 BC, were found at Tell Aswad, in the upper Tigris region and in southeast Anatolia close to Diyarbakir.Mud-brick construction was used at Çatalhöyük, from c."
What is the term for a rectangular building unit composed of clay or concrete?,Brickwork,Block,Mortar,Adhesive,Binder,B,"A brick is a type of construction material used to build walls, pavements and other elements in masonry construction. Properly, the term brick denotes a unit primarily composed of clay, but is now also used informally to denote units made of other materials or other chemically cured construction blocks. Bricks can be joined using mortar, adhesives or by interlocking. Bricks are usually produced at brickworks in numerous classes, types, materials, and sizes which vary with region, and are produced in bulk quantities.Block is a similar term referring to a rectangular building unit composed of clay or concrete, but is usually larger than a brick. Lightweight bricks (also called lightweight blocks) are made from expanded clay aggregate.
Fired bricks are one of the longest-lasting and strongest building materials, sometimes referred to as artificial stone, and have been used since circa 4000 BC. Air-dried bricks, also known as mud-bricks, have a history older than fired bricks, and have an additional ingredient of a mechanical binder such as straw.
Bricks are laid in courses and numerous patterns known as bonds, collectively known as brickwork, and may be laid in various kinds of mortar to hold the bricks together to make a durable structure.

History
Middle East and South Asia
The earliest bricks were dried mud-bricks, meaning that they were formed from clay-bearing earth or mud and dried (usually in the sun) until they were strong enough for use. The oldest discovered bricks, originally made from shaped mud and dating before 7500 BC, were found at Tell Aswad, in the upper Tigris region and in southeast Anatolia close to Diyarbakir.Mud-brick construction was used at Çatalhöyük, from c."
What is the additional ingredient in air-dried bricks?,Straw,Concrete,Clay,Stone,Wood,A,"A brick is a type of construction material used to build walls, pavements and other elements in masonry construction. Properly, the term brick denotes a unit primarily composed of clay, but is now also used informally to denote units made of other materials or other chemically cured construction blocks. Bricks can be joined using mortar, adhesives or by interlocking. Bricks are usually produced at brickworks in numerous classes, types, materials, and sizes which vary with region, and are produced in bulk quantities.Block is a similar term referring to a rectangular building unit composed of clay or concrete, but is usually larger than a brick. Lightweight bricks (also called lightweight blocks) are made from expanded clay aggregate.
Fired bricks are one of the longest-lasting and strongest building materials, sometimes referred to as artificial stone, and have been used since circa 4000 BC. Air-dried bricks, also known as mud-bricks, have a history older than fired bricks, and have an additional ingredient of a mechanical binder such as straw.
Bricks are laid in courses and numerous patterns known as bonds, collectively known as brickwork, and may be laid in various kinds of mortar to hold the bricks together to make a durable structure.

History
Middle East and South Asia
The earliest bricks were dried mud-bricks, meaning that they were formed from clay-bearing earth or mud and dried (usually in the sun) until they were strong enough for use. The oldest discovered bricks, originally made from shaped mud and dating before 7500 BC, were found at Tell Aswad, in the upper Tigris region and in southeast Anatolia close to Diyarbakir.Mud-brick construction was used at Çatalhöyük, from c."
What is entropy?,A scientific concept associated with disorder and randomness,A measurable physical property,A concept used in diverse fields,A term used in information theory,All of the above,E,"Entropy is a scientific concept, as well as a measurable physical property, that is most commonly associated with a state of disorder, randomness, or uncertainty. The term and the concept are used in diverse fields, from classical thermodynamics, where it was first recognized, to the microscopic description of nature in statistical physics, and to the principles of information theory. It has found far-ranging applications in chemistry and physics, in biological systems and their relation to life, in cosmology, economics, sociology, weather science, climate change, and information systems including the transmission of information in telecommunication.The thermodynamic concept was referred to by Scottish scientist and engineer William Rankine in 1850 with the names thermodynamic function and heat-potential. In 1865, German physicist Rudolf Clausius, one of the leading founders of the field of thermodynamics, defined it as the quotient of an infinitesimal amount of heat to the instantaneous temperature. He initially described it as transformation-content, in German Verwandlungsinhalt, and later coined the term entropy from a Greek word for transformation. Referring to microscopic constitution and structure, in 1862, Clausius interpreted the concept as meaning disgregation.Entropy is central to the second law of thermodynamics, which states that the entropy of an isolated system left to spontaneous evolution cannot decrease with time. As a result, isolated systems evolve toward thermodynamic equilibrium, where the entropy is highest."
Who first recognized the concept of entropy?,William Rankine,Rudolf Clausius,Scottish scientist and engineer,German physicist,Both A and B,E,"Entropy is a scientific concept, as well as a measurable physical property, that is most commonly associated with a state of disorder, randomness, or uncertainty. The term and the concept are used in diverse fields, from classical thermodynamics, where it was first recognized, to the microscopic description of nature in statistical physics, and to the principles of information theory. It has found far-ranging applications in chemistry and physics, in biological systems and their relation to life, in cosmology, economics, sociology, weather science, climate change, and information systems including the transmission of information in telecommunication.The thermodynamic concept was referred to by Scottish scientist and engineer William Rankine in 1850 with the names thermodynamic function and heat-potential. In 1865, German physicist Rudolf Clausius, one of the leading founders of the field of thermodynamics, defined it as the quotient of an infinitesimal amount of heat to the instantaneous temperature. He initially described it as transformation-content, in German Verwandlungsinhalt, and later coined the term entropy from a Greek word for transformation. Referring to microscopic constitution and structure, in 1862, Clausius interpreted the concept as meaning disgregation.Entropy is central to the second law of thermodynamics, which states that the entropy of an isolated system left to spontaneous evolution cannot decrease with time. As a result, isolated systems evolve toward thermodynamic equilibrium, where the entropy is highest."
What did Rudolf Clausius initially call the thermodynamic concept?,Thermodynamic function,Heat-potential,Transformation-content,Entropy,All of the above,C,"Entropy is a scientific concept, as well as a measurable physical property, that is most commonly associated with a state of disorder, randomness, or uncertainty. The term and the concept are used in diverse fields, from classical thermodynamics, where it was first recognized, to the microscopic description of nature in statistical physics, and to the principles of information theory. It has found far-ranging applications in chemistry and physics, in biological systems and their relation to life, in cosmology, economics, sociology, weather science, climate change, and information systems including the transmission of information in telecommunication.The thermodynamic concept was referred to by Scottish scientist and engineer William Rankine in 1850 with the names thermodynamic function and heat-potential. In 1865, German physicist Rudolf Clausius, one of the leading founders of the field of thermodynamics, defined it as the quotient of an infinitesimal amount of heat to the instantaneous temperature. He initially described it as transformation-content, in German Verwandlungsinhalt, and later coined the term entropy from a Greek word for transformation. Referring to microscopic constitution and structure, in 1862, Clausius interpreted the concept as meaning disgregation.Entropy is central to the second law of thermodynamics, which states that the entropy of an isolated system left to spontaneous evolution cannot decrease with time. As a result, isolated systems evolve toward thermodynamic equilibrium, where the entropy is highest."
How did Clausius interpret the concept of entropy in 1862?,As meaning disgregation,As transformation-content,As a quotient of infinitesimal heat to temperature,As related to microscopic constitution and structure,None of the above,D,"Entropy is a scientific concept, as well as a measurable physical property, that is most commonly associated with a state of disorder, randomness, or uncertainty. The term and the concept are used in diverse fields, from classical thermodynamics, where it was first recognized, to the microscopic description of nature in statistical physics, and to the principles of information theory. It has found far-ranging applications in chemistry and physics, in biological systems and their relation to life, in cosmology, economics, sociology, weather science, climate change, and information systems including the transmission of information in telecommunication.The thermodynamic concept was referred to by Scottish scientist and engineer William Rankine in 1850 with the names thermodynamic function and heat-potential. In 1865, German physicist Rudolf Clausius, one of the leading founders of the field of thermodynamics, defined it as the quotient of an infinitesimal amount of heat to the instantaneous temperature. He initially described it as transformation-content, in German Verwandlungsinhalt, and later coined the term entropy from a Greek word for transformation. Referring to microscopic constitution and structure, in 1862, Clausius interpreted the concept as meaning disgregation.Entropy is central to the second law of thermodynamics, which states that the entropy of an isolated system left to spontaneous evolution cannot decrease with time. As a result, isolated systems evolve toward thermodynamic equilibrium, where the entropy is highest."
What does the second law of thermodynamics state?,Entropy of an isolated system cannot decrease with time,Isolated systems evolve towards thermodynamic equilibrium,Entropy is highest at thermodynamic equilibrium,All of the above,None of the above,D,"Entropy is a scientific concept, as well as a measurable physical property, that is most commonly associated with a state of disorder, randomness, or uncertainty. The term and the concept are used in diverse fields, from classical thermodynamics, where it was first recognized, to the microscopic description of nature in statistical physics, and to the principles of information theory. It has found far-ranging applications in chemistry and physics, in biological systems and their relation to life, in cosmology, economics, sociology, weather science, climate change, and information systems including the transmission of information in telecommunication.The thermodynamic concept was referred to by Scottish scientist and engineer William Rankine in 1850 with the names thermodynamic function and heat-potential. In 1865, German physicist Rudolf Clausius, one of the leading founders of the field of thermodynamics, defined it as the quotient of an infinitesimal amount of heat to the instantaneous temperature. He initially described it as transformation-content, in German Verwandlungsinhalt, and later coined the term entropy from a Greek word for transformation. Referring to microscopic constitution and structure, in 1862, Clausius interpreted the concept as meaning disgregation.Entropy is central to the second law of thermodynamics, which states that the entropy of an isolated system left to spontaneous evolution cannot decrease with time. As a result, isolated systems evolve toward thermodynamic equilibrium, where the entropy is highest."
Who proposed the Crocco's Multiplanetary Trajectory?,Walter Hohmann,Gaetano Crocco,G. A. Crocco,Space Pioneer,Aeronautics and Space Pioneer,C,"Crocco's Multiplanetary Trajectory, sometimes named Crocco's Mission and Crocco's ""Grand Tour"", is a mathematical description of an hypothetical Earth-Mars-Venus-Earth-Research Mission, which was first proposed in 1956 by the Aeronautics and Space Pioneer G. A. Crocco during the VII. International Astronautical Congress in Rome.

History
With the beginning of spaceflight in the first half of the 20th century, theoretical mathematicians estimated the amount of energy required for interplanetary spaceflight for the first time, the energy requirement being significantly higher than that of any chemical fuel available at that time. It remained questionable if humanity would ever be capable of reaching certain locations in the Solar System.Even though Walter Hohmann had already calculated the most energy-efficient trajectory between two similar circular orbits (the Hohmann transfer orbit) at the beginning of the century, an Earth-Mars-Earth round trip utilizing this flight path would have made it a necessity to remain on the surface of Mars for a duration of 425 days, waiting for the planets to align and the next launch window to open, in addition to 259 days for both the journey to Mars and the return to Earth.
For this reason, Gaetano Crocco developed the concept of a Nonstop-Round-Trip to Mars, which would have had a far lower energy requirement, as the rocket engines of the craft would only have to accelerate it in a single maneuver to attain the necessary velocity to reach Mars. During the passing of the spacecraft, onboard crew would have performed analysis of the martian surface. Even this trajectory would have amounted to a flight time of a little over one year, if the spacecraft were not accelerated in another orbital maneuver.

Properties
Crocco searched for an orbit with the following properties:

Crosses the orbit of the Earth
Crosses the orbit of Mars
Orbital period of one yearWith proper selection of a certain launch window, this would also have allowed the spacecraft to return one year after departure."
When was the Crocco's Multiplanetary Trajectory proposed?,1956,20th century,VII. International Astronautical Congress,Rome,1956 by the Aeronautics and Space Pioneer G. A. Crocco during the VII. International Astronautical Congress in Rome.,E,"Crocco's Multiplanetary Trajectory, sometimes named Crocco's Mission and Crocco's ""Grand Tour"", is a mathematical description of an hypothetical Earth-Mars-Venus-Earth-Research Mission, which was first proposed in 1956 by the Aeronautics and Space Pioneer G. A. Crocco during the VII. International Astronautical Congress in Rome.

History
With the beginning of spaceflight in the first half of the 20th century, theoretical mathematicians estimated the amount of energy required for interplanetary spaceflight for the first time, the energy requirement being significantly higher than that of any chemical fuel available at that time. It remained questionable if humanity would ever be capable of reaching certain locations in the Solar System.Even though Walter Hohmann had already calculated the most energy-efficient trajectory between two similar circular orbits (the Hohmann transfer orbit) at the beginning of the century, an Earth-Mars-Earth round trip utilizing this flight path would have made it a necessity to remain on the surface of Mars for a duration of 425 days, waiting for the planets to align and the next launch window to open, in addition to 259 days for both the journey to Mars and the return to Earth.
For this reason, Gaetano Crocco developed the concept of a Nonstop-Round-Trip to Mars, which would have had a far lower energy requirement, as the rocket engines of the craft would only have to accelerate it in a single maneuver to attain the necessary velocity to reach Mars. During the passing of the spacecraft, onboard crew would have performed analysis of the martian surface. Even this trajectory would have amounted to a flight time of a little over one year, if the spacecraft were not accelerated in another orbital maneuver.

Properties
Crocco searched for an orbit with the following properties:

Crosses the orbit of the Earth
Crosses the orbit of Mars
Orbital period of one yearWith proper selection of a certain launch window, this would also have allowed the spacecraft to return one year after departure."
What is the concept of a Nonstop-Round-Trip to Mars?,Waiting for the planets to align and the next launch window to open,Acceleration in another orbital maneuver,Flight time of a little over one year,Crosses the orbit of Mars,Acceleration in a single maneuver to reach Mars,E,"Crocco's Multiplanetary Trajectory, sometimes named Crocco's Mission and Crocco's ""Grand Tour"", is a mathematical description of an hypothetical Earth-Mars-Venus-Earth-Research Mission, which was first proposed in 1956 by the Aeronautics and Space Pioneer G. A. Crocco during the VII. International Astronautical Congress in Rome.

History
With the beginning of spaceflight in the first half of the 20th century, theoretical mathematicians estimated the amount of energy required for interplanetary spaceflight for the first time, the energy requirement being significantly higher than that of any chemical fuel available at that time. It remained questionable if humanity would ever be capable of reaching certain locations in the Solar System.Even though Walter Hohmann had already calculated the most energy-efficient trajectory between two similar circular orbits (the Hohmann transfer orbit) at the beginning of the century, an Earth-Mars-Earth round trip utilizing this flight path would have made it a necessity to remain on the surface of Mars for a duration of 425 days, waiting for the planets to align and the next launch window to open, in addition to 259 days for both the journey to Mars and the return to Earth.
For this reason, Gaetano Crocco developed the concept of a Nonstop-Round-Trip to Mars, which would have had a far lower energy requirement, as the rocket engines of the craft would only have to accelerate it in a single maneuver to attain the necessary velocity to reach Mars. During the passing of the spacecraft, onboard crew would have performed analysis of the martian surface. Even this trajectory would have amounted to a flight time of a little over one year, if the spacecraft were not accelerated in another orbital maneuver.

Properties
Crocco searched for an orbit with the following properties:

Crosses the orbit of the Earth
Crosses the orbit of Mars
Orbital period of one yearWith proper selection of a certain launch window, this would also have allowed the spacecraft to return one year after departure."
What were the energy requirements for interplanetary spaceflight at that time?,Significantly higher than any chemical fuel available,Calculating the most energy-efficient trajectory,Flight time of a little over one year,The rockets engines only have to accelerate in a single maneuver,Waiting for the planets to align and the next launch window to open,A,"Crocco's Multiplanetary Trajectory, sometimes named Crocco's Mission and Crocco's ""Grand Tour"", is a mathematical description of an hypothetical Earth-Mars-Venus-Earth-Research Mission, which was first proposed in 1956 by the Aeronautics and Space Pioneer G. A. Crocco during the VII. International Astronautical Congress in Rome.

History
With the beginning of spaceflight in the first half of the 20th century, theoretical mathematicians estimated the amount of energy required for interplanetary spaceflight for the first time, the energy requirement being significantly higher than that of any chemical fuel available at that time. It remained questionable if humanity would ever be capable of reaching certain locations in the Solar System.Even though Walter Hohmann had already calculated the most energy-efficient trajectory between two similar circular orbits (the Hohmann transfer orbit) at the beginning of the century, an Earth-Mars-Earth round trip utilizing this flight path would have made it a necessity to remain on the surface of Mars for a duration of 425 days, waiting for the planets to align and the next launch window to open, in addition to 259 days for both the journey to Mars and the return to Earth.
For this reason, Gaetano Crocco developed the concept of a Nonstop-Round-Trip to Mars, which would have had a far lower energy requirement, as the rocket engines of the craft would only have to accelerate it in a single maneuver to attain the necessary velocity to reach Mars. During the passing of the spacecraft, onboard crew would have performed analysis of the martian surface. Even this trajectory would have amounted to a flight time of a little over one year, if the spacecraft were not accelerated in another orbital maneuver.

Properties
Crocco searched for an orbit with the following properties:

Crosses the orbit of the Earth
Crosses the orbit of Mars
Orbital period of one yearWith proper selection of a certain launch window, this would also have allowed the spacecraft to return one year after departure."
What were the properties that Crocco searched for in the orbit?,Crosses the orbit of Mars,Orbital period of one year,Crosses the orbit of Earth,Proper selection of a certain launch window,Proper selection of a certain launch window and crosses the orbit of Mars,E,"Crocco's Multiplanetary Trajectory, sometimes named Crocco's Mission and Crocco's ""Grand Tour"", is a mathematical description of an hypothetical Earth-Mars-Venus-Earth-Research Mission, which was first proposed in 1956 by the Aeronautics and Space Pioneer G. A. Crocco during the VII. International Astronautical Congress in Rome.

History
With the beginning of spaceflight in the first half of the 20th century, theoretical mathematicians estimated the amount of energy required for interplanetary spaceflight for the first time, the energy requirement being significantly higher than that of any chemical fuel available at that time. It remained questionable if humanity would ever be capable of reaching certain locations in the Solar System.Even though Walter Hohmann had already calculated the most energy-efficient trajectory between two similar circular orbits (the Hohmann transfer orbit) at the beginning of the century, an Earth-Mars-Earth round trip utilizing this flight path would have made it a necessity to remain on the surface of Mars for a duration of 425 days, waiting for the planets to align and the next launch window to open, in addition to 259 days for both the journey to Mars and the return to Earth.
For this reason, Gaetano Crocco developed the concept of a Nonstop-Round-Trip to Mars, which would have had a far lower energy requirement, as the rocket engines of the craft would only have to accelerate it in a single maneuver to attain the necessary velocity to reach Mars. During the passing of the spacecraft, onboard crew would have performed analysis of the martian surface. Even this trajectory would have amounted to a flight time of a little over one year, if the spacecraft were not accelerated in another orbital maneuver.

Properties
Crocco searched for an orbit with the following properties:

Crosses the orbit of the Earth
Crosses the orbit of Mars
Orbital period of one yearWith proper selection of a certain launch window, this would also have allowed the spacecraft to return one year after departure."
What is quantum state discrimination?,A technique to identify the specific quantum state of a system,A method to measure multiple quantum states simultaneously,A process to create orthogonal vectors in a quantum system,A way to determine the state of a system without additional measurements,A technique used in quantum tomography,A,"The term quantum state discrimination collectively refers to quantum-informatics techniques, with the help of which, by performing a small number of measurements on a physical system , its specific quantum state can be identified . And this is provided that the set of states in which the system can be is known in advance, and we only need to determine which one it is. This assumption distinguishes such techniques from quantum tomography, which does not impose additional requirements on the state of the system, but requires many times more measurements.
If the set of states in which the investigated system can be is represented by orthogonal vectors , the situation is particularly simple. To unambiguously determine the state of the system, it is enough to perform a quantum measurement in the basis formed by these vectors. The given quantum state can then be flawlessly identified from the measured value. Moreover, it can be easily shown that if the individual states are not orthogonal to each other, there is no way to tell them apart with certainty. Therefore, in such a case, it is always necessary to take into account the possibility of incorrect or inconclusive determination of the state of the system."
What distinguishes quantum state discrimination from quantum tomography?,The number of measurements required,The basis formed by orthogonal vectors,The possibility of incorrect determination of the system state,The use of quantum-informatics techniques,None of the above,B,"The term quantum state discrimination collectively refers to quantum-informatics techniques, with the help of which, by performing a small number of measurements on a physical system , its specific quantum state can be identified . And this is provided that the set of states in which the system can be is known in advance, and we only need to determine which one it is. This assumption distinguishes such techniques from quantum tomography, which does not impose additional requirements on the state of the system, but requires many times more measurements.
If the set of states in which the investigated system can be is represented by orthogonal vectors , the situation is particularly simple. To unambiguously determine the state of the system, it is enough to perform a quantum measurement in the basis formed by these vectors. The given quantum state can then be flawlessly identified from the measured value. Moreover, it can be easily shown that if the individual states are not orthogonal to each other, there is no way to tell them apart with certainty. Therefore, in such a case, it is always necessary to take into account the possibility of incorrect or inconclusive determination of the state of the system."
What is required in order to unambiguously determine the state of a system with orthogonal vectors?,Performing multiple quantum measurements,Knowing the set of states in advance,Using quantum tomography techniques,Creating a quantum measurement basis,None of the above,B,"The term quantum state discrimination collectively refers to quantum-informatics techniques, with the help of which, by performing a small number of measurements on a physical system , its specific quantum state can be identified . And this is provided that the set of states in which the system can be is known in advance, and we only need to determine which one it is. This assumption distinguishes such techniques from quantum tomography, which does not impose additional requirements on the state of the system, but requires many times more measurements.
If the set of states in which the investigated system can be is represented by orthogonal vectors , the situation is particularly simple. To unambiguously determine the state of the system, it is enough to perform a quantum measurement in the basis formed by these vectors. The given quantum state can then be flawlessly identified from the measured value. Moreover, it can be easily shown that if the individual states are not orthogonal to each other, there is no way to tell them apart with certainty. Therefore, in such a case, it is always necessary to take into account the possibility of incorrect or inconclusive determination of the state of the system."
What happens when individual quantum states are not orthogonal to each other?,They can be easily identified through a quantum measurement,They cannot be distinguished with certainty,They can still be determined through quantum tomography,They form a basis for quantum state discrimination,None of the above,B,"The term quantum state discrimination collectively refers to quantum-informatics techniques, with the help of which, by performing a small number of measurements on a physical system , its specific quantum state can be identified . And this is provided that the set of states in which the system can be is known in advance, and we only need to determine which one it is. This assumption distinguishes such techniques from quantum tomography, which does not impose additional requirements on the state of the system, but requires many times more measurements.
If the set of states in which the investigated system can be is represented by orthogonal vectors , the situation is particularly simple. To unambiguously determine the state of the system, it is enough to perform a quantum measurement in the basis formed by these vectors. The given quantum state can then be flawlessly identified from the measured value. Moreover, it can be easily shown that if the individual states are not orthogonal to each other, there is no way to tell them apart with certainty. Therefore, in such a case, it is always necessary to take into account the possibility of incorrect or inconclusive determination of the state of the system."
When is it necessary to consider incorrect or inconclusive determination of the system state?,When the quantum states are orthogonal to each other,When performing quantum measurements in a different basis,When the set of states is known in advance,When the individual states are not orthogonal to each other,None of the above,D,"The term quantum state discrimination collectively refers to quantum-informatics techniques, with the help of which, by performing a small number of measurements on a physical system , its specific quantum state can be identified . And this is provided that the set of states in which the system can be is known in advance, and we only need to determine which one it is. This assumption distinguishes such techniques from quantum tomography, which does not impose additional requirements on the state of the system, but requires many times more measurements.
If the set of states in which the investigated system can be is represented by orthogonal vectors , the situation is particularly simple. To unambiguously determine the state of the system, it is enough to perform a quantum measurement in the basis formed by these vectors. The given quantum state can then be flawlessly identified from the measured value. Moreover, it can be easily shown that if the individual states are not orthogonal to each other, there is no way to tell them apart with certainty. Therefore, in such a case, it is always necessary to take into account the possibility of incorrect or inconclusive determination of the state of the system."
What is control engineering?,A field of study that focuses only on control theory,An engineering discipline that deals with control systems,A branch of mechanical engineering,A practice that uses sensors and detectors to measure output performance,A field that gained significant attention during the 21st century,B,"Control engineering or control systems engineering is an engineering discipline that deals with control systems, applying control theory to design equipment and systems with desired behaviors in control environments. The discipline of controls overlaps and is usually taught along with electrical engineering and mechanical engineering at many institutions around the world.The practice uses sensors and detectors to measure the output performance of the process being controlled; these measurements are used to provide corrective feedback helping to achieve the desired performance. Systems designed to perform without requiring human input are called automatic control systems (such as cruise control for regulating the speed of a car). Multi-disciplinary in nature, control systems engineering activities focus on implementation of control systems mainly derived by mathematical modeling of a diverse range of systems.

Overview
Modern day control engineering is a relatively new field of study that gained significant attention during the 20th century with the advancement of technology. It can be broadly defined or classified as practical application of control theory. Control engineering plays an essential role in a wide range of control systems, from simple household washing machines to high-performance fighter aircraft. It seeks to understand physical systems, using mathematical modelling, in terms of inputs, outputs and various components with different behaviors; to use control system design tools to develop controllers for those systems; and to implement controllers in physical systems employing available technology."
What are control systems designed to do?,Provide human input to achieve desired performance,Measure the output performance of a process,Use feedback to achieve the desired performance,Automate control systems without human intervention,Model a diverse range of systems,C,"Control engineering or control systems engineering is an engineering discipline that deals with control systems, applying control theory to design equipment and systems with desired behaviors in control environments. The discipline of controls overlaps and is usually taught along with electrical engineering and mechanical engineering at many institutions around the world.The practice uses sensors and detectors to measure the output performance of the process being controlled; these measurements are used to provide corrective feedback helping to achieve the desired performance. Systems designed to perform without requiring human input are called automatic control systems (such as cruise control for regulating the speed of a car). Multi-disciplinary in nature, control systems engineering activities focus on implementation of control systems mainly derived by mathematical modeling of a diverse range of systems.

Overview
Modern day control engineering is a relatively new field of study that gained significant attention during the 20th century with the advancement of technology. It can be broadly defined or classified as practical application of control theory. Control engineering plays an essential role in a wide range of control systems, from simple household washing machines to high-performance fighter aircraft. It seeks to understand physical systems, using mathematical modelling, in terms of inputs, outputs and various components with different behaviors; to use control system design tools to develop controllers for those systems; and to implement controllers in physical systems employing available technology."
What is the main focus of control systems engineering activities?,Mathematical modeling of physical systems,Developing controllers for physical systems,Implementing controllers in physical systems,Understanding inputs and outputs of physical systems,Using control system design tools,C,"Control engineering or control systems engineering is an engineering discipline that deals with control systems, applying control theory to design equipment and systems with desired behaviors in control environments. The discipline of controls overlaps and is usually taught along with electrical engineering and mechanical engineering at many institutions around the world.The practice uses sensors and detectors to measure the output performance of the process being controlled; these measurements are used to provide corrective feedback helping to achieve the desired performance. Systems designed to perform without requiring human input are called automatic control systems (such as cruise control for regulating the speed of a car). Multi-disciplinary in nature, control systems engineering activities focus on implementation of control systems mainly derived by mathematical modeling of a diverse range of systems.

Overview
Modern day control engineering is a relatively new field of study that gained significant attention during the 20th century with the advancement of technology. It can be broadly defined or classified as practical application of control theory. Control engineering plays an essential role in a wide range of control systems, from simple household washing machines to high-performance fighter aircraft. It seeks to understand physical systems, using mathematical modelling, in terms of inputs, outputs and various components with different behaviors; to use control system design tools to develop controllers for those systems; and to implement controllers in physical systems employing available technology."
What is the role of control engineering in modern technology?,To focus on control theory only,To develop high-performance fighter aircraft,To automate household washing machines,To understand physical systems using mathematical modeling,To implement controllers in physical systems,E,"Control engineering or control systems engineering is an engineering discipline that deals with control systems, applying control theory to design equipment and systems with desired behaviors in control environments. The discipline of controls overlaps and is usually taught along with electrical engineering and mechanical engineering at many institutions around the world.The practice uses sensors and detectors to measure the output performance of the process being controlled; these measurements are used to provide corrective feedback helping to achieve the desired performance. Systems designed to perform without requiring human input are called automatic control systems (such as cruise control for regulating the speed of a car). Multi-disciplinary in nature, control systems engineering activities focus on implementation of control systems mainly derived by mathematical modeling of a diverse range of systems.

Overview
Modern day control engineering is a relatively new field of study that gained significant attention during the 20th century with the advancement of technology. It can be broadly defined or classified as practical application of control theory. Control engineering plays an essential role in a wide range of control systems, from simple household washing machines to high-performance fighter aircraft. It seeks to understand physical systems, using mathematical modelling, in terms of inputs, outputs and various components with different behaviors; to use control system design tools to develop controllers for those systems; and to implement controllers in physical systems employing available technology."
When did control engineering gain significant attention?,During the 19th century,During the 21st century,During the 20th century,During the Renaissance period,During the Industrial Revolution,C,"Control engineering or control systems engineering is an engineering discipline that deals with control systems, applying control theory to design equipment and systems with desired behaviors in control environments. The discipline of controls overlaps and is usually taught along with electrical engineering and mechanical engineering at many institutions around the world.The practice uses sensors and detectors to measure the output performance of the process being controlled; these measurements are used to provide corrective feedback helping to achieve the desired performance. Systems designed to perform without requiring human input are called automatic control systems (such as cruise control for regulating the speed of a car). Multi-disciplinary in nature, control systems engineering activities focus on implementation of control systems mainly derived by mathematical modeling of a diverse range of systems.

Overview
Modern day control engineering is a relatively new field of study that gained significant attention during the 20th century with the advancement of technology. It can be broadly defined or classified as practical application of control theory. Control engineering plays an essential role in a wide range of control systems, from simple household washing machines to high-performance fighter aircraft. It seeks to understand physical systems, using mathematical modelling, in terms of inputs, outputs and various components with different behaviors; to use control system design tools to develop controllers for those systems; and to implement controllers in physical systems employing available technology."
What is virtual environment software?,A software that creates an interactive and immersive experience for administrators and users,"A program that implements, manages and controls multiple virtual environment instances",A system installed within an organization's existing IT infrastructure,A software platform that enables organizations to extend their market and industry reach,A software used for virtual military training and virtual classrooms,B,"Virtual environment software refers to any software, program or system that implements, manages and controls multiple virtual environment instances (self definition). The software is installed within an organization's existing IT infrastructure and controlled from within the organization itself. From a central interface, the software creates an interactive and immersive experience for administrators and users.

Uses
Virtual environment software can be purposed for any use, from advanced military training in a virtual environment simulator to virtual classrooms. Many Virtual Environments are being purposed as branding channels for products and services by enterprise corporations and non-profit groups.Currently, virtual events and virtual tradeshows have been the early accepted uses of virtual event services. More recently, virtual environment software platforms have offered choices to enterprises – with the ability to connect people across the Internet. Virtual environment software enables organizations to extend their market and industry reach while reducing (all travel-related) costs and time.

Background
Providers of virtual environments have tended to focus on the early marketplace adoption of virtual events. These providers are typically software as a service (SaaS)-based."
What are some uses of virtual environment software?,Advanced military training in a virtual environment simulator,Virtual classrooms,Branding channels for products and services by enterprises corporations and non-profit groups,Virtual events and virtual tradeshows,All of the above,E,"Virtual environment software refers to any software, program or system that implements, manages and controls multiple virtual environment instances (self definition). The software is installed within an organization's existing IT infrastructure and controlled from within the organization itself. From a central interface, the software creates an interactive and immersive experience for administrators and users.

Uses
Virtual environment software can be purposed for any use, from advanced military training in a virtual environment simulator to virtual classrooms. Many Virtual Environments are being purposed as branding channels for products and services by enterprise corporations and non-profit groups.Currently, virtual events and virtual tradeshows have been the early accepted uses of virtual event services. More recently, virtual environment software platforms have offered choices to enterprises – with the ability to connect people across the Internet. Virtual environment software enables organizations to extend their market and industry reach while reducing (all travel-related) costs and time.

Background
Providers of virtual environments have tended to focus on the early marketplace adoption of virtual events. These providers are typically software as a service (SaaS)-based."
Which type of providers have tended to focus on the early marketplace adoption of virtual events?,Software as a service (SaaS) providers,Enterprise corporations,Non-profit groups,Virtual environment software platforms,All of the above,A,"Virtual environment software refers to any software, program or system that implements, manages and controls multiple virtual environment instances (self definition). The software is installed within an organization's existing IT infrastructure and controlled from within the organization itself. From a central interface, the software creates an interactive and immersive experience for administrators and users.

Uses
Virtual environment software can be purposed for any use, from advanced military training in a virtual environment simulator to virtual classrooms. Many Virtual Environments are being purposed as branding channels for products and services by enterprise corporations and non-profit groups.Currently, virtual events and virtual tradeshows have been the early accepted uses of virtual event services. More recently, virtual environment software platforms have offered choices to enterprises – with the ability to connect people across the Internet. Virtual environment software enables organizations to extend their market and industry reach while reducing (all travel-related) costs and time.

Background
Providers of virtual environments have tended to focus on the early marketplace adoption of virtual events. These providers are typically software as a service (SaaS)-based."
How can virtual environment software benefit organizations?,By reducing travel-related costs and time,By extending their market and industry reach,By creating an interactive and immersive experience for administrators and users,"By implementing, managing and controlling multiple virtual environment instances",All of the above,E,"Virtual environment software refers to any software, program or system that implements, manages and controls multiple virtual environment instances (self definition). The software is installed within an organization's existing IT infrastructure and controlled from within the organization itself. From a central interface, the software creates an interactive and immersive experience for administrators and users.

Uses
Virtual environment software can be purposed for any use, from advanced military training in a virtual environment simulator to virtual classrooms. Many Virtual Environments are being purposed as branding channels for products and services by enterprise corporations and non-profit groups.Currently, virtual events and virtual tradeshows have been the early accepted uses of virtual event services. More recently, virtual environment software platforms have offered choices to enterprises – with the ability to connect people across the Internet. Virtual environment software enables organizations to extend their market and industry reach while reducing (all travel-related) costs and time.

Background
Providers of virtual environments have tended to focus on the early marketplace adoption of virtual events. These providers are typically software as a service (SaaS)-based."
What are some examples of virtual environment software uses?,Advanced military training in a virtual environment simulator,Virtual classrooms,Branding channels for products and services by enterprises corporations and non-profit groups,Virtual events and virtual tradeshows,All of the above,E,"Virtual environment software refers to any software, program or system that implements, manages and controls multiple virtual environment instances (self definition). The software is installed within an organization's existing IT infrastructure and controlled from within the organization itself. From a central interface, the software creates an interactive and immersive experience for administrators and users.

Uses
Virtual environment software can be purposed for any use, from advanced military training in a virtual environment simulator to virtual classrooms. Many Virtual Environments are being purposed as branding channels for products and services by enterprise corporations and non-profit groups.Currently, virtual events and virtual tradeshows have been the early accepted uses of virtual event services. More recently, virtual environment software platforms have offered choices to enterprises – with the ability to connect people across the Internet. Virtual environment software enables organizations to extend their market and industry reach while reducing (all travel-related) costs and time.

Background
Providers of virtual environments have tended to focus on the early marketplace adoption of virtual events. These providers are typically software as a service (SaaS)-based."
Who is traditionally regarded as the first marine engineer?,Robert Fulton,Archimedes,James Watt,Isambard Kingdom Brunel,Nikola Tesla,B,"Marine engineering is the engineering of boats, ships, submarines, and any other marine vessel. Here it is also taken to include the engineering of other ocean systems and structures – referred to in certain academic and professional circles as “ocean engineering.”
Marine engineering applies a number of engineering sciences, including mechanical engineering, electrical engineering, electronic engineering, and computer science, to the development, design, operation and maintenance of watercraft propulsion and ocean systems. It includes but is not limited to power and propulsion plants, machinery, piping, automation and control systems for marine vehicles of any kind, as well as coastal and offshore structures.

History
Archimedes is traditionally regarded as the first marine engineer, having developed a number of marine engineering systems in antiquity. Modern marine engineering dates back to the beginning of the Industrial Revolution (early 1700s).
In 1807, Robert Fulton successfully used a steam engine to propel a vessel through the water. Fulton's ship used the engine to power a small wooden paddle wheel as its marine propulsion system. The integration of a steam engine into a watercraft to create a marine steam engine was the start of the marine engineering profession. Only twelve years after Fulton’s Clermont had her first voyage, the Savannah marked the first sea voyage from America to Europe."
When did modern marine engineering begin?,1807,1700s,1850s,1900s,2000s,B,"Marine engineering is the engineering of boats, ships, submarines, and any other marine vessel. Here it is also taken to include the engineering of other ocean systems and structures – referred to in certain academic and professional circles as “ocean engineering.”
Marine engineering applies a number of engineering sciences, including mechanical engineering, electrical engineering, electronic engineering, and computer science, to the development, design, operation and maintenance of watercraft propulsion and ocean systems. It includes but is not limited to power and propulsion plants, machinery, piping, automation and control systems for marine vehicles of any kind, as well as coastal and offshore structures.

History
Archimedes is traditionally regarded as the first marine engineer, having developed a number of marine engineering systems in antiquity. Modern marine engineering dates back to the beginning of the Industrial Revolution (early 1700s).
In 1807, Robert Fulton successfully used a steam engine to propel a vessel through the water. Fulton's ship used the engine to power a small wooden paddle wheel as its marine propulsion system. The integration of a steam engine into a watercraft to create a marine steam engine was the start of the marine engineering profession. Only twelve years after Fulton’s Clermont had her first voyage, the Savannah marked the first sea voyage from America to Europe."
What type of engineering sciences are applied in marine engineering?,Mechanical engineering,Electrical engineering,Electronic engineering,Computer science,All of the above,E,"Marine engineering is the engineering of boats, ships, submarines, and any other marine vessel. Here it is also taken to include the engineering of other ocean systems and structures – referred to in certain academic and professional circles as “ocean engineering.”
Marine engineering applies a number of engineering sciences, including mechanical engineering, electrical engineering, electronic engineering, and computer science, to the development, design, operation and maintenance of watercraft propulsion and ocean systems. It includes but is not limited to power and propulsion plants, machinery, piping, automation and control systems for marine vehicles of any kind, as well as coastal and offshore structures.

History
Archimedes is traditionally regarded as the first marine engineer, having developed a number of marine engineering systems in antiquity. Modern marine engineering dates back to the beginning of the Industrial Revolution (early 1700s).
In 1807, Robert Fulton successfully used a steam engine to propel a vessel through the water. Fulton's ship used the engine to power a small wooden paddle wheel as its marine propulsion system. The integration of a steam engine into a watercraft to create a marine steam engine was the start of the marine engineering profession. Only twelve years after Fulton’s Clermont had her first voyage, the Savannah marked the first sea voyage from America to Europe."
What is the term used to refer to the engineering of other ocean systems and structures?,Marine engineering,Watercraft engineering,Ocean engineering,Coastal engineering,Offshore engineering,C,"Marine engineering is the engineering of boats, ships, submarines, and any other marine vessel. Here it is also taken to include the engineering of other ocean systems and structures – referred to in certain academic and professional circles as “ocean engineering.”
Marine engineering applies a number of engineering sciences, including mechanical engineering, electrical engineering, electronic engineering, and computer science, to the development, design, operation and maintenance of watercraft propulsion and ocean systems. It includes but is not limited to power and propulsion plants, machinery, piping, automation and control systems for marine vehicles of any kind, as well as coastal and offshore structures.

History
Archimedes is traditionally regarded as the first marine engineer, having developed a number of marine engineering systems in antiquity. Modern marine engineering dates back to the beginning of the Industrial Revolution (early 1700s).
In 1807, Robert Fulton successfully used a steam engine to propel a vessel through the water. Fulton's ship used the engine to power a small wooden paddle wheel as its marine propulsion system. The integration of a steam engine into a watercraft to create a marine steam engine was the start of the marine engineering profession. Only twelve years after Fulton’s Clermont had her first voyage, the Savannah marked the first sea voyage from America to Europe."
What marked the first sea voyage from America to Europe?,Archimedes,Robert Fulton,Savannah,Clermont,The Industrial Revolution,C,"Marine engineering is the engineering of boats, ships, submarines, and any other marine vessel. Here it is also taken to include the engineering of other ocean systems and structures – referred to in certain academic and professional circles as “ocean engineering.”
Marine engineering applies a number of engineering sciences, including mechanical engineering, electrical engineering, electronic engineering, and computer science, to the development, design, operation and maintenance of watercraft propulsion and ocean systems. It includes but is not limited to power and propulsion plants, machinery, piping, automation and control systems for marine vehicles of any kind, as well as coastal and offshore structures.

History
Archimedes is traditionally regarded as the first marine engineer, having developed a number of marine engineering systems in antiquity. Modern marine engineering dates back to the beginning of the Industrial Revolution (early 1700s).
In 1807, Robert Fulton successfully used a steam engine to propel a vessel through the water. Fulton's ship used the engine to power a small wooden paddle wheel as its marine propulsion system. The integration of a steam engine into a watercraft to create a marine steam engine was the start of the marine engineering profession. Only twelve years after Fulton’s Clermont had her first voyage, the Savannah marked the first sea voyage from America to Europe."
What is synthetic biology?,The study of genetic modification of biological organisms,The process of designing and constructing biological systems for various applications,The engineering discipline focused on manipulating information,The field of research that relies on trial-and-error approaches,The science of harnessing living systems for manufacturing,B,"Engineering biology is the set of methods for designing, building, and testing engineered biological systems which have been used to manipulate information, construct materials, process chemicals, produce energy, provide food, and help maintain or enhance human health and environment.

History
Rapid advances in the ability to genetically modify biological organisms have advanced a new engineering discipline, commonly referred to as synthetic biology. This approach seeks to harness the power of living systems for a variety of manufacturing applications, such as advanced therapeutics, sustainable fuels, chemical feedstocks, and advanced materials. To date, research in synthetic biology has typically relied on trial-and-error approaches, which are costly, laborious, and inefficient.

References
Bibliography
H.R.4521 - America COMPETES Act of 2022https://www.congress.gov/congressional-record/2022/03/17/senate-section/article/S1237-5

Schuergers, N., Werlang, C., Ajo-Franklin, C., & Boghossian, A. (2017). A Synthetic Biology Approach to Engineering Living Photovoltaics. Energy & Environmental Science. doi:10.1039/C7EE00282C
Teague, B."
What are some applications of engineering biology?,Producing sustainable fuels,Enhancing human health and environment,Processing chemicals,Designing advanced materials,All of the above,E,"Engineering biology is the set of methods for designing, building, and testing engineered biological systems which have been used to manipulate information, construct materials, process chemicals, produce energy, provide food, and help maintain or enhance human health and environment.

History
Rapid advances in the ability to genetically modify biological organisms have advanced a new engineering discipline, commonly referred to as synthetic biology. This approach seeks to harness the power of living systems for a variety of manufacturing applications, such as advanced therapeutics, sustainable fuels, chemical feedstocks, and advanced materials. To date, research in synthetic biology has typically relied on trial-and-error approaches, which are costly, laborious, and inefficient.

References
Bibliography
H.R.4521 - America COMPETES Act of 2022https://www.congress.gov/congressional-record/2022/03/17/senate-section/article/S1237-5

Schuergers, N., Werlang, C., Ajo-Franklin, C., & Boghossian, A. (2017). A Synthetic Biology Approach to Engineering Living Photovoltaics. Energy & Environmental Science. doi:10.1039/C7EE00282C
Teague, B."
What are the drawbacks of trial-and-error approaches in synthetic biology research?,They are cost-effective,They are efficient,They can be laborious,They are reliable,They have limited applications,C,"Engineering biology is the set of methods for designing, building, and testing engineered biological systems which have been used to manipulate information, construct materials, process chemicals, produce energy, provide food, and help maintain or enhance human health and environment.

History
Rapid advances in the ability to genetically modify biological organisms have advanced a new engineering discipline, commonly referred to as synthetic biology. This approach seeks to harness the power of living systems for a variety of manufacturing applications, such as advanced therapeutics, sustainable fuels, chemical feedstocks, and advanced materials. To date, research in synthetic biology has typically relied on trial-and-error approaches, which are costly, laborious, and inefficient.

References
Bibliography
H.R.4521 - America COMPETES Act of 2022https://www.congress.gov/congressional-record/2022/03/17/senate-section/article/S1237-5

Schuergers, N., Werlang, C., Ajo-Franklin, C., & Boghossian, A. (2017). A Synthetic Biology Approach to Engineering Living Photovoltaics. Energy & Environmental Science. doi:10.1039/C7EE00282C
Teague, B."
Which legislation is referenced in the article?,The America COMPETES Act of 2022,The Synthetic Biology Act,The Genetic Modification Act,The Manufacturing Applications Act,The Sustainable Fuels Act,A,"Engineering biology is the set of methods for designing, building, and testing engineered biological systems which have been used to manipulate information, construct materials, process chemicals, produce energy, provide food, and help maintain or enhance human health and environment.

History
Rapid advances in the ability to genetically modify biological organisms have advanced a new engineering discipline, commonly referred to as synthetic biology. This approach seeks to harness the power of living systems for a variety of manufacturing applications, such as advanced therapeutics, sustainable fuels, chemical feedstocks, and advanced materials. To date, research in synthetic biology has typically relied on trial-and-error approaches, which are costly, laborious, and inefficient.

References
Bibliography
H.R.4521 - America COMPETES Act of 2022https://www.congress.gov/congressional-record/2022/03/17/senate-section/article/S1237-5

Schuergers, N., Werlang, C., Ajo-Franklin, C., & Boghossian, A. (2017). A Synthetic Biology Approach to Engineering Living Photovoltaics. Energy & Environmental Science. doi:10.1039/C7EE00282C
Teague, B."
What is the title of the referenced research paper?,Synthetic Biology and Photovoltaics,The Role of Living Systems in Energy Production,Engineering Living Photovoltaics,The Advancements in Sustainable Energy,The Future of Renewable Materials,C,"Engineering biology is the set of methods for designing, building, and testing engineered biological systems which have been used to manipulate information, construct materials, process chemicals, produce energy, provide food, and help maintain or enhance human health and environment.

History
Rapid advances in the ability to genetically modify biological organisms have advanced a new engineering discipline, commonly referred to as synthetic biology. This approach seeks to harness the power of living systems for a variety of manufacturing applications, such as advanced therapeutics, sustainable fuels, chemical feedstocks, and advanced materials. To date, research in synthetic biology has typically relied on trial-and-error approaches, which are costly, laborious, and inefficient.

References
Bibliography
H.R.4521 - America COMPETES Act of 2022https://www.congress.gov/congressional-record/2022/03/17/senate-section/article/S1237-5

Schuergers, N., Werlang, C., Ajo-Franklin, C., & Boghossian, A. (2017). A Synthetic Biology Approach to Engineering Living Photovoltaics. Energy & Environmental Science. doi:10.1039/C7EE00282C
Teague, B."
What is the purpose of the mathematical language?,To express scientific laws and theorems,To create new words,To confuse people with neologisms,To use symbols instead of words,To express mathematical formulas,A,"The language of mathematics or mathematical language is an extension of the natural language (for example English) that is used in mathematics and in science for expressing results (scientific laws, theorems, proofs, logical deductions, etc) with concision, precision and unambiguity.

Features
The main features of the mathematical language are the following.

Use of common words with a derived meaning, generally more specific and more precise. For example, ""or"" means ""one, the other or both"", while, in common language, ""both"" is sometimes included and sometimes not. Also, a ""line"" is straight and has zero width.
Use of common words with a meaning that is completely different from their common meaning. For example, a mathematical ring is not related to any other meaning of ""ring"". Real numbers and imaginary numbers are two sorts of numbers, none being more real or more imaginary than the others.
Use of neologisms. For example polynomial, homomorphism.
Use of symbols as words or phrases. For example, 
  
    
      
        A
        =
        B
      
    
    {\displaystyle A=B}
   and 
  
    
      
        ∀
        x
      
    
    {\displaystyle \forall x}
   are respectively read as ""
  
    
      
        A
      
    
    {\displaystyle A}
   equals 
  
    
      
        B
      
    
    {\displaystyle B}
  "" and ""for all 
  
    
      
        x
      
    
    {\displaystyle x}
  "".
Use of formulas as part of sentences."
"What does the word ""or"" mean in mathematical language?",One and the other,One or the other,Both,Neither,None of the above,C,"The language of mathematics or mathematical language is an extension of the natural language (for example English) that is used in mathematics and in science for expressing results (scientific laws, theorems, proofs, logical deductions, etc) with concision, precision and unambiguity.

Features
The main features of the mathematical language are the following.

Use of common words with a derived meaning, generally more specific and more precise. For example, ""or"" means ""one, the other or both"", while, in common language, ""both"" is sometimes included and sometimes not. Also, a ""line"" is straight and has zero width.
Use of common words with a meaning that is completely different from their common meaning. For example, a mathematical ring is not related to any other meaning of ""ring"". Real numbers and imaginary numbers are two sorts of numbers, none being more real or more imaginary than the others.
Use of neologisms. For example polynomial, homomorphism.
Use of symbols as words or phrases. For example, 
  
    
      
        A
        =
        B
      
    
    {\displaystyle A=B}
   and 
  
    
      
        ∀
        x
      
    
    {\displaystyle \forall x}
   are respectively read as ""
  
    
      
        A
      
    
    {\displaystyle A}
   equals 
  
    
      
        B
      
    
    {\displaystyle B}
  "" and ""for all 
  
    
      
        x
      
    
    {\displaystyle x}
  "".
Use of formulas as part of sentences."
"What does the word ""line"" mean in mathematical language?",A straight and narrow path,A straight object with no width,A curved object,A connection between two points,None of the above,B,"The language of mathematics or mathematical language is an extension of the natural language (for example English) that is used in mathematics and in science for expressing results (scientific laws, theorems, proofs, logical deductions, etc) with concision, precision and unambiguity.

Features
The main features of the mathematical language are the following.

Use of common words with a derived meaning, generally more specific and more precise. For example, ""or"" means ""one, the other or both"", while, in common language, ""both"" is sometimes included and sometimes not. Also, a ""line"" is straight and has zero width.
Use of common words with a meaning that is completely different from their common meaning. For example, a mathematical ring is not related to any other meaning of ""ring"". Real numbers and imaginary numbers are two sorts of numbers, none being more real or more imaginary than the others.
Use of neologisms. For example polynomial, homomorphism.
Use of symbols as words or phrases. For example, 
  
    
      
        A
        =
        B
      
    
    {\displaystyle A=B}
   and 
  
    
      
        ∀
        x
      
    
    {\displaystyle \forall x}
   are respectively read as ""
  
    
      
        A
      
    
    {\displaystyle A}
   equals 
  
    
      
        B
      
    
    {\displaystyle B}
  "" and ""for all 
  
    
      
        x
      
    
    {\displaystyle x}
  "".
Use of formulas as part of sentences."
What are real numbers and imaginary numbers in mathematical language?,Two different types of numbers,Numbers that are more real or more imaginary,Numbers that are not related to each other,Numbers that are derived from common words,None of the above,A,"The language of mathematics or mathematical language is an extension of the natural language (for example English) that is used in mathematics and in science for expressing results (scientific laws, theorems, proofs, logical deductions, etc) with concision, precision and unambiguity.

Features
The main features of the mathematical language are the following.

Use of common words with a derived meaning, generally more specific and more precise. For example, ""or"" means ""one, the other or both"", while, in common language, ""both"" is sometimes included and sometimes not. Also, a ""line"" is straight and has zero width.
Use of common words with a meaning that is completely different from their common meaning. For example, a mathematical ring is not related to any other meaning of ""ring"". Real numbers and imaginary numbers are two sorts of numbers, none being more real or more imaginary than the others.
Use of neologisms. For example polynomial, homomorphism.
Use of symbols as words or phrases. For example, 
  
    
      
        A
        =
        B
      
    
    {\displaystyle A=B}
   and 
  
    
      
        ∀
        x
      
    
    {\displaystyle \forall x}
   are respectively read as ""
  
    
      
        A
      
    
    {\displaystyle A}
   equals 
  
    
      
        B
      
    
    {\displaystyle B}
  "" and ""for all 
  
    
      
        x
      
    
    {\displaystyle x}
  "".
Use of formulas as part of sentences."
How are symbols used in mathematical language?,As words or phrases,To confuse people with neologisms,To replace common words,To represent formulas,None of the above,D,"The language of mathematics or mathematical language is an extension of the natural language (for example English) that is used in mathematics and in science for expressing results (scientific laws, theorems, proofs, logical deductions, etc) with concision, precision and unambiguity.

Features
The main features of the mathematical language are the following.

Use of common words with a derived meaning, generally more specific and more precise. For example, ""or"" means ""one, the other or both"", while, in common language, ""both"" is sometimes included and sometimes not. Also, a ""line"" is straight and has zero width.
Use of common words with a meaning that is completely different from their common meaning. For example, a mathematical ring is not related to any other meaning of ""ring"". Real numbers and imaginary numbers are two sorts of numbers, none being more real or more imaginary than the others.
Use of neologisms. For example polynomial, homomorphism.
Use of symbols as words or phrases. For example, 
  
    
      
        A
        =
        B
      
    
    {\displaystyle A=B}
   and 
  
    
      
        ∀
        x
      
    
    {\displaystyle \forall x}
   are respectively read as ""
  
    
      
        A
      
    
    {\displaystyle A}
   equals 
  
    
      
        B
      
    
    {\displaystyle B}
  "" and ""for all 
  
    
      
        x
      
    
    {\displaystyle x}
  "".
Use of formulas as part of sentences."
What is the definition of absolute electrode potential?,The electrode potential of a metal measured with respect to a universal reference system,The electrode potential of a metal measured with respect to a specific metal-solution interface,The electrode potential of a metal measured with respect to a vacuum,The electrode potential of a metal measured with respect to atmospheric pressure,The electrode potential of a metal measured with respect to a perfect vacuum,A,"This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.

A
Absolute electrode potentialIn electrochemistry, according to an IUPAC definition, is the electrode potential of a metal measured with respect to a universal reference system (without any additional metal–solution interface). 
Absolute pressureIs zero-referenced against a perfect vacuum, using an absolute scale, so it is equal to gauge pressure plus atmospheric pressure. 
Absolute zeroThe lower limit of the thermodynamic temperature scale, a state at which the enthalpy and entropy of a cooled ideal gas reach their minimum value, taken as 0. Absolute zero is the point at which the fundamental particles of nature have minimal vibrational motion, retaining only quantum mechanical, zero-point energy-induced particle motion. The theoretical temperature is determined by extrapolating the ideal gas law; by international agreement, absolute zero is taken as −273.15° on the Celsius scale (International System of Units), which equals −459.67° on the Fahrenheit scale (United States customary units or Imperial units). The corresponding Kelvin and Rankine temperature scales set their zero points at absolute zero by definition.
AbsorbanceAbsorbance or decadic absorbance is the common logarithm of the ratio of incident to transmitted radiant power through a material, and spectral absorbance or spectral decadic absorbance is the common logarithm of the ratio of incident to transmitted spectral radiant power through a material."
How is absolute pressure defined?,Pressure measured with respect to a universal reference system,Pressure measured with respect to a specific metal-solution interface,Pressure measured with respect to a vacuum,Pressure measured with respect to atmospheric pressure,Pressure measured with respect to a perfect vacuum,D,"This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.

A
Absolute electrode potentialIn electrochemistry, according to an IUPAC definition, is the electrode potential of a metal measured with respect to a universal reference system (without any additional metal–solution interface). 
Absolute pressureIs zero-referenced against a perfect vacuum, using an absolute scale, so it is equal to gauge pressure plus atmospheric pressure. 
Absolute zeroThe lower limit of the thermodynamic temperature scale, a state at which the enthalpy and entropy of a cooled ideal gas reach their minimum value, taken as 0. Absolute zero is the point at which the fundamental particles of nature have minimal vibrational motion, retaining only quantum mechanical, zero-point energy-induced particle motion. The theoretical temperature is determined by extrapolating the ideal gas law; by international agreement, absolute zero is taken as −273.15° on the Celsius scale (International System of Units), which equals −459.67° on the Fahrenheit scale (United States customary units or Imperial units). The corresponding Kelvin and Rankine temperature scales set their zero points at absolute zero by definition.
AbsorbanceAbsorbance or decadic absorbance is the common logarithm of the ratio of incident to transmitted radiant power through a material, and spectral absorbance or spectral decadic absorbance is the common logarithm of the ratio of incident to transmitted spectral radiant power through a material."
What is absolute zero?,The temperature at which the enthalpy and entropy of a gas reach their minimum value,The temperature at which the particles of nature have minimal vibrational motion,The temperature at which the ideal gas law is extrapolated,The temperature at which the Kelvin and Rankine scales set their zero points,The temperature at which the Celsius and Fahrenheit scales set their zero points,B,"This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.

A
Absolute electrode potentialIn electrochemistry, according to an IUPAC definition, is the electrode potential of a metal measured with respect to a universal reference system (without any additional metal–solution interface). 
Absolute pressureIs zero-referenced against a perfect vacuum, using an absolute scale, so it is equal to gauge pressure plus atmospheric pressure. 
Absolute zeroThe lower limit of the thermodynamic temperature scale, a state at which the enthalpy and entropy of a cooled ideal gas reach their minimum value, taken as 0. Absolute zero is the point at which the fundamental particles of nature have minimal vibrational motion, retaining only quantum mechanical, zero-point energy-induced particle motion. The theoretical temperature is determined by extrapolating the ideal gas law; by international agreement, absolute zero is taken as −273.15° on the Celsius scale (International System of Units), which equals −459.67° on the Fahrenheit scale (United States customary units or Imperial units). The corresponding Kelvin and Rankine temperature scales set their zero points at absolute zero by definition.
AbsorbanceAbsorbance or decadic absorbance is the common logarithm of the ratio of incident to transmitted radiant power through a material, and spectral absorbance or spectral decadic absorbance is the common logarithm of the ratio of incident to transmitted spectral radiant power through a material."
What is absorbance?,The ratio of incident to transmitted radiant power through a material,The ratio of incident to transmitted spectral radiant power through a material,The logarithm of the ratio of incident to transmitted radiant power through a material,The logarithm of the ratio of incident to transmitted spectral radiant power through a material,The logarithm of the ratio of incident to transmitted radiant power through a vacuum,C,"This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.

A
Absolute electrode potentialIn electrochemistry, according to an IUPAC definition, is the electrode potential of a metal measured with respect to a universal reference system (without any additional metal–solution interface). 
Absolute pressureIs zero-referenced against a perfect vacuum, using an absolute scale, so it is equal to gauge pressure plus atmospheric pressure. 
Absolute zeroThe lower limit of the thermodynamic temperature scale, a state at which the enthalpy and entropy of a cooled ideal gas reach their minimum value, taken as 0. Absolute zero is the point at which the fundamental particles of nature have minimal vibrational motion, retaining only quantum mechanical, zero-point energy-induced particle motion. The theoretical temperature is determined by extrapolating the ideal gas law; by international agreement, absolute zero is taken as −273.15° on the Celsius scale (International System of Units), which equals −459.67° on the Fahrenheit scale (United States customary units or Imperial units). The corresponding Kelvin and Rankine temperature scales set their zero points at absolute zero by definition.
AbsorbanceAbsorbance or decadic absorbance is the common logarithm of the ratio of incident to transmitted radiant power through a material, and spectral absorbance or spectral decadic absorbance is the common logarithm of the ratio of incident to transmitted spectral radiant power through a material."
What is spectral absorbance?,The ratio of incident to transmitted radiant power through a material,The ratio of incident to transmitted spectral radiant power through a material,The logarithm of the ratio of incident to transmitted radiant power through a material,The logarithm of the ratio of incident to transmitted spectral radiant power through a material,The logarithm of the ratio of incident to transmitted radiant power through a vacuum,D,"This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.

A
Absolute electrode potentialIn electrochemistry, according to an IUPAC definition, is the electrode potential of a metal measured with respect to a universal reference system (without any additional metal–solution interface). 
Absolute pressureIs zero-referenced against a perfect vacuum, using an absolute scale, so it is equal to gauge pressure plus atmospheric pressure. 
Absolute zeroThe lower limit of the thermodynamic temperature scale, a state at which the enthalpy and entropy of a cooled ideal gas reach their minimum value, taken as 0. Absolute zero is the point at which the fundamental particles of nature have minimal vibrational motion, retaining only quantum mechanical, zero-point energy-induced particle motion. The theoretical temperature is determined by extrapolating the ideal gas law; by international agreement, absolute zero is taken as −273.15° on the Celsius scale (International System of Units), which equals −459.67° on the Fahrenheit scale (United States customary units or Imperial units). The corresponding Kelvin and Rankine temperature scales set their zero points at absolute zero by definition.
AbsorbanceAbsorbance or decadic absorbance is the common logarithm of the ratio of incident to transmitted radiant power through a material, and spectral absorbance or spectral decadic absorbance is the common logarithm of the ratio of incident to transmitted spectral radiant power through a material."
What are the two main branches of natural science?,Chemistry and physics,Biology and chemistry,Physics and biology,Physics and astronomy,Chemistry and astronomy,C,"Natural science is one of the branches of science concerned with the description, understanding and prediction of natural phenomena, based on empirical evidence from observation and experimentation. Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances.
Natural science can be divided into two main branches: life science and physical science. Life science is alternatively known as biology, and physical science is subdivided into branches: physics, chemistry, earth science, and astronomy. These branches of natural science may be further divided into more specialized branches (also known as fields). As empirical sciences, natural sciences use tools from the formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the ""laws of nature"".Modern natural science succeeded more classical approaches to natural philosophy, usually traced to Taoist traditions in Asia and to ancient Greece in Europe. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science."
Which branch of natural science studies the Earth?,Physics,Chemistry,Biology,Earth science,Astronomy,D,"Natural science is one of the branches of science concerned with the description, understanding and prediction of natural phenomena, based on empirical evidence from observation and experimentation. Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances.
Natural science can be divided into two main branches: life science and physical science. Life science is alternatively known as biology, and physical science is subdivided into branches: physics, chemistry, earth science, and astronomy. These branches of natural science may be further divided into more specialized branches (also known as fields). As empirical sciences, natural sciences use tools from the formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the ""laws of nature"".Modern natural science succeeded more classical approaches to natural philosophy, usually traced to Taoist traditions in Asia and to ancient Greece in Europe. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science."
What are the tools used by natural sciences to convert information about nature into measurements?,Mathematics and logic,Observation and experimentation,Peer review and repeatability,Measurement and explanation,Laws of nature and empirical evidence,A,"Natural science is one of the branches of science concerned with the description, understanding and prediction of natural phenomena, based on empirical evidence from observation and experimentation. Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances.
Natural science can be divided into two main branches: life science and physical science. Life science is alternatively known as biology, and physical science is subdivided into branches: physics, chemistry, earth science, and astronomy. These branches of natural science may be further divided into more specialized branches (also known as fields). As empirical sciences, natural sciences use tools from the formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the ""laws of nature"".Modern natural science succeeded more classical approaches to natural philosophy, usually traced to Taoist traditions in Asia and to ancient Greece in Europe. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science."
Who debated about using more mathematical and experimental approaches in natural science?,Galileo and Descartes,Descartes and Bacon,Bacon and Newton,Newton and Galileo,Galileo and Bacon,A,"Natural science is one of the branches of science concerned with the description, understanding and prediction of natural phenomena, based on empirical evidence from observation and experimentation. Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances.
Natural science can be divided into two main branches: life science and physical science. Life science is alternatively known as biology, and physical science is subdivided into branches: physics, chemistry, earth science, and astronomy. These branches of natural science may be further divided into more specialized branches (also known as fields). As empirical sciences, natural sciences use tools from the formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the ""laws of nature"".Modern natural science succeeded more classical approaches to natural philosophy, usually traced to Taoist traditions in Asia and to ancient Greece in Europe. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science."
What are the branches of physical science?,"Physics, chemistry, and biology","Chemistry, earth science, and astronomy","Physics, chemistry, earth science, and astronomy","Physics, earth science, and astronomy","Physics, chemistry, and astronomy",E,"Natural science is one of the branches of science concerned with the description, understanding and prediction of natural phenomena, based on empirical evidence from observation and experimentation. Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances.
Natural science can be divided into two main branches: life science and physical science. Life science is alternatively known as biology, and physical science is subdivided into branches: physics, chemistry, earth science, and astronomy. These branches of natural science may be further divided into more specialized branches (also known as fields). As empirical sciences, natural sciences use tools from the formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the ""laws of nature"".Modern natural science succeeded more classical approaches to natural philosophy, usually traced to Taoist traditions in Asia and to ancient Greece in Europe. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science."
What are rain gardens designed for?,To increase the flow rate of runoff,To decrease the pollutant load of runoff,To absorb heat in urban areas,To release water vapor into the atmosphere,To treat polluted stormwater runoff,E,"Rain gardens, also called bioretention facilities, are one of a variety of practices designed to increase rain runoff reabsorption by the soil. They can also be used to treat polluted stormwater runoff. Rain gardens are designed landscape sites that reduce the flow rate, total quantity, and pollutant load of runoff from impervious urban areas like roofs, driveways, walkways, parking lots, and compacted lawn areas. Rain gardens rely on plants and natural or engineered soil medium to retain  stormwater and increase the lag time of infiltration, while remediating and filtering pollutants carried by urban runoff. Rain gardens provide a method to reuse and optimize any rain that falls, reducing or avoiding the need for additional irrigation. A benefit of planting rain gardens is the consequential decrease in ambient air and water temperature, a mitigation that is especially effective in urban areas containing an abundance of impervious surfaces that absorb heat in a phenomenon known as the heat-island effect.Rain garden plantings commonly include wetland edge vegetation, such as wildflowers, sedges, rushes, ferns, shrubs and small trees. These plants take up nutrients and water that flow into the rain garden, and they release water vapor back to the atmosphere through the process of transpiration."
What is one benefit of planting rain gardens?,Increase in ambient air and water temperature,Decrease in the flow rate of runoff,Decrease in the pollutant load of runoff,Increase in the need for additional irrigation,Decrease in the heat-island effect,C,"Rain gardens, also called bioretention facilities, are one of a variety of practices designed to increase rain runoff reabsorption by the soil. They can also be used to treat polluted stormwater runoff. Rain gardens are designed landscape sites that reduce the flow rate, total quantity, and pollutant load of runoff from impervious urban areas like roofs, driveways, walkways, parking lots, and compacted lawn areas. Rain gardens rely on plants and natural or engineered soil medium to retain  stormwater and increase the lag time of infiltration, while remediating and filtering pollutants carried by urban runoff. Rain gardens provide a method to reuse and optimize any rain that falls, reducing or avoiding the need for additional irrigation. A benefit of planting rain gardens is the consequential decrease in ambient air and water temperature, a mitigation that is especially effective in urban areas containing an abundance of impervious surfaces that absorb heat in a phenomenon known as the heat-island effect.Rain garden plantings commonly include wetland edge vegetation, such as wildflowers, sedges, rushes, ferns, shrubs and small trees. These plants take up nutrients and water that flow into the rain garden, and they release water vapor back to the atmosphere through the process of transpiration."
What types of vegetation are commonly found in rain gardens?,Conifers and evergreen trees,Grass and moss,Cacti and succulents,Wildflowers and ferns,Palm trees and shrubs,D,"Rain gardens, also called bioretention facilities, are one of a variety of practices designed to increase rain runoff reabsorption by the soil. They can also be used to treat polluted stormwater runoff. Rain gardens are designed landscape sites that reduce the flow rate, total quantity, and pollutant load of runoff from impervious urban areas like roofs, driveways, walkways, parking lots, and compacted lawn areas. Rain gardens rely on plants and natural or engineered soil medium to retain  stormwater and increase the lag time of infiltration, while remediating and filtering pollutants carried by urban runoff. Rain gardens provide a method to reuse and optimize any rain that falls, reducing or avoiding the need for additional irrigation. A benefit of planting rain gardens is the consequential decrease in ambient air and water temperature, a mitigation that is especially effective in urban areas containing an abundance of impervious surfaces that absorb heat in a phenomenon known as the heat-island effect.Rain garden plantings commonly include wetland edge vegetation, such as wildflowers, sedges, rushes, ferns, shrubs and small trees. These plants take up nutrients and water that flow into the rain garden, and they release water vapor back to the atmosphere through the process of transpiration."
How do rain gardens help to reduce the need for irrigation?,By increasing the pollutant load of runoff,By decreasing the flow rate of runoff,By absorbing heat in urban areas,By retaining stormwater and increasing infiltration,By releasing water vapor into the atmosphere,D,"Rain gardens, also called bioretention facilities, are one of a variety of practices designed to increase rain runoff reabsorption by the soil. They can also be used to treat polluted stormwater runoff. Rain gardens are designed landscape sites that reduce the flow rate, total quantity, and pollutant load of runoff from impervious urban areas like roofs, driveways, walkways, parking lots, and compacted lawn areas. Rain gardens rely on plants and natural or engineered soil medium to retain  stormwater and increase the lag time of infiltration, while remediating and filtering pollutants carried by urban runoff. Rain gardens provide a method to reuse and optimize any rain that falls, reducing or avoiding the need for additional irrigation. A benefit of planting rain gardens is the consequential decrease in ambient air and water temperature, a mitigation that is especially effective in urban areas containing an abundance of impervious surfaces that absorb heat in a phenomenon known as the heat-island effect.Rain garden plantings commonly include wetland edge vegetation, such as wildflowers, sedges, rushes, ferns, shrubs and small trees. These plants take up nutrients and water that flow into the rain garden, and they release water vapor back to the atmosphere through the process of transpiration."
What is the purpose of the wetland edge vegetation in rain gardens?,To increase the flow rate of runoff,To decrease the pollutant load of runoff,To absorb heat in urban areas,To release water vapor into the atmosphere,To take up nutrients and water and release water vapor,E,"Rain gardens, also called bioretention facilities, are one of a variety of practices designed to increase rain runoff reabsorption by the soil. They can also be used to treat polluted stormwater runoff. Rain gardens are designed landscape sites that reduce the flow rate, total quantity, and pollutant load of runoff from impervious urban areas like roofs, driveways, walkways, parking lots, and compacted lawn areas. Rain gardens rely on plants and natural or engineered soil medium to retain  stormwater and increase the lag time of infiltration, while remediating and filtering pollutants carried by urban runoff. Rain gardens provide a method to reuse and optimize any rain that falls, reducing or avoiding the need for additional irrigation. A benefit of planting rain gardens is the consequential decrease in ambient air and water temperature, a mitigation that is especially effective in urban areas containing an abundance of impervious surfaces that absorb heat in a phenomenon known as the heat-island effect.Rain garden plantings commonly include wetland edge vegetation, such as wildflowers, sedges, rushes, ferns, shrubs and small trees. These plants take up nutrients and water that flow into the rain garden, and they release water vapor back to the atmosphere through the process of transpiration."
What is the formula for calculating homologous temperature?,T_H = T(K) / Tmp(K),T_H = T(K) - Tmp(K),T_H = T(K) * Tmp(K),T_H = T(K) + Tmp(K),T_H = T(K) % Tmp(K),A,"Homologous temperature expresses the thermodynamic temperature of a material as a fraction of the thermodynamic temperature of its melting point (i.e. using the Kelvin scale):

  
    
      
        
          T
          
            H
          
        
        =
        
          
            
              T
              (
              
                K
              
              )
            
            
              
                T
                
                  m
                  p
                
              
              (
              
                K
              
              )
            
          
        
      
    
    {\displaystyle T_{H}={\frac {T({\text{K}})}{T_{mp}({\text{K}})}}}
  
For example, the homologous temperature of lead at room temperature (25 °C) is approximately 0.50 (TH = T/Tmp = 298 K/601 K = 0.50).

Significance of the homologous temperature
The homologous temperature of a substance is useful for determining the rate of steady state creep (diffusion dependent deformation). A higher homologous temperature results in an exponentially higher rate of diffusion dependent deformation.Additionally, for a given fixed homologous temperature, two materials with different melting points would have similar diffusion-dependent deformation behaviour. For example, solder (Tmp = 456 K) at 115 °C would have comparable mechanical properties to copper (Tmp = 1358 K) at 881 °C, because they would both be at 0.85Tmp despite being at different absolute temperatures.
In electronics applications, where circuits typically operate over a −55 °C to +125 °C range, eutectic tin-lead (Sn63) solder is working at 0.48Tmp to 0.87Tmp. The upper temperature is high relative to the melting point; from this we can deduce that solder will have limited mechanical strength (as a bulk material) and significant creep under stress. This is borne out by its comparatively low values for tensile strength, shear strength and modulus of elasticity. Copper, on the other hand, has a much higher melting point, so foils are working at only 0.16Tmp to 0.29Tmp and their properties are little affected by temperature.


== References ==."
What is the significance of homologous temperature?,It determines the melting point of a material,It determines the rate of diffusion dependent deformation,It determines the boiling point of a material,It determines the density of a material,It determines the specific heat capacity of a material,B,"Homologous temperature expresses the thermodynamic temperature of a material as a fraction of the thermodynamic temperature of its melting point (i.e. using the Kelvin scale):

  
    
      
        
          T
          
            H
          
        
        =
        
          
            
              T
              (
              
                K
              
              )
            
            
              
                T
                
                  m
                  p
                
              
              (
              
                K
              
              )
            
          
        
      
    
    {\displaystyle T_{H}={\frac {T({\text{K}})}{T_{mp}({\text{K}})}}}
  
For example, the homologous temperature of lead at room temperature (25 °C) is approximately 0.50 (TH = T/Tmp = 298 K/601 K = 0.50).

Significance of the homologous temperature
The homologous temperature of a substance is useful for determining the rate of steady state creep (diffusion dependent deformation). A higher homologous temperature results in an exponentially higher rate of diffusion dependent deformation.Additionally, for a given fixed homologous temperature, two materials with different melting points would have similar diffusion-dependent deformation behaviour. For example, solder (Tmp = 456 K) at 115 °C would have comparable mechanical properties to copper (Tmp = 1358 K) at 881 °C, because they would both be at 0.85Tmp despite being at different absolute temperatures.
In electronics applications, where circuits typically operate over a −55 °C to +125 °C range, eutectic tin-lead (Sn63) solder is working at 0.48Tmp to 0.87Tmp. The upper temperature is high relative to the melting point; from this we can deduce that solder will have limited mechanical strength (as a bulk material) and significant creep under stress. This is borne out by its comparatively low values for tensile strength, shear strength and modulus of elasticity. Copper, on the other hand, has a much higher melting point, so foils are working at only 0.16Tmp to 0.29Tmp and their properties are little affected by temperature.


== References ==."
"At room temperature, what is the homologous temperature of lead?",0.50,0.25,0.75,1.00,0.10,A,"Homologous temperature expresses the thermodynamic temperature of a material as a fraction of the thermodynamic temperature of its melting point (i.e. using the Kelvin scale):

  
    
      
        
          T
          
            H
          
        
        =
        
          
            
              T
              (
              
                K
              
              )
            
            
              
                T
                
                  m
                  p
                
              
              (
              
                K
              
              )
            
          
        
      
    
    {\displaystyle T_{H}={\frac {T({\text{K}})}{T_{mp}({\text{K}})}}}
  
For example, the homologous temperature of lead at room temperature (25 °C) is approximately 0.50 (TH = T/Tmp = 298 K/601 K = 0.50).

Significance of the homologous temperature
The homologous temperature of a substance is useful for determining the rate of steady state creep (diffusion dependent deformation). A higher homologous temperature results in an exponentially higher rate of diffusion dependent deformation.Additionally, for a given fixed homologous temperature, two materials with different melting points would have similar diffusion-dependent deformation behaviour. For example, solder (Tmp = 456 K) at 115 °C would have comparable mechanical properties to copper (Tmp = 1358 K) at 881 °C, because they would both be at 0.85Tmp despite being at different absolute temperatures.
In electronics applications, where circuits typically operate over a −55 °C to +125 °C range, eutectic tin-lead (Sn63) solder is working at 0.48Tmp to 0.87Tmp. The upper temperature is high relative to the melting point; from this we can deduce that solder will have limited mechanical strength (as a bulk material) and significant creep under stress. This is borne out by its comparatively low values for tensile strength, shear strength and modulus of elasticity. Copper, on the other hand, has a much higher melting point, so foils are working at only 0.16Tmp to 0.29Tmp and their properties are little affected by temperature.


== References ==."
What would be the homologous temperature for solder at 115 °C?,0.85Tmp,0.48Tmp,0.87Tmp,0.16Tmp,0.29Tmp,C,"Homologous temperature expresses the thermodynamic temperature of a material as a fraction of the thermodynamic temperature of its melting point (i.e. using the Kelvin scale):

  
    
      
        
          T
          
            H
          
        
        =
        
          
            
              T
              (
              
                K
              
              )
            
            
              
                T
                
                  m
                  p
                
              
              (
              
                K
              
              )
            
          
        
      
    
    {\displaystyle T_{H}={\frac {T({\text{K}})}{T_{mp}({\text{K}})}}}
  
For example, the homologous temperature of lead at room temperature (25 °C) is approximately 0.50 (TH = T/Tmp = 298 K/601 K = 0.50).

Significance of the homologous temperature
The homologous temperature of a substance is useful for determining the rate of steady state creep (diffusion dependent deformation). A higher homologous temperature results in an exponentially higher rate of diffusion dependent deformation.Additionally, for a given fixed homologous temperature, two materials with different melting points would have similar diffusion-dependent deformation behaviour. For example, solder (Tmp = 456 K) at 115 °C would have comparable mechanical properties to copper (Tmp = 1358 K) at 881 °C, because they would both be at 0.85Tmp despite being at different absolute temperatures.
In electronics applications, where circuits typically operate over a −55 °C to +125 °C range, eutectic tin-lead (Sn63) solder is working at 0.48Tmp to 0.87Tmp. The upper temperature is high relative to the melting point; from this we can deduce that solder will have limited mechanical strength (as a bulk material) and significant creep under stress. This is borne out by its comparatively low values for tensile strength, shear strength and modulus of elasticity. Copper, on the other hand, has a much higher melting point, so foils are working at only 0.16Tmp to 0.29Tmp and their properties are little affected by temperature.


== References ==."
What is the property of solder that is affected by its high temperature relative to its melting point?,Tensile strength,Shear strength,Modulus of elasticity,Density,Specific heat capacity,C,"Homologous temperature expresses the thermodynamic temperature of a material as a fraction of the thermodynamic temperature of its melting point (i.e. using the Kelvin scale):

  
    
      
        
          T
          
            H
          
        
        =
        
          
            
              T
              (
              
                K
              
              )
            
            
              
                T
                
                  m
                  p
                
              
              (
              
                K
              
              )
            
          
        
      
    
    {\displaystyle T_{H}={\frac {T({\text{K}})}{T_{mp}({\text{K}})}}}
  
For example, the homologous temperature of lead at room temperature (25 °C) is approximately 0.50 (TH = T/Tmp = 298 K/601 K = 0.50).

Significance of the homologous temperature
The homologous temperature of a substance is useful for determining the rate of steady state creep (diffusion dependent deformation). A higher homologous temperature results in an exponentially higher rate of diffusion dependent deformation.Additionally, for a given fixed homologous temperature, two materials with different melting points would have similar diffusion-dependent deformation behaviour. For example, solder (Tmp = 456 K) at 115 °C would have comparable mechanical properties to copper (Tmp = 1358 K) at 881 °C, because they would both be at 0.85Tmp despite being at different absolute temperatures.
In electronics applications, where circuits typically operate over a −55 °C to +125 °C range, eutectic tin-lead (Sn63) solder is working at 0.48Tmp to 0.87Tmp. The upper temperature is high relative to the melting point; from this we can deduce that solder will have limited mechanical strength (as a bulk material) and significant creep under stress. This is borne out by its comparatively low values for tensile strength, shear strength and modulus of elasticity. Copper, on the other hand, has a much higher melting point, so foils are working at only 0.16Tmp to 0.29Tmp and their properties are little affected by temperature.


== References ==."
What are the Clebsch-Gordan coefficients used for in representation theory?,To calculate the eigenvalues of total angular momentum,To perform direct sum decomposition of irreducible representations,To solve the Schrödinger equation for particle systems,To calculate the expectation values of angular momentum operators,To calculate the eigenvalues of the Hamiltonian operator,B,"In physics, the Clebsch–Gordan (CG) coefficients are numbers that arise in angular momentum coupling in quantum mechanics.  They appear as the expansion coefficients of total angular momentum eigenstates in an uncoupled tensor product basis. In more mathematical terms, the CG coefficients are used in representation theory, particularly of compact Lie groups, to perform the explicit direct sum decomposition of the tensor product of two irreducible representations (i.e., a reducible representation into irreducible representations, in cases where the numbers and types of irreducible components are already known abstractly). The name derives from the German mathematicians Alfred Clebsch and Paul Gordan, who encountered an equivalent problem in invariant theory.
From a vector calculus perspective, the CG coefficients associated with the SO(3) group can be defined simply in terms of integrals of products of spherical harmonics and their complex conjugates. The addition of spins in quantum-mechanical terms can be read directly from this approach as spherical harmonics are eigenfunctions of total angular momentum and projection thereof onto an axis, and the integrals correspond to the Hilbert space inner product. From the formal definition of angular momentum, recursion relations for the Clebsch–Gordan coefficients can be found. There also exist complicated explicit formulas for their direct calculation.The formulas below use Dirac's bra–ket notation and the Condon–Shortley phase convention is adopted.

Review of the angular momentum operators
Angular momentum operators are self-adjoint operators jx, jy, and jz that satisfy the commutation relations

where εklm is the Levi-Civita symbol."
How can the CG coefficients associated with the SO(3) group be defined?,In terms of integrals of products of spherical harmonics,In terms of eigenvalues of the Hamiltonian operator,In terms of the expectation values of angular momentum operators,In terms of the eigenvalues of total angular momentum,In terms of integrals of products of spherical Bessel functions,A,"In physics, the Clebsch–Gordan (CG) coefficients are numbers that arise in angular momentum coupling in quantum mechanics.  They appear as the expansion coefficients of total angular momentum eigenstates in an uncoupled tensor product basis. In more mathematical terms, the CG coefficients are used in representation theory, particularly of compact Lie groups, to perform the explicit direct sum decomposition of the tensor product of two irreducible representations (i.e., a reducible representation into irreducible representations, in cases where the numbers and types of irreducible components are already known abstractly). The name derives from the German mathematicians Alfred Clebsch and Paul Gordan, who encountered an equivalent problem in invariant theory.
From a vector calculus perspective, the CG coefficients associated with the SO(3) group can be defined simply in terms of integrals of products of spherical harmonics and their complex conjugates. The addition of spins in quantum-mechanical terms can be read directly from this approach as spherical harmonics are eigenfunctions of total angular momentum and projection thereof onto an axis, and the integrals correspond to the Hilbert space inner product. From the formal definition of angular momentum, recursion relations for the Clebsch–Gordan coefficients can be found. There also exist complicated explicit formulas for their direct calculation.The formulas below use Dirac's bra–ket notation and the Condon–Shortley phase convention is adopted.

Review of the angular momentum operators
Angular momentum operators are self-adjoint operators jx, jy, and jz that satisfy the commutation relations

where εklm is the Levi-Civita symbol."
What do the CG coefficients relate to in vector calculus?,The addition of spins in quantum-mechanical terms,The expectation values of angular momentum operators,The eigenvalues of the Hamiltonian operator,The eigenvalues of total angular momentum,The integrals of products of spherical harmonics,A,"In physics, the Clebsch–Gordan (CG) coefficients are numbers that arise in angular momentum coupling in quantum mechanics.  They appear as the expansion coefficients of total angular momentum eigenstates in an uncoupled tensor product basis. In more mathematical terms, the CG coefficients are used in representation theory, particularly of compact Lie groups, to perform the explicit direct sum decomposition of the tensor product of two irreducible representations (i.e., a reducible representation into irreducible representations, in cases where the numbers and types of irreducible components are already known abstractly). The name derives from the German mathematicians Alfred Clebsch and Paul Gordan, who encountered an equivalent problem in invariant theory.
From a vector calculus perspective, the CG coefficients associated with the SO(3) group can be defined simply in terms of integrals of products of spherical harmonics and their complex conjugates. The addition of spins in quantum-mechanical terms can be read directly from this approach as spherical harmonics are eigenfunctions of total angular momentum and projection thereof onto an axis, and the integrals correspond to the Hilbert space inner product. From the formal definition of angular momentum, recursion relations for the Clebsch–Gordan coefficients can be found. There also exist complicated explicit formulas for their direct calculation.The formulas below use Dirac's bra–ket notation and the Condon–Shortley phase convention is adopted.

Review of the angular momentum operators
Angular momentum operators are self-adjoint operators jx, jy, and jz that satisfy the commutation relations

where εklm is the Levi-Civita symbol."
How are the Clebsch-Gordan coefficients defined in terms of angular momentum operators?,"Through the commutation relations of jx, jy, and jz operators",Through the eigenvalues of the Hamiltonian operator,Through the expectation values of angular momentum operators,Through the eigenvalues of total angular momentum,Through the integrals of products of spherical harmonics,A,"In physics, the Clebsch–Gordan (CG) coefficients are numbers that arise in angular momentum coupling in quantum mechanics.  They appear as the expansion coefficients of total angular momentum eigenstates in an uncoupled tensor product basis. In more mathematical terms, the CG coefficients are used in representation theory, particularly of compact Lie groups, to perform the explicit direct sum decomposition of the tensor product of two irreducible representations (i.e., a reducible representation into irreducible representations, in cases where the numbers and types of irreducible components are already known abstractly). The name derives from the German mathematicians Alfred Clebsch and Paul Gordan, who encountered an equivalent problem in invariant theory.
From a vector calculus perspective, the CG coefficients associated with the SO(3) group can be defined simply in terms of integrals of products of spherical harmonics and their complex conjugates. The addition of spins in quantum-mechanical terms can be read directly from this approach as spherical harmonics are eigenfunctions of total angular momentum and projection thereof onto an axis, and the integrals correspond to the Hilbert space inner product. From the formal definition of angular momentum, recursion relations for the Clebsch–Gordan coefficients can be found. There also exist complicated explicit formulas for their direct calculation.The formulas below use Dirac's bra–ket notation and the Condon–Shortley phase convention is adopted.

Review of the angular momentum operators
Angular momentum operators are self-adjoint operators jx, jy, and jz that satisfy the commutation relations

where εklm is the Levi-Civita symbol."
Which mathematicians are credited with encountering an equivalent problem in invariant theory?,Alfred Clebsch and Paul Gordan,Isaac Newton and Albert Einstein,Niels Bohr and Werner Heisenberg,Erwin Schrödinger and Max Planck,Richard Feynman and Murray Gell-Mann,A,"In physics, the Clebsch–Gordan (CG) coefficients are numbers that arise in angular momentum coupling in quantum mechanics.  They appear as the expansion coefficients of total angular momentum eigenstates in an uncoupled tensor product basis. In more mathematical terms, the CG coefficients are used in representation theory, particularly of compact Lie groups, to perform the explicit direct sum decomposition of the tensor product of two irreducible representations (i.e., a reducible representation into irreducible representations, in cases where the numbers and types of irreducible components are already known abstractly). The name derives from the German mathematicians Alfred Clebsch and Paul Gordan, who encountered an equivalent problem in invariant theory.
From a vector calculus perspective, the CG coefficients associated with the SO(3) group can be defined simply in terms of integrals of products of spherical harmonics and their complex conjugates. The addition of spins in quantum-mechanical terms can be read directly from this approach as spherical harmonics are eigenfunctions of total angular momentum and projection thereof onto an axis, and the integrals correspond to the Hilbert space inner product. From the formal definition of angular momentum, recursion relations for the Clebsch–Gordan coefficients can be found. There also exist complicated explicit formulas for their direct calculation.The formulas below use Dirac's bra–ket notation and the Condon–Shortley phase convention is adopted.

Review of the angular momentum operators
Angular momentum operators are self-adjoint operators jx, jy, and jz that satisfy the commutation relations

where εklm is the Levi-Civita symbol."
What is the main responsibility of structural engineers?,Creating the form and shape of human-made structures,Calculating the stability of built structures,Supervising the construction of projects,Designing machinery and vehicles,Ensuring structural integrity affects functioning and safety,A,"Structural engineering is a sub-discipline of civil engineering in which structural engineers are trained to design the 'bones and muscles' that create the form and shape of human-made structures. Structural engineers also must understand and calculate the stability, strength, rigidity and earthquake-susceptibility of built structures for buildings and nonbuilding structures. The structural designs are integrated with those of other designers such as architects and building services engineer and often supervise the construction of projects by contractors on site. They can also be involved in the design of machinery, medical equipment, and vehicles where structural integrity affects functioning and safety.  See glossary of structural engineering.
Structural engineering theory is based upon applied physical laws and empirical knowledge of the structural performance of different materials and geometries. Structural engineering design uses a number of relatively simple structural concepts to build complex structural systems. Structural engineers are responsible for making creative and efficient use of funds, structural elements and materials to achieve these goals.

History
Structural engineering dates back to 2700 B.C.E."
Who do structural engineers collaborate with in the design process?,Architects,Building services engineer,Contractors,All of the above,None of the above,D,"Structural engineering is a sub-discipline of civil engineering in which structural engineers are trained to design the 'bones and muscles' that create the form and shape of human-made structures. Structural engineers also must understand and calculate the stability, strength, rigidity and earthquake-susceptibility of built structures for buildings and nonbuilding structures. The structural designs are integrated with those of other designers such as architects and building services engineer and often supervise the construction of projects by contractors on site. They can also be involved in the design of machinery, medical equipment, and vehicles where structural integrity affects functioning and safety.  See glossary of structural engineering.
Structural engineering theory is based upon applied physical laws and empirical knowledge of the structural performance of different materials and geometries. Structural engineering design uses a number of relatively simple structural concepts to build complex structural systems. Structural engineers are responsible for making creative and efficient use of funds, structural elements and materials to achieve these goals.

History
Structural engineering dates back to 2700 B.C.E."
What is the basis of structural engineering theory?,Applied physical laws,Empirical knowledge of materials,Geometries of structures,All of the above,None of the above,D,"Structural engineering is a sub-discipline of civil engineering in which structural engineers are trained to design the 'bones and muscles' that create the form and shape of human-made structures. Structural engineers also must understand and calculate the stability, strength, rigidity and earthquake-susceptibility of built structures for buildings and nonbuilding structures. The structural designs are integrated with those of other designers such as architects and building services engineer and often supervise the construction of projects by contractors on site. They can also be involved in the design of machinery, medical equipment, and vehicles where structural integrity affects functioning and safety.  See glossary of structural engineering.
Structural engineering theory is based upon applied physical laws and empirical knowledge of the structural performance of different materials and geometries. Structural engineering design uses a number of relatively simple structural concepts to build complex structural systems. Structural engineers are responsible for making creative and efficient use of funds, structural elements and materials to achieve these goals.

History
Structural engineering dates back to 2700 B.C.E."
What do structural engineers use to build complex structural systems?,Simple structural concepts,Funds,Structural elements,Materials,All of the above,E,"Structural engineering is a sub-discipline of civil engineering in which structural engineers are trained to design the 'bones and muscles' that create the form and shape of human-made structures. Structural engineers also must understand and calculate the stability, strength, rigidity and earthquake-susceptibility of built structures for buildings and nonbuilding structures. The structural designs are integrated with those of other designers such as architects and building services engineer and often supervise the construction of projects by contractors on site. They can also be involved in the design of machinery, medical equipment, and vehicles where structural integrity affects functioning and safety.  See glossary of structural engineering.
Structural engineering theory is based upon applied physical laws and empirical knowledge of the structural performance of different materials and geometries. Structural engineering design uses a number of relatively simple structural concepts to build complex structural systems. Structural engineers are responsible for making creative and efficient use of funds, structural elements and materials to achieve these goals.

History
Structural engineering dates back to 2700 B.C.E."
When did structural engineering originate?,500 B.C.E.,1000 B.C.E.,2700 B.C.E.,4000 B.C.E.,6000 B.C.E.,C,"Structural engineering is a sub-discipline of civil engineering in which structural engineers are trained to design the 'bones and muscles' that create the form and shape of human-made structures. Structural engineers also must understand and calculate the stability, strength, rigidity and earthquake-susceptibility of built structures for buildings and nonbuilding structures. The structural designs are integrated with those of other designers such as architects and building services engineer and often supervise the construction of projects by contractors on site. They can also be involved in the design of machinery, medical equipment, and vehicles where structural integrity affects functioning and safety.  See glossary of structural engineering.
Structural engineering theory is based upon applied physical laws and empirical knowledge of the structural performance of different materials and geometries. Structural engineering design uses a number of relatively simple structural concepts to build complex structural systems. Structural engineers are responsible for making creative and efficient use of funds, structural elements and materials to achieve these goals.

History
Structural engineering dates back to 2700 B.C.E."
Where did Kelsey Hightower grow up?,"San Francisco, California","Long Beach, California","Atlanta, Georgia","New York City, New York","Chicago, Illinois",B,"Kelsey Hightower (born February 27, 1981) is an American software engineer, developer advocate, and speaker known for his work with Kubernetes, open-source software, and cloud computing.

Early life and education
Hightower grew up in Long Beach, California, then moved to Atlanta, Georgia with his mother as he was beginning high school. After high school he enrolled at Clayton State University, but found the technology courses to be lacking and didn't continue. He then began courses to earn his CompTIA A+ information technology (IT) certification.

Career
When Hightower was 19 years old, after earning his CompTIA A+ certification, he got a job with BellSouth installing DSL service. He continued to work with BellSouth for several years, then began his own IT consultancy. He hired several others, and ultimately opened a store in Jonesboro, Georgia. Also in his early career, Hightower briefly worked as a technician for Google, then at Total Systems (now called TSYS).Hightower began to give talks at Python meetups in Atlanta, where he was noticed by James Turnbull for how he and his colleagues were using Puppet, Python, and their own code to manage and automate deploys. In 2013, Puppet, Inc."
What certification did Kelsey Hightower earn?,CompTIA A+,Microsoft Certified Professional,Cisco Certified Network Associate (CCNA),Oracle Certified Professional (OCP),Amazon Web Services (AWS) Certified Solutions Architect,A,"Kelsey Hightower (born February 27, 1981) is an American software engineer, developer advocate, and speaker known for his work with Kubernetes, open-source software, and cloud computing.

Early life and education
Hightower grew up in Long Beach, California, then moved to Atlanta, Georgia with his mother as he was beginning high school. After high school he enrolled at Clayton State University, but found the technology courses to be lacking and didn't continue. He then began courses to earn his CompTIA A+ information technology (IT) certification.

Career
When Hightower was 19 years old, after earning his CompTIA A+ certification, he got a job with BellSouth installing DSL service. He continued to work with BellSouth for several years, then began his own IT consultancy. He hired several others, and ultimately opened a store in Jonesboro, Georgia. Also in his early career, Hightower briefly worked as a technician for Google, then at Total Systems (now called TSYS).Hightower began to give talks at Python meetups in Atlanta, where he was noticed by James Turnbull for how he and his colleagues were using Puppet, Python, and their own code to manage and automate deploys. In 2013, Puppet, Inc."
Where did Kelsey Hightower work as a technician?,Google,Microsoft,Apple,Facebook,Amazon,A,"Kelsey Hightower (born February 27, 1981) is an American software engineer, developer advocate, and speaker known for his work with Kubernetes, open-source software, and cloud computing.

Early life and education
Hightower grew up in Long Beach, California, then moved to Atlanta, Georgia with his mother as he was beginning high school. After high school he enrolled at Clayton State University, but found the technology courses to be lacking and didn't continue. He then began courses to earn his CompTIA A+ information technology (IT) certification.

Career
When Hightower was 19 years old, after earning his CompTIA A+ certification, he got a job with BellSouth installing DSL service. He continued to work with BellSouth for several years, then began his own IT consultancy. He hired several others, and ultimately opened a store in Jonesboro, Georgia. Also in his early career, Hightower briefly worked as a technician for Google, then at Total Systems (now called TSYS).Hightower began to give talks at Python meetups in Atlanta, where he was noticed by James Turnbull for how he and his colleagues were using Puppet, Python, and their own code to manage and automate deploys. In 2013, Puppet, Inc."
"Who noticed Kelsey Hightower for his use of Puppet, Python, and their own code?",James Turner,Mark Zuckerberg,Larry Page,Satya Nadella,Tim Cook,A,"Kelsey Hightower (born February 27, 1981) is an American software engineer, developer advocate, and speaker known for his work with Kubernetes, open-source software, and cloud computing.

Early life and education
Hightower grew up in Long Beach, California, then moved to Atlanta, Georgia with his mother as he was beginning high school. After high school he enrolled at Clayton State University, but found the technology courses to be lacking and didn't continue. He then began courses to earn his CompTIA A+ information technology (IT) certification.

Career
When Hightower was 19 years old, after earning his CompTIA A+ certification, he got a job with BellSouth installing DSL service. He continued to work with BellSouth for several years, then began his own IT consultancy. He hired several others, and ultimately opened a store in Jonesboro, Georgia. Also in his early career, Hightower briefly worked as a technician for Google, then at Total Systems (now called TSYS).Hightower began to give talks at Python meetups in Atlanta, where he was noticed by James Turnbull for how he and his colleagues were using Puppet, Python, and their own code to manage and automate deploys. In 2013, Puppet, Inc."
"Which company did Puppet, Inc. acquire in 2013?",Red Hat,Cloudera,VMware,Ansible,Puppet Labs,E,"Kelsey Hightower (born February 27, 1981) is an American software engineer, developer advocate, and speaker known for his work with Kubernetes, open-source software, and cloud computing.

Early life and education
Hightower grew up in Long Beach, California, then moved to Atlanta, Georgia with his mother as he was beginning high school. After high school he enrolled at Clayton State University, but found the technology courses to be lacking and didn't continue. He then began courses to earn his CompTIA A+ information technology (IT) certification.

Career
When Hightower was 19 years old, after earning his CompTIA A+ certification, he got a job with BellSouth installing DSL service. He continued to work with BellSouth for several years, then began his own IT consultancy. He hired several others, and ultimately opened a store in Jonesboro, Georgia. Also in his early career, Hightower briefly worked as a technician for Google, then at Total Systems (now called TSYS).Hightower began to give talks at Python meetups in Atlanta, where he was noticed by James Turnbull for how he and his colleagues were using Puppet, Python, and their own code to manage and automate deploys. In 2013, Puppet, Inc."
What is the purpose of the CHKDSK command?,To create a backup of the file system,To scan and fix logical file system errors,To compress files and folders,To encrypt files and folders,To defragment the hard drive,B,"In computing, CHKDSK (short for ""check disk"") is a system tool and command in DOS, Digital Research FlexOS, IBM/Toshiba 4690 OS, IBM OS/2, Microsoft Windows and related operating systems. It verifies the file system integrity of a volume and attempts to fix logical file system errors. It is similar to the fsck command in Unix and similar to Microsoft ScanDisk, which co-existed with CHKDSK in Windows 9x and MS-DOS 6.x.

Implementations
An early implementation of a 'CheckDisk' was the CHECKDSK that was a part of Digital Equipment Corporation hardware's diagnostics, running on early 1970s TENEX and TOPS-20.

SCP 86-DOS
The CHKDSK command was first implemented in 1980 by Tim Paterson and included in Seattle Computer Products 86-DOS.

MS-DOS / IBM PC DOS
The command is available in MS-DOS versions 1 and later.CHKDSK is implemented as an external command. MS-DOS versions 2.x - 4.x use chkdsk.com as the executable file. MS-DOS versions 5.x and later use chkdsk.exe as the executable file.CHKDSK can also show the memory usage, this was used before the command MEM.EXE was introduced in MS-DOS 4.0 to show the memory usage. In DR DOS the parameter /A limited the output to only show the memory usage.

MS-DOS 5.0 bug
CHKDSK and UNDELETE in MS-DOS 5.0 have a bug which can corrupt data: If the file allocation table of a disk uses 256 sectors, running CHKDSK /F can cause data loss and running UNDELETE can cause unpredictable results. This normally affects disks with a capacity of approximately a multiple of 128 MB."
Who first implemented the CHKDSK command?,Digital Equipment Corporation,IBM,Microsoft,Seattle Computer Products,Tim Paterson,E,"In computing, CHKDSK (short for ""check disk"") is a system tool and command in DOS, Digital Research FlexOS, IBM/Toshiba 4690 OS, IBM OS/2, Microsoft Windows and related operating systems. It verifies the file system integrity of a volume and attempts to fix logical file system errors. It is similar to the fsck command in Unix and similar to Microsoft ScanDisk, which co-existed with CHKDSK in Windows 9x and MS-DOS 6.x.

Implementations
An early implementation of a 'CheckDisk' was the CHECKDSK that was a part of Digital Equipment Corporation hardware's diagnostics, running on early 1970s TENEX and TOPS-20.

SCP 86-DOS
The CHKDSK command was first implemented in 1980 by Tim Paterson and included in Seattle Computer Products 86-DOS.

MS-DOS / IBM PC DOS
The command is available in MS-DOS versions 1 and later.CHKDSK is implemented as an external command. MS-DOS versions 2.x - 4.x use chkdsk.com as the executable file. MS-DOS versions 5.x and later use chkdsk.exe as the executable file.CHKDSK can also show the memory usage, this was used before the command MEM.EXE was introduced in MS-DOS 4.0 to show the memory usage. In DR DOS the parameter /A limited the output to only show the memory usage.

MS-DOS 5.0 bug
CHKDSK and UNDELETE in MS-DOS 5.0 have a bug which can corrupt data: If the file allocation table of a disk uses 256 sectors, running CHKDSK /F can cause data loss and running UNDELETE can cause unpredictable results. This normally affects disks with a capacity of approximately a multiple of 128 MB."
Which operating systems include the CHKDSK command?,Unix and Linux,Windows 9x and MS-DOS 6.x,MacOS,iOS and Android,Windows and related systems,E,"In computing, CHKDSK (short for ""check disk"") is a system tool and command in DOS, Digital Research FlexOS, IBM/Toshiba 4690 OS, IBM OS/2, Microsoft Windows and related operating systems. It verifies the file system integrity of a volume and attempts to fix logical file system errors. It is similar to the fsck command in Unix and similar to Microsoft ScanDisk, which co-existed with CHKDSK in Windows 9x and MS-DOS 6.x.

Implementations
An early implementation of a 'CheckDisk' was the CHECKDSK that was a part of Digital Equipment Corporation hardware's diagnostics, running on early 1970s TENEX and TOPS-20.

SCP 86-DOS
The CHKDSK command was first implemented in 1980 by Tim Paterson and included in Seattle Computer Products 86-DOS.

MS-DOS / IBM PC DOS
The command is available in MS-DOS versions 1 and later.CHKDSK is implemented as an external command. MS-DOS versions 2.x - 4.x use chkdsk.com as the executable file. MS-DOS versions 5.x and later use chkdsk.exe as the executable file.CHKDSK can also show the memory usage, this was used before the command MEM.EXE was introduced in MS-DOS 4.0 to show the memory usage. In DR DOS the parameter /A limited the output to only show the memory usage.

MS-DOS 5.0 bug
CHKDSK and UNDELETE in MS-DOS 5.0 have a bug which can corrupt data: If the file allocation table of a disk uses 256 sectors, running CHKDSK /F can cause data loss and running UNDELETE can cause unpredictable results. This normally affects disks with a capacity of approximately a multiple of 128 MB."
What was the early implementation of CHKDSK?,Digital Research FlexOS,TENEX and TOPS-20,IBM/Toshiba 4690 OS,Seattle Computer Products 86-DOS,Microsoft Windows,D,"In computing, CHKDSK (short for ""check disk"") is a system tool and command in DOS, Digital Research FlexOS, IBM/Toshiba 4690 OS, IBM OS/2, Microsoft Windows and related operating systems. It verifies the file system integrity of a volume and attempts to fix logical file system errors. It is similar to the fsck command in Unix and similar to Microsoft ScanDisk, which co-existed with CHKDSK in Windows 9x and MS-DOS 6.x.

Implementations
An early implementation of a 'CheckDisk' was the CHECKDSK that was a part of Digital Equipment Corporation hardware's diagnostics, running on early 1970s TENEX and TOPS-20.

SCP 86-DOS
The CHKDSK command was first implemented in 1980 by Tim Paterson and included in Seattle Computer Products 86-DOS.

MS-DOS / IBM PC DOS
The command is available in MS-DOS versions 1 and later.CHKDSK is implemented as an external command. MS-DOS versions 2.x - 4.x use chkdsk.com as the executable file. MS-DOS versions 5.x and later use chkdsk.exe as the executable file.CHKDSK can also show the memory usage, this was used before the command MEM.EXE was introduced in MS-DOS 4.0 to show the memory usage. In DR DOS the parameter /A limited the output to only show the memory usage.

MS-DOS 5.0 bug
CHKDSK and UNDELETE in MS-DOS 5.0 have a bug which can corrupt data: If the file allocation table of a disk uses 256 sectors, running CHKDSK /F can cause data loss and running UNDELETE can cause unpredictable results. This normally affects disks with a capacity of approximately a multiple of 128 MB."
Which version of MS-DOS introduced the bug in CHKDSK?,MS-DOS 1.0,MS-DOS 2.0,MS-DOS 3.0,MS-DOS 4.0,MS-DOS 5.0,E,"In computing, CHKDSK (short for ""check disk"") is a system tool and command in DOS, Digital Research FlexOS, IBM/Toshiba 4690 OS, IBM OS/2, Microsoft Windows and related operating systems. It verifies the file system integrity of a volume and attempts to fix logical file system errors. It is similar to the fsck command in Unix and similar to Microsoft ScanDisk, which co-existed with CHKDSK in Windows 9x and MS-DOS 6.x.

Implementations
An early implementation of a 'CheckDisk' was the CHECKDSK that was a part of Digital Equipment Corporation hardware's diagnostics, running on early 1970s TENEX and TOPS-20.

SCP 86-DOS
The CHKDSK command was first implemented in 1980 by Tim Paterson and included in Seattle Computer Products 86-DOS.

MS-DOS / IBM PC DOS
The command is available in MS-DOS versions 1 and later.CHKDSK is implemented as an external command. MS-DOS versions 2.x - 4.x use chkdsk.com as the executable file. MS-DOS versions 5.x and later use chkdsk.exe as the executable file.CHKDSK can also show the memory usage, this was used before the command MEM.EXE was introduced in MS-DOS 4.0 to show the memory usage. In DR DOS the parameter /A limited the output to only show the memory usage.

MS-DOS 5.0 bug
CHKDSK and UNDELETE in MS-DOS 5.0 have a bug which can corrupt data: If the file allocation table of a disk uses 256 sectors, running CHKDSK /F can cause data loss and running UNDELETE can cause unpredictable results. This normally affects disks with a capacity of approximately a multiple of 128 MB."
What is data mining?,The extraction of patterns and knowledge from large amounts of data,The extraction of data itself from a large dataset,The process of collecting and storing large-scale data,The application of computer decision support system,"The analysis step of the ""knowledge discovery in databases"" process",A,"Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.The term ""data mining"" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data Mining: Practical Machine Learning Tools and Techniques with Java (which covers mostly machine learning material) was originally to be named Practical Machine Learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining)."
What are the main fields that data mining intersects?,Artificial intelligence and machine learning,Computer science and statistics,Database systems and data management,Complexity considerations and visualization,Model and inference considerations and interestingness metrics,B,"Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.The term ""data mining"" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data Mining: Practical Machine Learning Tools and Techniques with Java (which covers mostly machine learning material) was originally to be named Practical Machine Learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining)."
What is the goal of data mining?,The extraction of patterns from a data set,The transformation of information into a comprehensible structure,"The analysis step of the ""knowledge discovery in databases"" process",The extraction of data itself from a large dataset,The application of computer decision support system,B,"Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.The term ""data mining"" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data Mining: Practical Machine Learning Tools and Techniques with Java (which covers mostly machine learning material) was originally to be named Practical Machine Learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining)."
"What is the book ""Data Mining: Practical Machine Learning Tools and Techniques with Java"" originally named?",Data Mining: Practical Machine Learning Tools and Techniques,Practical Machine Learning,Java: Practical Machine Learning Tools and Techniques,Machine Learning: Practical Tools and Techniques with Java,Data Mining: Practical Tools and Techniques with Java,B,"Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.The term ""data mining"" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data Mining: Practical Machine Learning Tools and Techniques with Java (which covers mostly machine learning material) was originally to be named Practical Machine Learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining)."
What are some examples of data mining tasks?,"Cluster analysis, anomaly detection, and association rule mining","Data pre-processing, model and inference considerations, and visualization","Complexity considerations, interestingness metrics, and online updating","Machine learning, business intelligence, and artificial intelligence","Large-scale data analysis, analytics, and statistics",A,"Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.The term ""data mining"" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data Mining: Practical Machine Learning Tools and Techniques with Java (which covers mostly machine learning material) was originally to be named Practical Machine Learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining)."
What is homeokinetics?,The study of separate levels in physics,"The study of self-organizing, complex systems",The study of atomic physics,The study of social physics,The study of galactic physics,B,"Homeokinetics is the study of self-organizing, complex systems. Standard physics studies systems at separate levels, such as atomic physics, nuclear physics, biophysics, social physics, and galactic physics. Homeokinetic physics studies the up-down processes that bind these levels. Tools such as mechanics, quantum field theory, and the laws of thermodynamics provide the key relationships. The subject, described as the physics and thermodynamics associated with the up down movement between levels of systems, originated in the late 1970s work of American physicists Harry Soodak and Arthur Iberall. Complex systems are universes, galaxies, social systems, people, or even those that seem as simple as gases. The basic premise is that the entire universe consists of atomistic-like units bound in interactive ensembles to form systems, level by level, in a nested hierarchy."
What are some tools used in homeokinetic physics?,"Mechanics, quantum field theory, and laws of thermodynamics",Separate levels in physics,Atomic physics and biophysics,Social physics and galactic physics,Thermodynamics and quantum mechanics,A,"Homeokinetics is the study of self-organizing, complex systems. Standard physics studies systems at separate levels, such as atomic physics, nuclear physics, biophysics, social physics, and galactic physics. Homeokinetic physics studies the up-down processes that bind these levels. Tools such as mechanics, quantum field theory, and the laws of thermodynamics provide the key relationships. The subject, described as the physics and thermodynamics associated with the up down movement between levels of systems, originated in the late 1970s work of American physicists Harry Soodak and Arthur Iberall. Complex systems are universes, galaxies, social systems, people, or even those that seem as simple as gases. The basic premise is that the entire universe consists of atomistic-like units bound in interactive ensembles to form systems, level by level, in a nested hierarchy."
Who originated the concept of homeokinetics?,Harry Soodak and Arthur Iberall,Physicists from the late 1970s,"Universes, galaxies, social systems, and people",Those that seem as simple as gases,Atomistic-like units bound in interactive ensembles,A,"Homeokinetics is the study of self-organizing, complex systems. Standard physics studies systems at separate levels, such as atomic physics, nuclear physics, biophysics, social physics, and galactic physics. Homeokinetic physics studies the up-down processes that bind these levels. Tools such as mechanics, quantum field theory, and the laws of thermodynamics provide the key relationships. The subject, described as the physics and thermodynamics associated with the up down movement between levels of systems, originated in the late 1970s work of American physicists Harry Soodak and Arthur Iberall. Complex systems are universes, galaxies, social systems, people, or even those that seem as simple as gases. The basic premise is that the entire universe consists of atomistic-like units bound in interactive ensembles to form systems, level by level, in a nested hierarchy."
What are some examples of complex systems?,"Universes, galaxies, social systems, and people",Separate levels in physics,Atomic physics and biophysics,Social physics and galactic physics,Thermodynamics and quantum mechanics,A,"Homeokinetics is the study of self-organizing, complex systems. Standard physics studies systems at separate levels, such as atomic physics, nuclear physics, biophysics, social physics, and galactic physics. Homeokinetic physics studies the up-down processes that bind these levels. Tools such as mechanics, quantum field theory, and the laws of thermodynamics provide the key relationships. The subject, described as the physics and thermodynamics associated with the up down movement between levels of systems, originated in the late 1970s work of American physicists Harry Soodak and Arthur Iberall. Complex systems are universes, galaxies, social systems, people, or even those that seem as simple as gases. The basic premise is that the entire universe consists of atomistic-like units bound in interactive ensembles to form systems, level by level, in a nested hierarchy."
What is the basic premise of homeokinetics?,The entire universe consists of atomistic-like units,Separate levels in physics,Atomic physics and biophysics,Social physics and galactic physics,Thermodynamics and quantum mechanics,A,"Homeokinetics is the study of self-organizing, complex systems. Standard physics studies systems at separate levels, such as atomic physics, nuclear physics, biophysics, social physics, and galactic physics. Homeokinetic physics studies the up-down processes that bind these levels. Tools such as mechanics, quantum field theory, and the laws of thermodynamics provide the key relationships. The subject, described as the physics and thermodynamics associated with the up down movement between levels of systems, originated in the late 1970s work of American physicists Harry Soodak and Arthur Iberall. Complex systems are universes, galaxies, social systems, people, or even those that seem as simple as gases. The basic premise is that the entire universe consists of atomistic-like units bound in interactive ensembles to form systems, level by level, in a nested hierarchy."
What is vacuum engineering?,The field of engineering that deals with the use of air in industrial and scientific applications,The field of engineering that deals with the use of vacuum in industrial and scientific applications,The field of engineering that deals with the use of pressure in industrial and scientific applications,The field of engineering that deals with the use of gas in industrial and scientific applications,The field of engineering that deals with the use of steam in industrial and scientific applications,B,"Vacuum engineering is the field of engineering that deals with the practical use of vacuum in industrial and scientific applications. Vacuum may improve the productivity and performance of processes otherwise carried out at normal air pressure, or may make possible processes that could not be done in the presence of air. Vacuum engineering techniques are widely applied in materials processing such as drying or filtering, chemical processing, application of metal coatings to objects, manufacture of electron devices and incandescent lamps, and in scientific research. 
Vacuum techniques vary depending on the desired vacuum pressure to be achieved. For a ""rough"" vacuum, over 100 Pascals pressure, conventional methods of analysis, materials, pumps and measuring instruments can be used, whereas ultrahigh vacuum systems use specialized equipment to achieve pressures below one-millionth of one Pascal.  At such low pressures, even metals may emit enough gas to cause serious contamination.

Design and mechanism
Vacuum systems usually consist of gauges, vapor jet and pumps, vapor traps and valves along with other extensional piping.  A vessel that is operating under vacuum system may be any of these types such as processing tank, steam simulator, particle accelerator, or any other type of space that has an enclosed chamber to maintain the system in less than atmospheric gas pressure."
How can vacuum improve processes?,By increasing the pressure,By decreasing the pressure,By maintaining normal air pressure,By increasing the temperature,By decreasing the temperature,B,"Vacuum engineering is the field of engineering that deals with the practical use of vacuum in industrial and scientific applications. Vacuum may improve the productivity and performance of processes otherwise carried out at normal air pressure, or may make possible processes that could not be done in the presence of air. Vacuum engineering techniques are widely applied in materials processing such as drying or filtering, chemical processing, application of metal coatings to objects, manufacture of electron devices and incandescent lamps, and in scientific research. 
Vacuum techniques vary depending on the desired vacuum pressure to be achieved. For a ""rough"" vacuum, over 100 Pascals pressure, conventional methods of analysis, materials, pumps and measuring instruments can be used, whereas ultrahigh vacuum systems use specialized equipment to achieve pressures below one-millionth of one Pascal.  At such low pressures, even metals may emit enough gas to cause serious contamination.

Design and mechanism
Vacuum systems usually consist of gauges, vapor jet and pumps, vapor traps and valves along with other extensional piping.  A vessel that is operating under vacuum system may be any of these types such as processing tank, steam simulator, particle accelerator, or any other type of space that has an enclosed chamber to maintain the system in less than atmospheric gas pressure."
What are some applications of vacuum engineering techniques?,Manufacturing of electron devices and incandescent lamps,Chemical processing,Drying and filtering,All of the above,None of the above,D,"Vacuum engineering is the field of engineering that deals with the practical use of vacuum in industrial and scientific applications. Vacuum may improve the productivity and performance of processes otherwise carried out at normal air pressure, or may make possible processes that could not be done in the presence of air. Vacuum engineering techniques are widely applied in materials processing such as drying or filtering, chemical processing, application of metal coatings to objects, manufacture of electron devices and incandescent lamps, and in scientific research. 
Vacuum techniques vary depending on the desired vacuum pressure to be achieved. For a ""rough"" vacuum, over 100 Pascals pressure, conventional methods of analysis, materials, pumps and measuring instruments can be used, whereas ultrahigh vacuum systems use specialized equipment to achieve pressures below one-millionth of one Pascal.  At such low pressures, even metals may emit enough gas to cause serious contamination.

Design and mechanism
Vacuum systems usually consist of gauges, vapor jet and pumps, vapor traps and valves along with other extensional piping.  A vessel that is operating under vacuum system may be any of these types such as processing tank, steam simulator, particle accelerator, or any other type of space that has an enclosed chamber to maintain the system in less than atmospheric gas pressure."
"What pressure range is considered a ""rough"" vacuum?",Below one-millionth of one Pascal,Over 100 Pascals,One Pascal,Atmospheric pressure,None of the above,B,"Vacuum engineering is the field of engineering that deals with the practical use of vacuum in industrial and scientific applications. Vacuum may improve the productivity and performance of processes otherwise carried out at normal air pressure, or may make possible processes that could not be done in the presence of air. Vacuum engineering techniques are widely applied in materials processing such as drying or filtering, chemical processing, application of metal coatings to objects, manufacture of electron devices and incandescent lamps, and in scientific research. 
Vacuum techniques vary depending on the desired vacuum pressure to be achieved. For a ""rough"" vacuum, over 100 Pascals pressure, conventional methods of analysis, materials, pumps and measuring instruments can be used, whereas ultrahigh vacuum systems use specialized equipment to achieve pressures below one-millionth of one Pascal.  At such low pressures, even metals may emit enough gas to cause serious contamination.

Design and mechanism
Vacuum systems usually consist of gauges, vapor jet and pumps, vapor traps and valves along with other extensional piping.  A vessel that is operating under vacuum system may be any of these types such as processing tank, steam simulator, particle accelerator, or any other type of space that has an enclosed chamber to maintain the system in less than atmospheric gas pressure."
What type of equipment is used in ultrahigh vacuum systems?,"Conventional methods of analysis, materials, pumps and measuring instruments",Specialized equipment to achieve pressures below one-millionth of one Pascal,"Gauges, vapor jet and pumps, vapor traps and valves","Processing tank, steam simulator, particle accelerator",None of the above,B,"Vacuum engineering is the field of engineering that deals with the practical use of vacuum in industrial and scientific applications. Vacuum may improve the productivity and performance of processes otherwise carried out at normal air pressure, or may make possible processes that could not be done in the presence of air. Vacuum engineering techniques are widely applied in materials processing such as drying or filtering, chemical processing, application of metal coatings to objects, manufacture of electron devices and incandescent lamps, and in scientific research. 
Vacuum techniques vary depending on the desired vacuum pressure to be achieved. For a ""rough"" vacuum, over 100 Pascals pressure, conventional methods of analysis, materials, pumps and measuring instruments can be used, whereas ultrahigh vacuum systems use specialized equipment to achieve pressures below one-millionth of one Pascal.  At such low pressures, even metals may emit enough gas to cause serious contamination.

Design and mechanism
Vacuum systems usually consist of gauges, vapor jet and pumps, vapor traps and valves along with other extensional piping.  A vessel that is operating under vacuum system may be any of these types such as processing tank, steam simulator, particle accelerator, or any other type of space that has an enclosed chamber to maintain the system in less than atmospheric gas pressure."
What is a persistence barcode?,A barcode used for scanning products in a store,An algebraic invariant of a persistence module that characterizes the stability of topological features throughout a growing family of spaces,A barcode used to track the lifetime of a topological feature in a filtration,A barcode used to measure the amount of noise in data,A barcode used to capture all the topological information in a filtration,B,"In topological data analysis, a persistence barcode, sometimes shortened to barcode, is an algebraic invariant of a persistence module that characterizes the stability of topological features throughout a growing family of spaces. Formally, a persistence barcode consists of a multiset of intervals in the extended real line, where the length of each interval corresponds to the lifetime of a topological feature in a filtration, usually built on a point cloud, a  graph, a  function, or, more generally, a simplicial complex or a chain complex. Generally, longer intervals in a barcode correspond to more robust features, whereas shorter intervals are more likely to be noise in the data. A persistence barcode is a complete invariant that captures all the topological information in a filtration. In algebraic topology, the persistence barcodes were first introduced by Sergey Barannikov in 1994 as the ""canonical forms"" invariants consisting of a multiset of line segments with ends on two parallel lines, and later, in geometry processing, by Gunnar Carlsson et al. in 2004.

Definition
Let 
  
    
      F
    
    \mathbb {F}
   be a fixed field. Then a persistence module 
  
    M
    M
   indexed over 
  
    
      R
    
    \mathbb {R}
   consists of a family of 
  
    
      F
    
    \mathbb {F}
  -vector spaces 
  
    
      
        {
        
          M
          
            t
          
        
        
          }
          
            t
            ∈
            
              R
            
          
        
      
    
    {\displaystyle \{M_{t}\}_{t\in \mathbb {R} }}
   and linear maps 
  
    
      
        
          φ
          
            s
            ,
            t
          
        
        :
        
          M
          
            s
          
        
        →
        
          M
          
            t
          
        
      
    
    {\displaystyle \varphi _{s,t}:M_{s}\to M_{t}}
   for each 
  
    
      s
      ≤
      t
    
    s\leq t
   such that 
  
    
      
        
          φ
          
            s
            ,
            t
          
        
        ∘
        
          φ
          
            r
            ,
            s
          
        
        =
        
          φ
          
            r
            ,
            t
          
        
      
    
    {\displaystyle \varphi _{s,t}\circ \varphi _{r,s}=\varphi _{r,t}}
   for all 
  
    
      r
      ≤
      s
      ≤
      t
    
    r\leq s\leq t
  ."
What do longer intervals in a persistence barcode correspond to?,More robust features,Less robust features,Noise in the data,No correlation,None of the above,A,"In topological data analysis, a persistence barcode, sometimes shortened to barcode, is an algebraic invariant of a persistence module that characterizes the stability of topological features throughout a growing family of spaces. Formally, a persistence barcode consists of a multiset of intervals in the extended real line, where the length of each interval corresponds to the lifetime of a topological feature in a filtration, usually built on a point cloud, a  graph, a  function, or, more generally, a simplicial complex or a chain complex. Generally, longer intervals in a barcode correspond to more robust features, whereas shorter intervals are more likely to be noise in the data. A persistence barcode is a complete invariant that captures all the topological information in a filtration. In algebraic topology, the persistence barcodes were first introduced by Sergey Barannikov in 1994 as the ""canonical forms"" invariants consisting of a multiset of line segments with ends on two parallel lines, and later, in geometry processing, by Gunnar Carlsson et al. in 2004.

Definition
Let 
  
    
      F
    
    \mathbb {F}
   be a fixed field. Then a persistence module 
  
    M
    M
   indexed over 
  
    
      R
    
    \mathbb {R}
   consists of a family of 
  
    
      F
    
    \mathbb {F}
  -vector spaces 
  
    
      
        {
        
          M
          
            t
          
        
        
          }
          
            t
            ∈
            
              R
            
          
        
      
    
    {\displaystyle \{M_{t}\}_{t\in \mathbb {R} }}
   and linear maps 
  
    
      
        
          φ
          
            s
            ,
            t
          
        
        :
        
          M
          
            s
          
        
        →
        
          M
          
            t
          
        
      
    
    {\displaystyle \varphi _{s,t}:M_{s}\to M_{t}}
   for each 
  
    
      s
      ≤
      t
    
    s\leq t
   such that 
  
    
      
        
          φ
          
            s
            ,
            t
          
        
        ∘
        
          φ
          
            r
            ,
            s
          
        
        =
        
          φ
          
            r
            ,
            t
          
        
      
    
    {\displaystyle \varphi _{s,t}\circ \varphi _{r,s}=\varphi _{r,t}}
   for all 
  
    
      r
      ≤
      s
      ≤
      t
    
    r\leq s\leq t
  ."
Who first introduced the concept of persistence barcodes?,Sergey Barannikov in 1994,Gunnar Carlsson et al. in 2004,Both Sergey Barannikov in 1994 and Gunnar Carlsson et al. in 2004,Neither Sergey Barannikov nor Gunnar Carlsson,Unknown,C,"In topological data analysis, a persistence barcode, sometimes shortened to barcode, is an algebraic invariant of a persistence module that characterizes the stability of topological features throughout a growing family of spaces. Formally, a persistence barcode consists of a multiset of intervals in the extended real line, where the length of each interval corresponds to the lifetime of a topological feature in a filtration, usually built on a point cloud, a  graph, a  function, or, more generally, a simplicial complex or a chain complex. Generally, longer intervals in a barcode correspond to more robust features, whereas shorter intervals are more likely to be noise in the data. A persistence barcode is a complete invariant that captures all the topological information in a filtration. In algebraic topology, the persistence barcodes were first introduced by Sergey Barannikov in 1994 as the ""canonical forms"" invariants consisting of a multiset of line segments with ends on two parallel lines, and later, in geometry processing, by Gunnar Carlsson et al. in 2004.

Definition
Let 
  
    
      F
    
    \mathbb {F}
   be a fixed field. Then a persistence module 
  
    M
    M
   indexed over 
  
    
      R
    
    \mathbb {R}
   consists of a family of 
  
    
      F
    
    \mathbb {F}
  -vector spaces 
  
    
      
        {
        
          M
          
            t
          
        
        
          }
          
            t
            ∈
            
              R
            
          
        
      
    
    {\displaystyle \{M_{t}\}_{t\in \mathbb {R} }}
   and linear maps 
  
    
      
        
          φ
          
            s
            ,
            t
          
        
        :
        
          M
          
            s
          
        
        →
        
          M
          
            t
          
        
      
    
    {\displaystyle \varphi _{s,t}:M_{s}\to M_{t}}
   for each 
  
    
      s
      ≤
      t
    
    s\leq t
   such that 
  
    
      
        
          φ
          
            s
            ,
            t
          
        
        ∘
        
          φ
          
            r
            ,
            s
          
        
        =
        
          φ
          
            r
            ,
            t
          
        
      
    
    {\displaystyle \varphi _{s,t}\circ \varphi _{r,s}=\varphi _{r,t}}
   for all 
  
    
      r
      ≤
      s
      ≤
      t
    
    r\leq s\leq t
  ."
What is a persistence module?,A collection of F-vector spaces indexed over R,A collection of linear maps between F-vector spaces,A collection of intervals in the extended real line,A collection of topological features in a filtration,None of the above,A,"In topological data analysis, a persistence barcode, sometimes shortened to barcode, is an algebraic invariant of a persistence module that characterizes the stability of topological features throughout a growing family of spaces. Formally, a persistence barcode consists of a multiset of intervals in the extended real line, where the length of each interval corresponds to the lifetime of a topological feature in a filtration, usually built on a point cloud, a  graph, a  function, or, more generally, a simplicial complex or a chain complex. Generally, longer intervals in a barcode correspond to more robust features, whereas shorter intervals are more likely to be noise in the data. A persistence barcode is a complete invariant that captures all the topological information in a filtration. In algebraic topology, the persistence barcodes were first introduced by Sergey Barannikov in 1994 as the ""canonical forms"" invariants consisting of a multiset of line segments with ends on two parallel lines, and later, in geometry processing, by Gunnar Carlsson et al. in 2004.

Definition
Let 
  
    
      F
    
    \mathbb {F}
   be a fixed field. Then a persistence module 
  
    M
    M
   indexed over 
  
    
      R
    
    \mathbb {R}
   consists of a family of 
  
    
      F
    
    \mathbb {F}
  -vector spaces 
  
    
      
        {
        
          M
          
            t
          
        
        
          }
          
            t
            ∈
            
              R
            
          
        
      
    
    {\displaystyle \{M_{t}\}_{t\in \mathbb {R} }}
   and linear maps 
  
    
      
        
          φ
          
            s
            ,
            t
          
        
        :
        
          M
          
            s
          
        
        →
        
          M
          
            t
          
        
      
    
    {\displaystyle \varphi _{s,t}:M_{s}\to M_{t}}
   for each 
  
    
      s
      ≤
      t
    
    s\leq t
   such that 
  
    
      
        
          φ
          
            s
            ,
            t
          
        
        ∘
        
          φ
          
            r
            ,
            s
          
        
        =
        
          φ
          
            r
            ,
            t
          
        
      
    
    {\displaystyle \varphi _{s,t}\circ \varphi _{r,s}=\varphi _{r,t}}
   for all 
  
    
      r
      ≤
      s
      ≤
      t
    
    r\leq s\leq t
  ."
What property must the linear maps in a persistence module satisfy?,"φs,t ∘ φr,s = φr,t","φs,t + φr,s = φr,t","φs,t * φr,s = φr,t","φs,t / φr,s = φr,t",None of the above,A,"In topological data analysis, a persistence barcode, sometimes shortened to barcode, is an algebraic invariant of a persistence module that characterizes the stability of topological features throughout a growing family of spaces. Formally, a persistence barcode consists of a multiset of intervals in the extended real line, where the length of each interval corresponds to the lifetime of a topological feature in a filtration, usually built on a point cloud, a  graph, a  function, or, more generally, a simplicial complex or a chain complex. Generally, longer intervals in a barcode correspond to more robust features, whereas shorter intervals are more likely to be noise in the data. A persistence barcode is a complete invariant that captures all the topological information in a filtration. In algebraic topology, the persistence barcodes were first introduced by Sergey Barannikov in 1994 as the ""canonical forms"" invariants consisting of a multiset of line segments with ends on two parallel lines, and later, in geometry processing, by Gunnar Carlsson et al. in 2004.

Definition
Let 
  
    
      F
    
    \mathbb {F}
   be a fixed field. Then a persistence module 
  
    M
    M
   indexed over 
  
    
      R
    
    \mathbb {R}
   consists of a family of 
  
    
      F
    
    \mathbb {F}
  -vector spaces 
  
    
      
        {
        
          M
          
            t
          
        
        
          }
          
            t
            ∈
            
              R
            
          
        
      
    
    {\displaystyle \{M_{t}\}_{t\in \mathbb {R} }}
   and linear maps 
  
    
      
        
          φ
          
            s
            ,
            t
          
        
        :
        
          M
          
            s
          
        
        →
        
          M
          
            t
          
        
      
    
    {\displaystyle \varphi _{s,t}:M_{s}\to M_{t}}
   for each 
  
    
      s
      ≤
      t
    
    s\leq t
   such that 
  
    
      
        
          φ
          
            s
            ,
            t
          
        
        ∘
        
          φ
          
            r
            ,
            s
          
        
        =
        
          φ
          
            r
            ,
            t
          
        
      
    
    {\displaystyle \varphi _{s,t}\circ \varphi _{r,s}=\varphi _{r,t}}
   for all 
  
    
      r
      ≤
      s
      ≤
      t
    
    r\leq s\leq t
  ."
What is the main objective of the UniverseMachine project?,To study the evolution of the universe,To discover the role of dark matter,To compare different universes,To develop insights into the possible beginning of our universe,To simulate various models of possible universes,A,"The UniverseMachine (also known as the Universe Machine) is a project carrying out astrophysical supercomputer simulations of various models of possible universes, created by astronomer Peter Behroozi and his research team at the Steward Observatory and the University of Arizona. Numerous universes with different physical characteristics may be simulated in order to develop insights into the possible beginning and evolution of our universe. A major objective is to better understand the role of dark matter in the development of the universe. According to Behroozi, ""On the computer, we can create many different universes and compare them to the actual one, and that lets us infer which rules lead to the one we see.""Besides lead investigator Behroozi, research team members include astronomer Charlie Conroy of Harvard University, physicist Andrew Hearin of the Argonne National Laboratory and physicist Risa Wechsler of Stanford University. Support funding for the project is provided by NASA, the National Science Foundation and the Munich Institute for Astro- and Particle Physics.

Description
Besides using computers and related resources at the NASA Ames Research Center and the Leibniz-Rechenzentrum in Garching, Germany, the research team used the High-Performance Computing cluster at the University of Arizona. Two-thousand processors simultaneously processed the data over three weeks. In this way, the research team generated over 8 million universes, and at least 9.6×1013 galaxies."
Who is the lead investigator of the UniverseMachine project?,Peter Behroozi,Charlie Conroy,Andrew Hearin,Risa Wechsler,None of the above,A,"The UniverseMachine (also known as the Universe Machine) is a project carrying out astrophysical supercomputer simulations of various models of possible universes, created by astronomer Peter Behroozi and his research team at the Steward Observatory and the University of Arizona. Numerous universes with different physical characteristics may be simulated in order to develop insights into the possible beginning and evolution of our universe. A major objective is to better understand the role of dark matter in the development of the universe. According to Behroozi, ""On the computer, we can create many different universes and compare them to the actual one, and that lets us infer which rules lead to the one we see.""Besides lead investigator Behroozi, research team members include astronomer Charlie Conroy of Harvard University, physicist Andrew Hearin of the Argonne National Laboratory and physicist Risa Wechsler of Stanford University. Support funding for the project is provided by NASA, the National Science Foundation and the Munich Institute for Astro- and Particle Physics.

Description
Besides using computers and related resources at the NASA Ames Research Center and the Leibniz-Rechenzentrum in Garching, Germany, the research team used the High-Performance Computing cluster at the University of Arizona. Two-thousand processors simultaneously processed the data over three weeks. In this way, the research team generated over 8 million universes, and at least 9.6×1013 galaxies."
How many universes were generated by the research team?,Over 8 million,At least 9.6×1013,2000,3 weeks,None of the above,A,"The UniverseMachine (also known as the Universe Machine) is a project carrying out astrophysical supercomputer simulations of various models of possible universes, created by astronomer Peter Behroozi and his research team at the Steward Observatory and the University of Arizona. Numerous universes with different physical characteristics may be simulated in order to develop insights into the possible beginning and evolution of our universe. A major objective is to better understand the role of dark matter in the development of the universe. According to Behroozi, ""On the computer, we can create many different universes and compare them to the actual one, and that lets us infer which rules lead to the one we see.""Besides lead investigator Behroozi, research team members include astronomer Charlie Conroy of Harvard University, physicist Andrew Hearin of the Argonne National Laboratory and physicist Risa Wechsler of Stanford University. Support funding for the project is provided by NASA, the National Science Foundation and the Munich Institute for Astro- and Particle Physics.

Description
Besides using computers and related resources at the NASA Ames Research Center and the Leibniz-Rechenzentrum in Garching, Germany, the research team used the High-Performance Computing cluster at the University of Arizona. Two-thousand processors simultaneously processed the data over three weeks. In this way, the research team generated over 8 million universes, and at least 9.6×1013 galaxies."
What resources did the research team use to process the data?,Computers at NASA Ames Research Center,"Computers at Leibniz-Rechenzentrum in Garching, Germany",High-Performance Computing cluster at the University of Arizona,All of the above,None of the above,D,"The UniverseMachine (also known as the Universe Machine) is a project carrying out astrophysical supercomputer simulations of various models of possible universes, created by astronomer Peter Behroozi and his research team at the Steward Observatory and the University of Arizona. Numerous universes with different physical characteristics may be simulated in order to develop insights into the possible beginning and evolution of our universe. A major objective is to better understand the role of dark matter in the development of the universe. According to Behroozi, ""On the computer, we can create many different universes and compare them to the actual one, and that lets us infer which rules lead to the one we see.""Besides lead investigator Behroozi, research team members include astronomer Charlie Conroy of Harvard University, physicist Andrew Hearin of the Argonne National Laboratory and physicist Risa Wechsler of Stanford University. Support funding for the project is provided by NASA, the National Science Foundation and the Munich Institute for Astro- and Particle Physics.

Description
Besides using computers and related resources at the NASA Ames Research Center and the Leibniz-Rechenzentrum in Garching, Germany, the research team used the High-Performance Computing cluster at the University of Arizona. Two-thousand processors simultaneously processed the data over three weeks. In this way, the research team generated over 8 million universes, and at least 9.6×1013 galaxies."
Which organizations provide support funding for the UniverseMachine project?,NASA,National Science Foundation,Munich Institute for Astro- and Particle Physics,All of the above,None of the above,D,"The UniverseMachine (also known as the Universe Machine) is a project carrying out astrophysical supercomputer simulations of various models of possible universes, created by astronomer Peter Behroozi and his research team at the Steward Observatory and the University of Arizona. Numerous universes with different physical characteristics may be simulated in order to develop insights into the possible beginning and evolution of our universe. A major objective is to better understand the role of dark matter in the development of the universe. According to Behroozi, ""On the computer, we can create many different universes and compare them to the actual one, and that lets us infer which rules lead to the one we see.""Besides lead investigator Behroozi, research team members include astronomer Charlie Conroy of Harvard University, physicist Andrew Hearin of the Argonne National Laboratory and physicist Risa Wechsler of Stanford University. Support funding for the project is provided by NASA, the National Science Foundation and the Munich Institute for Astro- and Particle Physics.

Description
Besides using computers and related resources at the NASA Ames Research Center and the Leibniz-Rechenzentrum in Garching, Germany, the research team used the High-Performance Computing cluster at the University of Arizona. Two-thousand processors simultaneously processed the data over three weeks. In this way, the research team generated over 8 million universes, and at least 9.6×1013 galaxies."
What is the main focus of digital topology?,Properties and features of digital images,Properties and features of analog images,Properties and features of 2D images,Properties and features of 3D images,Properties and features of topological objects,A,"Digital topology deals with properties and features of two-dimensional (2D) or three-dimensional (3D) digital images
that correspond to topological properties (e.g., connectedness) or topological features (e.g., boundaries) of objects.
Concepts and results of digital topology are used to specify and justify important (low-level) image analysis algorithms, 
including algorithms for thinning, border or surface tracing, counting of components or tunnels, or region-filling.

History
Digital topology was first studied in the late 1960s by the computer image analysis researcher Azriel Rosenfeld (1931–2004), whose publications on the subject played a major role in establishing and developing the field. The term ""digital topology"" was itself invented by Rosenfeld, who used it in a 1973 publication for the first time.
A related work called the grid cell topology, which could be considered as a link to classic combinatorial topology, appeared in the book of Pavel Alexandrov and Heinz Hopf, Topologie I (1935).  Rosenfeld et al. proposed digital connectivity such as 4-connectivity and 8-connectivity in two dimensions as well as 6-connectivity and 26-connectivity in three dimensions. The labeling method for inferring a connected component was studied in the 1970s. Theodosios Pavlidis (1982) suggested the use of graph-theoretic algorithms such as the depth-first search method for finding connected components. Vladimir A."
Who is considered a pioneer in the field of digital topology?,Azriel Rosenfeld,Pavel Alexandrov,Heinz Hopf,Theodosios Pavlidis,Vladimir A.,A,"Digital topology deals with properties and features of two-dimensional (2D) or three-dimensional (3D) digital images
that correspond to topological properties (e.g., connectedness) or topological features (e.g., boundaries) of objects.
Concepts and results of digital topology are used to specify and justify important (low-level) image analysis algorithms, 
including algorithms for thinning, border or surface tracing, counting of components or tunnels, or region-filling.

History
Digital topology was first studied in the late 1960s by the computer image analysis researcher Azriel Rosenfeld (1931–2004), whose publications on the subject played a major role in establishing and developing the field. The term ""digital topology"" was itself invented by Rosenfeld, who used it in a 1973 publication for the first time.
A related work called the grid cell topology, which could be considered as a link to classic combinatorial topology, appeared in the book of Pavel Alexandrov and Heinz Hopf, Topologie I (1935).  Rosenfeld et al. proposed digital connectivity such as 4-connectivity and 8-connectivity in two dimensions as well as 6-connectivity and 26-connectivity in three dimensions. The labeling method for inferring a connected component was studied in the 1970s. Theodosios Pavlidis (1982) suggested the use of graph-theoretic algorithms such as the depth-first search method for finding connected components. Vladimir A."
"When was the term ""digital topology"" first used?",1960s,1970s,1980s,1990s,2000s,B,"Digital topology deals with properties and features of two-dimensional (2D) or three-dimensional (3D) digital images
that correspond to topological properties (e.g., connectedness) or topological features (e.g., boundaries) of objects.
Concepts and results of digital topology are used to specify and justify important (low-level) image analysis algorithms, 
including algorithms for thinning, border or surface tracing, counting of components or tunnels, or region-filling.

History
Digital topology was first studied in the late 1960s by the computer image analysis researcher Azriel Rosenfeld (1931–2004), whose publications on the subject played a major role in establishing and developing the field. The term ""digital topology"" was itself invented by Rosenfeld, who used it in a 1973 publication for the first time.
A related work called the grid cell topology, which could be considered as a link to classic combinatorial topology, appeared in the book of Pavel Alexandrov and Heinz Hopf, Topologie I (1935).  Rosenfeld et al. proposed digital connectivity such as 4-connectivity and 8-connectivity in two dimensions as well as 6-connectivity and 26-connectivity in three dimensions. The labeling method for inferring a connected component was studied in the 1970s. Theodosios Pavlidis (1982) suggested the use of graph-theoretic algorithms such as the depth-first search method for finding connected components. Vladimir A."
What is an example of digital connectivity in two dimensions?,4-connectivity,8-connectivity,6-connectivity,26-connectivity,Depth-first search,B,"Digital topology deals with properties and features of two-dimensional (2D) or three-dimensional (3D) digital images
that correspond to topological properties (e.g., connectedness) or topological features (e.g., boundaries) of objects.
Concepts and results of digital topology are used to specify and justify important (low-level) image analysis algorithms, 
including algorithms for thinning, border or surface tracing, counting of components or tunnels, or region-filling.

History
Digital topology was first studied in the late 1960s by the computer image analysis researcher Azriel Rosenfeld (1931–2004), whose publications on the subject played a major role in establishing and developing the field. The term ""digital topology"" was itself invented by Rosenfeld, who used it in a 1973 publication for the first time.
A related work called the grid cell topology, which could be considered as a link to classic combinatorial topology, appeared in the book of Pavel Alexandrov and Heinz Hopf, Topologie I (1935).  Rosenfeld et al. proposed digital connectivity such as 4-connectivity and 8-connectivity in two dimensions as well as 6-connectivity and 26-connectivity in three dimensions. The labeling method for inferring a connected component was studied in the 1970s. Theodosios Pavlidis (1982) suggested the use of graph-theoretic algorithms such as the depth-first search method for finding connected components. Vladimir A."
Which method is used for finding connected components?,Depth-first search,Breadth-first search,Dijkstra's algorithm,Prim's algorithm,Kruskal's algorithm,A,"Digital topology deals with properties and features of two-dimensional (2D) or three-dimensional (3D) digital images
that correspond to topological properties (e.g., connectedness) or topological features (e.g., boundaries) of objects.
Concepts and results of digital topology are used to specify and justify important (low-level) image analysis algorithms, 
including algorithms for thinning, border or surface tracing, counting of components or tunnels, or region-filling.

History
Digital topology was first studied in the late 1960s by the computer image analysis researcher Azriel Rosenfeld (1931–2004), whose publications on the subject played a major role in establishing and developing the field. The term ""digital topology"" was itself invented by Rosenfeld, who used it in a 1973 publication for the first time.
A related work called the grid cell topology, which could be considered as a link to classic combinatorial topology, appeared in the book of Pavel Alexandrov and Heinz Hopf, Topologie I (1935).  Rosenfeld et al. proposed digital connectivity such as 4-connectivity and 8-connectivity in two dimensions as well as 6-connectivity and 26-connectivity in three dimensions. The labeling method for inferring a connected component was studied in the 1970s. Theodosios Pavlidis (1982) suggested the use of graph-theoretic algorithms such as the depth-first search method for finding connected components. Vladimir A."
What is the main purpose of forensic science?,To analyze financial and banking data,To testify as expert witnesses in both criminal and civil cases,To travel to the scene of the crime to collect evidence,"To analyze DNA, fingerprints, and bloodstain patterns",To analyze firearms and ballistics,D,"Forensic science, also known as criminalistics, is the application of science to criminal and civil laws. During criminal investigation in particular, it is governed by the legal standards of admissible evidence and criminal procedure. It is a broad field utilizing numerous practices such as the analysis of DNA, fingerprints, bloodstain patterns, firearms, ballistics, and toxicology.
Forensic scientists collect, preserve, and analyze scientific evidence during the course of an investigation. While some forensic scientists travel to the scene of the crime to collect the evidence themselves, others occupy a laboratory role, performing analysis on objects brought to them by other individuals. Still others are involved in analysis of financial, banking, or other numerical data for use in financial crime investigation, and can be employed as consultants from private firms, academia, or as government employees.In addition to their laboratory role, forensic scientists testify as expert witnesses in both criminal and civil cases and can work for either the prosecution or the defense. While any field could technically be forensic, certain sections have developed over time to encompass the majority of forensically related cases.

Etymology
The term forensic stems from the Latin word, forēnsis (3rd declension, adjective), meaning ""of a forum, place of assembly"". The history of the term originates in Roman times, when a criminal charge meant presenting the case before a group of public individuals in the forum."
What is the etymology of the term forensic?,"It comes from the Latin word meaning ""of a forum, place of assembly""",It originates from Roman times when presenting a criminal case in front of a group of public individuals,It is derived from the legal standards of admissible evidence and criminal procedure,"It is based on the analysis of financial, banking, and numerical data","It is associated with the analysis of DNA, fingerprints, and bloodstain patterns",B,"Forensic science, also known as criminalistics, is the application of science to criminal and civil laws. During criminal investigation in particular, it is governed by the legal standards of admissible evidence and criminal procedure. It is a broad field utilizing numerous practices such as the analysis of DNA, fingerprints, bloodstain patterns, firearms, ballistics, and toxicology.
Forensic scientists collect, preserve, and analyze scientific evidence during the course of an investigation. While some forensic scientists travel to the scene of the crime to collect the evidence themselves, others occupy a laboratory role, performing analysis on objects brought to them by other individuals. Still others are involved in analysis of financial, banking, or other numerical data for use in financial crime investigation, and can be employed as consultants from private firms, academia, or as government employees.In addition to their laboratory role, forensic scientists testify as expert witnesses in both criminal and civil cases and can work for either the prosecution or the defense. While any field could technically be forensic, certain sections have developed over time to encompass the majority of forensically related cases.

Etymology
The term forensic stems from the Latin word, forēnsis (3rd declension, adjective), meaning ""of a forum, place of assembly"". The history of the term originates in Roman times, when a criminal charge meant presenting the case before a group of public individuals in the forum."
What role do some forensic scientists play in criminal investigations?,They testify as expert witnesses in both criminal and civil cases,They analyze firearms and ballistics,"They analyze DNA, fingerprints, and bloodstain patterns",They travel to the scene of the crime to collect evidence,"They analyze financial, banking, and numerical data",C,"Forensic science, also known as criminalistics, is the application of science to criminal and civil laws. During criminal investigation in particular, it is governed by the legal standards of admissible evidence and criminal procedure. It is a broad field utilizing numerous practices such as the analysis of DNA, fingerprints, bloodstain patterns, firearms, ballistics, and toxicology.
Forensic scientists collect, preserve, and analyze scientific evidence during the course of an investigation. While some forensic scientists travel to the scene of the crime to collect the evidence themselves, others occupy a laboratory role, performing analysis on objects brought to them by other individuals. Still others are involved in analysis of financial, banking, or other numerical data for use in financial crime investigation, and can be employed as consultants from private firms, academia, or as government employees.In addition to their laboratory role, forensic scientists testify as expert witnesses in both criminal and civil cases and can work for either the prosecution or the defense. While any field could technically be forensic, certain sections have developed over time to encompass the majority of forensically related cases.

Etymology
The term forensic stems from the Latin word, forēnsis (3rd declension, adjective), meaning ""of a forum, place of assembly"". The history of the term originates in Roman times, when a criminal charge meant presenting the case before a group of public individuals in the forum."
What types of evidence can forensic scientists analyze?,Financial and banking data,"DNA, fingerprints, and bloodstain patterns",Firearms and ballistics,Numerical data for financial crime investigation,All of the above,E,"Forensic science, also known as criminalistics, is the application of science to criminal and civil laws. During criminal investigation in particular, it is governed by the legal standards of admissible evidence and criminal procedure. It is a broad field utilizing numerous practices such as the analysis of DNA, fingerprints, bloodstain patterns, firearms, ballistics, and toxicology.
Forensic scientists collect, preserve, and analyze scientific evidence during the course of an investigation. While some forensic scientists travel to the scene of the crime to collect the evidence themselves, others occupy a laboratory role, performing analysis on objects brought to them by other individuals. Still others are involved in analysis of financial, banking, or other numerical data for use in financial crime investigation, and can be employed as consultants from private firms, academia, or as government employees.In addition to their laboratory role, forensic scientists testify as expert witnesses in both criminal and civil cases and can work for either the prosecution or the defense. While any field could technically be forensic, certain sections have developed over time to encompass the majority of forensically related cases.

Etymology
The term forensic stems from the Latin word, forēnsis (3rd declension, adjective), meaning ""of a forum, place of assembly"". The history of the term originates in Roman times, when a criminal charge meant presenting the case before a group of public individuals in the forum."
In what capacity can forensic scientists work?,"As consultants from private firms, academia, or government employees",As expert witnesses in both criminal and civil cases,"In the analysis of DNA, fingerprints, and bloodstain patterns","In the analysis of financial, banking, and numerical data",All of the above,E,"Forensic science, also known as criminalistics, is the application of science to criminal and civil laws. During criminal investigation in particular, it is governed by the legal standards of admissible evidence and criminal procedure. It is a broad field utilizing numerous practices such as the analysis of DNA, fingerprints, bloodstain patterns, firearms, ballistics, and toxicology.
Forensic scientists collect, preserve, and analyze scientific evidence during the course of an investigation. While some forensic scientists travel to the scene of the crime to collect the evidence themselves, others occupy a laboratory role, performing analysis on objects brought to them by other individuals. Still others are involved in analysis of financial, banking, or other numerical data for use in financial crime investigation, and can be employed as consultants from private firms, academia, or as government employees.In addition to their laboratory role, forensic scientists testify as expert witnesses in both criminal and civil cases and can work for either the prosecution or the defense. While any field could technically be forensic, certain sections have developed over time to encompass the majority of forensically related cases.

Etymology
The term forensic stems from the Latin word, forēnsis (3rd declension, adjective), meaning ""of a forum, place of assembly"". The history of the term originates in Roman times, when a criminal charge meant presenting the case before a group of public individuals in the forum."
What is Abel's test used for?,Determining the derivative of a function,Testing for the convergence of an infinite series,Solving differential equations,Finding the maximum value of a function,Integrating a function,B,"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.
This glossary of calculus is a list of definitions about calculus, its sub-disciplines, and related fields.

A
Abel's test
A method of testing for the convergence of an infinite series.
absolute convergence
An infinite series of numbers is said to converge absolutely (or to be absolutely convergent) if the sum of the absolute values of the summands is finite. More precisely, a real or complex series 
  
    
      
        ∑
        
          n
          =
          0
        
        
          ∞
        
      
      
        a
        
          n
        
      
    
    \textstyle \sum _{n=0}^{\infty }a_{n}
   is said to converge absolutely if 
  
    
      
        ∑
        
          n
          =
          0
        
        
          ∞
        
      
      
        |
        
          a
          
            n
          
        
        |
      
      =
      L
    
    \textstyle \sum _{n=0}^{\infty }\left|a_{n}\right|=L
   for some real number 
  
    
      L
    
    \textstyle L
  . Similarly, an improper integral of a function, 
  
    
      
        ∫
        
          0
        
        
          ∞
        
      
      f
      (
      x
      )
      
      d
      x
    
    \textstyle \int _{0}^{\infty }f(x)\,dx
  , is said to converge absolutely if the integral of the absolute value of the integrand is finite—that is, if 
  
    
      
        ∫
        
          0
        
        
          ∞
        
      
      
        |
        
          f
          (
          x
          )
        
        |
      
      d
      x
      =
      L
      .
    
    \textstyle \int _{0}^{\infty }\left|f(x)\right|dx=L.
  
absolute maximum
The highest value a function attains.
absolute minimum
The lowest value a function attains.
absolute value
The absolute value or modulus |x| of a real number x is the non-negative value of x without regard to its sign. Namely, |x| = x for a positive x, |x| = −x for a negative x (in which case −x is positive), and |0| = 0. For example, the absolute value of 3 is 3, and the absolute value of −3 is also 3."
How is absolute convergence defined?,The sum of the absolute values of the summands is finite,The sum of the absolute values of the summands is infinite,The sum of the absolute values of the summands is zero,The sum of the absolute values of the summands is a complex number,The sum of the absolute values of the summands is undefined,A,"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.
This glossary of calculus is a list of definitions about calculus, its sub-disciplines, and related fields.

A
Abel's test
A method of testing for the convergence of an infinite series.
absolute convergence
An infinite series of numbers is said to converge absolutely (or to be absolutely convergent) if the sum of the absolute values of the summands is finite. More precisely, a real or complex series 
  
    
      
        ∑
        
          n
          =
          0
        
        
          ∞
        
      
      
        a
        
          n
        
      
    
    \textstyle \sum _{n=0}^{\infty }a_{n}
   is said to converge absolutely if 
  
    
      
        ∑
        
          n
          =
          0
        
        
          ∞
        
      
      
        |
        
          a
          
            n
          
        
        |
      
      =
      L
    
    \textstyle \sum _{n=0}^{\infty }\left|a_{n}\right|=L
   for some real number 
  
    
      L
    
    \textstyle L
  . Similarly, an improper integral of a function, 
  
    
      
        ∫
        
          0
        
        
          ∞
        
      
      f
      (
      x
      )
      
      d
      x
    
    \textstyle \int _{0}^{\infty }f(x)\,dx
  , is said to converge absolutely if the integral of the absolute value of the integrand is finite—that is, if 
  
    
      
        ∫
        
          0
        
        
          ∞
        
      
      
        |
        
          f
          (
          x
          )
        
        |
      
      d
      x
      =
      L
      .
    
    \textstyle \int _{0}^{\infty }\left|f(x)\right|dx=L.
  
absolute maximum
The highest value a function attains.
absolute minimum
The lowest value a function attains.
absolute value
The absolute value or modulus |x| of a real number x is the non-negative value of x without regard to its sign. Namely, |x| = x for a positive x, |x| = −x for a negative x (in which case −x is positive), and |0| = 0. For example, the absolute value of 3 is 3, and the absolute value of −3 is also 3."
What is the definition of absolute maximum?,The lowest value a function attains,The highest value a function attains,The derivative of a function,The integral of a function,The rate of change of a function,B,"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.
This glossary of calculus is a list of definitions about calculus, its sub-disciplines, and related fields.

A
Abel's test
A method of testing for the convergence of an infinite series.
absolute convergence
An infinite series of numbers is said to converge absolutely (or to be absolutely convergent) if the sum of the absolute values of the summands is finite. More precisely, a real or complex series 
  
    
      
        ∑
        
          n
          =
          0
        
        
          ∞
        
      
      
        a
        
          n
        
      
    
    \textstyle \sum _{n=0}^{\infty }a_{n}
   is said to converge absolutely if 
  
    
      
        ∑
        
          n
          =
          0
        
        
          ∞
        
      
      
        |
        
          a
          
            n
          
        
        |
      
      =
      L
    
    \textstyle \sum _{n=0}^{\infty }\left|a_{n}\right|=L
   for some real number 
  
    
      L
    
    \textstyle L
  . Similarly, an improper integral of a function, 
  
    
      
        ∫
        
          0
        
        
          ∞
        
      
      f
      (
      x
      )
      
      d
      x
    
    \textstyle \int _{0}^{\infty }f(x)\,dx
  , is said to converge absolutely if the integral of the absolute value of the integrand is finite—that is, if 
  
    
      
        ∫
        
          0
        
        
          ∞
        
      
      
        |
        
          f
          (
          x
          )
        
        |
      
      d
      x
      =
      L
      .
    
    \textstyle \int _{0}^{\infty }\left|f(x)\right|dx=L.
  
absolute maximum
The highest value a function attains.
absolute minimum
The lowest value a function attains.
absolute value
The absolute value or modulus |x| of a real number x is the non-negative value of x without regard to its sign. Namely, |x| = x for a positive x, |x| = −x for a negative x (in which case −x is positive), and |0| = 0. For example, the absolute value of 3 is 3, and the absolute value of −3 is also 3."
What is the definition of absolute minimum?,The highest value a function attains,The lowest value a function attains,The derivative of a function,The integral of a function,The rate of change of a function,B,"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.
This glossary of calculus is a list of definitions about calculus, its sub-disciplines, and related fields.

A
Abel's test
A method of testing for the convergence of an infinite series.
absolute convergence
An infinite series of numbers is said to converge absolutely (or to be absolutely convergent) if the sum of the absolute values of the summands is finite. More precisely, a real or complex series 
  
    
      
        ∑
        
          n
          =
          0
        
        
          ∞
        
      
      
        a
        
          n
        
      
    
    \textstyle \sum _{n=0}^{\infty }a_{n}
   is said to converge absolutely if 
  
    
      
        ∑
        
          n
          =
          0
        
        
          ∞
        
      
      
        |
        
          a
          
            n
          
        
        |
      
      =
      L
    
    \textstyle \sum _{n=0}^{\infty }\left|a_{n}\right|=L
   for some real number 
  
    
      L
    
    \textstyle L
  . Similarly, an improper integral of a function, 
  
    
      
        ∫
        
          0
        
        
          ∞
        
      
      f
      (
      x
      )
      
      d
      x
    
    \textstyle \int _{0}^{\infty }f(x)\,dx
  , is said to converge absolutely if the integral of the absolute value of the integrand is finite—that is, if 
  
    
      
        ∫
        
          0
        
        
          ∞
        
      
      
        |
        
          f
          (
          x
          )
        
        |
      
      d
      x
      =
      L
      .
    
    \textstyle \int _{0}^{\infty }\left|f(x)\right|dx=L.
  
absolute maximum
The highest value a function attains.
absolute minimum
The lowest value a function attains.
absolute value
The absolute value or modulus |x| of a real number x is the non-negative value of x without regard to its sign. Namely, |x| = x for a positive x, |x| = −x for a negative x (in which case −x is positive), and |0| = 0. For example, the absolute value of 3 is 3, and the absolute value of −3 is also 3."
What is the definition of absolute value?,The non-negative value of a real number without regard to its sign,The negative value of a real number without regard to its sign,The positive value of a real number without regard to its sign,The value of a complex number,The value of zero,A,"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.
This glossary of calculus is a list of definitions about calculus, its sub-disciplines, and related fields.

A
Abel's test
A method of testing for the convergence of an infinite series.
absolute convergence
An infinite series of numbers is said to converge absolutely (or to be absolutely convergent) if the sum of the absolute values of the summands is finite. More precisely, a real or complex series 
  
    
      
        ∑
        
          n
          =
          0
        
        
          ∞
        
      
      
        a
        
          n
        
      
    
    \textstyle \sum _{n=0}^{\infty }a_{n}
   is said to converge absolutely if 
  
    
      
        ∑
        
          n
          =
          0
        
        
          ∞
        
      
      
        |
        
          a
          
            n
          
        
        |
      
      =
      L
    
    \textstyle \sum _{n=0}^{\infty }\left|a_{n}\right|=L
   for some real number 
  
    
      L
    
    \textstyle L
  . Similarly, an improper integral of a function, 
  
    
      
        ∫
        
          0
        
        
          ∞
        
      
      f
      (
      x
      )
      
      d
      x
    
    \textstyle \int _{0}^{\infty }f(x)\,dx
  , is said to converge absolutely if the integral of the absolute value of the integrand is finite—that is, if 
  
    
      
        ∫
        
          0
        
        
          ∞
        
      
      
        |
        
          f
          (
          x
          )
        
        |
      
      d
      x
      =
      L
      .
    
    \textstyle \int _{0}^{\infty }\left|f(x)\right|dx=L.
  
absolute maximum
The highest value a function attains.
absolute minimum
The lowest value a function attains.
absolute value
The absolute value or modulus |x| of a real number x is the non-negative value of x without regard to its sign. Namely, |x| = x for a positive x, |x| = −x for a negative x (in which case −x is positive), and |0| = 0. For example, the absolute value of 3 is 3, and the absolute value of −3 is also 3."
What is the main objective of interval arithmetic?,To provide a simple way of calculating exact values of a function,To provide a simple way of calculating upper and lower bounds of a function's range,To provide a simple way of calculating the mean of a function,To provide a simple way of calculating the derivative of a function,To provide a simple way of calculating the integral of a function,B,"Interval arithmetic (also known as interval mathematics; interval analysis or interval computation) is a mathematical technique used to mitigate rounding and measurement errors in mathematical computation by computing function bounds. Numerical methods involving interval arithmetic can guarantee relatively reliable and mathematically correct results. Instead of representing a value as a single number, interval arithmetic represents each value as a range of possibilities.
Mathematically, instead of working with an uncertain real-valued variable 
  
    x
    x
  , interval arithmetic works with an interval 
  
    
      [
      a
      ,
      b
      ]
    
    [a,b]
   that defines the range of values that 
  
    x
    x
   can have. In other words, any value of the variable 
  
    x
    x
   lies in the closed interval between 
  
    a
    a
   and 
  
    b
    b
  . A function 
  
    f
    f
  , when applied to 
  
    x
    x
  , produces an interval 
  
    
      [
      c
      ,
      d
      ]
    
    [c,d]
   which includes all the possible values for 
  
    
      f
      (
      x
      )
    
    f(x)
   for all 
  
    
      x
      ∈
      [
      a
      ,
      b
      ]
    
    x\in [a,b]
  .
Interval arithmetic is suitable for a variety of purposes; the most common use is in scientific works, particularly when the calculations are handled by software, where it is used to keep track of rounding errors in calculations and of uncertainties in the knowledge of the exact values of physical and technical parameters. The latter often arise from measurement errors and tolerances for components or due to limits on computational accuracy. Interval arithmetic also helps find guaranteed solutions to equations (such as differential equations) and optimization problems.

Introduction
The main objective of interval arithmetic is to provide a simple way of calculating upper and lower bounds of a function's range in one or more variables."
What is the purpose of interval arithmetic in scientific works?,To calculate exact values of physical and technical parameters,To find guaranteed solutions to equations,To keep track of rounding errors and uncertainties in calculations,To optimize mathematical functions,To calculate the mean of a function's range,C,"Interval arithmetic (also known as interval mathematics; interval analysis or interval computation) is a mathematical technique used to mitigate rounding and measurement errors in mathematical computation by computing function bounds. Numerical methods involving interval arithmetic can guarantee relatively reliable and mathematically correct results. Instead of representing a value as a single number, interval arithmetic represents each value as a range of possibilities.
Mathematically, instead of working with an uncertain real-valued variable 
  
    x
    x
  , interval arithmetic works with an interval 
  
    
      [
      a
      ,
      b
      ]
    
    [a,b]
   that defines the range of values that 
  
    x
    x
   can have. In other words, any value of the variable 
  
    x
    x
   lies in the closed interval between 
  
    a
    a
   and 
  
    b
    b
  . A function 
  
    f
    f
  , when applied to 
  
    x
    x
  , produces an interval 
  
    
      [
      c
      ,
      d
      ]
    
    [c,d]
   which includes all the possible values for 
  
    
      f
      (
      x
      )
    
    f(x)
   for all 
  
    
      x
      ∈
      [
      a
      ,
      b
      ]
    
    x\in [a,b]
  .
Interval arithmetic is suitable for a variety of purposes; the most common use is in scientific works, particularly when the calculations are handled by software, where it is used to keep track of rounding errors in calculations and of uncertainties in the knowledge of the exact values of physical and technical parameters. The latter often arise from measurement errors and tolerances for components or due to limits on computational accuracy. Interval arithmetic also helps find guaranteed solutions to equations (such as differential equations) and optimization problems.

Introduction
The main objective of interval arithmetic is to provide a simple way of calculating upper and lower bounds of a function's range in one or more variables."
What is an interval in interval arithmetic?,A range of values that a variable can have,A single number that represents a value,The exact value of a physical or technical parameter,The uncertainty in the knowledge of the exact values of variables,The upper and lower bounds of a function's range,A,"Interval arithmetic (also known as interval mathematics; interval analysis or interval computation) is a mathematical technique used to mitigate rounding and measurement errors in mathematical computation by computing function bounds. Numerical methods involving interval arithmetic can guarantee relatively reliable and mathematically correct results. Instead of representing a value as a single number, interval arithmetic represents each value as a range of possibilities.
Mathematically, instead of working with an uncertain real-valued variable 
  
    x
    x
  , interval arithmetic works with an interval 
  
    
      [
      a
      ,
      b
      ]
    
    [a,b]
   that defines the range of values that 
  
    x
    x
   can have. In other words, any value of the variable 
  
    x
    x
   lies in the closed interval between 
  
    a
    a
   and 
  
    b
    b
  . A function 
  
    f
    f
  , when applied to 
  
    x
    x
  , produces an interval 
  
    
      [
      c
      ,
      d
      ]
    
    [c,d]
   which includes all the possible values for 
  
    
      f
      (
      x
      )
    
    f(x)
   for all 
  
    
      x
      ∈
      [
      a
      ,
      b
      ]
    
    x\in [a,b]
  .
Interval arithmetic is suitable for a variety of purposes; the most common use is in scientific works, particularly when the calculations are handled by software, where it is used to keep track of rounding errors in calculations and of uncertainties in the knowledge of the exact values of physical and technical parameters. The latter often arise from measurement errors and tolerances for components or due to limits on computational accuracy. Interval arithmetic also helps find guaranteed solutions to equations (such as differential equations) and optimization problems.

Introduction
The main objective of interval arithmetic is to provide a simple way of calculating upper and lower bounds of a function's range in one or more variables."
What does interval arithmetic help to mitigate in mathematical computation?,Measurement errors in physical and technical parameters,Rounding errors in calculations,Uncertainties in the knowledge of the exact values of variables,Limits on computational accuracy,All of the above,E,"Interval arithmetic (also known as interval mathematics; interval analysis or interval computation) is a mathematical technique used to mitigate rounding and measurement errors in mathematical computation by computing function bounds. Numerical methods involving interval arithmetic can guarantee relatively reliable and mathematically correct results. Instead of representing a value as a single number, interval arithmetic represents each value as a range of possibilities.
Mathematically, instead of working with an uncertain real-valued variable 
  
    x
    x
  , interval arithmetic works with an interval 
  
    
      [
      a
      ,
      b
      ]
    
    [a,b]
   that defines the range of values that 
  
    x
    x
   can have. In other words, any value of the variable 
  
    x
    x
   lies in the closed interval between 
  
    a
    a
   and 
  
    b
    b
  . A function 
  
    f
    f
  , when applied to 
  
    x
    x
  , produces an interval 
  
    
      [
      c
      ,
      d
      ]
    
    [c,d]
   which includes all the possible values for 
  
    
      f
      (
      x
      )
    
    f(x)
   for all 
  
    
      x
      ∈
      [
      a
      ,
      b
      ]
    
    x\in [a,b]
  .
Interval arithmetic is suitable for a variety of purposes; the most common use is in scientific works, particularly when the calculations are handled by software, where it is used to keep track of rounding errors in calculations and of uncertainties in the knowledge of the exact values of physical and technical parameters. The latter often arise from measurement errors and tolerances for components or due to limits on computational accuracy. Interval arithmetic also helps find guaranteed solutions to equations (such as differential equations) and optimization problems.

Introduction
The main objective of interval arithmetic is to provide a simple way of calculating upper and lower bounds of a function's range in one or more variables."
When is interval arithmetic commonly used?,In scientific works,In optimization problems,In finding guaranteed solutions to equations,In calculating the mean of a function,In calculating the derivative of a function,A,"Interval arithmetic (also known as interval mathematics; interval analysis or interval computation) is a mathematical technique used to mitigate rounding and measurement errors in mathematical computation by computing function bounds. Numerical methods involving interval arithmetic can guarantee relatively reliable and mathematically correct results. Instead of representing a value as a single number, interval arithmetic represents each value as a range of possibilities.
Mathematically, instead of working with an uncertain real-valued variable 
  
    x
    x
  , interval arithmetic works with an interval 
  
    
      [
      a
      ,
      b
      ]
    
    [a,b]
   that defines the range of values that 
  
    x
    x
   can have. In other words, any value of the variable 
  
    x
    x
   lies in the closed interval between 
  
    a
    a
   and 
  
    b
    b
  . A function 
  
    f
    f
  , when applied to 
  
    x
    x
  , produces an interval 
  
    
      [
      c
      ,
      d
      ]
    
    [c,d]
   which includes all the possible values for 
  
    
      f
      (
      x
      )
    
    f(x)
   for all 
  
    
      x
      ∈
      [
      a
      ,
      b
      ]
    
    x\in [a,b]
  .
Interval arithmetic is suitable for a variety of purposes; the most common use is in scientific works, particularly when the calculations are handled by software, where it is used to keep track of rounding errors in calculations and of uncertainties in the knowledge of the exact values of physical and technical parameters. The latter often arise from measurement errors and tolerances for components or due to limits on computational accuracy. Interval arithmetic also helps find guaranteed solutions to equations (such as differential equations) and optimization problems.

Introduction
The main objective of interval arithmetic is to provide a simple way of calculating upper and lower bounds of a function's range in one or more variables."
Where did Sandy Munro start his engineering career?,Ford Motor Company,Munro & Associates,Valiant Machine & Tool company,General Motors,"Troy, Michigan",C,"Sandy Munro is an automotive engineer who specializes in machine tools and manufacturing.
He started as a toolmaker at the Valiant Machine & Tool company – a General Motors supplier in Windsor.  In 1978, he joined the Ford Motor Company where he improved methods of engine assembly.
In 1988, he started his own consultancy, Munro & Associates, in Troy, Michigan, specializing in lean design, tearing down automotive products to study and suggest improvements and innovations. Now located in Auburn Hills, Michigan, the company performs electric vehicle benchmarking and consults in the aerospace, defense and medical sectors.In 2018, he started broadcasting video analyses and interviews on his YouTube channel, Munro Live. The channel has over 300,000 subscribers and raised the profile of his consultancy during the COVID-19 pandemic, when meetings and trade shows were restricted.

Early life
Munro was born on 19. January 1949 and grew up in Windsor, Ontario, Canada.  Munro said he first started working by picking tomatoes at age 9.  He started his engineering career as a toolmaker at the Valiant Machine & Tool company which mainly supplied General Motors."
What does Munro & Associates specialize in?,Electric vehicle benchmarking,Aerospace and defense,Medical sector,Lean design and tearing down automotive products,YouTube channel,D,"Sandy Munro is an automotive engineer who specializes in machine tools and manufacturing.
He started as a toolmaker at the Valiant Machine & Tool company – a General Motors supplier in Windsor.  In 1978, he joined the Ford Motor Company where he improved methods of engine assembly.
In 1988, he started his own consultancy, Munro & Associates, in Troy, Michigan, specializing in lean design, tearing down automotive products to study and suggest improvements and innovations. Now located in Auburn Hills, Michigan, the company performs electric vehicle benchmarking and consults in the aerospace, defense and medical sectors.In 2018, he started broadcasting video analyses and interviews on his YouTube channel, Munro Live. The channel has over 300,000 subscribers and raised the profile of his consultancy during the COVID-19 pandemic, when meetings and trade shows were restricted.

Early life
Munro was born on 19. January 1949 and grew up in Windsor, Ontario, Canada.  Munro said he first started working by picking tomatoes at age 9.  He started his engineering career as a toolmaker at the Valiant Machine & Tool company which mainly supplied General Motors."
In which year did Sandy Munro start his own consultancy?,1978,1988,2018,1949,"300,000",B,"Sandy Munro is an automotive engineer who specializes in machine tools and manufacturing.
He started as a toolmaker at the Valiant Machine & Tool company – a General Motors supplier in Windsor.  In 1978, he joined the Ford Motor Company where he improved methods of engine assembly.
In 1988, he started his own consultancy, Munro & Associates, in Troy, Michigan, specializing in lean design, tearing down automotive products to study and suggest improvements and innovations. Now located in Auburn Hills, Michigan, the company performs electric vehicle benchmarking and consults in the aerospace, defense and medical sectors.In 2018, he started broadcasting video analyses and interviews on his YouTube channel, Munro Live. The channel has over 300,000 subscribers and raised the profile of his consultancy during the COVID-19 pandemic, when meetings and trade shows were restricted.

Early life
Munro was born on 19. January 1949 and grew up in Windsor, Ontario, Canada.  Munro said he first started working by picking tomatoes at age 9.  He started his engineering career as a toolmaker at the Valiant Machine & Tool company which mainly supplied General Motors."
Where is Munro & Associates currently located?,"Windsor, Ontario, Canada","Troy, Michigan","Auburn Hills, Michigan",General Motors,COVID-19 pandemic,C,"Sandy Munro is an automotive engineer who specializes in machine tools and manufacturing.
He started as a toolmaker at the Valiant Machine & Tool company – a General Motors supplier in Windsor.  In 1978, he joined the Ford Motor Company where he improved methods of engine assembly.
In 1988, he started his own consultancy, Munro & Associates, in Troy, Michigan, specializing in lean design, tearing down automotive products to study and suggest improvements and innovations. Now located in Auburn Hills, Michigan, the company performs electric vehicle benchmarking and consults in the aerospace, defense and medical sectors.In 2018, he started broadcasting video analyses and interviews on his YouTube channel, Munro Live. The channel has over 300,000 subscribers and raised the profile of his consultancy during the COVID-19 pandemic, when meetings and trade shows were restricted.

Early life
Munro was born on 19. January 1949 and grew up in Windsor, Ontario, Canada.  Munro said he first started working by picking tomatoes at age 9.  He started his engineering career as a toolmaker at the Valiant Machine & Tool company which mainly supplied General Motors."
What did Sandy Munro do to raise the profile of his consultancy during the COVID-19 pandemic?,Started consulting in the aerospace and defense sectors,Started broadcasting video analyses and interviews on his YouTube channel,Performed electric vehicle benchmarking,"Moved his consultancy to Auburn Hills, Michigan",Joined the Ford Motor Company,B,"Sandy Munro is an automotive engineer who specializes in machine tools and manufacturing.
He started as a toolmaker at the Valiant Machine & Tool company – a General Motors supplier in Windsor.  In 1978, he joined the Ford Motor Company where he improved methods of engine assembly.
In 1988, he started his own consultancy, Munro & Associates, in Troy, Michigan, specializing in lean design, tearing down automotive products to study and suggest improvements and innovations. Now located in Auburn Hills, Michigan, the company performs electric vehicle benchmarking and consults in the aerospace, defense and medical sectors.In 2018, he started broadcasting video analyses and interviews on his YouTube channel, Munro Live. The channel has over 300,000 subscribers and raised the profile of his consultancy during the COVID-19 pandemic, when meetings and trade shows were restricted.

Early life
Munro was born on 19. January 1949 and grew up in Windsor, Ontario, Canada.  Munro said he first started working by picking tomatoes at age 9.  He started his engineering career as a toolmaker at the Valiant Machine & Tool company which mainly supplied General Motors."
What is process design?,The choice and sequencing of units for desired physical and/or chemical transformation of materials,The design of new facilities,The modification or expansion of existing facilities,The design of unit operations,The design of fabrication and construction plans,A,"In chemical engineering, process design is the choice and sequencing of units for desired physical and/or chemical transformation of materials. Process design is central to chemical engineering, and it can be considered to be the summit of that field, bringing together all of the field's components.
Process design can be the design of new facilities or it can be the modification or expansion of existing facilities.  The design starts at a conceptual level and ultimately ends in the form of fabrication and construction plans.
Process design is distinct from equipment design, which is closer in spirit to the design of unit operations.  Processes often include many unit operations.

Documentation
Process design documents serve to define the design and they ensure that the design components fit together.  They are useful in communicating ideas and plans to other engineers involved with the design, to external regulatory agencies, to equipment vendors and to construction contractors.
In order of increasing detail, process design documents include:

Block flow diagrams (BFD): Very simple diagrams composed of rectangles and lines indicating major material or energy flows.
Process flow diagrams (PFD): Typically more complex  diagrams of major unit operations as well as flow lines.  They usually include a material balance, and sometimes an energy balance, showing typical or design flowrates, stream compositions, and stream and equipment pressures and temperatures.
Piping and instrumentation diagrams (P&ID): Diagrams showing each and every pipeline with piping class (carbon steel or stainless steel) and pipe size (diameter). They also show valving along with instrument locations and process control schemes.
Specifications: Written design requirements of all major equipment items.Process designers typically write operating manuals on how to start-up, operate and shut-down the process."
What is the purpose of process design documents?,To define the design and ensure that the design components fit together,"To communicate ideas and plans to other engineers, regulatory agencies, equipment vendors, and construction contractors","To provide operating manuals on how to start-up, operate, and shut-down the process",To show valving along with instrument locations and process control schemes,To specify written design requirements of all major equipment items,A,"In chemical engineering, process design is the choice and sequencing of units for desired physical and/or chemical transformation of materials. Process design is central to chemical engineering, and it can be considered to be the summit of that field, bringing together all of the field's components.
Process design can be the design of new facilities or it can be the modification or expansion of existing facilities.  The design starts at a conceptual level and ultimately ends in the form of fabrication and construction plans.
Process design is distinct from equipment design, which is closer in spirit to the design of unit operations.  Processes often include many unit operations.

Documentation
Process design documents serve to define the design and they ensure that the design components fit together.  They are useful in communicating ideas and plans to other engineers involved with the design, to external regulatory agencies, to equipment vendors and to construction contractors.
In order of increasing detail, process design documents include:

Block flow diagrams (BFD): Very simple diagrams composed of rectangles and lines indicating major material or energy flows.
Process flow diagrams (PFD): Typically more complex  diagrams of major unit operations as well as flow lines.  They usually include a material balance, and sometimes an energy balance, showing typical or design flowrates, stream compositions, and stream and equipment pressures and temperatures.
Piping and instrumentation diagrams (P&ID): Diagrams showing each and every pipeline with piping class (carbon steel or stainless steel) and pipe size (diameter). They also show valving along with instrument locations and process control schemes.
Specifications: Written design requirements of all major equipment items.Process designers typically write operating manuals on how to start-up, operate and shut-down the process."
What is a block flow diagram?,A diagram showing each and every pipeline with piping class and pipe size,A very simple diagram composed of rectangles and lines indicating major material or energy flows,A diagram showing major unit operations as well as flow lines,A diagram showing valving along with instrument locations and process control schemes,A written design requirement of all major equipment items,B,"In chemical engineering, process design is the choice and sequencing of units for desired physical and/or chemical transformation of materials. Process design is central to chemical engineering, and it can be considered to be the summit of that field, bringing together all of the field's components.
Process design can be the design of new facilities or it can be the modification or expansion of existing facilities.  The design starts at a conceptual level and ultimately ends in the form of fabrication and construction plans.
Process design is distinct from equipment design, which is closer in spirit to the design of unit operations.  Processes often include many unit operations.

Documentation
Process design documents serve to define the design and they ensure that the design components fit together.  They are useful in communicating ideas and plans to other engineers involved with the design, to external regulatory agencies, to equipment vendors and to construction contractors.
In order of increasing detail, process design documents include:

Block flow diagrams (BFD): Very simple diagrams composed of rectangles and lines indicating major material or energy flows.
Process flow diagrams (PFD): Typically more complex  diagrams of major unit operations as well as flow lines.  They usually include a material balance, and sometimes an energy balance, showing typical or design flowrates, stream compositions, and stream and equipment pressures and temperatures.
Piping and instrumentation diagrams (P&ID): Diagrams showing each and every pipeline with piping class (carbon steel or stainless steel) and pipe size (diameter). They also show valving along with instrument locations and process control schemes.
Specifications: Written design requirements of all major equipment items.Process designers typically write operating manuals on how to start-up, operate and shut-down the process."
What is a process flow diagram (PFD)?,A diagram showing each and every pipeline with piping class and pipe size,A very simple diagram composed of rectangles and lines indicating major material or energy flows,A diagram showing major unit operations as well as flow lines,A diagram showing valving along with instrument locations and process control schemes,A written design requirement of all major equipment items,C,"In chemical engineering, process design is the choice and sequencing of units for desired physical and/or chemical transformation of materials. Process design is central to chemical engineering, and it can be considered to be the summit of that field, bringing together all of the field's components.
Process design can be the design of new facilities or it can be the modification or expansion of existing facilities.  The design starts at a conceptual level and ultimately ends in the form of fabrication and construction plans.
Process design is distinct from equipment design, which is closer in spirit to the design of unit operations.  Processes often include many unit operations.

Documentation
Process design documents serve to define the design and they ensure that the design components fit together.  They are useful in communicating ideas and plans to other engineers involved with the design, to external regulatory agencies, to equipment vendors and to construction contractors.
In order of increasing detail, process design documents include:

Block flow diagrams (BFD): Very simple diagrams composed of rectangles and lines indicating major material or energy flows.
Process flow diagrams (PFD): Typically more complex  diagrams of major unit operations as well as flow lines.  They usually include a material balance, and sometimes an energy balance, showing typical or design flowrates, stream compositions, and stream and equipment pressures and temperatures.
Piping and instrumentation diagrams (P&ID): Diagrams showing each and every pipeline with piping class (carbon steel or stainless steel) and pipe size (diameter). They also show valving along with instrument locations and process control schemes.
Specifications: Written design requirements of all major equipment items.Process designers typically write operating manuals on how to start-up, operate and shut-down the process."
What are piping and instrumentation diagrams (P&ID)?,Diagrams showing each and every pipeline with piping class and pipe size,Very simple diagrams composed of rectangles and lines indicating major material or energy flows,Diagrams showing major unit operations as well as flow lines,Diagrams showing valving along with instrument locations and process control schemes,Written design requirements of all major equipment items,D,"In chemical engineering, process design is the choice and sequencing of units for desired physical and/or chemical transformation of materials. Process design is central to chemical engineering, and it can be considered to be the summit of that field, bringing together all of the field's components.
Process design can be the design of new facilities or it can be the modification or expansion of existing facilities.  The design starts at a conceptual level and ultimately ends in the form of fabrication and construction plans.
Process design is distinct from equipment design, which is closer in spirit to the design of unit operations.  Processes often include many unit operations.

Documentation
Process design documents serve to define the design and they ensure that the design components fit together.  They are useful in communicating ideas and plans to other engineers involved with the design, to external regulatory agencies, to equipment vendors and to construction contractors.
In order of increasing detail, process design documents include:

Block flow diagrams (BFD): Very simple diagrams composed of rectangles and lines indicating major material or energy flows.
Process flow diagrams (PFD): Typically more complex  diagrams of major unit operations as well as flow lines.  They usually include a material balance, and sometimes an energy balance, showing typical or design flowrates, stream compositions, and stream and equipment pressures and temperatures.
Piping and instrumentation diagrams (P&ID): Diagrams showing each and every pipeline with piping class (carbon steel or stainless steel) and pipe size (diameter). They also show valving along with instrument locations and process control schemes.
Specifications: Written design requirements of all major equipment items.Process designers typically write operating manuals on how to start-up, operate and shut-down the process."
What are the two primary types of marine heat exchangers used aboard vessels in the maritime industry?,Plate and tube heat exchangers,Shell and tube heat exchangers,Plate and shell heat exchangers,Parallel and compressed heat exchangers,Marine and non-marine heat exchangers,C,"Marine heat exchangers are no different than non-marine heat exchangers except for the simple fact that they are found aboard ships. Heat exchangers can be used for a wide variety of uses. As the name implies, these can be used for heating as well as cooling. The two primary types of marine heat exchangers used aboard vessels in the maritime industry are plate, and shell and tube.  Maintenance for heat exchangers prevents fouling and galvanic corrosion from dissimilar metals.

Types
Though there are many more types of heat exchangers that are used shore side, plate and shell and tube heat exchangers are the most common type of heat exchangers found aboard ocean-going vessels.

Plate
Plate-type marine heat exchangers are designed with sets of multiple parallel plates that are compressed to form the main cooler unit. This type has a small footprint in comparison to other types of heat exchangers.  The plates are designed in such a way that when placed next to each other they create passageways to the fluid to flow between the plates."
What is the main advantage of plate-type marine heat exchangers?,They have a small footprint,They are more efficient than other types of heat exchangers,They are less prone to fouling and galvanic corrosion,They can be used for both heating and cooling,They are cheaper to maintain,A,"Marine heat exchangers are no different than non-marine heat exchangers except for the simple fact that they are found aboard ships. Heat exchangers can be used for a wide variety of uses. As the name implies, these can be used for heating as well as cooling. The two primary types of marine heat exchangers used aboard vessels in the maritime industry are plate, and shell and tube.  Maintenance for heat exchangers prevents fouling and galvanic corrosion from dissimilar metals.

Types
Though there are many more types of heat exchangers that are used shore side, plate and shell and tube heat exchangers are the most common type of heat exchangers found aboard ocean-going vessels.

Plate
Plate-type marine heat exchangers are designed with sets of multiple parallel plates that are compressed to form the main cooler unit. This type has a small footprint in comparison to other types of heat exchangers.  The plates are designed in such a way that when placed next to each other they create passageways to the fluid to flow between the plates."
What is the purpose of maintenance for heat exchangers?,To increase their efficiency,To prevent fouling and galvanic corrosion,To improve their cooling capacity,To reduce their footprint,To make them compatible with non-marine applications,B,"Marine heat exchangers are no different than non-marine heat exchangers except for the simple fact that they are found aboard ships. Heat exchangers can be used for a wide variety of uses. As the name implies, these can be used for heating as well as cooling. The two primary types of marine heat exchangers used aboard vessels in the maritime industry are plate, and shell and tube.  Maintenance for heat exchangers prevents fouling and galvanic corrosion from dissimilar metals.

Types
Though there are many more types of heat exchangers that are used shore side, plate and shell and tube heat exchangers are the most common type of heat exchangers found aboard ocean-going vessels.

Plate
Plate-type marine heat exchangers are designed with sets of multiple parallel plates that are compressed to form the main cooler unit. This type has a small footprint in comparison to other types of heat exchangers.  The plates are designed in such a way that when placed next to each other they create passageways to the fluid to flow between the plates."
What determines the passageways for fluid flow in plate-type marine heat exchangers?,The compression of the plates,The design of the plates,The temperature of the fluid,The type of metal used in the plates,The pressure inside the heat exchanger,B,"Marine heat exchangers are no different than non-marine heat exchangers except for the simple fact that they are found aboard ships. Heat exchangers can be used for a wide variety of uses. As the name implies, these can be used for heating as well as cooling. The two primary types of marine heat exchangers used aboard vessels in the maritime industry are plate, and shell and tube.  Maintenance for heat exchangers prevents fouling and galvanic corrosion from dissimilar metals.

Types
Though there are many more types of heat exchangers that are used shore side, plate and shell and tube heat exchangers are the most common type of heat exchangers found aboard ocean-going vessels.

Plate
Plate-type marine heat exchangers are designed with sets of multiple parallel plates that are compressed to form the main cooler unit. This type has a small footprint in comparison to other types of heat exchangers.  The plates are designed in such a way that when placed next to each other they create passageways to the fluid to flow between the plates."
What is the most common type of heat exchangers found aboard ocean-going vessels?,Parallel heat exchangers,Compressed heat exchangers,Shore side heat exchangers,Plate-type heat exchangers,Tube heat exchangers,D,"Marine heat exchangers are no different than non-marine heat exchangers except for the simple fact that they are found aboard ships. Heat exchangers can be used for a wide variety of uses. As the name implies, these can be used for heating as well as cooling. The two primary types of marine heat exchangers used aboard vessels in the maritime industry are plate, and shell and tube.  Maintenance for heat exchangers prevents fouling and galvanic corrosion from dissimilar metals.

Types
Though there are many more types of heat exchangers that are used shore side, plate and shell and tube heat exchangers are the most common type of heat exchangers found aboard ocean-going vessels.

Plate
Plate-type marine heat exchangers are designed with sets of multiple parallel plates that are compressed to form the main cooler unit. This type has a small footprint in comparison to other types of heat exchangers.  The plates are designed in such a way that when placed next to each other they create passageways to the fluid to flow between the plates."
When was Nature founded?,1869,1980,2012,1969,2000,A,"Nature is a British weekly scientific journal founded and based in London, England. As a multidisciplinary publication, Nature features peer-reviewed research from a variety of academic disciplines, mainly in science and technology. It has core editorial offices across the United States, continental Europe, and Asia under the international scientific publishing company Springer Nature. Nature was one of the world's most cited scientific journals by the Science Edition of the 2022 Journal Citation Reports (with an ascribed impact factor of 64.8), making it one of the world's most-read and most prestigious academic journals. As of 2012, it claimed an online readership of about three million unique readers per month.Founded in autumn 1869, Nature was first circulated by Norman Lockyer and Alexander Macmillan as a public forum for scientific innovations. The mid-20th century facilitated an editorial expansion for the journal; Nature redoubled its efforts in explanatory and scientific journalism. The late 1980s and early 1990s saw the creation of a network of editorial offices outside of Britain and the establishment of ten new supplementary, speciality publications (e.g."
Where is Nature based?,"Paris, France","New York, USA","Tokyo, Japan","London, England","Berlin, Germany",D,"Nature is a British weekly scientific journal founded and based in London, England. As a multidisciplinary publication, Nature features peer-reviewed research from a variety of academic disciplines, mainly in science and technology. It has core editorial offices across the United States, continental Europe, and Asia under the international scientific publishing company Springer Nature. Nature was one of the world's most cited scientific journals by the Science Edition of the 2022 Journal Citation Reports (with an ascribed impact factor of 64.8), making it one of the world's most-read and most prestigious academic journals. As of 2012, it claimed an online readership of about three million unique readers per month.Founded in autumn 1869, Nature was first circulated by Norman Lockyer and Alexander Macmillan as a public forum for scientific innovations. The mid-20th century facilitated an editorial expansion for the journal; Nature redoubled its efforts in explanatory and scientific journalism. The late 1980s and early 1990s saw the creation of a network of editorial offices outside of Britain and the establishment of ten new supplementary, speciality publications (e.g."
Which publishing company is Nature under?,Springer Nature,Elsevier,Wiley,Taylor & Francis,Oxford University Press,A,"Nature is a British weekly scientific journal founded and based in London, England. As a multidisciplinary publication, Nature features peer-reviewed research from a variety of academic disciplines, mainly in science and technology. It has core editorial offices across the United States, continental Europe, and Asia under the international scientific publishing company Springer Nature. Nature was one of the world's most cited scientific journals by the Science Edition of the 2022 Journal Citation Reports (with an ascribed impact factor of 64.8), making it one of the world's most-read and most prestigious academic journals. As of 2012, it claimed an online readership of about three million unique readers per month.Founded in autumn 1869, Nature was first circulated by Norman Lockyer and Alexander Macmillan as a public forum for scientific innovations. The mid-20th century facilitated an editorial expansion for the journal; Nature redoubled its efforts in explanatory and scientific journalism. The late 1980s and early 1990s saw the creation of a network of editorial offices outside of Britain and the establishment of ten new supplementary, speciality publications (e.g."
What is the readership of Nature as of 2012?,1 million unique readers per month,2 million unique readers per month,3 million unique readers per month,4 million unique readers per month,5 million unique readers per month,C,"Nature is a British weekly scientific journal founded and based in London, England. As a multidisciplinary publication, Nature features peer-reviewed research from a variety of academic disciplines, mainly in science and technology. It has core editorial offices across the United States, continental Europe, and Asia under the international scientific publishing company Springer Nature. Nature was one of the world's most cited scientific journals by the Science Edition of the 2022 Journal Citation Reports (with an ascribed impact factor of 64.8), making it one of the world's most-read and most prestigious academic journals. As of 2012, it claimed an online readership of about three million unique readers per month.Founded in autumn 1869, Nature was first circulated by Norman Lockyer and Alexander Macmillan as a public forum for scientific innovations. The mid-20th century facilitated an editorial expansion for the journal; Nature redoubled its efforts in explanatory and scientific journalism. The late 1980s and early 1990s saw the creation of a network of editorial offices outside of Britain and the establishment of ten new supplementary, speciality publications (e.g."
What is the impact factor of Nature according to the Science Edition of the 2022 Journal Citation Reports?,30.5,45.2,64.8,82.1,95.6,C,"Nature is a British weekly scientific journal founded and based in London, England. As a multidisciplinary publication, Nature features peer-reviewed research from a variety of academic disciplines, mainly in science and technology. It has core editorial offices across the United States, continental Europe, and Asia under the international scientific publishing company Springer Nature. Nature was one of the world's most cited scientific journals by the Science Edition of the 2022 Journal Citation Reports (with an ascribed impact factor of 64.8), making it one of the world's most-read and most prestigious academic journals. As of 2012, it claimed an online readership of about three million unique readers per month.Founded in autumn 1869, Nature was first circulated by Norman Lockyer and Alexander Macmillan as a public forum for scientific innovations. The mid-20th century facilitated an editorial expansion for the journal; Nature redoubled its efforts in explanatory and scientific journalism. The late 1980s and early 1990s saw the creation of a network of editorial offices outside of Britain and the establishment of ten new supplementary, speciality publications (e.g."
What is the definition of a materials property?,A property that depends on the amount of the material,A property that is a function of temperature,A physical property that does not depend on the amount of the material,A property that varies according to the direction in the material,A property that behaves linearly in a given operating range,C,"A materials property is an intensive property of a material, i.e., a physical property that does not depend on the amount of the material. These quantitative properties may be used as a metric by which the benefits of one material versus another can be compared, thereby aiding in materials selection.
A property may be a constant or may be a function of one or more independent variables, such as temperature. Materials properties often vary to some degree according to the direction in the material in which they are measured, a condition referred to as anisotropy. Materials properties that relate to different physical phenomena often behave linearly (or approximately so) in a given operating range. Modeling them as linear functions can significantly simplify the differential constitutive equations that are used to describe the property.
Equations describing relevant materials properties are often used to predict the attributes of a system.
The properties are measured by standardized test methods. Many such methods have been documented by their respective user communities and published through the Internet; see ASTM International.

Acoustical properties
Acoustical absorption
Speed of sound
Sound reflection
Sound transfer
Third order elasticity (Acoustoelastic effect)

Atomic properties
Atomic mass: (applies to each element) the average mass of the atoms of an element, in daltons (Da), a.k.a. atomic mass units (amu).
Atomic number: (applies to individual atoms or pure elements) the number of protons in each nucleus
Relative atomic mass, a.k.a."
What is anisotropy in materials properties?,A condition in which properties vary with temperature,A condition in which properties vary with direction in the material,A condition in which properties are constant,A condition in which properties are function of one or more independent variables,A condition in which properties behave linearly in a given operating range,B,"A materials property is an intensive property of a material, i.e., a physical property that does not depend on the amount of the material. These quantitative properties may be used as a metric by which the benefits of one material versus another can be compared, thereby aiding in materials selection.
A property may be a constant or may be a function of one or more independent variables, such as temperature. Materials properties often vary to some degree according to the direction in the material in which they are measured, a condition referred to as anisotropy. Materials properties that relate to different physical phenomena often behave linearly (or approximately so) in a given operating range. Modeling them as linear functions can significantly simplify the differential constitutive equations that are used to describe the property.
Equations describing relevant materials properties are often used to predict the attributes of a system.
The properties are measured by standardized test methods. Many such methods have been documented by their respective user communities and published through the Internet; see ASTM International.

Acoustical properties
Acoustical absorption
Speed of sound
Sound reflection
Sound transfer
Third order elasticity (Acoustoelastic effect)

Atomic properties
Atomic mass: (applies to each element) the average mass of the atoms of an element, in daltons (Da), a.k.a. atomic mass units (amu).
Atomic number: (applies to individual atoms or pure elements) the number of protons in each nucleus
Relative atomic mass, a.k.a."
Why are equations describing relevant materials properties often used?,To predict the attributes of a system,To simplify the differential constitutive equations,To compare different materials,To measure standardized test methods,To model linear functions,A,"A materials property is an intensive property of a material, i.e., a physical property that does not depend on the amount of the material. These quantitative properties may be used as a metric by which the benefits of one material versus another can be compared, thereby aiding in materials selection.
A property may be a constant or may be a function of one or more independent variables, such as temperature. Materials properties often vary to some degree according to the direction in the material in which they are measured, a condition referred to as anisotropy. Materials properties that relate to different physical phenomena often behave linearly (or approximately so) in a given operating range. Modeling them as linear functions can significantly simplify the differential constitutive equations that are used to describe the property.
Equations describing relevant materials properties are often used to predict the attributes of a system.
The properties are measured by standardized test methods. Many such methods have been documented by their respective user communities and published through the Internet; see ASTM International.

Acoustical properties
Acoustical absorption
Speed of sound
Sound reflection
Sound transfer
Third order elasticity (Acoustoelastic effect)

Atomic properties
Atomic mass: (applies to each element) the average mass of the atoms of an element, in daltons (Da), a.k.a. atomic mass units (amu).
Atomic number: (applies to individual atoms or pure elements) the number of protons in each nucleus
Relative atomic mass, a.k.a."
What is the atomic mass of an element?,The average mass of the atoms of an element,The number of protons in each nucleus,The mass of each atom in daltons,The relative mass of an atom in atomic mass units,The mass of the electrons in each atom,A,"A materials property is an intensive property of a material, i.e., a physical property that does not depend on the amount of the material. These quantitative properties may be used as a metric by which the benefits of one material versus another can be compared, thereby aiding in materials selection.
A property may be a constant or may be a function of one or more independent variables, such as temperature. Materials properties often vary to some degree according to the direction in the material in which they are measured, a condition referred to as anisotropy. Materials properties that relate to different physical phenomena often behave linearly (or approximately so) in a given operating range. Modeling them as linear functions can significantly simplify the differential constitutive equations that are used to describe the property.
Equations describing relevant materials properties are often used to predict the attributes of a system.
The properties are measured by standardized test methods. Many such methods have been documented by their respective user communities and published through the Internet; see ASTM International.

Acoustical properties
Acoustical absorption
Speed of sound
Sound reflection
Sound transfer
Third order elasticity (Acoustoelastic effect)

Atomic properties
Atomic mass: (applies to each element) the average mass of the atoms of an element, in daltons (Da), a.k.a. atomic mass units (amu).
Atomic number: (applies to individual atoms or pure elements) the number of protons in each nucleus
Relative atomic mass, a.k.a."
What is the atomic number of an atom or pure element?,The average mass of the atoms of an element,The number of protons in each nucleus,The mass of each atom in daltons,The relative mass of an atom in atomic mass units,The mass of the electrons in each atom,B,"A materials property is an intensive property of a material, i.e., a physical property that does not depend on the amount of the material. These quantitative properties may be used as a metric by which the benefits of one material versus another can be compared, thereby aiding in materials selection.
A property may be a constant or may be a function of one or more independent variables, such as temperature. Materials properties often vary to some degree according to the direction in the material in which they are measured, a condition referred to as anisotropy. Materials properties that relate to different physical phenomena often behave linearly (or approximately so) in a given operating range. Modeling them as linear functions can significantly simplify the differential constitutive equations that are used to describe the property.
Equations describing relevant materials properties are often used to predict the attributes of a system.
The properties are measured by standardized test methods. Many such methods have been documented by their respective user communities and published through the Internet; see ASTM International.

Acoustical properties
Acoustical absorption
Speed of sound
Sound reflection
Sound transfer
Third order elasticity (Acoustoelastic effect)

Atomic properties
Atomic mass: (applies to each element) the average mass of the atoms of an element, in daltons (Da), a.k.a. atomic mass units (amu).
Atomic number: (applies to individual atoms or pure elements) the number of protons in each nucleus
Relative atomic mass, a.k.a."
What is perceptual control theory based on?,Properties of positive feedback control loops,Properties of negative feedback control loops,Properties of both positive and negative feedback control loops,Properties of open-loop control systems,Properties of closed-loop control systems,B,"Perceptual control theory (PCT) is a model of behavior based on the properties of negative feedback control loops. A control loop maintains a sensed variable at or near a reference value by means of the effects of its outputs upon that variable, as mediated by physical properties of the environment. In engineering control theory, reference values are set by a user outside the system. An example is a thermostat. In a living organism, reference values for controlled perceptual variables are endogenously maintained. Biological homeostasis and reflexes are simple, low-level examples. The discovery of mathematical principles of control introduced a way to model a negative feedback loop closed through the environment (circular causation), which spawned perceptual control theory."
What is the purpose of a control loop?,To maintain a sensed variable at a reference value,To maximize the effects of its outputs on the variable,To minimize the effects of its outputs on the variable,To set reference values for the system,To control multiple variables simultaneously,A,"Perceptual control theory (PCT) is a model of behavior based on the properties of negative feedback control loops. A control loop maintains a sensed variable at or near a reference value by means of the effects of its outputs upon that variable, as mediated by physical properties of the environment. In engineering control theory, reference values are set by a user outside the system. An example is a thermostat. In a living organism, reference values for controlled perceptual variables are endogenously maintained. Biological homeostasis and reflexes are simple, low-level examples. The discovery of mathematical principles of control introduced a way to model a negative feedback loop closed through the environment (circular causation), which spawned perceptual control theory."
How are reference values set in engineering control theory?,By a user outside the system,By the physical properties of the environment,By the control loop itself,By the endogenous maintenance of the organism,By feedback from other variables in the system,A,"Perceptual control theory (PCT) is a model of behavior based on the properties of negative feedback control loops. A control loop maintains a sensed variable at or near a reference value by means of the effects of its outputs upon that variable, as mediated by physical properties of the environment. In engineering control theory, reference values are set by a user outside the system. An example is a thermostat. In a living organism, reference values for controlled perceptual variables are endogenously maintained. Biological homeostasis and reflexes are simple, low-level examples. The discovery of mathematical principles of control introduced a way to model a negative feedback loop closed through the environment (circular causation), which spawned perceptual control theory."
What is an example of a control loop?,A thermostat,Biological homeostasis,Reflexes,Mathematical principles of control,Perceptual control theory,A,"Perceptual control theory (PCT) is a model of behavior based on the properties of negative feedback control loops. A control loop maintains a sensed variable at or near a reference value by means of the effects of its outputs upon that variable, as mediated by physical properties of the environment. In engineering control theory, reference values are set by a user outside the system. An example is a thermostat. In a living organism, reference values for controlled perceptual variables are endogenously maintained. Biological homeostasis and reflexes are simple, low-level examples. The discovery of mathematical principles of control introduced a way to model a negative feedback loop closed through the environment (circular causation), which spawned perceptual control theory."
What type of feedback loop is closed through the environment?,Positive feedback loop,Negative feedback loop,Open-loop control system,Closed-loop control system,Endogenous feedback loop,B,"Perceptual control theory (PCT) is a model of behavior based on the properties of negative feedback control loops. A control loop maintains a sensed variable at or near a reference value by means of the effects of its outputs upon that variable, as mediated by physical properties of the environment. In engineering control theory, reference values are set by a user outside the system. An example is a thermostat. In a living organism, reference values for controlled perceptual variables are endogenously maintained. Biological homeostasis and reflexes are simple, low-level examples. The discovery of mathematical principles of control introduced a way to model a negative feedback loop closed through the environment (circular causation), which spawned perceptual control theory."
What is the focus of wireless engineering?,Designing and applying optical technologies,Researching and developing radio communications,Creating radar systems,Utilizing acoustic technologies,Designing and applying infrared technologies,B,"Wireless Engineering is the branch of engineering which addresses the design, application, and research of wireless communication systems and technologies.

Overview
Wireless engineering is an engineering subject dealing with engineering problems using wireless technology such as radio communications and radar, but it is more general than the conventional radio engineering. It may include using other techniques such as acoustic, infrared, and optical technologies.

History
Wireless technologies have skyrocketed since their late 19th Century advancements. With the invention of the FM radio in 1935, wireless communications have become a concentrated focus of both private and government sectors.

In Education
Auburn University's Samuel Ginn College of Engineering was the first in the United States to offer a formalized undergraduate degree in such a field. The program was initiated by Samuel Ginn, an Auburn Alumni, in 2001. Auburn University's college of engineering divides their wireless engineering program into two areas of applicable study:  electrical engineering (pertaining to circuit design, digital signal processing, antenna design, etc.), and software-oriented wireless engineering (communications networks, mobile platform applications, systems software, etc.)Macquarie University in Sydney, was the first University to offer Wireless Engineering in Australia. The university works closely with nearby industries in research and teaching development in wireless engineering.Universiti Teknikal Malaysia Melaka in Malacca, was the first University to offer Wireless Communication Engineering in Malaysia.

Applications
Wireless engineering contains a wide spectrum of application, most notably cellular networks. The recent popularity of cellular networks has created a vast career demand with a large repository."
When did wireless technologies experience significant advancements?,Late 19th Century,Early 20th Century,Mid 20th Century,Late 20th Century,Early 21st Century,D,"Wireless Engineering is the branch of engineering which addresses the design, application, and research of wireless communication systems and technologies.

Overview
Wireless engineering is an engineering subject dealing with engineering problems using wireless technology such as radio communications and radar, but it is more general than the conventional radio engineering. It may include using other techniques such as acoustic, infrared, and optical technologies.

History
Wireless technologies have skyrocketed since their late 19th Century advancements. With the invention of the FM radio in 1935, wireless communications have become a concentrated focus of both private and government sectors.

In Education
Auburn University's Samuel Ginn College of Engineering was the first in the United States to offer a formalized undergraduate degree in such a field. The program was initiated by Samuel Ginn, an Auburn Alumni, in 2001. Auburn University's college of engineering divides their wireless engineering program into two areas of applicable study:  electrical engineering (pertaining to circuit design, digital signal processing, antenna design, etc.), and software-oriented wireless engineering (communications networks, mobile platform applications, systems software, etc.)Macquarie University in Sydney, was the first University to offer Wireless Engineering in Australia. The university works closely with nearby industries in research and teaching development in wireless engineering.Universiti Teknikal Malaysia Melaka in Malacca, was the first University to offer Wireless Communication Engineering in Malaysia.

Applications
Wireless engineering contains a wide spectrum of application, most notably cellular networks. The recent popularity of cellular networks has created a vast career demand with a large repository."
Which university was the first in the United States to offer a formalized undergraduate degree in wireless engineering?,Macquarie University,Universiti Teknikal Malaysia Melaka,Auburn University,Stanford University,"University of California, Berkeley",C,"Wireless Engineering is the branch of engineering which addresses the design, application, and research of wireless communication systems and technologies.

Overview
Wireless engineering is an engineering subject dealing with engineering problems using wireless technology such as radio communications and radar, but it is more general than the conventional radio engineering. It may include using other techniques such as acoustic, infrared, and optical technologies.

History
Wireless technologies have skyrocketed since their late 19th Century advancements. With the invention of the FM radio in 1935, wireless communications have become a concentrated focus of both private and government sectors.

In Education
Auburn University's Samuel Ginn College of Engineering was the first in the United States to offer a formalized undergraduate degree in such a field. The program was initiated by Samuel Ginn, an Auburn Alumni, in 2001. Auburn University's college of engineering divides their wireless engineering program into two areas of applicable study:  electrical engineering (pertaining to circuit design, digital signal processing, antenna design, etc.), and software-oriented wireless engineering (communications networks, mobile platform applications, systems software, etc.)Macquarie University in Sydney, was the first University to offer Wireless Engineering in Australia. The university works closely with nearby industries in research and teaching development in wireless engineering.Universiti Teknikal Malaysia Melaka in Malacca, was the first University to offer Wireless Communication Engineering in Malaysia.

Applications
Wireless engineering contains a wide spectrum of application, most notably cellular networks. The recent popularity of cellular networks has created a vast career demand with a large repository."
What are the two areas of study in Auburn University's wireless engineering program?,Acoustic engineering and infrared engineering,Radio engineering and radar engineering,Radar engineering and software engineering,Electrical engineering and software-oriented wireless engineering,Optical engineering and communications engineering,D,"Wireless Engineering is the branch of engineering which addresses the design, application, and research of wireless communication systems and technologies.

Overview
Wireless engineering is an engineering subject dealing with engineering problems using wireless technology such as radio communications and radar, but it is more general than the conventional radio engineering. It may include using other techniques such as acoustic, infrared, and optical technologies.

History
Wireless technologies have skyrocketed since their late 19th Century advancements. With the invention of the FM radio in 1935, wireless communications have become a concentrated focus of both private and government sectors.

In Education
Auburn University's Samuel Ginn College of Engineering was the first in the United States to offer a formalized undergraduate degree in such a field. The program was initiated by Samuel Ginn, an Auburn Alumni, in 2001. Auburn University's college of engineering divides their wireless engineering program into two areas of applicable study:  electrical engineering (pertaining to circuit design, digital signal processing, antenna design, etc.), and software-oriented wireless engineering (communications networks, mobile platform applications, systems software, etc.)Macquarie University in Sydney, was the first University to offer Wireless Engineering in Australia. The university works closely with nearby industries in research and teaching development in wireless engineering.Universiti Teknikal Malaysia Melaka in Malacca, was the first University to offer Wireless Communication Engineering in Malaysia.

Applications
Wireless engineering contains a wide spectrum of application, most notably cellular networks. The recent popularity of cellular networks has created a vast career demand with a large repository."
What is one of the most notable applications of wireless engineering?,Satellite communication,Wireless power transfer,Wireless home automation,Wireless charging,Cellular networks,E,"Wireless Engineering is the branch of engineering which addresses the design, application, and research of wireless communication systems and technologies.

Overview
Wireless engineering is an engineering subject dealing with engineering problems using wireless technology such as radio communications and radar, but it is more general than the conventional radio engineering. It may include using other techniques such as acoustic, infrared, and optical technologies.

History
Wireless technologies have skyrocketed since their late 19th Century advancements. With the invention of the FM radio in 1935, wireless communications have become a concentrated focus of both private and government sectors.

In Education
Auburn University's Samuel Ginn College of Engineering was the first in the United States to offer a formalized undergraduate degree in such a field. The program was initiated by Samuel Ginn, an Auburn Alumni, in 2001. Auburn University's college of engineering divides their wireless engineering program into two areas of applicable study:  electrical engineering (pertaining to circuit design, digital signal processing, antenna design, etc.), and software-oriented wireless engineering (communications networks, mobile platform applications, systems software, etc.)Macquarie University in Sydney, was the first University to offer Wireless Engineering in Australia. The university works closely with nearby industries in research and teaching development in wireless engineering.Universiti Teknikal Malaysia Melaka in Malacca, was the first University to offer Wireless Communication Engineering in Malaysia.

Applications
Wireless engineering contains a wide spectrum of application, most notably cellular networks. The recent popularity of cellular networks has created a vast career demand with a large repository."
What is the role of gauge bosons in particle physics?,To generate strong interactions between elementary particles,To carry the electromagnetic interaction,To provide mass to elementary fermions,To create composite bosons made of quarks,To carry the weak interaction,B,"In particle physics, a gauge boson is a bosonic elementary particle that acts as the force carrier for elementary fermions. Elementary particles, whose interactions are described by a gauge theory, interact with each other by the exchange of gauge bosons, usually as virtual particles.
Photons, W and Z bosons, and gluons are gauge bosons. All known gauge bosons have a spin of 1; for comparison, the Higgs boson has spin zero and the hypothetical graviton has a spin of 2. Therefore, all known gauge bosons are vector bosons.
Gauge bosons are different from the other kinds of bosons: first, fundamental scalar bosons (the Higgs boson); second, mesons, which are composite bosons, made of quarks; third, larger composite, non-force-carrying bosons, such as certain atoms.

Gauge bosons in the Standard Model
The Standard Model of particle physics recognizes four kinds of gauge bosons: photons, which carry the electromagnetic interaction; W and Z bosons, which carry the weak interaction; and gluons, which carry the strong interaction.Isolated gluons do not occur because they are colour-charged and subject to colour confinement.

Multiplicity of gauge bosons
In a quantized gauge theory, gauge bosons are quanta of the gauge fields. Consequently, there are as many gauge bosons as there are generators of the gauge field. In quantum electrodynamics, the gauge group is U(1); in this simple case, there is only one gauge boson, the photon. In quantum chromodynamics, the more complicated group SU(3) has eight generators, corresponding to the eight gluons."
What is the spin of all known gauge bosons?,Spin 0,Spin 1,Spin 2,Spin 3/2,Spin 1/2,B,"In particle physics, a gauge boson is a bosonic elementary particle that acts as the force carrier for elementary fermions. Elementary particles, whose interactions are described by a gauge theory, interact with each other by the exchange of gauge bosons, usually as virtual particles.
Photons, W and Z bosons, and gluons are gauge bosons. All known gauge bosons have a spin of 1; for comparison, the Higgs boson has spin zero and the hypothetical graviton has a spin of 2. Therefore, all known gauge bosons are vector bosons.
Gauge bosons are different from the other kinds of bosons: first, fundamental scalar bosons (the Higgs boson); second, mesons, which are composite bosons, made of quarks; third, larger composite, non-force-carrying bosons, such as certain atoms.

Gauge bosons in the Standard Model
The Standard Model of particle physics recognizes four kinds of gauge bosons: photons, which carry the electromagnetic interaction; W and Z bosons, which carry the weak interaction; and gluons, which carry the strong interaction.Isolated gluons do not occur because they are colour-charged and subject to colour confinement.

Multiplicity of gauge bosons
In a quantized gauge theory, gauge bosons are quanta of the gauge fields. Consequently, there are as many gauge bosons as there are generators of the gauge field. In quantum electrodynamics, the gauge group is U(1); in this simple case, there is only one gauge boson, the photon. In quantum chromodynamics, the more complicated group SU(3) has eight generators, corresponding to the eight gluons."
Which of the following is not a gauge boson?,Photon,W boson,Z boson,Gluon,Higgs boson,E,"In particle physics, a gauge boson is a bosonic elementary particle that acts as the force carrier for elementary fermions. Elementary particles, whose interactions are described by a gauge theory, interact with each other by the exchange of gauge bosons, usually as virtual particles.
Photons, W and Z bosons, and gluons are gauge bosons. All known gauge bosons have a spin of 1; for comparison, the Higgs boson has spin zero and the hypothetical graviton has a spin of 2. Therefore, all known gauge bosons are vector bosons.
Gauge bosons are different from the other kinds of bosons: first, fundamental scalar bosons (the Higgs boson); second, mesons, which are composite bosons, made of quarks; third, larger composite, non-force-carrying bosons, such as certain atoms.

Gauge bosons in the Standard Model
The Standard Model of particle physics recognizes four kinds of gauge bosons: photons, which carry the electromagnetic interaction; W and Z bosons, which carry the weak interaction; and gluons, which carry the strong interaction.Isolated gluons do not occur because they are colour-charged and subject to colour confinement.

Multiplicity of gauge bosons
In a quantized gauge theory, gauge bosons are quanta of the gauge fields. Consequently, there are as many gauge bosons as there are generators of the gauge field. In quantum electrodynamics, the gauge group is U(1); in this simple case, there is only one gauge boson, the photon. In quantum chromodynamics, the more complicated group SU(3) has eight generators, corresponding to the eight gluons."
How many gauge bosons are there in quantum electrodynamics?,One,Two,Four,Eight,Ten,A,"In particle physics, a gauge boson is a bosonic elementary particle that acts as the force carrier for elementary fermions. Elementary particles, whose interactions are described by a gauge theory, interact with each other by the exchange of gauge bosons, usually as virtual particles.
Photons, W and Z bosons, and gluons are gauge bosons. All known gauge bosons have a spin of 1; for comparison, the Higgs boson has spin zero and the hypothetical graviton has a spin of 2. Therefore, all known gauge bosons are vector bosons.
Gauge bosons are different from the other kinds of bosons: first, fundamental scalar bosons (the Higgs boson); second, mesons, which are composite bosons, made of quarks; third, larger composite, non-force-carrying bosons, such as certain atoms.

Gauge bosons in the Standard Model
The Standard Model of particle physics recognizes four kinds of gauge bosons: photons, which carry the electromagnetic interaction; W and Z bosons, which carry the weak interaction; and gluons, which carry the strong interaction.Isolated gluons do not occur because they are colour-charged and subject to colour confinement.

Multiplicity of gauge bosons
In a quantized gauge theory, gauge bosons are quanta of the gauge fields. Consequently, there are as many gauge bosons as there are generators of the gauge field. In quantum electrodynamics, the gauge group is U(1); in this simple case, there is only one gauge boson, the photon. In quantum chromodynamics, the more complicated group SU(3) has eight generators, corresponding to the eight gluons."
Why don't isolated gluons occur?,Because they have a spin of 2,Because they are composite bosons,Because they carry the electromagnetic interaction,Because they are subject to colour confinement,Because they have a mass of zero,D,"In particle physics, a gauge boson is a bosonic elementary particle that acts as the force carrier for elementary fermions. Elementary particles, whose interactions are described by a gauge theory, interact with each other by the exchange of gauge bosons, usually as virtual particles.
Photons, W and Z bosons, and gluons are gauge bosons. All known gauge bosons have a spin of 1; for comparison, the Higgs boson has spin zero and the hypothetical graviton has a spin of 2. Therefore, all known gauge bosons are vector bosons.
Gauge bosons are different from the other kinds of bosons: first, fundamental scalar bosons (the Higgs boson); second, mesons, which are composite bosons, made of quarks; third, larger composite, non-force-carrying bosons, such as certain atoms.

Gauge bosons in the Standard Model
The Standard Model of particle physics recognizes four kinds of gauge bosons: photons, which carry the electromagnetic interaction; W and Z bosons, which carry the weak interaction; and gluons, which carry the strong interaction.Isolated gluons do not occur because they are colour-charged and subject to colour confinement.

Multiplicity of gauge bosons
In a quantized gauge theory, gauge bosons are quanta of the gauge fields. Consequently, there are as many gauge bosons as there are generators of the gauge field. In quantum electrodynamics, the gauge group is U(1); in this simple case, there is only one gauge boson, the photon. In quantum chromodynamics, the more complicated group SU(3) has eight generators, corresponding to the eight gluons."
What is the Annie Jump Cannon Award in Astronomy?,An annual award for women in the STEM fields,An award for outstanding contributions to astronomy by a woman within five years of earning a doctorate degree,An award for outstanding contributions to astronomy by a woman within ten years of earning a master's degree,An award for women studying for a Masters or PhD in atmospheric science,An award for women in the field of medicine,B,"This list of science and technology awards for women is an index to articles about notable awards made to women for work in science and the STEM (Science, technology, engineering, and mathematics) fields generally. It includes awards for astronomy, space and atmospheric science; biology and medicine; chemistry; engineering; mathematics; neuroscience; physics; technology; and general or multiple fields.

Astronomy, space, atmospheric science
Annie Jump Cannon Award in Astronomy – annual award for outstanding contributions to astronomy by a woman within five years of earning a doctorate degree
Peter B. Wagner Memorial Award for Women in Atmospheric Sciences – awarded annually since 1998, based on paper completion, to a woman studying for a Masters or PhD in atmospheric science at a university in the United States

Biology and medicine
Elizabeth Blackwell Medal, given by the American Medical Women's Association to a woman physician ""who has made the most outstanding contributions to the cause of women in the field of medicine""
Federation of American Societies for Experimental Biology (FASEB) Excellence in Science Award
Group on Women in Medicine and Science Leadership Awards, Association of American Medical Colleges
Margaret Oakley Dayhoff Award from the Biophysical Society, Rockville, Maryland – given to a woman who ""has achieved prominence for 'substantial contributions to science'"" and showing high promise in the early part of her career
Pearl Meister Greengard Prize – established 2004
WICB Junior and Senior Awards from Women in Cell Biology (WICB)

Chemistry
ACS Award for Encouraging Women into Careers in the Chemical Sciences, sponsored by the Camille and Henry Dreyfus Foundation
Garvan–Olin Medal – annual award that recognizes distinguished service to chemistry by women chemists
Awards by the Iota Sigma Pi honorary society for women in chemistry:
Agnes Fay Morgan Research Award
Anna Louise Hoffman Award for Outstanding Achievement in Graduate Research
Centennial Award for Excellence in Undergraduate Teaching
Gladys Anderson Emerson Undergraduate Scholarship
Members-at-Large Re-entry Award
National Honorary Member
Outstanding Young Women in Chemistry award.
Undergraduate Excellence in Chemistry
Violet Diller Professional Excellence Award

Engineering
Sharon Keillor Award for Women in Engineering Education
Achievement Award of the Society of Women Engineers
Young Woman Engineer of the Year Award

Mathematics
Awards by the Association for Women in Mathematics:
Alice T. Schafer Prize – established 1991
Biographies of Contemporary Women in Mathematics Essay Contest – established in 2001 for biographical essays
Emmy Noether Lectures – an honorary lecture award
M. Gweneth Humphreys Award
Louise Hay Award for Contributions to Mathematics Education – established 1991
Ruth I. Michler Memorial Prize
Ruth Lyttle Satter Prize in Mathematics – established 1990
additional awards by the AWM
Awards sponsored by the Kovalevskaia Fund in Mexico, Peru, and southern Africa
Krieger–Nelson Prize for Distinguished Research by Women in Mathematics – established 1995 by the Canadian Mathematical Society

Neuroscience
Awards by the Society for Neuroscience:Bernice Grafstein Award for Outstanding Accomplishments in Mentoring – for dedication to mentoring women neuroscientists
Louise Hanson Marshall Special Recognition Award – honors an individual who has significantly promoted the professional development of women in neuroscience through teaching, organizational leadership, public advocacy, or other efforts that are not necessarily related to research
Mika Salpeter Lifetime Achievement Award – presented for outstanding career achievements in neuroscience who has also significantly promoted the professional advancement of women in neuroscience
Patricia Goldman-Rakic Hall of Honor – posthumously recognizes a neuroscientist who has pursued career excellence and exhibited dedication and advancement of women in neuroscience

Physics
Jocelyn Bell Burnell Medal and Prize, Institute of Physics

Technology
BlackBerry Women and Technology Awards
WITI@UC Athena Awards – Awards recognize those who embody, encourage, and promote the inclusion of women in technology. Awardees are leaders who inspire others to pursue and persist in technical careers.
Lori Bunch ""Recognized for Innovation in Cerner Millennium Implementation."" She started with Cerner as an installation director then was promoted to senior project architect in 1997."
What is the Elizabeth Blackwell Medal?,An award for women who have made outstanding contributions to the cause of women in the field of medicine,An award for women who have made outstanding contributions to the cause of women in the field of biology,An award for women who have made outstanding contributions to the cause of women in the STEM fields,An award for women who have made outstanding contributions to the cause of women in the field of chemistry,An award for women who have made outstanding contributions to the cause of women in the field of engineering,A,"This list of science and technology awards for women is an index to articles about notable awards made to women for work in science and the STEM (Science, technology, engineering, and mathematics) fields generally. It includes awards for astronomy, space and atmospheric science; biology and medicine; chemistry; engineering; mathematics; neuroscience; physics; technology; and general or multiple fields.

Astronomy, space, atmospheric science
Annie Jump Cannon Award in Astronomy – annual award for outstanding contributions to astronomy by a woman within five years of earning a doctorate degree
Peter B. Wagner Memorial Award for Women in Atmospheric Sciences – awarded annually since 1998, based on paper completion, to a woman studying for a Masters or PhD in atmospheric science at a university in the United States

Biology and medicine
Elizabeth Blackwell Medal, given by the American Medical Women's Association to a woman physician ""who has made the most outstanding contributions to the cause of women in the field of medicine""
Federation of American Societies for Experimental Biology (FASEB) Excellence in Science Award
Group on Women in Medicine and Science Leadership Awards, Association of American Medical Colleges
Margaret Oakley Dayhoff Award from the Biophysical Society, Rockville, Maryland – given to a woman who ""has achieved prominence for 'substantial contributions to science'"" and showing high promise in the early part of her career
Pearl Meister Greengard Prize – established 2004
WICB Junior and Senior Awards from Women in Cell Biology (WICB)

Chemistry
ACS Award for Encouraging Women into Careers in the Chemical Sciences, sponsored by the Camille and Henry Dreyfus Foundation
Garvan–Olin Medal – annual award that recognizes distinguished service to chemistry by women chemists
Awards by the Iota Sigma Pi honorary society for women in chemistry:
Agnes Fay Morgan Research Award
Anna Louise Hoffman Award for Outstanding Achievement in Graduate Research
Centennial Award for Excellence in Undergraduate Teaching
Gladys Anderson Emerson Undergraduate Scholarship
Members-at-Large Re-entry Award
National Honorary Member
Outstanding Young Women in Chemistry award.
Undergraduate Excellence in Chemistry
Violet Diller Professional Excellence Award

Engineering
Sharon Keillor Award for Women in Engineering Education
Achievement Award of the Society of Women Engineers
Young Woman Engineer of the Year Award

Mathematics
Awards by the Association for Women in Mathematics:
Alice T. Schafer Prize – established 1991
Biographies of Contemporary Women in Mathematics Essay Contest – established in 2001 for biographical essays
Emmy Noether Lectures – an honorary lecture award
M. Gweneth Humphreys Award
Louise Hay Award for Contributions to Mathematics Education – established 1991
Ruth I. Michler Memorial Prize
Ruth Lyttle Satter Prize in Mathematics – established 1990
additional awards by the AWM
Awards sponsored by the Kovalevskaia Fund in Mexico, Peru, and southern Africa
Krieger–Nelson Prize for Distinguished Research by Women in Mathematics – established 1995 by the Canadian Mathematical Society

Neuroscience
Awards by the Society for Neuroscience:Bernice Grafstein Award for Outstanding Accomplishments in Mentoring – for dedication to mentoring women neuroscientists
Louise Hanson Marshall Special Recognition Award – honors an individual who has significantly promoted the professional development of women in neuroscience through teaching, organizational leadership, public advocacy, or other efforts that are not necessarily related to research
Mika Salpeter Lifetime Achievement Award – presented for outstanding career achievements in neuroscience who has also significantly promoted the professional advancement of women in neuroscience
Patricia Goldman-Rakic Hall of Honor – posthumously recognizes a neuroscientist who has pursued career excellence and exhibited dedication and advancement of women in neuroscience

Physics
Jocelyn Bell Burnell Medal and Prize, Institute of Physics

Technology
BlackBerry Women and Technology Awards
WITI@UC Athena Awards – Awards recognize those who embody, encourage, and promote the inclusion of women in technology. Awardees are leaders who inspire others to pursue and persist in technical careers.
Lori Bunch ""Recognized for Innovation in Cerner Millennium Implementation."" She started with Cerner as an installation director then was promoted to senior project architect in 1997."
Which organization gives the FASEB Excellence in Science Award?,The American Medical Women's Association,The Federation of American Societies for Experimental Biology,The Biophysical Society,The Association of American Medical Colleges,The American Chemical Society,B,"This list of science and technology awards for women is an index to articles about notable awards made to women for work in science and the STEM (Science, technology, engineering, and mathematics) fields generally. It includes awards for astronomy, space and atmospheric science; biology and medicine; chemistry; engineering; mathematics; neuroscience; physics; technology; and general or multiple fields.

Astronomy, space, atmospheric science
Annie Jump Cannon Award in Astronomy – annual award for outstanding contributions to astronomy by a woman within five years of earning a doctorate degree
Peter B. Wagner Memorial Award for Women in Atmospheric Sciences – awarded annually since 1998, based on paper completion, to a woman studying for a Masters or PhD in atmospheric science at a university in the United States

Biology and medicine
Elizabeth Blackwell Medal, given by the American Medical Women's Association to a woman physician ""who has made the most outstanding contributions to the cause of women in the field of medicine""
Federation of American Societies for Experimental Biology (FASEB) Excellence in Science Award
Group on Women in Medicine and Science Leadership Awards, Association of American Medical Colleges
Margaret Oakley Dayhoff Award from the Biophysical Society, Rockville, Maryland – given to a woman who ""has achieved prominence for 'substantial contributions to science'"" and showing high promise in the early part of her career
Pearl Meister Greengard Prize – established 2004
WICB Junior and Senior Awards from Women in Cell Biology (WICB)

Chemistry
ACS Award for Encouraging Women into Careers in the Chemical Sciences, sponsored by the Camille and Henry Dreyfus Foundation
Garvan–Olin Medal – annual award that recognizes distinguished service to chemistry by women chemists
Awards by the Iota Sigma Pi honorary society for women in chemistry:
Agnes Fay Morgan Research Award
Anna Louise Hoffman Award for Outstanding Achievement in Graduate Research
Centennial Award for Excellence in Undergraduate Teaching
Gladys Anderson Emerson Undergraduate Scholarship
Members-at-Large Re-entry Award
National Honorary Member
Outstanding Young Women in Chemistry award.
Undergraduate Excellence in Chemistry
Violet Diller Professional Excellence Award

Engineering
Sharon Keillor Award for Women in Engineering Education
Achievement Award of the Society of Women Engineers
Young Woman Engineer of the Year Award

Mathematics
Awards by the Association for Women in Mathematics:
Alice T. Schafer Prize – established 1991
Biographies of Contemporary Women in Mathematics Essay Contest – established in 2001 for biographical essays
Emmy Noether Lectures – an honorary lecture award
M. Gweneth Humphreys Award
Louise Hay Award for Contributions to Mathematics Education – established 1991
Ruth I. Michler Memorial Prize
Ruth Lyttle Satter Prize in Mathematics – established 1990
additional awards by the AWM
Awards sponsored by the Kovalevskaia Fund in Mexico, Peru, and southern Africa
Krieger–Nelson Prize for Distinguished Research by Women in Mathematics – established 1995 by the Canadian Mathematical Society

Neuroscience
Awards by the Society for Neuroscience:Bernice Grafstein Award for Outstanding Accomplishments in Mentoring – for dedication to mentoring women neuroscientists
Louise Hanson Marshall Special Recognition Award – honors an individual who has significantly promoted the professional development of women in neuroscience through teaching, organizational leadership, public advocacy, or other efforts that are not necessarily related to research
Mika Salpeter Lifetime Achievement Award – presented for outstanding career achievements in neuroscience who has also significantly promoted the professional advancement of women in neuroscience
Patricia Goldman-Rakic Hall of Honor – posthumously recognizes a neuroscientist who has pursued career excellence and exhibited dedication and advancement of women in neuroscience

Physics
Jocelyn Bell Burnell Medal and Prize, Institute of Physics

Technology
BlackBerry Women and Technology Awards
WITI@UC Athena Awards – Awards recognize those who embody, encourage, and promote the inclusion of women in technology. Awardees are leaders who inspire others to pursue and persist in technical careers.
Lori Bunch ""Recognized for Innovation in Cerner Millennium Implementation."" She started with Cerner as an installation director then was promoted to senior project architect in 1997."
What is the Violet Diller Professional Excellence Award?,An award for women in the field of medicine,An award for women in the field of biology,An award for women in the field of chemistry,An award for women in the field of mathematics,An award for women in the field of engineering,C,"This list of science and technology awards for women is an index to articles about notable awards made to women for work in science and the STEM (Science, technology, engineering, and mathematics) fields generally. It includes awards for astronomy, space and atmospheric science; biology and medicine; chemistry; engineering; mathematics; neuroscience; physics; technology; and general or multiple fields.

Astronomy, space, atmospheric science
Annie Jump Cannon Award in Astronomy – annual award for outstanding contributions to astronomy by a woman within five years of earning a doctorate degree
Peter B. Wagner Memorial Award for Women in Atmospheric Sciences – awarded annually since 1998, based on paper completion, to a woman studying for a Masters or PhD in atmospheric science at a university in the United States

Biology and medicine
Elizabeth Blackwell Medal, given by the American Medical Women's Association to a woman physician ""who has made the most outstanding contributions to the cause of women in the field of medicine""
Federation of American Societies for Experimental Biology (FASEB) Excellence in Science Award
Group on Women in Medicine and Science Leadership Awards, Association of American Medical Colleges
Margaret Oakley Dayhoff Award from the Biophysical Society, Rockville, Maryland – given to a woman who ""has achieved prominence for 'substantial contributions to science'"" and showing high promise in the early part of her career
Pearl Meister Greengard Prize – established 2004
WICB Junior and Senior Awards from Women in Cell Biology (WICB)

Chemistry
ACS Award for Encouraging Women into Careers in the Chemical Sciences, sponsored by the Camille and Henry Dreyfus Foundation
Garvan–Olin Medal – annual award that recognizes distinguished service to chemistry by women chemists
Awards by the Iota Sigma Pi honorary society for women in chemistry:
Agnes Fay Morgan Research Award
Anna Louise Hoffman Award for Outstanding Achievement in Graduate Research
Centennial Award for Excellence in Undergraduate Teaching
Gladys Anderson Emerson Undergraduate Scholarship
Members-at-Large Re-entry Award
National Honorary Member
Outstanding Young Women in Chemistry award.
Undergraduate Excellence in Chemistry
Violet Diller Professional Excellence Award

Engineering
Sharon Keillor Award for Women in Engineering Education
Achievement Award of the Society of Women Engineers
Young Woman Engineer of the Year Award

Mathematics
Awards by the Association for Women in Mathematics:
Alice T. Schafer Prize – established 1991
Biographies of Contemporary Women in Mathematics Essay Contest – established in 2001 for biographical essays
Emmy Noether Lectures – an honorary lecture award
M. Gweneth Humphreys Award
Louise Hay Award for Contributions to Mathematics Education – established 1991
Ruth I. Michler Memorial Prize
Ruth Lyttle Satter Prize in Mathematics – established 1990
additional awards by the AWM
Awards sponsored by the Kovalevskaia Fund in Mexico, Peru, and southern Africa
Krieger–Nelson Prize for Distinguished Research by Women in Mathematics – established 1995 by the Canadian Mathematical Society

Neuroscience
Awards by the Society for Neuroscience:Bernice Grafstein Award for Outstanding Accomplishments in Mentoring – for dedication to mentoring women neuroscientists
Louise Hanson Marshall Special Recognition Award – honors an individual who has significantly promoted the professional development of women in neuroscience through teaching, organizational leadership, public advocacy, or other efforts that are not necessarily related to research
Mika Salpeter Lifetime Achievement Award – presented for outstanding career achievements in neuroscience who has also significantly promoted the professional advancement of women in neuroscience
Patricia Goldman-Rakic Hall of Honor – posthumously recognizes a neuroscientist who has pursued career excellence and exhibited dedication and advancement of women in neuroscience

Physics
Jocelyn Bell Burnell Medal and Prize, Institute of Physics

Technology
BlackBerry Women and Technology Awards
WITI@UC Athena Awards – Awards recognize those who embody, encourage, and promote the inclusion of women in technology. Awardees are leaders who inspire others to pursue and persist in technical careers.
Lori Bunch ""Recognized for Innovation in Cerner Millennium Implementation."" She started with Cerner as an installation director then was promoted to senior project architect in 1997."
Who established the BlackBerry Women and Technology Awards?,The American Medical Women's Association,The Institute of Physics,The Biophysical Society,The Federation of American Societies for Experimental Biology,BlackBerry,E,"This list of science and technology awards for women is an index to articles about notable awards made to women for work in science and the STEM (Science, technology, engineering, and mathematics) fields generally. It includes awards for astronomy, space and atmospheric science; biology and medicine; chemistry; engineering; mathematics; neuroscience; physics; technology; and general or multiple fields.

Astronomy, space, atmospheric science
Annie Jump Cannon Award in Astronomy – annual award for outstanding contributions to astronomy by a woman within five years of earning a doctorate degree
Peter B. Wagner Memorial Award for Women in Atmospheric Sciences – awarded annually since 1998, based on paper completion, to a woman studying for a Masters or PhD in atmospheric science at a university in the United States

Biology and medicine
Elizabeth Blackwell Medal, given by the American Medical Women's Association to a woman physician ""who has made the most outstanding contributions to the cause of women in the field of medicine""
Federation of American Societies for Experimental Biology (FASEB) Excellence in Science Award
Group on Women in Medicine and Science Leadership Awards, Association of American Medical Colleges
Margaret Oakley Dayhoff Award from the Biophysical Society, Rockville, Maryland – given to a woman who ""has achieved prominence for 'substantial contributions to science'"" and showing high promise in the early part of her career
Pearl Meister Greengard Prize – established 2004
WICB Junior and Senior Awards from Women in Cell Biology (WICB)

Chemistry
ACS Award for Encouraging Women into Careers in the Chemical Sciences, sponsored by the Camille and Henry Dreyfus Foundation
Garvan–Olin Medal – annual award that recognizes distinguished service to chemistry by women chemists
Awards by the Iota Sigma Pi honorary society for women in chemistry:
Agnes Fay Morgan Research Award
Anna Louise Hoffman Award for Outstanding Achievement in Graduate Research
Centennial Award for Excellence in Undergraduate Teaching
Gladys Anderson Emerson Undergraduate Scholarship
Members-at-Large Re-entry Award
National Honorary Member
Outstanding Young Women in Chemistry award.
Undergraduate Excellence in Chemistry
Violet Diller Professional Excellence Award

Engineering
Sharon Keillor Award for Women in Engineering Education
Achievement Award of the Society of Women Engineers
Young Woman Engineer of the Year Award

Mathematics
Awards by the Association for Women in Mathematics:
Alice T. Schafer Prize – established 1991
Biographies of Contemporary Women in Mathematics Essay Contest – established in 2001 for biographical essays
Emmy Noether Lectures – an honorary lecture award
M. Gweneth Humphreys Award
Louise Hay Award for Contributions to Mathematics Education – established 1991
Ruth I. Michler Memorial Prize
Ruth Lyttle Satter Prize in Mathematics – established 1990
additional awards by the AWM
Awards sponsored by the Kovalevskaia Fund in Mexico, Peru, and southern Africa
Krieger–Nelson Prize for Distinguished Research by Women in Mathematics – established 1995 by the Canadian Mathematical Society

Neuroscience
Awards by the Society for Neuroscience:Bernice Grafstein Award for Outstanding Accomplishments in Mentoring – for dedication to mentoring women neuroscientists
Louise Hanson Marshall Special Recognition Award – honors an individual who has significantly promoted the professional development of women in neuroscience through teaching, organizational leadership, public advocacy, or other efforts that are not necessarily related to research
Mika Salpeter Lifetime Achievement Award – presented for outstanding career achievements in neuroscience who has also significantly promoted the professional advancement of women in neuroscience
Patricia Goldman-Rakic Hall of Honor – posthumously recognizes a neuroscientist who has pursued career excellence and exhibited dedication and advancement of women in neuroscience

Physics
Jocelyn Bell Burnell Medal and Prize, Institute of Physics

Technology
BlackBerry Women and Technology Awards
WITI@UC Athena Awards – Awards recognize those who embody, encourage, and promote the inclusion of women in technology. Awardees are leaders who inspire others to pursue and persist in technical careers.
Lori Bunch ""Recognized for Innovation in Cerner Millennium Implementation."" She started with Cerner as an installation director then was promoted to senior project architect in 1997."
What is the Gibbs Brothers Medal awarded for?,Outstanding contributions in the field of naval architecture and marine engineering,Excellence in theoretical hydromechanics,Wide-ranging original contributions in the practical arts of ocean systems,Environmental protection in naval architecture,Contributions in yacht design and education,A,"The Gibbs Brothers Medal is awarded by the U.S. National Academy of Sciences for ""outstanding contributions in the field of naval architecture and marine engineering"".  It was established by a gift from William Francis Gibbs and Frederic Herbert Gibbs.

Recipients
Jerome H. Milgram (2017)
For wide-ranging original contributions to naval architecture in theoretical hydromechanics, education, yacht design, environmental protection, and the practical arts of ocean systems.Robert G. Keane, Jr. (2012)
For continued excellence as a naval architect over many years, exemplified by the outstanding naval warships that he had a major part in designing, helping to make the U.S. Navy the most powerful in the world.Keith W."
Who established the Gibbs Brothers Medal?,Jerome H. Milgram,William Francis Gibbs,Frederic Herbert Gibbs,"Robert G. Keane, Jr.",Keith W.,B,"The Gibbs Brothers Medal is awarded by the U.S. National Academy of Sciences for ""outstanding contributions in the field of naval architecture and marine engineering"".  It was established by a gift from William Francis Gibbs and Frederic Herbert Gibbs.

Recipients
Jerome H. Milgram (2017)
For wide-ranging original contributions to naval architecture in theoretical hydromechanics, education, yacht design, environmental protection, and the practical arts of ocean systems.Robert G. Keane, Jr. (2012)
For continued excellence as a naval architect over many years, exemplified by the outstanding naval warships that he had a major part in designing, helping to make the U.S. Navy the most powerful in the world.Keith W."
What did Jerome H. Milgram receive the Gibbs Brothers Medal for?,Wide-ranging original contributions to naval architecture in theoretical hydromechanics,Excellence in education and yacht design,Environmental protection in the field of naval architecture,Outstanding contributions in the practical arts of ocean systems,Contributions in the field of marine engineering,A,"The Gibbs Brothers Medal is awarded by the U.S. National Academy of Sciences for ""outstanding contributions in the field of naval architecture and marine engineering"".  It was established by a gift from William Francis Gibbs and Frederic Herbert Gibbs.

Recipients
Jerome H. Milgram (2017)
For wide-ranging original contributions to naval architecture in theoretical hydromechanics, education, yacht design, environmental protection, and the practical arts of ocean systems.Robert G. Keane, Jr. (2012)
For continued excellence as a naval architect over many years, exemplified by the outstanding naval warships that he had a major part in designing, helping to make the U.S. Navy the most powerful in the world.Keith W."
"What is Robert G. Keane, Jr. known for in the field of naval architecture?",Excellence in theoretical hydromechanics,Wide-ranging original contributions in the practical arts of ocean systems,Outstanding contributions in the field of marine engineering,Designing outstanding naval warships,Contributions in the field of environmental protection,D,"The Gibbs Brothers Medal is awarded by the U.S. National Academy of Sciences for ""outstanding contributions in the field of naval architecture and marine engineering"".  It was established by a gift from William Francis Gibbs and Frederic Herbert Gibbs.

Recipients
Jerome H. Milgram (2017)
For wide-ranging original contributions to naval architecture in theoretical hydromechanics, education, yacht design, environmental protection, and the practical arts of ocean systems.Robert G. Keane, Jr. (2012)
For continued excellence as a naval architect over many years, exemplified by the outstanding naval warships that he had a major part in designing, helping to make the U.S. Navy the most powerful in the world.Keith W."
Which recipient of the Gibbs Brothers Medal had a major part in designing outstanding naval warships?,Jerome H. Milgram,William Francis Gibbs,Frederic Herbert Gibbs,"Robert G. Keane, Jr.",Keith W.,D,"The Gibbs Brothers Medal is awarded by the U.S. National Academy of Sciences for ""outstanding contributions in the field of naval architecture and marine engineering"".  It was established by a gift from William Francis Gibbs and Frederic Herbert Gibbs.

Recipients
Jerome H. Milgram (2017)
For wide-ranging original contributions to naval architecture in theoretical hydromechanics, education, yacht design, environmental protection, and the practical arts of ocean systems.Robert G. Keane, Jr. (2012)
For continued excellence as a naval architect over many years, exemplified by the outstanding naval warships that he had a major part in designing, helping to make the U.S. Navy the most powerful in the world.Keith W."
"What does the term ""differential"" refer to in mathematics?",The derivatives of functions,Infinitesimal differences,The concept of infinitely small changes,The linearization of a function,The generalization for functions of multiple variables,C,"In mathematics, differential refers to several related notions derived from the early days of calculus, put on a rigorous footing, such as infinitesimal differences and the derivatives of functions.The term is used in various branches of mathematics such as calculus, differential geometry, algebraic geometry and algebraic topology.

Introduction
The term differential is used nonrigorously in calculus to refer to an infinitesimal (""infinitely small"") change in some varying quantity. For example, if x is a variable, then a change in the value of x is often denoted Δx (pronounced delta x). The differential dx represents an infinitely small change in the variable x. The idea of an infinitely small or infinitely slow change is, intuitively, extremely useful, and there are a number of ways to make the notion mathematically precise.
Using calculus, it is possible to relate the infinitely small changes of various variables to each other mathematically using derivatives. If y is a function of x, then the differential dy of y is related to dx by the formula

where 
  
    
      
        
          
            
              d
              y
            
            
              d
              x
            
          
        
        
      
    
    {\displaystyle {\frac {dy}{dx}}\,}
  denotes the derivative of y with respect to x. This formula summarizes the intuitive idea that the derivative of y with respect to x is the limit of the ratio of differences Δy/Δx as Δx becomes infinitesimal.

Basic notions
In calculus, the differential represents a change in the linearization of a function.
The total differential is its generalization for functions of multiple variables.
In traditional approaches to calculus, the differentials (e.g. dx, dy, dt, etc.) are interpreted as infinitesimals."
Which branches of mathematics use the term differential?,Calculus,Differential geometry,Algebraic geometry,Algebraic topology,All of the above,E,"In mathematics, differential refers to several related notions derived from the early days of calculus, put on a rigorous footing, such as infinitesimal differences and the derivatives of functions.The term is used in various branches of mathematics such as calculus, differential geometry, algebraic geometry and algebraic topology.

Introduction
The term differential is used nonrigorously in calculus to refer to an infinitesimal (""infinitely small"") change in some varying quantity. For example, if x is a variable, then a change in the value of x is often denoted Δx (pronounced delta x). The differential dx represents an infinitely small change in the variable x. The idea of an infinitely small or infinitely slow change is, intuitively, extremely useful, and there are a number of ways to make the notion mathematically precise.
Using calculus, it is possible to relate the infinitely small changes of various variables to each other mathematically using derivatives. If y is a function of x, then the differential dy of y is related to dx by the formula

where 
  
    
      
        
          
            
              d
              y
            
            
              d
              x
            
          
        
        
      
    
    {\displaystyle {\frac {dy}{dx}}\,}
  denotes the derivative of y with respect to x. This formula summarizes the intuitive idea that the derivative of y with respect to x is the limit of the ratio of differences Δy/Δx as Δx becomes infinitesimal.

Basic notions
In calculus, the differential represents a change in the linearization of a function.
The total differential is its generalization for functions of multiple variables.
In traditional approaches to calculus, the differentials (e.g. dx, dy, dt, etc.) are interpreted as infinitesimals."
How is the differential dx defined in calculus?,An infinitely small change in the variable x,A change in the value of x,The derivative of y with respect to x,The ratio of differences Δy/Δx,The linearization of a function,A,"In mathematics, differential refers to several related notions derived from the early days of calculus, put on a rigorous footing, such as infinitesimal differences and the derivatives of functions.The term is used in various branches of mathematics such as calculus, differential geometry, algebraic geometry and algebraic topology.

Introduction
The term differential is used nonrigorously in calculus to refer to an infinitesimal (""infinitely small"") change in some varying quantity. For example, if x is a variable, then a change in the value of x is often denoted Δx (pronounced delta x). The differential dx represents an infinitely small change in the variable x. The idea of an infinitely small or infinitely slow change is, intuitively, extremely useful, and there are a number of ways to make the notion mathematically precise.
Using calculus, it is possible to relate the infinitely small changes of various variables to each other mathematically using derivatives. If y is a function of x, then the differential dy of y is related to dx by the formula

where 
  
    
      
        
          
            
              d
              y
            
            
              d
              x
            
          
        
        
      
    
    {\displaystyle {\frac {dy}{dx}}\,}
  denotes the derivative of y with respect to x. This formula summarizes the intuitive idea that the derivative of y with respect to x is the limit of the ratio of differences Δy/Δx as Δx becomes infinitesimal.

Basic notions
In calculus, the differential represents a change in the linearization of a function.
The total differential is its generalization for functions of multiple variables.
In traditional approaches to calculus, the differentials (e.g. dx, dy, dt, etc.) are interpreted as infinitesimals."
What does the formula dy/dx represent?,The derivative of y with respect to x,The ratio of differences Δy/Δx,The linearization of a function,An infinitely small change in the variable x,The generalization for functions of multiple variables,A,"In mathematics, differential refers to several related notions derived from the early days of calculus, put on a rigorous footing, such as infinitesimal differences and the derivatives of functions.The term is used in various branches of mathematics such as calculus, differential geometry, algebraic geometry and algebraic topology.

Introduction
The term differential is used nonrigorously in calculus to refer to an infinitesimal (""infinitely small"") change in some varying quantity. For example, if x is a variable, then a change in the value of x is often denoted Δx (pronounced delta x). The differential dx represents an infinitely small change in the variable x. The idea of an infinitely small or infinitely slow change is, intuitively, extremely useful, and there are a number of ways to make the notion mathematically precise.
Using calculus, it is possible to relate the infinitely small changes of various variables to each other mathematically using derivatives. If y is a function of x, then the differential dy of y is related to dx by the formula

where 
  
    
      
        
          
            
              d
              y
            
            
              d
              x
            
          
        
        
      
    
    {\displaystyle {\frac {dy}{dx}}\,}
  denotes the derivative of y with respect to x. This formula summarizes the intuitive idea that the derivative of y with respect to x is the limit of the ratio of differences Δy/Δx as Δx becomes infinitesimal.

Basic notions
In calculus, the differential represents a change in the linearization of a function.
The total differential is its generalization for functions of multiple variables.
In traditional approaches to calculus, the differentials (e.g. dx, dy, dt, etc.) are interpreted as infinitesimals."
How are the differentials interpreted in traditional approaches to calculus?,As derivatives,As infinitely small quantities,As linearizations of functions,As generalizations for functions of multiple variables,As ratios of differences,B,"In mathematics, differential refers to several related notions derived from the early days of calculus, put on a rigorous footing, such as infinitesimal differences and the derivatives of functions.The term is used in various branches of mathematics such as calculus, differential geometry, algebraic geometry and algebraic topology.

Introduction
The term differential is used nonrigorously in calculus to refer to an infinitesimal (""infinitely small"") change in some varying quantity. For example, if x is a variable, then a change in the value of x is often denoted Δx (pronounced delta x). The differential dx represents an infinitely small change in the variable x. The idea of an infinitely small or infinitely slow change is, intuitively, extremely useful, and there are a number of ways to make the notion mathematically precise.
Using calculus, it is possible to relate the infinitely small changes of various variables to each other mathematically using derivatives. If y is a function of x, then the differential dy of y is related to dx by the formula

where 
  
    
      
        
          
            
              d
              y
            
            
              d
              x
            
          
        
        
      
    
    {\displaystyle {\frac {dy}{dx}}\,}
  denotes the derivative of y with respect to x. This formula summarizes the intuitive idea that the derivative of y with respect to x is the limit of the ratio of differences Δy/Δx as Δx becomes infinitesimal.

Basic notions
In calculus, the differential represents a change in the linearization of a function.
The total differential is its generalization for functions of multiple variables.
In traditional approaches to calculus, the differentials (e.g. dx, dy, dt, etc.) are interpreted as infinitesimals."
What is electron cyclotron resonance (ECR)?,"A phenomenon observed in plasma physics, condensed matter physics, and accelerator physics",A phenomenon observed only in plasma physics,A phenomenon observed only in condensed matter physics,A phenomenon observed only in accelerator physics,A phenomenon observed in plasma physics and accelerator physics,A,"Electron cyclotron resonance (ECR) is a phenomenon observed in plasma physics, condensed matter physics, and accelerator physics. It happens when the frequency of incident radiation coincides with the natural frequency of rotation of electrons in magnetic fields. A free electron in a static and uniform magnetic field will move in a circle due to the Lorentz force.  The circular motion may be superimposed with a uniform axial motion, resulting in a helix, or with a uniform motion perpendicular to the field (e.g., in the presence of an electrical or gravitational field) resulting in a cycloid.  The angular frequency (ω = 2πf ) of this cyclotron motion for a given magnetic field strength B is given (in SI units) by

  
    
      
        
          ω
          
            ce
          
        
        =
        
          
            
              e
              B
            
            
              m
              
                e
              
            
          
        
      
    
    {\displaystyle \omega _{\text{ce}}={\frac {eB}{m_{\text{e}}}}}
  .where 
  
    e
    e
   is the elementary charge and 
  
    m
    m
   is the mass of the electron. For the commonly used microwave frequency 2.45 GHz and the bare electron charge and mass, the resonance condition is met when B = 875 G = 0.0875 T.
For particles of charge q, electron rest mass m0,e moving at relativistic speeds v, the formula needs to be adjusted according to the special theory of relativity to:

  
    
      
        
          ω
          
            ce
          
        
        =
        
          
            
              e
              B
            
            
              γ
              
                m
                
                  0
                  ,
                  
                    e
                  
                
              
            
          
        
      
    
    {\displaystyle \omega _{\text{ce}}={\frac {eB}{\gamma m_{0,{\text{e}}}}}}
  where

  
    
      
        γ
        =
        
          
            1
            
              1
              −
              
                
                  (
                  
                    
                      v
                      c
                    
                  
                  )
                
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \gamma ={\frac {1}{\sqrt {1-\left({\frac {v}{c}}\right)^{2}}}}}
  .

In plasma physics
An ionized plasma may be efficiently produced or heated by superimposing a static magnetic field and a high-frequency electromagnetic field at the electron cyclotron resonance frequency. In the toroidal magnetic fields used in magnetic fusion energy research, the magnetic field decreases with the major radius, so the location of the power deposition can be controlled within about a centimeter."
What causes an electron to move in a circle in a static and uniform magnetic field?,The Lorentz force,The electromagnetic force,The gravitational force,The electric force,The centrifugal force,A,"Electron cyclotron resonance (ECR) is a phenomenon observed in plasma physics, condensed matter physics, and accelerator physics. It happens when the frequency of incident radiation coincides with the natural frequency of rotation of electrons in magnetic fields. A free electron in a static and uniform magnetic field will move in a circle due to the Lorentz force.  The circular motion may be superimposed with a uniform axial motion, resulting in a helix, or with a uniform motion perpendicular to the field (e.g., in the presence of an electrical or gravitational field) resulting in a cycloid.  The angular frequency (ω = 2πf ) of this cyclotron motion for a given magnetic field strength B is given (in SI units) by

  
    
      
        
          ω
          
            ce
          
        
        =
        
          
            
              e
              B
            
            
              m
              
                e
              
            
          
        
      
    
    {\displaystyle \omega _{\text{ce}}={\frac {eB}{m_{\text{e}}}}}
  .where 
  
    e
    e
   is the elementary charge and 
  
    m
    m
   is the mass of the electron. For the commonly used microwave frequency 2.45 GHz and the bare electron charge and mass, the resonance condition is met when B = 875 G = 0.0875 T.
For particles of charge q, electron rest mass m0,e moving at relativistic speeds v, the formula needs to be adjusted according to the special theory of relativity to:

  
    
      
        
          ω
          
            ce
          
        
        =
        
          
            
              e
              B
            
            
              γ
              
                m
                
                  0
                  ,
                  
                    e
                  
                
              
            
          
        
      
    
    {\displaystyle \omega _{\text{ce}}={\frac {eB}{\gamma m_{0,{\text{e}}}}}}
  where

  
    
      
        γ
        =
        
          
            1
            
              1
              −
              
                
                  (
                  
                    
                      v
                      c
                    
                  
                  )
                
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \gamma ={\frac {1}{\sqrt {1-\left({\frac {v}{c}}\right)^{2}}}}}
  .

In plasma physics
An ionized plasma may be efficiently produced or heated by superimposing a static magnetic field and a high-frequency electromagnetic field at the electron cyclotron resonance frequency. In the toroidal magnetic fields used in magnetic fusion energy research, the magnetic field decreases with the major radius, so the location of the power deposition can be controlled within about a centimeter."
What is the resonance condition for electron cyclotron resonance (ECR) to occur?,B = 875 G = 0.0875 T,B = 2.45 GHz,B = eB/m,"B = γm0,e","B = eB/γm0,e",A,"Electron cyclotron resonance (ECR) is a phenomenon observed in plasma physics, condensed matter physics, and accelerator physics. It happens when the frequency of incident radiation coincides with the natural frequency of rotation of electrons in magnetic fields. A free electron in a static and uniform magnetic field will move in a circle due to the Lorentz force.  The circular motion may be superimposed with a uniform axial motion, resulting in a helix, or with a uniform motion perpendicular to the field (e.g., in the presence of an electrical or gravitational field) resulting in a cycloid.  The angular frequency (ω = 2πf ) of this cyclotron motion for a given magnetic field strength B is given (in SI units) by

  
    
      
        
          ω
          
            ce
          
        
        =
        
          
            
              e
              B
            
            
              m
              
                e
              
            
          
        
      
    
    {\displaystyle \omega _{\text{ce}}={\frac {eB}{m_{\text{e}}}}}
  .where 
  
    e
    e
   is the elementary charge and 
  
    m
    m
   is the mass of the electron. For the commonly used microwave frequency 2.45 GHz and the bare electron charge and mass, the resonance condition is met when B = 875 G = 0.0875 T.
For particles of charge q, electron rest mass m0,e moving at relativistic speeds v, the formula needs to be adjusted according to the special theory of relativity to:

  
    
      
        
          ω
          
            ce
          
        
        =
        
          
            
              e
              B
            
            
              γ
              
                m
                
                  0
                  ,
                  
                    e
                  
                
              
            
          
        
      
    
    {\displaystyle \omega _{\text{ce}}={\frac {eB}{\gamma m_{0,{\text{e}}}}}}
  where

  
    
      
        γ
        =
        
          
            1
            
              1
              −
              
                
                  (
                  
                    
                      v
                      c
                    
                  
                  )
                
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \gamma ={\frac {1}{\sqrt {1-\left({\frac {v}{c}}\right)^{2}}}}}
  .

In plasma physics
An ionized plasma may be efficiently produced or heated by superimposing a static magnetic field and a high-frequency electromagnetic field at the electron cyclotron resonance frequency. In the toroidal magnetic fields used in magnetic fusion energy research, the magnetic field decreases with the major radius, so the location of the power deposition can be controlled within about a centimeter."
How can an ionized plasma be efficiently produced or heated?,By superimposing a static magnetic field and a high-frequency electromagnetic field at the electron cyclotron resonance frequency,By superimposing a static magnetic field and a low-frequency electromagnetic field at the electron cyclotron resonance frequency,By superimposing a dynamic magnetic field and a high-frequency electromagnetic field at the electron cyclotron resonance frequency,By superimposing a dynamic magnetic field and a low-frequency electromagnetic field at the electron cyclotron resonance frequency,By superimposing a static magnetic field and a high-frequency electromagnetic field at a frequency other than the electron cyclotron resonance frequency,A,"Electron cyclotron resonance (ECR) is a phenomenon observed in plasma physics, condensed matter physics, and accelerator physics. It happens when the frequency of incident radiation coincides with the natural frequency of rotation of electrons in magnetic fields. A free electron in a static and uniform magnetic field will move in a circle due to the Lorentz force.  The circular motion may be superimposed with a uniform axial motion, resulting in a helix, or with a uniform motion perpendicular to the field (e.g., in the presence of an electrical or gravitational field) resulting in a cycloid.  The angular frequency (ω = 2πf ) of this cyclotron motion for a given magnetic field strength B is given (in SI units) by

  
    
      
        
          ω
          
            ce
          
        
        =
        
          
            
              e
              B
            
            
              m
              
                e
              
            
          
        
      
    
    {\displaystyle \omega _{\text{ce}}={\frac {eB}{m_{\text{e}}}}}
  .where 
  
    e
    e
   is the elementary charge and 
  
    m
    m
   is the mass of the electron. For the commonly used microwave frequency 2.45 GHz and the bare electron charge and mass, the resonance condition is met when B = 875 G = 0.0875 T.
For particles of charge q, electron rest mass m0,e moving at relativistic speeds v, the formula needs to be adjusted according to the special theory of relativity to:

  
    
      
        
          ω
          
            ce
          
        
        =
        
          
            
              e
              B
            
            
              γ
              
                m
                
                  0
                  ,
                  
                    e
                  
                
              
            
          
        
      
    
    {\displaystyle \omega _{\text{ce}}={\frac {eB}{\gamma m_{0,{\text{e}}}}}}
  where

  
    
      
        γ
        =
        
          
            1
            
              1
              −
              
                
                  (
                  
                    
                      v
                      c
                    
                  
                  )
                
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \gamma ={\frac {1}{\sqrt {1-\left({\frac {v}{c}}\right)^{2}}}}}
  .

In plasma physics
An ionized plasma may be efficiently produced or heated by superimposing a static magnetic field and a high-frequency electromagnetic field at the electron cyclotron resonance frequency. In the toroidal magnetic fields used in magnetic fusion energy research, the magnetic field decreases with the major radius, so the location of the power deposition can be controlled within about a centimeter."
"In magnetic fusion energy research, how can the location of power deposition be controlled?",By adjusting the static magnetic field strength,By adjusting the high-frequency electromagnetic field strength,By adjusting the toroidal magnetic field strength,By adjusting the major radius,By adjusting the axial motion,D,"Electron cyclotron resonance (ECR) is a phenomenon observed in plasma physics, condensed matter physics, and accelerator physics. It happens when the frequency of incident radiation coincides with the natural frequency of rotation of electrons in magnetic fields. A free electron in a static and uniform magnetic field will move in a circle due to the Lorentz force.  The circular motion may be superimposed with a uniform axial motion, resulting in a helix, or with a uniform motion perpendicular to the field (e.g., in the presence of an electrical or gravitational field) resulting in a cycloid.  The angular frequency (ω = 2πf ) of this cyclotron motion for a given magnetic field strength B is given (in SI units) by

  
    
      
        
          ω
          
            ce
          
        
        =
        
          
            
              e
              B
            
            
              m
              
                e
              
            
          
        
      
    
    {\displaystyle \omega _{\text{ce}}={\frac {eB}{m_{\text{e}}}}}
  .where 
  
    e
    e
   is the elementary charge and 
  
    m
    m
   is the mass of the electron. For the commonly used microwave frequency 2.45 GHz and the bare electron charge and mass, the resonance condition is met when B = 875 G = 0.0875 T.
For particles of charge q, electron rest mass m0,e moving at relativistic speeds v, the formula needs to be adjusted according to the special theory of relativity to:

  
    
      
        
          ω
          
            ce
          
        
        =
        
          
            
              e
              B
            
            
              γ
              
                m
                
                  0
                  ,
                  
                    e
                  
                
              
            
          
        
      
    
    {\displaystyle \omega _{\text{ce}}={\frac {eB}{\gamma m_{0,{\text{e}}}}}}
  where

  
    
      
        γ
        =
        
          
            1
            
              1
              −
              
                
                  (
                  
                    
                      v
                      c
                    
                  
                  )
                
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \gamma ={\frac {1}{\sqrt {1-\left({\frac {v}{c}}\right)^{2}}}}}
  .

In plasma physics
An ionized plasma may be efficiently produced or heated by superimposing a static magnetic field and a high-frequency electromagnetic field at the electron cyclotron resonance frequency. In the toroidal magnetic fields used in magnetic fusion energy research, the magnetic field decreases with the major radius, so the location of the power deposition can be controlled within about a centimeter."
What is process control?,"A discipline that uses industrial control systems and control theory to achieve consistency, economy, and safety in production processes","A technique used in industries such as automotive, mining, dredging, oil refining, pulp and paper manufacturing, chemical processing, and power generating plants",A method that enables a small number of operators to manage complex processes to a high degree of consistency,A system that allows for the design and operation of large high volume and complex processes,All of the above,E,"An industrial process control or simply process control in continuous production processes is a discipline that uses industrial control systems and control theory to achieve a production level of consistency, economy and safety which could not be achieved purely by human manual control. It is implemented widely in industries such as automotive, mining, dredging, oil refining, pulp and paper manufacturing, chemical processing and power generating plants.There is a wide range of size, type and complexity, but it enables a small number of operators to manage complex processes to a high degree of consistency. The development of large industrial process control systems was instrumental in enabling the design of large high volume and complex processes, which could not be otherwise economically or safely operated.The applications can range from controlling the temperature and level of a single process vessel, to a complete chemical processing plant with several thousand control loops.

History
Early process control breakthroughs came most frequently in the form of water control devices. Ktesibios of Alexandria is credited for inventing float valves to regulate water level of water clocks in the 3rd century BC. In the 1st century AD, Heron of Alexandria invented a water valve similar to the fill valve used in modern toilets.Later process controls inventions involved basic physics principles. In 1620, Cornelis Drebbel invented a bimetallic thermostat for controlling the temperature in a furnace. In 1681, Denis Papin discovered the pressure inside a vessel could be regulated by placing weights on top of the vessel lid."
What are some industries that use process control?,Automotive,Mining,Dredging,Oil refining,All of the above,E,"An industrial process control or simply process control in continuous production processes is a discipline that uses industrial control systems and control theory to achieve a production level of consistency, economy and safety which could not be achieved purely by human manual control. It is implemented widely in industries such as automotive, mining, dredging, oil refining, pulp and paper manufacturing, chemical processing and power generating plants.There is a wide range of size, type and complexity, but it enables a small number of operators to manage complex processes to a high degree of consistency. The development of large industrial process control systems was instrumental in enabling the design of large high volume and complex processes, which could not be otherwise economically or safely operated.The applications can range from controlling the temperature and level of a single process vessel, to a complete chemical processing plant with several thousand control loops.

History
Early process control breakthroughs came most frequently in the form of water control devices. Ktesibios of Alexandria is credited for inventing float valves to regulate water level of water clocks in the 3rd century BC. In the 1st century AD, Heron of Alexandria invented a water valve similar to the fill valve used in modern toilets.Later process controls inventions involved basic physics principles. In 1620, Cornelis Drebbel invented a bimetallic thermostat for controlling the temperature in a furnace. In 1681, Denis Papin discovered the pressure inside a vessel could be regulated by placing weights on top of the vessel lid."
What were some early process control breakthroughs?,Inventing float valves to regulate water level in water clocks,Inventing a water valve similar to the fill valve used in modern toilets,Inventing a bimetallic thermostat for controlling temperature in a furnace,Discovering the pressure inside a vessel could be regulated by placing weights on top of the vessel lid,All of the above,E,"An industrial process control or simply process control in continuous production processes is a discipline that uses industrial control systems and control theory to achieve a production level of consistency, economy and safety which could not be achieved purely by human manual control. It is implemented widely in industries such as automotive, mining, dredging, oil refining, pulp and paper manufacturing, chemical processing and power generating plants.There is a wide range of size, type and complexity, but it enables a small number of operators to manage complex processes to a high degree of consistency. The development of large industrial process control systems was instrumental in enabling the design of large high volume and complex processes, which could not be otherwise economically or safely operated.The applications can range from controlling the temperature and level of a single process vessel, to a complete chemical processing plant with several thousand control loops.

History
Early process control breakthroughs came most frequently in the form of water control devices. Ktesibios of Alexandria is credited for inventing float valves to regulate water level of water clocks in the 3rd century BC. In the 1st century AD, Heron of Alexandria invented a water valve similar to the fill valve used in modern toilets.Later process controls inventions involved basic physics principles. In 1620, Cornelis Drebbel invented a bimetallic thermostat for controlling the temperature in a furnace. In 1681, Denis Papin discovered the pressure inside a vessel could be regulated by placing weights on top of the vessel lid."
What was the role of large industrial process control systems in the development of complex processes?,They enabled the design of large high volume and complex processes,They made complex processes economically and safely operable,They allowed for the management of complex processes by a small number of operators,"They ensured consistency, economy, and safety in production processes",All of the above,E,"An industrial process control or simply process control in continuous production processes is a discipline that uses industrial control systems and control theory to achieve a production level of consistency, economy and safety which could not be achieved purely by human manual control. It is implemented widely in industries such as automotive, mining, dredging, oil refining, pulp and paper manufacturing, chemical processing and power generating plants.There is a wide range of size, type and complexity, but it enables a small number of operators to manage complex processes to a high degree of consistency. The development of large industrial process control systems was instrumental in enabling the design of large high volume and complex processes, which could not be otherwise economically or safely operated.The applications can range from controlling the temperature and level of a single process vessel, to a complete chemical processing plant with several thousand control loops.

History
Early process control breakthroughs came most frequently in the form of water control devices. Ktesibios of Alexandria is credited for inventing float valves to regulate water level of water clocks in the 3rd century BC. In the 1st century AD, Heron of Alexandria invented a water valve similar to the fill valve used in modern toilets.Later process controls inventions involved basic physics principles. In 1620, Cornelis Drebbel invented a bimetallic thermostat for controlling the temperature in a furnace. In 1681, Denis Papin discovered the pressure inside a vessel could be regulated by placing weights on top of the vessel lid."
What can the applications of process control range from?,Controlling the temperature and level of a single process vessel,Managing a complete chemical processing plant with several thousand control loops,"Achieving consistency, economy, and safety in production processes",Designing and operating large high volume and complex processes,All of the above,E,"An industrial process control or simply process control in continuous production processes is a discipline that uses industrial control systems and control theory to achieve a production level of consistency, economy and safety which could not be achieved purely by human manual control. It is implemented widely in industries such as automotive, mining, dredging, oil refining, pulp and paper manufacturing, chemical processing and power generating plants.There is a wide range of size, type and complexity, but it enables a small number of operators to manage complex processes to a high degree of consistency. The development of large industrial process control systems was instrumental in enabling the design of large high volume and complex processes, which could not be otherwise economically or safely operated.The applications can range from controlling the temperature and level of a single process vessel, to a complete chemical processing plant with several thousand control loops.

History
Early process control breakthroughs came most frequently in the form of water control devices. Ktesibios of Alexandria is credited for inventing float valves to regulate water level of water clocks in the 3rd century BC. In the 1st century AD, Heron of Alexandria invented a water valve similar to the fill valve used in modern toilets.Later process controls inventions involved basic physics principles. In 1620, Cornelis Drebbel invented a bimetallic thermostat for controlling the temperature in a furnace. In 1681, Denis Papin discovered the pressure inside a vessel could be regulated by placing weights on top of the vessel lid."
Who devised the Peterson Identification System?,John James Audubon,Roger Tory Peterson,Rachel Carson,Jane Goodall,David Attenborough,B,"The Peterson Identification System is a practical method for the field identification of animals, plants and other natural phenomena. It was devised by ornithologist Roger Tory Peterson in 1934 for the first of his series of Field Guides (See Peterson Field Guides.) Peterson devised his system ""so that live birds could be identified readily at a distance by their 'field marks' without resorting to the bird-in-hand characters that the early collectors relied on. During the last half century the binocular and the spotting scope have replaced the shotgun."" As such, it both reflected and contributed to awareness of the emerging early environmental movement. Another application of this system was made when Roger Tory Peterson was enlisted in the US Army Corps of Engineers from 1943 to 1945. “...plane identification—the aircraft spotting technique—was based on Roger’s bird identification method-the Peterson system.”.

The system
Created for use by amateur naturalists and laymen, rather than specialists, the ""Peterson System"" is essentially a pictorial key based upon readily noticed visual impressions rather than on the technical features of interest to scientists. The technique involves patternistic drawings with arrows that pinpoint the key field comparisons between similar species.

Influence
Since the first Peterson Field Guide, the system has been expanded to about three dozen volumes in the series as well as being emulated by many other publishers and authors of field guides. It has become the near-universally accepted standard, first in the United States and Europe and then around the world.


== References ==."
What is the purpose of the Peterson Identification System?,To identify birds by their field marks,To collect birds for scientific study,To study the behavior of birds,To protect birds from environmental threats,To train bird watchers,A,"The Peterson Identification System is a practical method for the field identification of animals, plants and other natural phenomena. It was devised by ornithologist Roger Tory Peterson in 1934 for the first of his series of Field Guides (See Peterson Field Guides.) Peterson devised his system ""so that live birds could be identified readily at a distance by their 'field marks' without resorting to the bird-in-hand characters that the early collectors relied on. During the last half century the binocular and the spotting scope have replaced the shotgun."" As such, it both reflected and contributed to awareness of the emerging early environmental movement. Another application of this system was made when Roger Tory Peterson was enlisted in the US Army Corps of Engineers from 1943 to 1945. “...plane identification—the aircraft spotting technique—was based on Roger’s bird identification method-the Peterson system.”.

The system
Created for use by amateur naturalists and laymen, rather than specialists, the ""Peterson System"" is essentially a pictorial key based upon readily noticed visual impressions rather than on the technical features of interest to scientists. The technique involves patternistic drawings with arrows that pinpoint the key field comparisons between similar species.

Influence
Since the first Peterson Field Guide, the system has been expanded to about three dozen volumes in the series as well as being emulated by many other publishers and authors of field guides. It has become the near-universally accepted standard, first in the United States and Europe and then around the world.


== References ==."
How did the Peterson Identification System contribute to the environmental movement?,By promoting the use of binoculars and spotting scopes,By replacing the shotgun as a bird watching tool,By raising awareness of the need for bird conservation,By advocating for the protection of natural habitats,By educating amateur naturalists and laymen,C,"The Peterson Identification System is a practical method for the field identification of animals, plants and other natural phenomena. It was devised by ornithologist Roger Tory Peterson in 1934 for the first of his series of Field Guides (See Peterson Field Guides.) Peterson devised his system ""so that live birds could be identified readily at a distance by their 'field marks' without resorting to the bird-in-hand characters that the early collectors relied on. During the last half century the binocular and the spotting scope have replaced the shotgun."" As such, it both reflected and contributed to awareness of the emerging early environmental movement. Another application of this system was made when Roger Tory Peterson was enlisted in the US Army Corps of Engineers from 1943 to 1945. “...plane identification—the aircraft spotting technique—was based on Roger’s bird identification method-the Peterson system.”.

The system
Created for use by amateur naturalists and laymen, rather than specialists, the ""Peterson System"" is essentially a pictorial key based upon readily noticed visual impressions rather than on the technical features of interest to scientists. The technique involves patternistic drawings with arrows that pinpoint the key field comparisons between similar species.

Influence
Since the first Peterson Field Guide, the system has been expanded to about three dozen volumes in the series as well as being emulated by many other publishers and authors of field guides. It has become the near-universally accepted standard, first in the United States and Europe and then around the world.


== References ==."
What was another application of the Peterson System?,Plane identification,Fish identification,Plant identification,Insect identification,Mammal identification,A,"The Peterson Identification System is a practical method for the field identification of animals, plants and other natural phenomena. It was devised by ornithologist Roger Tory Peterson in 1934 for the first of his series of Field Guides (See Peterson Field Guides.) Peterson devised his system ""so that live birds could be identified readily at a distance by their 'field marks' without resorting to the bird-in-hand characters that the early collectors relied on. During the last half century the binocular and the spotting scope have replaced the shotgun."" As such, it both reflected and contributed to awareness of the emerging early environmental movement. Another application of this system was made when Roger Tory Peterson was enlisted in the US Army Corps of Engineers from 1943 to 1945. “...plane identification—the aircraft spotting technique—was based on Roger’s bird identification method-the Peterson system.”.

The system
Created for use by amateur naturalists and laymen, rather than specialists, the ""Peterson System"" is essentially a pictorial key based upon readily noticed visual impressions rather than on the technical features of interest to scientists. The technique involves patternistic drawings with arrows that pinpoint the key field comparisons between similar species.

Influence
Since the first Peterson Field Guide, the system has been expanded to about three dozen volumes in the series as well as being emulated by many other publishers and authors of field guides. It has become the near-universally accepted standard, first in the United States and Europe and then around the world.


== References ==."
How widely accepted is the Peterson Identification System?,It is only accepted in the United States and Europe,It is the near-universally accepted standard worldwide,It is only accepted by specialists,It is not widely accepted by naturalists and laymen,It is accepted by some publishers and authors of field guides,B,"The Peterson Identification System is a practical method for the field identification of animals, plants and other natural phenomena. It was devised by ornithologist Roger Tory Peterson in 1934 for the first of his series of Field Guides (See Peterson Field Guides.) Peterson devised his system ""so that live birds could be identified readily at a distance by their 'field marks' without resorting to the bird-in-hand characters that the early collectors relied on. During the last half century the binocular and the spotting scope have replaced the shotgun."" As such, it both reflected and contributed to awareness of the emerging early environmental movement. Another application of this system was made when Roger Tory Peterson was enlisted in the US Army Corps of Engineers from 1943 to 1945. “...plane identification—the aircraft spotting technique—was based on Roger’s bird identification method-the Peterson system.”.

The system
Created for use by amateur naturalists and laymen, rather than specialists, the ""Peterson System"" is essentially a pictorial key based upon readily noticed visual impressions rather than on the technical features of interest to scientists. The technique involves patternistic drawings with arrows that pinpoint the key field comparisons between similar species.

Influence
Since the first Peterson Field Guide, the system has been expanded to about three dozen volumes in the series as well as being emulated by many other publishers and authors of field guides. It has become the near-universally accepted standard, first in the United States and Europe and then around the world.


== References ==."
What is the overall aim of gerontechnology?,To maximize the vital and productive years in the later years of life,To create technological environments for older people,To reduce the cost of care for older adults,To promote human health and well-being,To increase vitality and quality of life throughout the lifespan,A,"Gerontechnology, also called gerotechnology, is an inter- and multidisciplinary academic and professional field combining gerontology and technology. Sustainability of an aging society depends upon our effectiveness in creating technological environments, including assistive technology and inclusive design, for innovative and independent living and social participation of older adults in any state of health, comfort and safety. In short, gerontechnology concerns matching technological environments to health, housing, mobility, communication, leisure and work of older people. Gerontechnology is most frequently identified as a subset of HealthTech and is more commonly referred to as AgeTech in Europe and the United States. Research outcomes form the basis for designers, builders, engineers, manufacturers, and those in the health professions (nursing, medicine, gerontology, geriatrics, environmental psychology, developmental psychology, etc.), to provide an optimum living environment for the widest range of ages.

Description
Gerontechnology is considered an adjunct to the promotion of human health and well-being. It pertains to both human development and aging with the aim to compress morbidity and to increase vitality and quality of life throughout the lifespan. It creates solutions to extend the working phase in society by maximizing the vital and productive years in the later years of life, which consequently reduces the cost of care.
The overall framework of gerontechnology may be seen as a matrix of domains of human activity: (1) health & self-esteem, housing & activities of daily living, communication & governance, mobility & transport, work & leisure, as well as (2) technology interventions or impact levels (enhancement & satisfaction, prevention & engagement, compensation & assistance, care and care organisation)."
Which domains of human activity does gerontechnology cover?,"Health & self-esteem, housing & activities of daily living, communication & governance, mobility & transport, work & leisure","Health & self-esteem, communication & governance, work & leisure","Housing & activities of daily living, mobility & transport, work & leisure","Health & self-esteem, housing & activities of daily living, communication & governance","Housing & activities of daily living, communication & governance, mobility & transport",A,"Gerontechnology, also called gerotechnology, is an inter- and multidisciplinary academic and professional field combining gerontology and technology. Sustainability of an aging society depends upon our effectiveness in creating technological environments, including assistive technology and inclusive design, for innovative and independent living and social participation of older adults in any state of health, comfort and safety. In short, gerontechnology concerns matching technological environments to health, housing, mobility, communication, leisure and work of older people. Gerontechnology is most frequently identified as a subset of HealthTech and is more commonly referred to as AgeTech in Europe and the United States. Research outcomes form the basis for designers, builders, engineers, manufacturers, and those in the health professions (nursing, medicine, gerontology, geriatrics, environmental psychology, developmental psychology, etc.), to provide an optimum living environment for the widest range of ages.

Description
Gerontechnology is considered an adjunct to the promotion of human health and well-being. It pertains to both human development and aging with the aim to compress morbidity and to increase vitality and quality of life throughout the lifespan. It creates solutions to extend the working phase in society by maximizing the vital and productive years in the later years of life, which consequently reduces the cost of care.
The overall framework of gerontechnology may be seen as a matrix of domains of human activity: (1) health & self-esteem, housing & activities of daily living, communication & governance, mobility & transport, work & leisure, as well as (2) technology interventions or impact levels (enhancement & satisfaction, prevention & engagement, compensation & assistance, care and care organisation)."
What is another term commonly used for gerontechnology in Europe and the United States?,HealthTech,AgeTech,Gerotechnology,Assistive Technology,Inclusive Design,B,"Gerontechnology, also called gerotechnology, is an inter- and multidisciplinary academic and professional field combining gerontology and technology. Sustainability of an aging society depends upon our effectiveness in creating technological environments, including assistive technology and inclusive design, for innovative and independent living and social participation of older adults in any state of health, comfort and safety. In short, gerontechnology concerns matching technological environments to health, housing, mobility, communication, leisure and work of older people. Gerontechnology is most frequently identified as a subset of HealthTech and is more commonly referred to as AgeTech in Europe and the United States. Research outcomes form the basis for designers, builders, engineers, manufacturers, and those in the health professions (nursing, medicine, gerontology, geriatrics, environmental psychology, developmental psychology, etc.), to provide an optimum living environment for the widest range of ages.

Description
Gerontechnology is considered an adjunct to the promotion of human health and well-being. It pertains to both human development and aging with the aim to compress morbidity and to increase vitality and quality of life throughout the lifespan. It creates solutions to extend the working phase in society by maximizing the vital and productive years in the later years of life, which consequently reduces the cost of care.
The overall framework of gerontechnology may be seen as a matrix of domains of human activity: (1) health & self-esteem, housing & activities of daily living, communication & governance, mobility & transport, work & leisure, as well as (2) technology interventions or impact levels (enhancement & satisfaction, prevention & engagement, compensation & assistance, care and care organisation)."
Who benefits from the research outcomes of gerontechnology?,"Designers, builders, engineers, and manufacturers",Those in the health professions,Older adults,Gerontechnologists,All of the above,E,"Gerontechnology, also called gerotechnology, is an inter- and multidisciplinary academic and professional field combining gerontology and technology. Sustainability of an aging society depends upon our effectiveness in creating technological environments, including assistive technology and inclusive design, for innovative and independent living and social participation of older adults in any state of health, comfort and safety. In short, gerontechnology concerns matching technological environments to health, housing, mobility, communication, leisure and work of older people. Gerontechnology is most frequently identified as a subset of HealthTech and is more commonly referred to as AgeTech in Europe and the United States. Research outcomes form the basis for designers, builders, engineers, manufacturers, and those in the health professions (nursing, medicine, gerontology, geriatrics, environmental psychology, developmental psychology, etc.), to provide an optimum living environment for the widest range of ages.

Description
Gerontechnology is considered an adjunct to the promotion of human health and well-being. It pertains to both human development and aging with the aim to compress morbidity and to increase vitality and quality of life throughout the lifespan. It creates solutions to extend the working phase in society by maximizing the vital and productive years in the later years of life, which consequently reduces the cost of care.
The overall framework of gerontechnology may be seen as a matrix of domains of human activity: (1) health & self-esteem, housing & activities of daily living, communication & governance, mobility & transport, work & leisure, as well as (2) technology interventions or impact levels (enhancement & satisfaction, prevention & engagement, compensation & assistance, care and care organisation)."
What does gerontechnology contribute to the sustainability of an aging society?,Creating technological environments for older adults,Providing an optimum living environment for all ages,Maximizing the vital and productive years in the later years of life,Compressing morbidity and increasing vitality and quality of life,Reducing the cost of care for older people,D,"Gerontechnology, also called gerotechnology, is an inter- and multidisciplinary academic and professional field combining gerontology and technology. Sustainability of an aging society depends upon our effectiveness in creating technological environments, including assistive technology and inclusive design, for innovative and independent living and social participation of older adults in any state of health, comfort and safety. In short, gerontechnology concerns matching technological environments to health, housing, mobility, communication, leisure and work of older people. Gerontechnology is most frequently identified as a subset of HealthTech and is more commonly referred to as AgeTech in Europe and the United States. Research outcomes form the basis for designers, builders, engineers, manufacturers, and those in the health professions (nursing, medicine, gerontology, geriatrics, environmental psychology, developmental psychology, etc.), to provide an optimum living environment for the widest range of ages.

Description
Gerontechnology is considered an adjunct to the promotion of human health and well-being. It pertains to both human development and aging with the aim to compress morbidity and to increase vitality and quality of life throughout the lifespan. It creates solutions to extend the working phase in society by maximizing the vital and productive years in the later years of life, which consequently reduces the cost of care.
The overall framework of gerontechnology may be seen as a matrix of domains of human activity: (1) health & self-esteem, housing & activities of daily living, communication & governance, mobility & transport, work & leisure, as well as (2) technology interventions or impact levels (enhancement & satisfaction, prevention & engagement, compensation & assistance, care and care organisation)."
What is the purpose of a switch in an electrical circuit?,To increase the electric current in the circuit,To interrupt the electric current in the circuit,To amplify the electric current in the circuit,To generate electricity in the circuit,To store electricity in the circuit,B,"In electrical engineering, a switch is an electrical component that can disconnect or connect the conducting path in an electrical circuit, interrupting the electric current or diverting it from one conductor to another. The most common type of switch is an electromechanical device consisting of one or more sets of movable electrical contacts connected to external circuits. When a pair of contacts is touching current can pass between them, while when the contacts are separated no current can flow. 
Switches are made in many different configurations; they may have multiple sets of contacts controlled by the same knob or actuator, and the contacts may operate simultaneously, sequentially, or alternately. A switch may be operated manually, for example, a light switch or a keyboard button, or may function as a sensing element to sense the position of a machine part, liquid level, pressure, or temperature, such as a thermostat. Many specialized forms exist, such as the toggle switch, rotary switch, mercury switch, push-button switch, reversing switch, relay, and circuit breaker. A common use is control of lighting, where multiple switches may be wired into one circuit to allow convenient control of light fixtures."
What is the most common type of switch?,Toggle switch,Rotary switch,Mercury switch,Push-button switch,Relay,A,"In electrical engineering, a switch is an electrical component that can disconnect or connect the conducting path in an electrical circuit, interrupting the electric current or diverting it from one conductor to another. The most common type of switch is an electromechanical device consisting of one or more sets of movable electrical contacts connected to external circuits. When a pair of contacts is touching current can pass between them, while when the contacts are separated no current can flow. 
Switches are made in many different configurations; they may have multiple sets of contacts controlled by the same knob or actuator, and the contacts may operate simultaneously, sequentially, or alternately. A switch may be operated manually, for example, a light switch or a keyboard button, or may function as a sensing element to sense the position of a machine part, liquid level, pressure, or temperature, such as a thermostat. Many specialized forms exist, such as the toggle switch, rotary switch, mercury switch, push-button switch, reversing switch, relay, and circuit breaker. A common use is control of lighting, where multiple switches may be wired into one circuit to allow convenient control of light fixtures."
How can a switch be operated?,Manually,Automatically,Remotely,All of the above,None of the above,D,"In electrical engineering, a switch is an electrical component that can disconnect or connect the conducting path in an electrical circuit, interrupting the electric current or diverting it from one conductor to another. The most common type of switch is an electromechanical device consisting of one or more sets of movable electrical contacts connected to external circuits. When a pair of contacts is touching current can pass between them, while when the contacts are separated no current can flow. 
Switches are made in many different configurations; they may have multiple sets of contacts controlled by the same knob or actuator, and the contacts may operate simultaneously, sequentially, or alternately. A switch may be operated manually, for example, a light switch or a keyboard button, or may function as a sensing element to sense the position of a machine part, liquid level, pressure, or temperature, such as a thermostat. Many specialized forms exist, such as the toggle switch, rotary switch, mercury switch, push-button switch, reversing switch, relay, and circuit breaker. A common use is control of lighting, where multiple switches may be wired into one circuit to allow convenient control of light fixtures."
What is a common use of switches?,Control of lighting,Control of temperature,Control of pressure,Control of liquid level,All of the above,A,"In electrical engineering, a switch is an electrical component that can disconnect or connect the conducting path in an electrical circuit, interrupting the electric current or diverting it from one conductor to another. The most common type of switch is an electromechanical device consisting of one or more sets of movable electrical contacts connected to external circuits. When a pair of contacts is touching current can pass between them, while when the contacts are separated no current can flow. 
Switches are made in many different configurations; they may have multiple sets of contacts controlled by the same knob or actuator, and the contacts may operate simultaneously, sequentially, or alternately. A switch may be operated manually, for example, a light switch or a keyboard button, or may function as a sensing element to sense the position of a machine part, liquid level, pressure, or temperature, such as a thermostat. Many specialized forms exist, such as the toggle switch, rotary switch, mercury switch, push-button switch, reversing switch, relay, and circuit breaker. A common use is control of lighting, where multiple switches may be wired into one circuit to allow convenient control of light fixtures."
What is a circuit breaker?,A type of switch,A device that generates electricity,A device that amplifies the electric current,A device that stores electricity,A device that measures temperature,A,"In electrical engineering, a switch is an electrical component that can disconnect or connect the conducting path in an electrical circuit, interrupting the electric current or diverting it from one conductor to another. The most common type of switch is an electromechanical device consisting of one or more sets of movable electrical contacts connected to external circuits. When a pair of contacts is touching current can pass between them, while when the contacts are separated no current can flow. 
Switches are made in many different configurations; they may have multiple sets of contacts controlled by the same knob or actuator, and the contacts may operate simultaneously, sequentially, or alternately. A switch may be operated manually, for example, a light switch or a keyboard button, or may function as a sensing element to sense the position of a machine part, liquid level, pressure, or temperature, such as a thermostat. Many specialized forms exist, such as the toggle switch, rotary switch, mercury switch, push-button switch, reversing switch, relay, and circuit breaker. A common use is control of lighting, where multiple switches may be wired into one circuit to allow convenient control of light fixtures."
What is the main purpose of the Indy Autonomous Challenge (IAC)?,To develop software for autonomous vehicles,To race autonomous race cars in oval circuits,To provide a challenging environment for the development of autonomous vehicles,To test the maturity of algorithms in cloud computing,To demonstrate the ability to race autonomously,C,"The Indy Autonomous Challenge (IAC) is the main and, as of July 2023, the only active racing series for autonomous race cars. The vehicles partecipating in the IAC are SAE level 4 autonomous as they are capable of completing circuit laps and overtaking manoeuvres without any human intervention.Each team pratecipating in the competition uses the same vehicle, a custom-built Dallara AV21 single-seater derived from their IL-15 model with the addition of all the sensors, actuators and computing hardware necessary for fully autonomous driving. The partecipating teams are constituted by university researchers by top universities worldwite, including Massachusetts Institute of Technology, University of Pittsburgh, KAIST, and Politecnico di Milano. 
The first race took place on the Indianapolis Motor Speedway (IMS) in October 2021, after an initial period of simulator-only challenges, which started in November 2019 as a proving ground to allow the competing teams to demonstrate their ability to race autonomously before receiving the phisical race car. Since then, the competition has raced in several notable oval circuits as Las Vegas Motor Speedway and Texas Motor Speedway, and in June 2023 in its first road course circuit, at the Monza Circuit.

Motivations and achievements
As a successor of the DARPA Grand Challenge, the IAC aimed to provide a challenging environment for the development of autonomous vehicles. University teams were invited to develop software for solving the autonomous driving task, but in the challenging environment of a racetrack. 
During the competition, teams used simulation environments and cloud computing to test and prove the maturity of their algorithms."
Which level of autonomy do the vehicles participating in the IAC have?,Level 1 autonomous,Level 2 autonomous,Level 3 autonomous,Level 4 autonomous,Level 5 autonomous,D,"The Indy Autonomous Challenge (IAC) is the main and, as of July 2023, the only active racing series for autonomous race cars. The vehicles partecipating in the IAC are SAE level 4 autonomous as they are capable of completing circuit laps and overtaking manoeuvres without any human intervention.Each team pratecipating in the competition uses the same vehicle, a custom-built Dallara AV21 single-seater derived from their IL-15 model with the addition of all the sensors, actuators and computing hardware necessary for fully autonomous driving. The partecipating teams are constituted by university researchers by top universities worldwite, including Massachusetts Institute of Technology, University of Pittsburgh, KAIST, and Politecnico di Milano. 
The first race took place on the Indianapolis Motor Speedway (IMS) in October 2021, after an initial period of simulator-only challenges, which started in November 2019 as a proving ground to allow the competing teams to demonstrate their ability to race autonomously before receiving the phisical race car. Since then, the competition has raced in several notable oval circuits as Las Vegas Motor Speedway and Texas Motor Speedway, and in June 2023 in its first road course circuit, at the Monza Circuit.

Motivations and achievements
As a successor of the DARPA Grand Challenge, the IAC aimed to provide a challenging environment for the development of autonomous vehicles. University teams were invited to develop software for solving the autonomous driving task, but in the challenging environment of a racetrack. 
During the competition, teams used simulation environments and cloud computing to test and prove the maturity of their algorithms."
When did the first race of the Indy Autonomous Challenge take place?,November 2019,October 2021,June 2023,July 2023,December 2022,B,"The Indy Autonomous Challenge (IAC) is the main and, as of July 2023, the only active racing series for autonomous race cars. The vehicles partecipating in the IAC are SAE level 4 autonomous as they are capable of completing circuit laps and overtaking manoeuvres without any human intervention.Each team pratecipating in the competition uses the same vehicle, a custom-built Dallara AV21 single-seater derived from their IL-15 model with the addition of all the sensors, actuators and computing hardware necessary for fully autonomous driving. The partecipating teams are constituted by university researchers by top universities worldwite, including Massachusetts Institute of Technology, University of Pittsburgh, KAIST, and Politecnico di Milano. 
The first race took place on the Indianapolis Motor Speedway (IMS) in October 2021, after an initial period of simulator-only challenges, which started in November 2019 as a proving ground to allow the competing teams to demonstrate their ability to race autonomously before receiving the phisical race car. Since then, the competition has raced in several notable oval circuits as Las Vegas Motor Speedway and Texas Motor Speedway, and in June 2023 in its first road course circuit, at the Monza Circuit.

Motivations and achievements
As a successor of the DARPA Grand Challenge, the IAC aimed to provide a challenging environment for the development of autonomous vehicles. University teams were invited to develop software for solving the autonomous driving task, but in the challenging environment of a racetrack. 
During the competition, teams used simulation environments and cloud computing to test and prove the maturity of their algorithms."
Which university is not part of the teams participating in the IAC?,Massachusetts Institute of Technology,University of Pittsburgh,KAIST,Politecnico di Milano,Stanford University,E,"The Indy Autonomous Challenge (IAC) is the main and, as of July 2023, the only active racing series for autonomous race cars. The vehicles partecipating in the IAC are SAE level 4 autonomous as they are capable of completing circuit laps and overtaking manoeuvres without any human intervention.Each team pratecipating in the competition uses the same vehicle, a custom-built Dallara AV21 single-seater derived from their IL-15 model with the addition of all the sensors, actuators and computing hardware necessary for fully autonomous driving. The partecipating teams are constituted by university researchers by top universities worldwite, including Massachusetts Institute of Technology, University of Pittsburgh, KAIST, and Politecnico di Milano. 
The first race took place on the Indianapolis Motor Speedway (IMS) in October 2021, after an initial period of simulator-only challenges, which started in November 2019 as a proving ground to allow the competing teams to demonstrate their ability to race autonomously before receiving the phisical race car. Since then, the competition has raced in several notable oval circuits as Las Vegas Motor Speedway and Texas Motor Speedway, and in June 2023 in its first road course circuit, at the Monza Circuit.

Motivations and achievements
As a successor of the DARPA Grand Challenge, the IAC aimed to provide a challenging environment for the development of autonomous vehicles. University teams were invited to develop software for solving the autonomous driving task, but in the challenging environment of a racetrack. 
During the competition, teams used simulation environments and cloud computing to test and prove the maturity of their algorithms."
What was the first road course circuit where the IAC race took place?,Indianapolis Motor Speedway,Las Vegas Motor Speedway,Texas Motor Speedway,Monza Circuit,Daytona International Speedway,D,"The Indy Autonomous Challenge (IAC) is the main and, as of July 2023, the only active racing series for autonomous race cars. The vehicles partecipating in the IAC are SAE level 4 autonomous as they are capable of completing circuit laps and overtaking manoeuvres without any human intervention.Each team pratecipating in the competition uses the same vehicle, a custom-built Dallara AV21 single-seater derived from their IL-15 model with the addition of all the sensors, actuators and computing hardware necessary for fully autonomous driving. The partecipating teams are constituted by university researchers by top universities worldwite, including Massachusetts Institute of Technology, University of Pittsburgh, KAIST, and Politecnico di Milano. 
The first race took place on the Indianapolis Motor Speedway (IMS) in October 2021, after an initial period of simulator-only challenges, which started in November 2019 as a proving ground to allow the competing teams to demonstrate their ability to race autonomously before receiving the phisical race car. Since then, the competition has raced in several notable oval circuits as Las Vegas Motor Speedway and Texas Motor Speedway, and in June 2023 in its first road course circuit, at the Monza Circuit.

Motivations and achievements
As a successor of the DARPA Grand Challenge, the IAC aimed to provide a challenging environment for the development of autonomous vehicles. University teams were invited to develop software for solving the autonomous driving task, but in the challenging environment of a racetrack. 
During the competition, teams used simulation environments and cloud computing to test and prove the maturity of their algorithms."
What is the term used to describe an enzyme that catalyses a reaction so efficiently that the rate limiting step is that of substrate diffusion into the active site or product diffusion out?,Chemical limitation,Evolutionary limitation,Kinetic perfection,Catalytic perfection,Diffusion limitation,D,"A diffusion-limited enzyme catalyses a reaction so efficiently that the rate limiting step is that of substrate diffusion into the active site, or product diffusion out. This is also known as kinetic perfection or catalytic perfection. Since the rate of catalysis of such enzymes is set by the diffusion-controlled reaction, it therefore represents an intrinsic, physical constraint on evolution (a maximum peak height in the fitness landscape). Diffusion limited perfect enzymes are very rare.  Most enzymes catalyse their reactions to a rate that is 1,000-10,000 times slower than this limit. This is due to both the chemical limitations of difficult reactions, and the evolutionary limitations that such high reaction rates do not confer any extra fitness.

History
The theory of diffusion-controlled reaction was originally utilized by R.A. Alberty, Gordon Hammes, and Manfred Eigen to estimate the upper limit of enzyme-substrate reaction."
What does a diffusion-limited enzyme represent in terms of evolution?,"An intrinsic, physical constraint",A maximum peak height in the fitness landscape,An evolutionary limitation,A chemical limitation,A diffusion limitation,B,"A diffusion-limited enzyme catalyses a reaction so efficiently that the rate limiting step is that of substrate diffusion into the active site, or product diffusion out. This is also known as kinetic perfection or catalytic perfection. Since the rate of catalysis of such enzymes is set by the diffusion-controlled reaction, it therefore represents an intrinsic, physical constraint on evolution (a maximum peak height in the fitness landscape). Diffusion limited perfect enzymes are very rare.  Most enzymes catalyse their reactions to a rate that is 1,000-10,000 times slower than this limit. This is due to both the chemical limitations of difficult reactions, and the evolutionary limitations that such high reaction rates do not confer any extra fitness.

History
The theory of diffusion-controlled reaction was originally utilized by R.A. Alberty, Gordon Hammes, and Manfred Eigen to estimate the upper limit of enzyme-substrate reaction."
How rare are diffusion limited perfect enzymes?,Very common,Very rare,Moderately common,Moderately rare,Extremely rare,E,"A diffusion-limited enzyme catalyses a reaction so efficiently that the rate limiting step is that of substrate diffusion into the active site, or product diffusion out. This is also known as kinetic perfection or catalytic perfection. Since the rate of catalysis of such enzymes is set by the diffusion-controlled reaction, it therefore represents an intrinsic, physical constraint on evolution (a maximum peak height in the fitness landscape). Diffusion limited perfect enzymes are very rare.  Most enzymes catalyse their reactions to a rate that is 1,000-10,000 times slower than this limit. This is due to both the chemical limitations of difficult reactions, and the evolutionary limitations that such high reaction rates do not confer any extra fitness.

History
The theory of diffusion-controlled reaction was originally utilized by R.A. Alberty, Gordon Hammes, and Manfred Eigen to estimate the upper limit of enzyme-substrate reaction."
What is the typical rate at which most enzymes catalyse their reactions in comparison to the diffusion-limited rate?,Same rate as the diffusion-limited rate,10-100 times slower than the diffusion-limited rate,"100-1,000 times slower than the diffusion-limited rate","1,000-10,000 times slower than the diffusion-limited rate","10,000-100,000 times slower than the diffusion-limited rate",D,"A diffusion-limited enzyme catalyses a reaction so efficiently that the rate limiting step is that of substrate diffusion into the active site, or product diffusion out. This is also known as kinetic perfection or catalytic perfection. Since the rate of catalysis of such enzymes is set by the diffusion-controlled reaction, it therefore represents an intrinsic, physical constraint on evolution (a maximum peak height in the fitness landscape). Diffusion limited perfect enzymes are very rare.  Most enzymes catalyse their reactions to a rate that is 1,000-10,000 times slower than this limit. This is due to both the chemical limitations of difficult reactions, and the evolutionary limitations that such high reaction rates do not confer any extra fitness.

History
The theory of diffusion-controlled reaction was originally utilized by R.A. Alberty, Gordon Hammes, and Manfred Eigen to estimate the upper limit of enzyme-substrate reaction."
Who originally utilized the theory of diffusion-controlled reaction to estimate the upper limit of enzyme-substrate reaction?,R.A. Alberty,Gordon Hammes,Manfred Eigen,All of the above,None of the above,D,"A diffusion-limited enzyme catalyses a reaction so efficiently that the rate limiting step is that of substrate diffusion into the active site, or product diffusion out. This is also known as kinetic perfection or catalytic perfection. Since the rate of catalysis of such enzymes is set by the diffusion-controlled reaction, it therefore represents an intrinsic, physical constraint on evolution (a maximum peak height in the fitness landscape). Diffusion limited perfect enzymes are very rare.  Most enzymes catalyse their reactions to a rate that is 1,000-10,000 times slower than this limit. This is due to both the chemical limitations of difficult reactions, and the evolutionary limitations that such high reaction rates do not confer any extra fitness.

History
The theory of diffusion-controlled reaction was originally utilized by R.A. Alberty, Gordon Hammes, and Manfred Eigen to estimate the upper limit of enzyme-substrate reaction."
What is the study of arithmetic semigroups as a means to extend notions from classical analytic number theory called?,Analytic number theory,Abstract analytic number theory,Abstract algebra,Abstract differential geometry,Absolute differential calculus,B,"Mathematics is a broad subject that is commonly divided in many areas that may be defined by their objects of study, by the used methods, or by both. For example, analytic number theory is a subarea of number theory devoted to the use of methods of analysis for the study of natural numbers.
This glossary is alphabetically sorted. This hides a large part of the relationships between areas. For the broadest areas of mathematics, see Mathematics § Areas of mathematics. The Mathematics Subject Classification is a hierarchical list of areas and subjects of study that has been elaborated by the community of mathematicians. It is used by most publishers for classifying mathematical articles and books.

A
Absolute differential calculus
 An older name of Ricci calculus 
Absolute geometry
 Also called neutral geometry, a synthetic geometry similar to Euclidean geometry but without the parallel postulate.
Abstract algebra
 The part of algebra devoted to the study of algebraic structures in themselves. Occasionally named modern algebra in course titles.
Abstract analytic number theory
 The study of arithmetic semigroups as a means to extend notions from classical analytic number theory.
Abstract differential geometry
 A form of differential geometry without the notion of smoothness from calculus."
What is the name of synthetic geometry similar to Euclidean geometry but without the parallel postulate?,Absolute differential calculus,Abstract differential geometry,Absolute geometry,Abstract algebra,Abstract analytic number theory,C,"Mathematics is a broad subject that is commonly divided in many areas that may be defined by their objects of study, by the used methods, or by both. For example, analytic number theory is a subarea of number theory devoted to the use of methods of analysis for the study of natural numbers.
This glossary is alphabetically sorted. This hides a large part of the relationships between areas. For the broadest areas of mathematics, see Mathematics § Areas of mathematics. The Mathematics Subject Classification is a hierarchical list of areas and subjects of study that has been elaborated by the community of mathematicians. It is used by most publishers for classifying mathematical articles and books.

A
Absolute differential calculus
 An older name of Ricci calculus 
Absolute geometry
 Also called neutral geometry, a synthetic geometry similar to Euclidean geometry but without the parallel postulate.
Abstract algebra
 The part of algebra devoted to the study of algebraic structures in themselves. Occasionally named modern algebra in course titles.
Abstract analytic number theory
 The study of arithmetic semigroups as a means to extend notions from classical analytic number theory.
Abstract differential geometry
 A form of differential geometry without the notion of smoothness from calculus."
What is the part of algebra devoted to the study of algebraic structures in themselves called?,Absolute differential calculus,Abstract analytic number theory,Abstract algebra,Abstract differential geometry,Absolute geometry,C,"Mathematics is a broad subject that is commonly divided in many areas that may be defined by their objects of study, by the used methods, or by both. For example, analytic number theory is a subarea of number theory devoted to the use of methods of analysis for the study of natural numbers.
This glossary is alphabetically sorted. This hides a large part of the relationships between areas. For the broadest areas of mathematics, see Mathematics § Areas of mathematics. The Mathematics Subject Classification is a hierarchical list of areas and subjects of study that has been elaborated by the community of mathematicians. It is used by most publishers for classifying mathematical articles and books.

A
Absolute differential calculus
 An older name of Ricci calculus 
Absolute geometry
 Also called neutral geometry, a synthetic geometry similar to Euclidean geometry but without the parallel postulate.
Abstract algebra
 The part of algebra devoted to the study of algebraic structures in themselves. Occasionally named modern algebra in course titles.
Abstract analytic number theory
 The study of arithmetic semigroups as a means to extend notions from classical analytic number theory.
Abstract differential geometry
 A form of differential geometry without the notion of smoothness from calculus."
What is a form of differential geometry without the notion of smoothness from calculus called?,Abstract algebra,Abstract differential geometry,Absolute geometry,Absolute differential calculus,Abstract analytic number theory,B,"Mathematics is a broad subject that is commonly divided in many areas that may be defined by their objects of study, by the used methods, or by both. For example, analytic number theory is a subarea of number theory devoted to the use of methods of analysis for the study of natural numbers.
This glossary is alphabetically sorted. This hides a large part of the relationships between areas. For the broadest areas of mathematics, see Mathematics § Areas of mathematics. The Mathematics Subject Classification is a hierarchical list of areas and subjects of study that has been elaborated by the community of mathematicians. It is used by most publishers for classifying mathematical articles and books.

A
Absolute differential calculus
 An older name of Ricci calculus 
Absolute geometry
 Also called neutral geometry, a synthetic geometry similar to Euclidean geometry but without the parallel postulate.
Abstract algebra
 The part of algebra devoted to the study of algebraic structures in themselves. Occasionally named modern algebra in course titles.
Abstract analytic number theory
 The study of arithmetic semigroups as a means to extend notions from classical analytic number theory.
Abstract differential geometry
 A form of differential geometry without the notion of smoothness from calculus."
What is an older name of Ricci calculus?,Abstract differential geometry,Abstract algebra,Absolute differential calculus,Absolute geometry,Analytic number theory,C,"Mathematics is a broad subject that is commonly divided in many areas that may be defined by their objects of study, by the used methods, or by both. For example, analytic number theory is a subarea of number theory devoted to the use of methods of analysis for the study of natural numbers.
This glossary is alphabetically sorted. This hides a large part of the relationships between areas. For the broadest areas of mathematics, see Mathematics § Areas of mathematics. The Mathematics Subject Classification is a hierarchical list of areas and subjects of study that has been elaborated by the community of mathematicians. It is used by most publishers for classifying mathematical articles and books.

A
Absolute differential calculus
 An older name of Ricci calculus 
Absolute geometry
 Also called neutral geometry, a synthetic geometry similar to Euclidean geometry but without the parallel postulate.
Abstract algebra
 The part of algebra devoted to the study of algebraic structures in themselves. Occasionally named modern algebra in course titles.
Abstract analytic number theory
 The study of arithmetic semigroups as a means to extend notions from classical analytic number theory.
Abstract differential geometry
 A form of differential geometry without the notion of smoothness from calculus."
What was Office/36?,A suite of applications for IBM System/36 computers.,A suite of applications for IBM System/360 computers.,A suite of applications for IBM System/38 computers.,A suite of applications for IBM System/32 computers.,A suite of applications for IBM System/34 computers.,A,"Office/36 was a suite of applications marketed by IBM from 1983 to 2000 for the IBM System/36 family of midrange computers. IBM announced its System/36 Office Automation (OA) strategy in 1985.Office/36 could be purchased in its entirety, or piecemeal.  Components of Office/36 include:
IDDU/36, the Interactive Data Definition Utility.
Query/36, the Query utility.
DisplayWrite/36, a word processing program.
Personal Services/36, a calendaring system and an office messaging utility.Query/36 was not quite the same as SQL, but it had some similarities, especially the ability to very rapidly create a displayed recordset from a disk file.  Note that SQL, also an IBM development, had not been standardized prior to 1986.
DisplayWrite/36, in the same category as Microsoft Word, had online dictionaries and definition capabilities, and spell-check, and unlike the standard S/36 products, it would straighten spillover text and scroll in real time.
Considerable changes were required to S/36 design to support Office/36 functionality, not the least of which was the capability to manage new container objects called ""folders"" and produce multiple extents to them on demand.  Q/36 and DW/36 typically exceeded the 64K program limit of the S/36, both in editing and printing, so using Office products could heavily impact other applications.  DW/36 allowed use of bold, underline, and other display formatting characteristics in real time.


== References ==."
Which component of Office/36 was a word processing program?,IDDU/36,Query/36,DisplayWrite/36,Personal Services/36,None of the above,C,"Office/36 was a suite of applications marketed by IBM from 1983 to 2000 for the IBM System/36 family of midrange computers. IBM announced its System/36 Office Automation (OA) strategy in 1985.Office/36 could be purchased in its entirety, or piecemeal.  Components of Office/36 include:
IDDU/36, the Interactive Data Definition Utility.
Query/36, the Query utility.
DisplayWrite/36, a word processing program.
Personal Services/36, a calendaring system and an office messaging utility.Query/36 was not quite the same as SQL, but it had some similarities, especially the ability to very rapidly create a displayed recordset from a disk file.  Note that SQL, also an IBM development, had not been standardized prior to 1986.
DisplayWrite/36, in the same category as Microsoft Word, had online dictionaries and definition capabilities, and spell-check, and unlike the standard S/36 products, it would straighten spillover text and scroll in real time.
Considerable changes were required to S/36 design to support Office/36 functionality, not the least of which was the capability to manage new container objects called ""folders"" and produce multiple extents to them on demand.  Q/36 and DW/36 typically exceeded the 64K program limit of the S/36, both in editing and printing, so using Office products could heavily impact other applications.  DW/36 allowed use of bold, underline, and other display formatting characteristics in real time.


== References ==."
Which component of Office/36 had online dictionaries and spell-check capabilities?,IDDU/36,Query/36,DisplayWrite/36,Personal Services/36,None of the above,C,"Office/36 was a suite of applications marketed by IBM from 1983 to 2000 for the IBM System/36 family of midrange computers. IBM announced its System/36 Office Automation (OA) strategy in 1985.Office/36 could be purchased in its entirety, or piecemeal.  Components of Office/36 include:
IDDU/36, the Interactive Data Definition Utility.
Query/36, the Query utility.
DisplayWrite/36, a word processing program.
Personal Services/36, a calendaring system and an office messaging utility.Query/36 was not quite the same as SQL, but it had some similarities, especially the ability to very rapidly create a displayed recordset from a disk file.  Note that SQL, also an IBM development, had not been standardized prior to 1986.
DisplayWrite/36, in the same category as Microsoft Word, had online dictionaries and definition capabilities, and spell-check, and unlike the standard S/36 products, it would straighten spillover text and scroll in real time.
Considerable changes were required to S/36 design to support Office/36 functionality, not the least of which was the capability to manage new container objects called ""folders"" and produce multiple extents to them on demand.  Q/36 and DW/36 typically exceeded the 64K program limit of the S/36, both in editing and printing, so using Office products could heavily impact other applications.  DW/36 allowed use of bold, underline, and other display formatting characteristics in real time.


== References ==."
What were the changes required to S/36 design to support Office/36 functionality?,"Addition of new container objects called ""folders"".",Ability to produce multiple extents to folders on demand.,Both option 1 and option 2.,None of the above.,Not mentioned in the text.,C,"Office/36 was a suite of applications marketed by IBM from 1983 to 2000 for the IBM System/36 family of midrange computers. IBM announced its System/36 Office Automation (OA) strategy in 1985.Office/36 could be purchased in its entirety, or piecemeal.  Components of Office/36 include:
IDDU/36, the Interactive Data Definition Utility.
Query/36, the Query utility.
DisplayWrite/36, a word processing program.
Personal Services/36, a calendaring system and an office messaging utility.Query/36 was not quite the same as SQL, but it had some similarities, especially the ability to very rapidly create a displayed recordset from a disk file.  Note that SQL, also an IBM development, had not been standardized prior to 1986.
DisplayWrite/36, in the same category as Microsoft Word, had online dictionaries and definition capabilities, and spell-check, and unlike the standard S/36 products, it would straighten spillover text and scroll in real time.
Considerable changes were required to S/36 design to support Office/36 functionality, not the least of which was the capability to manage new container objects called ""folders"" and produce multiple extents to them on demand.  Q/36 and DW/36 typically exceeded the 64K program limit of the S/36, both in editing and printing, so using Office products could heavily impact other applications.  DW/36 allowed use of bold, underline, and other display formatting characteristics in real time.


== References ==."
"Which Office/36 component allowed use of bold, underline, and other display formatting characteristics in real time?",IDDU/36,Query/36,DisplayWrite/36,Personal Services/36,None of the above,C,"Office/36 was a suite of applications marketed by IBM from 1983 to 2000 for the IBM System/36 family of midrange computers. IBM announced its System/36 Office Automation (OA) strategy in 1985.Office/36 could be purchased in its entirety, or piecemeal.  Components of Office/36 include:
IDDU/36, the Interactive Data Definition Utility.
Query/36, the Query utility.
DisplayWrite/36, a word processing program.
Personal Services/36, a calendaring system and an office messaging utility.Query/36 was not quite the same as SQL, but it had some similarities, especially the ability to very rapidly create a displayed recordset from a disk file.  Note that SQL, also an IBM development, had not been standardized prior to 1986.
DisplayWrite/36, in the same category as Microsoft Word, had online dictionaries and definition capabilities, and spell-check, and unlike the standard S/36 products, it would straighten spillover text and scroll in real time.
Considerable changes were required to S/36 design to support Office/36 functionality, not the least of which was the capability to manage new container objects called ""folders"" and produce multiple extents to them on demand.  Q/36 and DW/36 typically exceeded the 64K program limit of the S/36, both in editing and printing, so using Office products could heavily impact other applications.  DW/36 allowed use of bold, underline, and other display formatting characteristics in real time.


== References ==."
What are the major subareas of mathematical logic?,"Algebra, calculus, and geometry","Model theory, proof theory, and set theory","Number theory, analysis, and topology","Combinatorics, graph theory, and statistics","Logic, philosophy, and linguistics",B,"Mathematical logic is the study of formal logic within mathematics. Major subareas include model theory, proof theory, set theory, and recursion theory. Research in mathematical logic commonly addresses the mathematical properties of formal systems of logic such as their expressive or deductive power. However, it can also include uses of logic to characterize correct mathematical reasoning or to establish foundations of mathematics.
Since its inception, mathematical logic has both contributed to and been motivated by the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency."
What does research in mathematical logic commonly address?,The history of mathematics,The application of logic in computer programming,The mathematical properties of formal systems of logic,The study of abstract algebra,The development of mathematical algorithms,C,"Mathematical logic is the study of formal logic within mathematics. Major subareas include model theory, proof theory, set theory, and recursion theory. Research in mathematical logic commonly addresses the mathematical properties of formal systems of logic such as their expressive or deductive power. However, it can also include uses of logic to characterize correct mathematical reasoning or to establish foundations of mathematics.
Since its inception, mathematical logic has both contributed to and been motivated by the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency."
What is the study of foundations of mathematics?,The study of mathematical proofs,The study of mathematical modeling,The study of mathematical logic,The study of mathematical analysis,The study of mathematical applications,C,"Mathematical logic is the study of formal logic within mathematics. Major subareas include model theory, proof theory, set theory, and recursion theory. Research in mathematical logic commonly addresses the mathematical properties of formal systems of logic such as their expressive or deductive power. However, it can also include uses of logic to characterize correct mathematical reasoning or to establish foundations of mathematics.
Since its inception, mathematical logic has both contributed to and been motivated by the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency."
Who developed the program to prove the consistency of foundational theories?,Kurt Gödel,Gerhard Gentzen,David Hilbert,Georg Cantor,Alfred Tarski,C,"Mathematical logic is the study of formal logic within mathematics. Major subareas include model theory, proof theory, set theory, and recursion theory. Research in mathematical logic commonly addresses the mathematical properties of formal systems of logic such as their expressive or deductive power. However, it can also include uses of logic to characterize correct mathematical reasoning or to establish foundations of mathematics.
Since its inception, mathematical logic has both contributed to and been motivated by the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency."
"What did the results of Kurt Gödel, Gerhard Gentzen, and others provide?",A complete resolution to the program,A proof of the consistency of foundational theories,A new axiomatic framework for geometry,A method to compute mathematical algorithms,Partial resolution to the program and clarified the issues involved in proving consistency,E,"Mathematical logic is the study of formal logic within mathematics. Major subareas include model theory, proof theory, set theory, and recursion theory. Research in mathematical logic commonly addresses the mathematical properties of formal systems of logic such as their expressive or deductive power. However, it can also include uses of logic to characterize correct mathematical reasoning or to establish foundations of mathematics.
Since its inception, mathematical logic has both contributed to and been motivated by the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency."
What is the purpose of the BREIN grid system?,To provide easy access to grid resources,To address obstacles to wide adoption of grid technologies,To build simple and reliable grid systems for business,To develop a new user-friendly grid-based research e-infrastructure,To deliver grid technology that can be used today by current users,C,"This is a comprehensive list of Grid computing infrastructure projects.

Grid computing infrastructure projects
BREIN uses the Semantic Web and multi-agent systems to build simple and reliable grid systems for business, with a focus on engineering and logistics management.
A-Ware is developing a stable, supported, commercially exploitable, high quality technology to give easy access to grid resources.
AssessGrid addresses obstacles to wide adoption of grid technologies by bringing risk management and assessment to this field, enabling use of grid computing in business and society.
Cohesion Platform – A Java-based modular peer-to-peer multi-application desktop grid computing platform for irregularly structured problems developed at the University of Tübingen (Germany)
The European Grid Infrastructure (EGI) – A series of projects funded by the European Commission which links over 70 institutions in 27 European countries to form a multi-science computing grid infrastructure for the European Research Area, letting researchers share computer resources
GridCOMP provides an advanced component platform for an effective invisible grid.
GridECON takes a user-oriented perspective and creates solutions to grid challenges to promote widespread use of grids.
neuGRID develops a new user-friendly grid-based research e-infrastructure enabling the European neuroscience community to perform research needed for the pressing study of degenerative brain diseases, for example, Alzheimer's disease.
OurGrid aims to deliver grid technology that can be used today by current users to solve present problems. To achieve this goal, it uses a different trade-off compared to most grid projects: it forfeits supporting arbitrary applications in favor of supporting only bag-of-tasks applications.
ScottNet NCG – A distributed neural computing grid. A private commercial effort in continuous operation since 1995. This system performs a series of functions including data synchronization amongst databases, mainframe systems, and other data repositories. E-commerce transaction processing, automated research and data retrieval, content analysis, web site monitoring, scripted and dynamic user emulation, shipping and fulfillment API integration and management, RSS and NNTP monitoring and analysis, real time security enforcement, and backup/restore functions.
BEinGRID Business Experiments in Grid.
Legion – A grid computing platform developed at the University of Virginia.

Open-source grid computing infrastructure projects
These projects attempt to make large physical computation infrastructures available for researchers to use:

3G Bridge An open-source core job bridging component between different grid infrastructures.
Berkeley NOW Project.
Debian Cluster Components.
DiaGrid grid computing network centered at Purdue University.
NESSI-GRID.
OMII-Europe – An EU-funded project established to source key software components that can interoperate across several heterogeneous grid middleware platforms.
OMII-UK Provide free open source software and support to enable a sustained future for the UK e-research community.
Open Science Grid.
SARA Computing and Networking Services in Netherlands.
Storage@home Distributed storage infrastructure developed to solve the problem of backing up and sharing petabytes of scientific results using a distributed model of volunteer managed hosts. Data is maintained by a mixture of replication and monitoring.
StrataGenie Searches for trading strategies in intraday stock market data and distributes trading signals to subscribers.
Worldwide LHC Computing Grid.
The Extreme Science and Engineering Discovery Environment (XSEDE), formerly Teragrid.

See also
List of volunteer computing projects
List of citizen science projects
List of crowdsourcing projects
List of free and open-source Android applications


== References ==."
Which project aims to enable the European neuroscience community to perform research on degenerative brain diseases?,BREIN,AssessGrid,GridCOMP,neuGRID,OurGrid,D,"This is a comprehensive list of Grid computing infrastructure projects.

Grid computing infrastructure projects
BREIN uses the Semantic Web and multi-agent systems to build simple and reliable grid systems for business, with a focus on engineering and logistics management.
A-Ware is developing a stable, supported, commercially exploitable, high quality technology to give easy access to grid resources.
AssessGrid addresses obstacles to wide adoption of grid technologies by bringing risk management and assessment to this field, enabling use of grid computing in business and society.
Cohesion Platform – A Java-based modular peer-to-peer multi-application desktop grid computing platform for irregularly structured problems developed at the University of Tübingen (Germany)
The European Grid Infrastructure (EGI) – A series of projects funded by the European Commission which links over 70 institutions in 27 European countries to form a multi-science computing grid infrastructure for the European Research Area, letting researchers share computer resources
GridCOMP provides an advanced component platform for an effective invisible grid.
GridECON takes a user-oriented perspective and creates solutions to grid challenges to promote widespread use of grids.
neuGRID develops a new user-friendly grid-based research e-infrastructure enabling the European neuroscience community to perform research needed for the pressing study of degenerative brain diseases, for example, Alzheimer's disease.
OurGrid aims to deliver grid technology that can be used today by current users to solve present problems. To achieve this goal, it uses a different trade-off compared to most grid projects: it forfeits supporting arbitrary applications in favor of supporting only bag-of-tasks applications.
ScottNet NCG – A distributed neural computing grid. A private commercial effort in continuous operation since 1995. This system performs a series of functions including data synchronization amongst databases, mainframe systems, and other data repositories. E-commerce transaction processing, automated research and data retrieval, content analysis, web site monitoring, scripted and dynamic user emulation, shipping and fulfillment API integration and management, RSS and NNTP monitoring and analysis, real time security enforcement, and backup/restore functions.
BEinGRID Business Experiments in Grid.
Legion – A grid computing platform developed at the University of Virginia.

Open-source grid computing infrastructure projects
These projects attempt to make large physical computation infrastructures available for researchers to use:

3G Bridge An open-source core job bridging component between different grid infrastructures.
Berkeley NOW Project.
Debian Cluster Components.
DiaGrid grid computing network centered at Purdue University.
NESSI-GRID.
OMII-Europe – An EU-funded project established to source key software components that can interoperate across several heterogeneous grid middleware platforms.
OMII-UK Provide free open source software and support to enable a sustained future for the UK e-research community.
Open Science Grid.
SARA Computing and Networking Services in Netherlands.
Storage@home Distributed storage infrastructure developed to solve the problem of backing up and sharing petabytes of scientific results using a distributed model of volunteer managed hosts. Data is maintained by a mixture of replication and monitoring.
StrataGenie Searches for trading strategies in intraday stock market data and distributes trading signals to subscribers.
Worldwide LHC Computing Grid.
The Extreme Science and Engineering Discovery Environment (XSEDE), formerly Teragrid.

See also
List of volunteer computing projects
List of citizen science projects
List of crowdsourcing projects
List of free and open-source Android applications


== References ==."
What is the purpose of the OurGrid project?,To provide easy access to grid resources,To address obstacles to wide adoption of grid technologies,To build simple and reliable grid systems for business,To develop a new user-friendly grid-based research e-infrastructure,To deliver grid technology that can be used today by current users,E,"This is a comprehensive list of Grid computing infrastructure projects.

Grid computing infrastructure projects
BREIN uses the Semantic Web and multi-agent systems to build simple and reliable grid systems for business, with a focus on engineering and logistics management.
A-Ware is developing a stable, supported, commercially exploitable, high quality technology to give easy access to grid resources.
AssessGrid addresses obstacles to wide adoption of grid technologies by bringing risk management and assessment to this field, enabling use of grid computing in business and society.
Cohesion Platform – A Java-based modular peer-to-peer multi-application desktop grid computing platform for irregularly structured problems developed at the University of Tübingen (Germany)
The European Grid Infrastructure (EGI) – A series of projects funded by the European Commission which links over 70 institutions in 27 European countries to form a multi-science computing grid infrastructure for the European Research Area, letting researchers share computer resources
GridCOMP provides an advanced component platform for an effective invisible grid.
GridECON takes a user-oriented perspective and creates solutions to grid challenges to promote widespread use of grids.
neuGRID develops a new user-friendly grid-based research e-infrastructure enabling the European neuroscience community to perform research needed for the pressing study of degenerative brain diseases, for example, Alzheimer's disease.
OurGrid aims to deliver grid technology that can be used today by current users to solve present problems. To achieve this goal, it uses a different trade-off compared to most grid projects: it forfeits supporting arbitrary applications in favor of supporting only bag-of-tasks applications.
ScottNet NCG – A distributed neural computing grid. A private commercial effort in continuous operation since 1995. This system performs a series of functions including data synchronization amongst databases, mainframe systems, and other data repositories. E-commerce transaction processing, automated research and data retrieval, content analysis, web site monitoring, scripted and dynamic user emulation, shipping and fulfillment API integration and management, RSS and NNTP monitoring and analysis, real time security enforcement, and backup/restore functions.
BEinGRID Business Experiments in Grid.
Legion – A grid computing platform developed at the University of Virginia.

Open-source grid computing infrastructure projects
These projects attempt to make large physical computation infrastructures available for researchers to use:

3G Bridge An open-source core job bridging component between different grid infrastructures.
Berkeley NOW Project.
Debian Cluster Components.
DiaGrid grid computing network centered at Purdue University.
NESSI-GRID.
OMII-Europe – An EU-funded project established to source key software components that can interoperate across several heterogeneous grid middleware platforms.
OMII-UK Provide free open source software and support to enable a sustained future for the UK e-research community.
Open Science Grid.
SARA Computing and Networking Services in Netherlands.
Storage@home Distributed storage infrastructure developed to solve the problem of backing up and sharing petabytes of scientific results using a distributed model of volunteer managed hosts. Data is maintained by a mixture of replication and monitoring.
StrataGenie Searches for trading strategies in intraday stock market data and distributes trading signals to subscribers.
Worldwide LHC Computing Grid.
The Extreme Science and Engineering Discovery Environment (XSEDE), formerly Teragrid.

See also
List of volunteer computing projects
List of citizen science projects
List of crowdsourcing projects
List of free and open-source Android applications


== References ==."
Which project is a distributed neural computing grid?,GridECON,ScottNet NCG,BEinGRID,Legion,Open Science Grid,B,"This is a comprehensive list of Grid computing infrastructure projects.

Grid computing infrastructure projects
BREIN uses the Semantic Web and multi-agent systems to build simple and reliable grid systems for business, with a focus on engineering and logistics management.
A-Ware is developing a stable, supported, commercially exploitable, high quality technology to give easy access to grid resources.
AssessGrid addresses obstacles to wide adoption of grid technologies by bringing risk management and assessment to this field, enabling use of grid computing in business and society.
Cohesion Platform – A Java-based modular peer-to-peer multi-application desktop grid computing platform for irregularly structured problems developed at the University of Tübingen (Germany)
The European Grid Infrastructure (EGI) – A series of projects funded by the European Commission which links over 70 institutions in 27 European countries to form a multi-science computing grid infrastructure for the European Research Area, letting researchers share computer resources
GridCOMP provides an advanced component platform for an effective invisible grid.
GridECON takes a user-oriented perspective and creates solutions to grid challenges to promote widespread use of grids.
neuGRID develops a new user-friendly grid-based research e-infrastructure enabling the European neuroscience community to perform research needed for the pressing study of degenerative brain diseases, for example, Alzheimer's disease.
OurGrid aims to deliver grid technology that can be used today by current users to solve present problems. To achieve this goal, it uses a different trade-off compared to most grid projects: it forfeits supporting arbitrary applications in favor of supporting only bag-of-tasks applications.
ScottNet NCG – A distributed neural computing grid. A private commercial effort in continuous operation since 1995. This system performs a series of functions including data synchronization amongst databases, mainframe systems, and other data repositories. E-commerce transaction processing, automated research and data retrieval, content analysis, web site monitoring, scripted and dynamic user emulation, shipping and fulfillment API integration and management, RSS and NNTP monitoring and analysis, real time security enforcement, and backup/restore functions.
BEinGRID Business Experiments in Grid.
Legion – A grid computing platform developed at the University of Virginia.

Open-source grid computing infrastructure projects
These projects attempt to make large physical computation infrastructures available for researchers to use:

3G Bridge An open-source core job bridging component between different grid infrastructures.
Berkeley NOW Project.
Debian Cluster Components.
DiaGrid grid computing network centered at Purdue University.
NESSI-GRID.
OMII-Europe – An EU-funded project established to source key software components that can interoperate across several heterogeneous grid middleware platforms.
OMII-UK Provide free open source software and support to enable a sustained future for the UK e-research community.
Open Science Grid.
SARA Computing and Networking Services in Netherlands.
Storage@home Distributed storage infrastructure developed to solve the problem of backing up and sharing petabytes of scientific results using a distributed model of volunteer managed hosts. Data is maintained by a mixture of replication and monitoring.
StrataGenie Searches for trading strategies in intraday stock market data and distributes trading signals to subscribers.
Worldwide LHC Computing Grid.
The Extreme Science and Engineering Discovery Environment (XSEDE), formerly Teragrid.

See also
List of volunteer computing projects
List of citizen science projects
List of crowdsourcing projects
List of free and open-source Android applications


== References ==."
Which project aims to provide free open source software and support to the UK e-research community?,OMII-Europe,OMII-UK,SARA Computing and Networking Services,Storage@home,StrataGenie,B,"This is a comprehensive list of Grid computing infrastructure projects.

Grid computing infrastructure projects
BREIN uses the Semantic Web and multi-agent systems to build simple and reliable grid systems for business, with a focus on engineering and logistics management.
A-Ware is developing a stable, supported, commercially exploitable, high quality technology to give easy access to grid resources.
AssessGrid addresses obstacles to wide adoption of grid technologies by bringing risk management and assessment to this field, enabling use of grid computing in business and society.
Cohesion Platform – A Java-based modular peer-to-peer multi-application desktop grid computing platform for irregularly structured problems developed at the University of Tübingen (Germany)
The European Grid Infrastructure (EGI) – A series of projects funded by the European Commission which links over 70 institutions in 27 European countries to form a multi-science computing grid infrastructure for the European Research Area, letting researchers share computer resources
GridCOMP provides an advanced component platform for an effective invisible grid.
GridECON takes a user-oriented perspective and creates solutions to grid challenges to promote widespread use of grids.
neuGRID develops a new user-friendly grid-based research e-infrastructure enabling the European neuroscience community to perform research needed for the pressing study of degenerative brain diseases, for example, Alzheimer's disease.
OurGrid aims to deliver grid technology that can be used today by current users to solve present problems. To achieve this goal, it uses a different trade-off compared to most grid projects: it forfeits supporting arbitrary applications in favor of supporting only bag-of-tasks applications.
ScottNet NCG – A distributed neural computing grid. A private commercial effort in continuous operation since 1995. This system performs a series of functions including data synchronization amongst databases, mainframe systems, and other data repositories. E-commerce transaction processing, automated research and data retrieval, content analysis, web site monitoring, scripted and dynamic user emulation, shipping and fulfillment API integration and management, RSS and NNTP monitoring and analysis, real time security enforcement, and backup/restore functions.
BEinGRID Business Experiments in Grid.
Legion – A grid computing platform developed at the University of Virginia.

Open-source grid computing infrastructure projects
These projects attempt to make large physical computation infrastructures available for researchers to use:

3G Bridge An open-source core job bridging component between different grid infrastructures.
Berkeley NOW Project.
Debian Cluster Components.
DiaGrid grid computing network centered at Purdue University.
NESSI-GRID.
OMII-Europe – An EU-funded project established to source key software components that can interoperate across several heterogeneous grid middleware platforms.
OMII-UK Provide free open source software and support to enable a sustained future for the UK e-research community.
Open Science Grid.
SARA Computing and Networking Services in Netherlands.
Storage@home Distributed storage infrastructure developed to solve the problem of backing up and sharing petabytes of scientific results using a distributed model of volunteer managed hosts. Data is maintained by a mixture of replication and monitoring.
StrataGenie Searches for trading strategies in intraday stock market data and distributes trading signals to subscribers.
Worldwide LHC Computing Grid.
The Extreme Science and Engineering Discovery Environment (XSEDE), formerly Teragrid.

See also
List of volunteer computing projects
List of citizen science projects
List of crowdsourcing projects
List of free and open-source Android applications


== References ==."
What is the common assumption stated as part of these puzzles?,The jugs are regularly shaped and marked,It is possible to accurately measure any quantity of water that does not completely fill a jug,Water can be spilled during the pouring process,Each step pouring water from a source jug to a destination jug stops when either the source jug is empty or the destination jug is full,The jugs have unknown integer capacities,D,"Water pouring puzzles (also called water jug problems, decanting problems, measuring puzzles, or Die Hard with a Vengeance puzzles) are a class of puzzle involving a finite collection of water jugs of known integer capacities (in terms of a liquid measure such as liters or gallons).
Initially each jug contains a known integer volume of liquid, not necessarily equal to its capacity.
Puzzles of this type ask how many steps of pouring water from one jug to another (until either one jug becomes empty or the other becomes full) are needed to reach a goal state, specified in terms of the volume of liquid that must be present in some jug or jugs.By Bézout's identity, such puzzles have solution if and only if the desired volume is a multiple of the greatest common divisor of all the integer volume capacities of jugs.

Rules
It is a common assumption, stated as part of these puzzles, that the jugs in the puzzle are irregularly shaped and unmarked, so that it is impossible to accurately measure any quantity of water that does not completely fill a jug. Other assumptions of these problems may include that no water can be spilled, and that each step pouring water from a source
jug to a destination jug stops when either the source jug is empty or the destination jug is full, whichever happens first.

Standard example
The standard puzzle of this kind works with three jugs of capacity 8, 5 and 3 liters. These are initially filled with 8, 0 and 0 liters. In the goal state they should be filled with 4, 4 and 0 liters. The puzzle may be solved in seven steps, passing through the following sequence of states (denoted as a bracketed triple of the three volumes of water in the three jugs):

[8,0,0] → [3,5,0] → [3,2,3] → [6,2,0] → [6,0,2] → [1,5,2] → [1,4,3] → [4,4,0].Cowley (1926) writes that this particular puzzle ""dates back to mediaeval times"" and notes its occurrence in Bachet's 17th-century mathematics textbook.

Reversibility of actions
Since the rules only allows stopping/turning on the boundaries of the Cartesian grid (i.e. at the full capacities of each jug), the only reversible actions (reversible in one step) are:

Transferring water from a full jug to any jug
Transferring water from any jug to an empty jugThe only irreversible actions that can't be reversed in one step are:

Transferring water from a partially full jug to another partially full jugBy restricting ourselves to reversible actions only, we can construct the solution to the problem from the desired result. From the point [4,4,0], there are only two reversible action: Transferring 3 liters from the 8 liter jug to the empty 3 liter jug [1,4,3], or transferring 3 liters from the 5 liter jug to the empty 3 liter jug [4,1,3]."
What does Bézout's identity state about the solution of water pouring puzzles?,The desired volume must be a multiple of the greatest common divisor of all the integer volume capacities of jugs,The desired volume can be any integer value,The desired volume must be equal to the sum of all the integer volume capacities of jugs,The desired volume must be less than the capacity of the largest jug,The desired volume must be greater than the sum of all the integer volume capacities of jugs,A,"Water pouring puzzles (also called water jug problems, decanting problems, measuring puzzles, or Die Hard with a Vengeance puzzles) are a class of puzzle involving a finite collection of water jugs of known integer capacities (in terms of a liquid measure such as liters or gallons).
Initially each jug contains a known integer volume of liquid, not necessarily equal to its capacity.
Puzzles of this type ask how many steps of pouring water from one jug to another (until either one jug becomes empty or the other becomes full) are needed to reach a goal state, specified in terms of the volume of liquid that must be present in some jug or jugs.By Bézout's identity, such puzzles have solution if and only if the desired volume is a multiple of the greatest common divisor of all the integer volume capacities of jugs.

Rules
It is a common assumption, stated as part of these puzzles, that the jugs in the puzzle are irregularly shaped and unmarked, so that it is impossible to accurately measure any quantity of water that does not completely fill a jug. Other assumptions of these problems may include that no water can be spilled, and that each step pouring water from a source
jug to a destination jug stops when either the source jug is empty or the destination jug is full, whichever happens first.

Standard example
The standard puzzle of this kind works with three jugs of capacity 8, 5 and 3 liters. These are initially filled with 8, 0 and 0 liters. In the goal state they should be filled with 4, 4 and 0 liters. The puzzle may be solved in seven steps, passing through the following sequence of states (denoted as a bracketed triple of the three volumes of water in the three jugs):

[8,0,0] → [3,5,0] → [3,2,3] → [6,2,0] → [6,0,2] → [1,5,2] → [1,4,3] → [4,4,0].Cowley (1926) writes that this particular puzzle ""dates back to mediaeval times"" and notes its occurrence in Bachet's 17th-century mathematics textbook.

Reversibility of actions
Since the rules only allows stopping/turning on the boundaries of the Cartesian grid (i.e. at the full capacities of each jug), the only reversible actions (reversible in one step) are:

Transferring water from a full jug to any jug
Transferring water from any jug to an empty jugThe only irreversible actions that can't be reversed in one step are:

Transferring water from a partially full jug to another partially full jugBy restricting ourselves to reversible actions only, we can construct the solution to the problem from the desired result. From the point [4,4,0], there are only two reversible action: Transferring 3 liters from the 8 liter jug to the empty 3 liter jug [1,4,3], or transferring 3 liters from the 5 liter jug to the empty 3 liter jug [4,1,3]."
What is the standard example of a water pouring puzzle?,"Three jugs with capacities 6, 4, and 2 liters","Three jugs with capacities 10, 7, and 5 liters","Three jugs with capacities 8, 5, and 3 liters","Three jugs with capacities 12, 9, and 6 liters","Three jugs with capacities 15, 12, and 9 liters",C,"Water pouring puzzles (also called water jug problems, decanting problems, measuring puzzles, or Die Hard with a Vengeance puzzles) are a class of puzzle involving a finite collection of water jugs of known integer capacities (in terms of a liquid measure such as liters or gallons).
Initially each jug contains a known integer volume of liquid, not necessarily equal to its capacity.
Puzzles of this type ask how many steps of pouring water from one jug to another (until either one jug becomes empty or the other becomes full) are needed to reach a goal state, specified in terms of the volume of liquid that must be present in some jug or jugs.By Bézout's identity, such puzzles have solution if and only if the desired volume is a multiple of the greatest common divisor of all the integer volume capacities of jugs.

Rules
It is a common assumption, stated as part of these puzzles, that the jugs in the puzzle are irregularly shaped and unmarked, so that it is impossible to accurately measure any quantity of water that does not completely fill a jug. Other assumptions of these problems may include that no water can be spilled, and that each step pouring water from a source
jug to a destination jug stops when either the source jug is empty or the destination jug is full, whichever happens first.

Standard example
The standard puzzle of this kind works with three jugs of capacity 8, 5 and 3 liters. These are initially filled with 8, 0 and 0 liters. In the goal state they should be filled with 4, 4 and 0 liters. The puzzle may be solved in seven steps, passing through the following sequence of states (denoted as a bracketed triple of the three volumes of water in the three jugs):

[8,0,0] → [3,5,0] → [3,2,3] → [6,2,0] → [6,0,2] → [1,5,2] → [1,4,3] → [4,4,0].Cowley (1926) writes that this particular puzzle ""dates back to mediaeval times"" and notes its occurrence in Bachet's 17th-century mathematics textbook.

Reversibility of actions
Since the rules only allows stopping/turning on the boundaries of the Cartesian grid (i.e. at the full capacities of each jug), the only reversible actions (reversible in one step) are:

Transferring water from a full jug to any jug
Transferring water from any jug to an empty jugThe only irreversible actions that can't be reversed in one step are:

Transferring water from a partially full jug to another partially full jugBy restricting ourselves to reversible actions only, we can construct the solution to the problem from the desired result. From the point [4,4,0], there are only two reversible action: Transferring 3 liters from the 8 liter jug to the empty 3 liter jug [1,4,3], or transferring 3 liters from the 5 liter jug to the empty 3 liter jug [4,1,3]."
How many steps are needed to solve the standard water pouring puzzle example?,Four steps,Six steps,Seven steps,Eight steps,Ten steps,C,"Water pouring puzzles (also called water jug problems, decanting problems, measuring puzzles, or Die Hard with a Vengeance puzzles) are a class of puzzle involving a finite collection of water jugs of known integer capacities (in terms of a liquid measure such as liters or gallons).
Initially each jug contains a known integer volume of liquid, not necessarily equal to its capacity.
Puzzles of this type ask how many steps of pouring water from one jug to another (until either one jug becomes empty or the other becomes full) are needed to reach a goal state, specified in terms of the volume of liquid that must be present in some jug or jugs.By Bézout's identity, such puzzles have solution if and only if the desired volume is a multiple of the greatest common divisor of all the integer volume capacities of jugs.

Rules
It is a common assumption, stated as part of these puzzles, that the jugs in the puzzle are irregularly shaped and unmarked, so that it is impossible to accurately measure any quantity of water that does not completely fill a jug. Other assumptions of these problems may include that no water can be spilled, and that each step pouring water from a source
jug to a destination jug stops when either the source jug is empty or the destination jug is full, whichever happens first.

Standard example
The standard puzzle of this kind works with three jugs of capacity 8, 5 and 3 liters. These are initially filled with 8, 0 and 0 liters. In the goal state they should be filled with 4, 4 and 0 liters. The puzzle may be solved in seven steps, passing through the following sequence of states (denoted as a bracketed triple of the three volumes of water in the three jugs):

[8,0,0] → [3,5,0] → [3,2,3] → [6,2,0] → [6,0,2] → [1,5,2] → [1,4,3] → [4,4,0].Cowley (1926) writes that this particular puzzle ""dates back to mediaeval times"" and notes its occurrence in Bachet's 17th-century mathematics textbook.

Reversibility of actions
Since the rules only allows stopping/turning on the boundaries of the Cartesian grid (i.e. at the full capacities of each jug), the only reversible actions (reversible in one step) are:

Transferring water from a full jug to any jug
Transferring water from any jug to an empty jugThe only irreversible actions that can't be reversed in one step are:

Transferring water from a partially full jug to another partially full jugBy restricting ourselves to reversible actions only, we can construct the solution to the problem from the desired result. From the point [4,4,0], there are only two reversible action: Transferring 3 liters from the 8 liter jug to the empty 3 liter jug [1,4,3], or transferring 3 liters from the 5 liter jug to the empty 3 liter jug [4,1,3]."
Which actions in water pouring puzzles are reversible?,Transferring water from a full jug to any jug,Transferring water from any jug to an empty jug,Transferring water from a partially full jug to another partially full jug,Spilling water during the pouring process,Stopping/turning on the boundaries of the Cartesian grid,A,"Water pouring puzzles (also called water jug problems, decanting problems, measuring puzzles, or Die Hard with a Vengeance puzzles) are a class of puzzle involving a finite collection of water jugs of known integer capacities (in terms of a liquid measure such as liters or gallons).
Initially each jug contains a known integer volume of liquid, not necessarily equal to its capacity.
Puzzles of this type ask how many steps of pouring water from one jug to another (until either one jug becomes empty or the other becomes full) are needed to reach a goal state, specified in terms of the volume of liquid that must be present in some jug or jugs.By Bézout's identity, such puzzles have solution if and only if the desired volume is a multiple of the greatest common divisor of all the integer volume capacities of jugs.

Rules
It is a common assumption, stated as part of these puzzles, that the jugs in the puzzle are irregularly shaped and unmarked, so that it is impossible to accurately measure any quantity of water that does not completely fill a jug. Other assumptions of these problems may include that no water can be spilled, and that each step pouring water from a source
jug to a destination jug stops when either the source jug is empty or the destination jug is full, whichever happens first.

Standard example
The standard puzzle of this kind works with three jugs of capacity 8, 5 and 3 liters. These are initially filled with 8, 0 and 0 liters. In the goal state they should be filled with 4, 4 and 0 liters. The puzzle may be solved in seven steps, passing through the following sequence of states (denoted as a bracketed triple of the three volumes of water in the three jugs):

[8,0,0] → [3,5,0] → [3,2,3] → [6,2,0] → [6,0,2] → [1,5,2] → [1,4,3] → [4,4,0].Cowley (1926) writes that this particular puzzle ""dates back to mediaeval times"" and notes its occurrence in Bachet's 17th-century mathematics textbook.

Reversibility of actions
Since the rules only allows stopping/turning on the boundaries of the Cartesian grid (i.e. at the full capacities of each jug), the only reversible actions (reversible in one step) are:

Transferring water from a full jug to any jug
Transferring water from any jug to an empty jugThe only irreversible actions that can't be reversed in one step are:

Transferring water from a partially full jug to another partially full jugBy restricting ourselves to reversible actions only, we can construct the solution to the problem from the desired result. From the point [4,4,0], there are only two reversible action: Transferring 3 liters from the 8 liter jug to the empty 3 liter jug [1,4,3], or transferring 3 liters from the 5 liter jug to the empty 3 liter jug [4,1,3]."
Who discovered Bloch's theorem?,Felix Bloch,Albert Einstein,Max Planck,Marie Curie,Erwin Schrödinger,A,"In condensed matter physics, Bloch's theorem states that solutions to the Schrödinger equation in a periodic potential take the form of a plane wave modulated by a periodic function. The theorem is named after the physicist Felix Bloch, who discovered the theorem in 1929. Mathematically, they are written

where 
  
    
      r
    
    \mathbf {r}
   is position, 
  
    ψ
    \psi
   is the wave function, 
  
    u
    u
   is a periodic function with the same periodicity as the crystal, the wave vector 
  
    
      k
    
    \mathbf {k}
   is the crystal momentum vector, 
  
    e
    e
   is Euler's number, and 
  
    i
    i
   is the imaginary unit.
Functions of this form are known as Bloch functions or Bloch states, and serve as a suitable basis for the wave functions or states of electrons in crystalline solids.
Named after Swiss physicist Felix Bloch, the description of electrons in terms of Bloch functions, termed Bloch electrons (or less often Bloch Waves),  underlies the concept of electronic band structures.
These eigenstates are written with subscripts as 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
  , where 
  
    n
    n
   is a discrete index, called the band index, which is present because there are many different wave functions with the same 
  
    
      k
    
    \mathbf {k}
   (each has a different periodic component 
  
    u
    u
  ). Within a band (i.e., for fixed 
  
    n
    n
  ), 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
   varies continuously with 
  
    
      k
    
    \mathbf {k}
  , as does its energy. Also, 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
   is unique only up to a constant reciprocal lattice vector 
  
    
      K
    
    \mathbf {K}
  , or, 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
        =
        
          ψ
          
            n
            (
            
              k
              +
              K
            
            )
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }=\psi _{n(\mathbf {k+K} )}}
  . Therefore, the wave vector 
  
    
      k
    
    \mathbf {k}
   can be restricted to the first Brillouin zone of the reciprocal lattice without loss of generality.

Applications and consequences
Applicability
The most common example of Bloch's theorem is describing electrons in a crystal, especially in characterizing the crystal's electronic properties, such as electronic band structure. However, a Bloch-wave description applies more generally to any wave-like phenomenon in a periodic medium."
What is the mathematical form of Bloch's theorem?,ψ = ue^{ik⋅r}u,ψ = ue^{ik⋅r},ψ = ie^{ik⋅r},ψ = e^{ik⋅r},ψ = u,B,"In condensed matter physics, Bloch's theorem states that solutions to the Schrödinger equation in a periodic potential take the form of a plane wave modulated by a periodic function. The theorem is named after the physicist Felix Bloch, who discovered the theorem in 1929. Mathematically, they are written

where 
  
    
      r
    
    \mathbf {r}
   is position, 
  
    ψ
    \psi
   is the wave function, 
  
    u
    u
   is a periodic function with the same periodicity as the crystal, the wave vector 
  
    
      k
    
    \mathbf {k}
   is the crystal momentum vector, 
  
    e
    e
   is Euler's number, and 
  
    i
    i
   is the imaginary unit.
Functions of this form are known as Bloch functions or Bloch states, and serve as a suitable basis for the wave functions or states of electrons in crystalline solids.
Named after Swiss physicist Felix Bloch, the description of electrons in terms of Bloch functions, termed Bloch electrons (or less often Bloch Waves),  underlies the concept of electronic band structures.
These eigenstates are written with subscripts as 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
  , where 
  
    n
    n
   is a discrete index, called the band index, which is present because there are many different wave functions with the same 
  
    
      k
    
    \mathbf {k}
   (each has a different periodic component 
  
    u
    u
  ). Within a band (i.e., for fixed 
  
    n
    n
  ), 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
   varies continuously with 
  
    
      k
    
    \mathbf {k}
  , as does its energy. Also, 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
   is unique only up to a constant reciprocal lattice vector 
  
    
      K
    
    \mathbf {K}
  , or, 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
        =
        
          ψ
          
            n
            (
            
              k
              +
              K
            
            )
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }=\psi _{n(\mathbf {k+K} )}}
  . Therefore, the wave vector 
  
    
      k
    
    \mathbf {k}
   can be restricted to the first Brillouin zone of the reciprocal lattice without loss of generality.

Applications and consequences
Applicability
The most common example of Bloch's theorem is describing electrons in a crystal, especially in characterizing the crystal's electronic properties, such as electronic band structure. However, a Bloch-wave description applies more generally to any wave-like phenomenon in a periodic medium."
What are solutions to the Schrödinger equation in a periodic potential called?,Bloch functions,Plane waves,Periodic functions,Eigenstates,Wave functions,A,"In condensed matter physics, Bloch's theorem states that solutions to the Schrödinger equation in a periodic potential take the form of a plane wave modulated by a periodic function. The theorem is named after the physicist Felix Bloch, who discovered the theorem in 1929. Mathematically, they are written

where 
  
    
      r
    
    \mathbf {r}
   is position, 
  
    ψ
    \psi
   is the wave function, 
  
    u
    u
   is a periodic function with the same periodicity as the crystal, the wave vector 
  
    
      k
    
    \mathbf {k}
   is the crystal momentum vector, 
  
    e
    e
   is Euler's number, and 
  
    i
    i
   is the imaginary unit.
Functions of this form are known as Bloch functions or Bloch states, and serve as a suitable basis for the wave functions or states of electrons in crystalline solids.
Named after Swiss physicist Felix Bloch, the description of electrons in terms of Bloch functions, termed Bloch electrons (or less often Bloch Waves),  underlies the concept of electronic band structures.
These eigenstates are written with subscripts as 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
  , where 
  
    n
    n
   is a discrete index, called the band index, which is present because there are many different wave functions with the same 
  
    
      k
    
    \mathbf {k}
   (each has a different periodic component 
  
    u
    u
  ). Within a band (i.e., for fixed 
  
    n
    n
  ), 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
   varies continuously with 
  
    
      k
    
    \mathbf {k}
  , as does its energy. Also, 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
   is unique only up to a constant reciprocal lattice vector 
  
    
      K
    
    \mathbf {K}
  , or, 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
        =
        
          ψ
          
            n
            (
            
              k
              +
              K
            
            )
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }=\psi _{n(\mathbf {k+K} )}}
  . Therefore, the wave vector 
  
    
      k
    
    \mathbf {k}
   can be restricted to the first Brillouin zone of the reciprocal lattice without loss of generality.

Applications and consequences
Applicability
The most common example of Bloch's theorem is describing electrons in a crystal, especially in characterizing the crystal's electronic properties, such as electronic band structure. However, a Bloch-wave description applies more generally to any wave-like phenomenon in a periodic medium."
What is the term for electrons described in terms of Bloch functions?,Bloch Waves,Periodic Electrons,Band Electrons,Bloch Electrons,Crystal Electrons,D,"In condensed matter physics, Bloch's theorem states that solutions to the Schrödinger equation in a periodic potential take the form of a plane wave modulated by a periodic function. The theorem is named after the physicist Felix Bloch, who discovered the theorem in 1929. Mathematically, they are written

where 
  
    
      r
    
    \mathbf {r}
   is position, 
  
    ψ
    \psi
   is the wave function, 
  
    u
    u
   is a periodic function with the same periodicity as the crystal, the wave vector 
  
    
      k
    
    \mathbf {k}
   is the crystal momentum vector, 
  
    e
    e
   is Euler's number, and 
  
    i
    i
   is the imaginary unit.
Functions of this form are known as Bloch functions or Bloch states, and serve as a suitable basis for the wave functions or states of electrons in crystalline solids.
Named after Swiss physicist Felix Bloch, the description of electrons in terms of Bloch functions, termed Bloch electrons (or less often Bloch Waves),  underlies the concept of electronic band structures.
These eigenstates are written with subscripts as 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
  , where 
  
    n
    n
   is a discrete index, called the band index, which is present because there are many different wave functions with the same 
  
    
      k
    
    \mathbf {k}
   (each has a different periodic component 
  
    u
    u
  ). Within a band (i.e., for fixed 
  
    n
    n
  ), 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
   varies continuously with 
  
    
      k
    
    \mathbf {k}
  , as does its energy. Also, 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
   is unique only up to a constant reciprocal lattice vector 
  
    
      K
    
    \mathbf {K}
  , or, 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
        =
        
          ψ
          
            n
            (
            
              k
              +
              K
            
            )
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }=\psi _{n(\mathbf {k+K} )}}
  . Therefore, the wave vector 
  
    
      k
    
    \mathbf {k}
   can be restricted to the first Brillouin zone of the reciprocal lattice without loss of generality.

Applications and consequences
Applicability
The most common example of Bloch's theorem is describing electrons in a crystal, especially in characterizing the crystal's electronic properties, such as electronic band structure. However, a Bloch-wave description applies more generally to any wave-like phenomenon in a periodic medium."
What is the band index in Bloch functions?,k,r,n,K,ψ,C,"In condensed matter physics, Bloch's theorem states that solutions to the Schrödinger equation in a periodic potential take the form of a plane wave modulated by a periodic function. The theorem is named after the physicist Felix Bloch, who discovered the theorem in 1929. Mathematically, they are written

where 
  
    
      r
    
    \mathbf {r}
   is position, 
  
    ψ
    \psi
   is the wave function, 
  
    u
    u
   is a periodic function with the same periodicity as the crystal, the wave vector 
  
    
      k
    
    \mathbf {k}
   is the crystal momentum vector, 
  
    e
    e
   is Euler's number, and 
  
    i
    i
   is the imaginary unit.
Functions of this form are known as Bloch functions or Bloch states, and serve as a suitable basis for the wave functions or states of electrons in crystalline solids.
Named after Swiss physicist Felix Bloch, the description of electrons in terms of Bloch functions, termed Bloch electrons (or less often Bloch Waves),  underlies the concept of electronic band structures.
These eigenstates are written with subscripts as 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
  , where 
  
    n
    n
   is a discrete index, called the band index, which is present because there are many different wave functions with the same 
  
    
      k
    
    \mathbf {k}
   (each has a different periodic component 
  
    u
    u
  ). Within a band (i.e., for fixed 
  
    n
    n
  ), 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
   varies continuously with 
  
    
      k
    
    \mathbf {k}
  , as does its energy. Also, 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }}
   is unique only up to a constant reciprocal lattice vector 
  
    
      K
    
    \mathbf {K}
  , or, 
  
    
      
        
          ψ
          
            n
            
              k
            
          
        
        =
        
          ψ
          
            n
            (
            
              k
              +
              K
            
            )
          
        
      
    
    {\displaystyle \psi _{n\mathbf {k} }=\psi _{n(\mathbf {k+K} )}}
  . Therefore, the wave vector 
  
    
      k
    
    \mathbf {k}
   can be restricted to the first Brillouin zone of the reciprocal lattice without loss of generality.

Applications and consequences
Applicability
The most common example of Bloch's theorem is describing electrons in a crystal, especially in characterizing the crystal's electronic properties, such as electronic band structure. However, a Bloch-wave description applies more generally to any wave-like phenomenon in a periodic medium."
Who edited and published the three-volume work on Japanese mathematics called Jinkōki?,Yoshida Mitsuyoshi,Kan'ei era,Idai,Shinpen Jinkōki,Edo period,A,"Jinkōki (塵劫記, Permanent Mathematics) is a three-volume work on Japanese mathematics, first edited and published by Yoshida Mitsuyoshi in 1627. Over his lifetime, Mitsuyoshi revised Jinkōki several times. The edition released in the eleventh year of the Kan'ei era (1641) became particularly widespread. The last version personally published by Mitsuyoshi was the Idai (井大), which came out in the eighteenth year of the Kan'ei (1634). Subsequent to that, various editions of Jinkōki were released, one of which includes Shinpen Jinkōki (新編塵劫記).The book contained instructions for dividing and multiplying with a soroban and mathematical problems relevant to merchants and craftsmen. The book also contained several interesting mathematical problems, and was the first Japanese book to use printing in colour. As a result, the Jinkōki became the most popular Japanese mathematics book ever and one of the most widely read books of the Edo period."
In which year was the edition of Jinkōki released that became particularly widespread?,1627,1641,1634,18th year of Kan'ei,Japanese mathematics,B,"Jinkōki (塵劫記, Permanent Mathematics) is a three-volume work on Japanese mathematics, first edited and published by Yoshida Mitsuyoshi in 1627. Over his lifetime, Mitsuyoshi revised Jinkōki several times. The edition released in the eleventh year of the Kan'ei era (1641) became particularly widespread. The last version personally published by Mitsuyoshi was the Idai (井大), which came out in the eighteenth year of the Kan'ei (1634). Subsequent to that, various editions of Jinkōki were released, one of which includes Shinpen Jinkōki (新編塵劫記).The book contained instructions for dividing and multiplying with a soroban and mathematical problems relevant to merchants and craftsmen. The book also contained several interesting mathematical problems, and was the first Japanese book to use printing in colour. As a result, the Jinkōki became the most popular Japanese mathematics book ever and one of the most widely read books of the Edo period."
What type of mathematical problems were included in Jinkōki?,Geometry problems,Calculus problems,Problems relevant to merchants and craftsmen,Number theory problems,Algebra problems,C,"Jinkōki (塵劫記, Permanent Mathematics) is a three-volume work on Japanese mathematics, first edited and published by Yoshida Mitsuyoshi in 1627. Over his lifetime, Mitsuyoshi revised Jinkōki several times. The edition released in the eleventh year of the Kan'ei era (1641) became particularly widespread. The last version personally published by Mitsuyoshi was the Idai (井大), which came out in the eighteenth year of the Kan'ei (1634). Subsequent to that, various editions of Jinkōki were released, one of which includes Shinpen Jinkōki (新編塵劫記).The book contained instructions for dividing and multiplying with a soroban and mathematical problems relevant to merchants and craftsmen. The book also contained several interesting mathematical problems, and was the first Japanese book to use printing in colour. As a result, the Jinkōki became the most popular Japanese mathematics book ever and one of the most widely read books of the Edo period."
What was significant about the printing of Jinkōki?,It was the first Japanese book to use printing in color,It was the first Japanese book to use printing,It was the first book to use printing in color,It was the first book to use printing,It was the first book to use color,A,"Jinkōki (塵劫記, Permanent Mathematics) is a three-volume work on Japanese mathematics, first edited and published by Yoshida Mitsuyoshi in 1627. Over his lifetime, Mitsuyoshi revised Jinkōki several times. The edition released in the eleventh year of the Kan'ei era (1641) became particularly widespread. The last version personally published by Mitsuyoshi was the Idai (井大), which came out in the eighteenth year of the Kan'ei (1634). Subsequent to that, various editions of Jinkōki were released, one of which includes Shinpen Jinkōki (新編塵劫記).The book contained instructions for dividing and multiplying with a soroban and mathematical problems relevant to merchants and craftsmen. The book also contained several interesting mathematical problems, and was the first Japanese book to use printing in colour. As a result, the Jinkōki became the most popular Japanese mathematics book ever and one of the most widely read books of the Edo period."
Which period was the Jinkōki considered to be one of the most widely read books?,Kan'ei era,Idai,Shinpen Jinkōki,Edo period,Yoshida Mitsuyoshi,D,"Jinkōki (塵劫記, Permanent Mathematics) is a three-volume work on Japanese mathematics, first edited and published by Yoshida Mitsuyoshi in 1627. Over his lifetime, Mitsuyoshi revised Jinkōki several times. The edition released in the eleventh year of the Kan'ei era (1641) became particularly widespread. The last version personally published by Mitsuyoshi was the Idai (井大), which came out in the eighteenth year of the Kan'ei (1634). Subsequent to that, various editions of Jinkōki were released, one of which includes Shinpen Jinkōki (新編塵劫記).The book contained instructions for dividing and multiplying with a soroban and mathematical problems relevant to merchants and craftsmen. The book also contained several interesting mathematical problems, and was the first Japanese book to use printing in colour. As a result, the Jinkōki became the most popular Japanese mathematics book ever and one of the most widely read books of the Edo period."
What is a constant function?,A function whose output value is different for every input value,A function whose output value is the same for every input value,A function whose output value is randomly generated,A function whose output value depends on the input value,A function whose output value is always negative,B,"In mathematics, a constant function is a function whose (output) value is the same for every input value. For example, the function y(x) = 4 is a constant function because the value of y(x) is 4 regardless of the input value x (see image).

Basic properties
As a real-valued function of a real-valued argument, a constant function has the general form y(x) = c or just y = c.
Example: The function y(x) = 2 or just y = 2 is the specific constant function where the output value is c = 2. The domain of this function is the set of all real numbers R. The codomain of this function is just {2}. The independent variable x does not appear on the right side of the function expression and so its value is ""vacuously substituted"". Namely y(0) = 2, y(−2.7) = 2, y(π) = 2, and so on. No matter what value of x is input, the output is ""2"".
Real-world example: A store where every item is sold for the price of 1 dollar.The graph of the constant function y = c is a horizontal line in the plane that passes through the point (0, c).In the context of a polynomial in one variable x, the non-zero constant function is a polynomial of degree 0 and its general form is f(x) = c where c is nonzero."
What is the general form of a constant function?,y(x) = mx + b,y(x) = c,y(x) = ax^2 + bx + c,y(x) = sin(x),y(x) = e^x,B,"In mathematics, a constant function is a function whose (output) value is the same for every input value. For example, the function y(x) = 4 is a constant function because the value of y(x) is 4 regardless of the input value x (see image).

Basic properties
As a real-valued function of a real-valued argument, a constant function has the general form y(x) = c or just y = c.
Example: The function y(x) = 2 or just y = 2 is the specific constant function where the output value is c = 2. The domain of this function is the set of all real numbers R. The codomain of this function is just {2}. The independent variable x does not appear on the right side of the function expression and so its value is ""vacuously substituted"". Namely y(0) = 2, y(−2.7) = 2, y(π) = 2, and so on. No matter what value of x is input, the output is ""2"".
Real-world example: A store where every item is sold for the price of 1 dollar.The graph of the constant function y = c is a horizontal line in the plane that passes through the point (0, c).In the context of a polynomial in one variable x, the non-zero constant function is a polynomial of degree 0 and its general form is f(x) = c where c is nonzero."
What is the domain of a constant function?,The set of all real numbers,The set of all positive numbers,The set of all negative numbers,The set of all integers,The set of all fractions,A,"In mathematics, a constant function is a function whose (output) value is the same for every input value. For example, the function y(x) = 4 is a constant function because the value of y(x) is 4 regardless of the input value x (see image).

Basic properties
As a real-valued function of a real-valued argument, a constant function has the general form y(x) = c or just y = c.
Example: The function y(x) = 2 or just y = 2 is the specific constant function where the output value is c = 2. The domain of this function is the set of all real numbers R. The codomain of this function is just {2}. The independent variable x does not appear on the right side of the function expression and so its value is ""vacuously substituted"". Namely y(0) = 2, y(−2.7) = 2, y(π) = 2, and so on. No matter what value of x is input, the output is ""2"".
Real-world example: A store where every item is sold for the price of 1 dollar.The graph of the constant function y = c is a horizontal line in the plane that passes through the point (0, c).In the context of a polynomial in one variable x, the non-zero constant function is a polynomial of degree 0 and its general form is f(x) = c where c is nonzero."
What is the codomain of a constant function?,The set of all real numbers,The set of all positive numbers,The set of all negative numbers,The set of all integers,The set of all fractions,A,"In mathematics, a constant function is a function whose (output) value is the same for every input value. For example, the function y(x) = 4 is a constant function because the value of y(x) is 4 regardless of the input value x (see image).

Basic properties
As a real-valued function of a real-valued argument, a constant function has the general form y(x) = c or just y = c.
Example: The function y(x) = 2 or just y = 2 is the specific constant function where the output value is c = 2. The domain of this function is the set of all real numbers R. The codomain of this function is just {2}. The independent variable x does not appear on the right side of the function expression and so its value is ""vacuously substituted"". Namely y(0) = 2, y(−2.7) = 2, y(π) = 2, and so on. No matter what value of x is input, the output is ""2"".
Real-world example: A store where every item is sold for the price of 1 dollar.The graph of the constant function y = c is a horizontal line in the plane that passes through the point (0, c).In the context of a polynomial in one variable x, the non-zero constant function is a polynomial of degree 0 and its general form is f(x) = c where c is nonzero."
What is the graph of a constant function?,A vertical line in the plane,A horizontal line in the plane,A parabola,A circle,A sine wave,B,"In mathematics, a constant function is a function whose (output) value is the same for every input value. For example, the function y(x) = 4 is a constant function because the value of y(x) is 4 regardless of the input value x (see image).

Basic properties
As a real-valued function of a real-valued argument, a constant function has the general form y(x) = c or just y = c.
Example: The function y(x) = 2 or just y = 2 is the specific constant function where the output value is c = 2. The domain of this function is the set of all real numbers R. The codomain of this function is just {2}. The independent variable x does not appear on the right side of the function expression and so its value is ""vacuously substituted"". Namely y(0) = 2, y(−2.7) = 2, y(π) = 2, and so on. No matter what value of x is input, the output is ""2"".
Real-world example: A store where every item is sold for the price of 1 dollar.The graph of the constant function y = c is a horizontal line in the plane that passes through the point (0, c).In the context of a polynomial in one variable x, the non-zero constant function is a polynomial of degree 0 and its general form is f(x) = c where c is nonzero."
What is a multi-valued treatment?,A treatment that can take on more than two values.,A treatment that can only take on two values.,A treatment that can take on any number of values.,A treatment that has a finite number of values.,A treatment that has an infinite number of values.,A,"In statistics, in particular in the design of experiments, a multi-valued treatment is a treatment that can take on more than two values. It is related to the dose-response model in the medical literature.

Description
Generally speaking, treatment levels may be finite or infinite as well as ordinal or cardinal, which leads to a large collection of possible treatment effects to be studied in applications. One example is the effect of different levels of program participation (e.g. full-time and part-time) in a job training program.Assume there exists a finite collection of multi-valued treatment status 
  
    
      
        T
        =
        {
        0
        ,
        1
        ,
        2
        ,
        …
        ,
        J
        ,
        }
      
    
    {\displaystyle T=\{0,1,2,\ldots ,J,\}}
   with J some fixed integer. As in the potential outcomes framework, denote  
  
    
      
        Y
        (
        j
        )
        ⊂
        R
      
    
    {\displaystyle Y(j)\subset R}
   the collection of potential outcomes under the treatment J, and 
  
    
      
        Y
        =
        
          
            ∑
            
              j
              =
              0
            
            
              J
            
          
          
            
              D
              
                j
              
            
            Y
            (
            j
            )
          
        
      
    
    {\displaystyle Y=\textstyle \sum _{j=0}^{J}\displaystyle D_{j}Y(j)}
   denotes the observed outcome and 
  
    
      
        
          D
          
            j
          
        
      
    
    {\displaystyle D_{j}}
   is an indicator that equals 1 when the treatment equals j and 0 when it does not equal j, leading to a fundamental problem of causal inference.  A general framework that analyzes ordered choice models in terms of marginal treatment effects and average treatment effects has been extensively discussed by Heckman and Vytlacil.Recent work in the econometrics and statistics literature has focused on estimation and inference for multivalued treatments and ignorability conditions for identifying the treatment effects. In the context of program evaluation, the propensity score has been generalized to allow for multi-valued treatments, while other work has also focused on the role of the conditional mean independence assumption."
Give an example of a multi-valued treatment.,The effect of different levels of program participation in a job training program.,The effect of different doses of medication on a patient.,The effect of different types of exercise on weight loss.,The effect of different educational backgrounds on job performance.,The effect of different marketing strategies on sales.,A,"In statistics, in particular in the design of experiments, a multi-valued treatment is a treatment that can take on more than two values. It is related to the dose-response model in the medical literature.

Description
Generally speaking, treatment levels may be finite or infinite as well as ordinal or cardinal, which leads to a large collection of possible treatment effects to be studied in applications. One example is the effect of different levels of program participation (e.g. full-time and part-time) in a job training program.Assume there exists a finite collection of multi-valued treatment status 
  
    
      
        T
        =
        {
        0
        ,
        1
        ,
        2
        ,
        …
        ,
        J
        ,
        }
      
    
    {\displaystyle T=\{0,1,2,\ldots ,J,\}}
   with J some fixed integer. As in the potential outcomes framework, denote  
  
    
      
        Y
        (
        j
        )
        ⊂
        R
      
    
    {\displaystyle Y(j)\subset R}
   the collection of potential outcomes under the treatment J, and 
  
    
      
        Y
        =
        
          
            ∑
            
              j
              =
              0
            
            
              J
            
          
          
            
              D
              
                j
              
            
            Y
            (
            j
            )
          
        
      
    
    {\displaystyle Y=\textstyle \sum _{j=0}^{J}\displaystyle D_{j}Y(j)}
   denotes the observed outcome and 
  
    
      
        
          D
          
            j
          
        
      
    
    {\displaystyle D_{j}}
   is an indicator that equals 1 when the treatment equals j and 0 when it does not equal j, leading to a fundamental problem of causal inference.  A general framework that analyzes ordered choice models in terms of marginal treatment effects and average treatment effects has been extensively discussed by Heckman and Vytlacil.Recent work in the econometrics and statistics literature has focused on estimation and inference for multivalued treatments and ignorability conditions for identifying the treatment effects. In the context of program evaluation, the propensity score has been generalized to allow for multi-valued treatments, while other work has also focused on the role of the conditional mean independence assumption."
How are potential outcomes under a multi-valued treatment denoted?,Y(j),Y[T],Y(j),Y{t},Y(t),A,"In statistics, in particular in the design of experiments, a multi-valued treatment is a treatment that can take on more than two values. It is related to the dose-response model in the medical literature.

Description
Generally speaking, treatment levels may be finite or infinite as well as ordinal or cardinal, which leads to a large collection of possible treatment effects to be studied in applications. One example is the effect of different levels of program participation (e.g. full-time and part-time) in a job training program.Assume there exists a finite collection of multi-valued treatment status 
  
    
      
        T
        =
        {
        0
        ,
        1
        ,
        2
        ,
        …
        ,
        J
        ,
        }
      
    
    {\displaystyle T=\{0,1,2,\ldots ,J,\}}
   with J some fixed integer. As in the potential outcomes framework, denote  
  
    
      
        Y
        (
        j
        )
        ⊂
        R
      
    
    {\displaystyle Y(j)\subset R}
   the collection of potential outcomes under the treatment J, and 
  
    
      
        Y
        =
        
          
            ∑
            
              j
              =
              0
            
            
              J
            
          
          
            
              D
              
                j
              
            
            Y
            (
            j
            )
          
        
      
    
    {\displaystyle Y=\textstyle \sum _{j=0}^{J}\displaystyle D_{j}Y(j)}
   denotes the observed outcome and 
  
    
      
        
          D
          
            j
          
        
      
    
    {\displaystyle D_{j}}
   is an indicator that equals 1 when the treatment equals j and 0 when it does not equal j, leading to a fundamental problem of causal inference.  A general framework that analyzes ordered choice models in terms of marginal treatment effects and average treatment effects has been extensively discussed by Heckman and Vytlacil.Recent work in the econometrics and statistics literature has focused on estimation and inference for multivalued treatments and ignorability conditions for identifying the treatment effects. In the context of program evaluation, the propensity score has been generalized to allow for multi-valued treatments, while other work has also focused on the role of the conditional mean independence assumption."
What is the observed outcome denoted as in the context of multi-valued treatments?,Y,D,T,J,R,A,"In statistics, in particular in the design of experiments, a multi-valued treatment is a treatment that can take on more than two values. It is related to the dose-response model in the medical literature.

Description
Generally speaking, treatment levels may be finite or infinite as well as ordinal or cardinal, which leads to a large collection of possible treatment effects to be studied in applications. One example is the effect of different levels of program participation (e.g. full-time and part-time) in a job training program.Assume there exists a finite collection of multi-valued treatment status 
  
    
      
        T
        =
        {
        0
        ,
        1
        ,
        2
        ,
        …
        ,
        J
        ,
        }
      
    
    {\displaystyle T=\{0,1,2,\ldots ,J,\}}
   with J some fixed integer. As in the potential outcomes framework, denote  
  
    
      
        Y
        (
        j
        )
        ⊂
        R
      
    
    {\displaystyle Y(j)\subset R}
   the collection of potential outcomes under the treatment J, and 
  
    
      
        Y
        =
        
          
            ∑
            
              j
              =
              0
            
            
              J
            
          
          
            
              D
              
                j
              
            
            Y
            (
            j
            )
          
        
      
    
    {\displaystyle Y=\textstyle \sum _{j=0}^{J}\displaystyle D_{j}Y(j)}
   denotes the observed outcome and 
  
    
      
        
          D
          
            j
          
        
      
    
    {\displaystyle D_{j}}
   is an indicator that equals 1 when the treatment equals j and 0 when it does not equal j, leading to a fundamental problem of causal inference.  A general framework that analyzes ordered choice models in terms of marginal treatment effects and average treatment effects has been extensively discussed by Heckman and Vytlacil.Recent work in the econometrics and statistics literature has focused on estimation and inference for multivalued treatments and ignorability conditions for identifying the treatment effects. In the context of program evaluation, the propensity score has been generalized to allow for multi-valued treatments, while other work has also focused on the role of the conditional mean independence assumption."
What is the indicator that represents the treatment level in causal inference?,J,T,D,R,Y,C,"In statistics, in particular in the design of experiments, a multi-valued treatment is a treatment that can take on more than two values. It is related to the dose-response model in the medical literature.

Description
Generally speaking, treatment levels may be finite or infinite as well as ordinal or cardinal, which leads to a large collection of possible treatment effects to be studied in applications. One example is the effect of different levels of program participation (e.g. full-time and part-time) in a job training program.Assume there exists a finite collection of multi-valued treatment status 
  
    
      
        T
        =
        {
        0
        ,
        1
        ,
        2
        ,
        …
        ,
        J
        ,
        }
      
    
    {\displaystyle T=\{0,1,2,\ldots ,J,\}}
   with J some fixed integer. As in the potential outcomes framework, denote  
  
    
      
        Y
        (
        j
        )
        ⊂
        R
      
    
    {\displaystyle Y(j)\subset R}
   the collection of potential outcomes under the treatment J, and 
  
    
      
        Y
        =
        
          
            ∑
            
              j
              =
              0
            
            
              J
            
          
          
            
              D
              
                j
              
            
            Y
            (
            j
            )
          
        
      
    
    {\displaystyle Y=\textstyle \sum _{j=0}^{J}\displaystyle D_{j}Y(j)}
   denotes the observed outcome and 
  
    
      
        
          D
          
            j
          
        
      
    
    {\displaystyle D_{j}}
   is an indicator that equals 1 when the treatment equals j and 0 when it does not equal j, leading to a fundamental problem of causal inference.  A general framework that analyzes ordered choice models in terms of marginal treatment effects and average treatment effects has been extensively discussed by Heckman and Vytlacil.Recent work in the econometrics and statistics literature has focused on estimation and inference for multivalued treatments and ignorability conditions for identifying the treatment effects. In the context of program evaluation, the propensity score has been generalized to allow for multi-valued treatments, while other work has also focused on the role of the conditional mean independence assumption."
What is the main focus of civil engineering students?,Construction procedures and methods,Design work and analysis,Project management and personnel management,Cost analysis and risk management,Construction management and allocation of funds,B,"Construction engineering, also known as construction operations, is a professional subdiscipline of civil engineering that deals with the designing, planning, construction, and operations management of infrastructure such as roadways, tunnels, bridges, airports, railroads, facilities, buildings, dams, utilities and other projects. Construction engineers learn some of the design aspects similar to civil engineers as well as project management aspects.
At the educational level, civil engineering students concentrate primarily on the design work which is more analytical, gearing them toward a career as a design professional. This essentially requires them to take a multitude of challenging engineering science and design courses as part of obtaining a 4-year accredited degree. Education for construction engineers is primarily focused on construction procedures, methods, costs, schedules and personnel management. Their primary concern is to deliver a project on time within budget and of the desired quality.
Regarding educational requirements, construction engineering students take basic design courses in civil engineering, as well as construction management courses.

Work activities
Being a sub-discipline of civil engineering, construction engineers apply their knowledge and business, technical and management skills obtained from their undergraduate degree to oversee projects that include bridges, buildings and housing projects. Construction engineers are heavily involved in the design and management/ allocation of funds in these projects. They are charged with risk analysis, costing and planning."
What is the main concern of construction engineers?,Delivering projects on time and within budget,Designing infrastructure such as roadways and tunnels,Managing construction procedures and methods,Overseeing risk analysis and costing,Allocating funds for construction projects,A,"Construction engineering, also known as construction operations, is a professional subdiscipline of civil engineering that deals with the designing, planning, construction, and operations management of infrastructure such as roadways, tunnels, bridges, airports, railroads, facilities, buildings, dams, utilities and other projects. Construction engineers learn some of the design aspects similar to civil engineers as well as project management aspects.
At the educational level, civil engineering students concentrate primarily on the design work which is more analytical, gearing them toward a career as a design professional. This essentially requires them to take a multitude of challenging engineering science and design courses as part of obtaining a 4-year accredited degree. Education for construction engineers is primarily focused on construction procedures, methods, costs, schedules and personnel management. Their primary concern is to deliver a project on time within budget and of the desired quality.
Regarding educational requirements, construction engineering students take basic design courses in civil engineering, as well as construction management courses.

Work activities
Being a sub-discipline of civil engineering, construction engineers apply their knowledge and business, technical and management skills obtained from their undergraduate degree to oversee projects that include bridges, buildings and housing projects. Construction engineers are heavily involved in the design and management/ allocation of funds in these projects. They are charged with risk analysis, costing and planning."
What is the primary focus of education for construction engineers?,Construction procedures and methods,Design work and analysis,Project management and personnel management,Cost analysis and risk management,Construction management and allocation of funds,C,"Construction engineering, also known as construction operations, is a professional subdiscipline of civil engineering that deals with the designing, planning, construction, and operations management of infrastructure such as roadways, tunnels, bridges, airports, railroads, facilities, buildings, dams, utilities and other projects. Construction engineers learn some of the design aspects similar to civil engineers as well as project management aspects.
At the educational level, civil engineering students concentrate primarily on the design work which is more analytical, gearing them toward a career as a design professional. This essentially requires them to take a multitude of challenging engineering science and design courses as part of obtaining a 4-year accredited degree. Education for construction engineers is primarily focused on construction procedures, methods, costs, schedules and personnel management. Their primary concern is to deliver a project on time within budget and of the desired quality.
Regarding educational requirements, construction engineering students take basic design courses in civil engineering, as well as construction management courses.

Work activities
Being a sub-discipline of civil engineering, construction engineers apply their knowledge and business, technical and management skills obtained from their undergraduate degree to oversee projects that include bridges, buildings and housing projects. Construction engineers are heavily involved in the design and management/ allocation of funds in these projects. They are charged with risk analysis, costing and planning."
What kind of projects do construction engineers oversee?,"Bridges, buildings, and housing projects","Roadways, tunnels, and airports","Facilities, dams, and utilities","Railroads, facilities, and buildings",Infrastructure projects of all kinds,A,"Construction engineering, also known as construction operations, is a professional subdiscipline of civil engineering that deals with the designing, planning, construction, and operations management of infrastructure such as roadways, tunnels, bridges, airports, railroads, facilities, buildings, dams, utilities and other projects. Construction engineers learn some of the design aspects similar to civil engineers as well as project management aspects.
At the educational level, civil engineering students concentrate primarily on the design work which is more analytical, gearing them toward a career as a design professional. This essentially requires them to take a multitude of challenging engineering science and design courses as part of obtaining a 4-year accredited degree. Education for construction engineers is primarily focused on construction procedures, methods, costs, schedules and personnel management. Their primary concern is to deliver a project on time within budget and of the desired quality.
Regarding educational requirements, construction engineering students take basic design courses in civil engineering, as well as construction management courses.

Work activities
Being a sub-discipline of civil engineering, construction engineers apply their knowledge and business, technical and management skills obtained from their undergraduate degree to oversee projects that include bridges, buildings and housing projects. Construction engineers are heavily involved in the design and management/ allocation of funds in these projects. They are charged with risk analysis, costing and planning."
What skills do construction engineers utilize in their work?,"Business, technical, and management skills",Analytical and design skills,Risk analysis and costing skills,Construction procedures and methods skills,Allocation of funds and planning skills,A,"Construction engineering, also known as construction operations, is a professional subdiscipline of civil engineering that deals with the designing, planning, construction, and operations management of infrastructure such as roadways, tunnels, bridges, airports, railroads, facilities, buildings, dams, utilities and other projects. Construction engineers learn some of the design aspects similar to civil engineers as well as project management aspects.
At the educational level, civil engineering students concentrate primarily on the design work which is more analytical, gearing them toward a career as a design professional. This essentially requires them to take a multitude of challenging engineering science and design courses as part of obtaining a 4-year accredited degree. Education for construction engineers is primarily focused on construction procedures, methods, costs, schedules and personnel management. Their primary concern is to deliver a project on time within budget and of the desired quality.
Regarding educational requirements, construction engineering students take basic design courses in civil engineering, as well as construction management courses.

Work activities
Being a sub-discipline of civil engineering, construction engineers apply their knowledge and business, technical and management skills obtained from their undergraduate degree to oversee projects that include bridges, buildings and housing projects. Construction engineers are heavily involved in the design and management/ allocation of funds in these projects. They are charged with risk analysis, costing and planning."
What is a simulacrum?,A representation of a person or thing,A type of statue or painting,An inferior image without substance,An imitation of a god,A type of artistic technique,A,"A simulacrum (plural: simulacra or simulacrums, from Latin simulacrum, which means ""likeness, semblance"") is a representation or imitation of a person or thing. The word was first recorded in the English language in the late 16th century, used to describe a representation, such as a statue or a painting, especially of a god. By the late 19th century, it had gathered a secondary association of inferiority: an image without the substance or qualities of the original. Literary critic Fredric Jameson offers photorealism as an example of artistic simulacrum, in which a painting is created by copying a photograph that is itself a copy of the real thing. Other art forms that play with simulacra include trompe-l'œil, pop art, Italian neorealism, and French New Wave.

Philosophy
Simulacra have long been of interest to philosophers. In his Sophist, Plato speaks of two kinds of image-making. The first is a faithful reproduction, attempted to copy precisely the original."
"When was the word ""simulacrum"" first recorded in the English language?",16th century,17th century,18th century,19th century,20th century,A,"A simulacrum (plural: simulacra or simulacrums, from Latin simulacrum, which means ""likeness, semblance"") is a representation or imitation of a person or thing. The word was first recorded in the English language in the late 16th century, used to describe a representation, such as a statue or a painting, especially of a god. By the late 19th century, it had gathered a secondary association of inferiority: an image without the substance or qualities of the original. Literary critic Fredric Jameson offers photorealism as an example of artistic simulacrum, in which a painting is created by copying a photograph that is itself a copy of the real thing. Other art forms that play with simulacra include trompe-l'œil, pop art, Italian neorealism, and French New Wave.

Philosophy
Simulacra have long been of interest to philosophers. In his Sophist, Plato speaks of two kinds of image-making. The first is a faithful reproduction, attempted to copy precisely the original."
"What association did the word ""simulacrum"" gather by the late 19th century?",Superiority,Originality,Inferiority,Authenticity,Quality,C,"A simulacrum (plural: simulacra or simulacrums, from Latin simulacrum, which means ""likeness, semblance"") is a representation or imitation of a person or thing. The word was first recorded in the English language in the late 16th century, used to describe a representation, such as a statue or a painting, especially of a god. By the late 19th century, it had gathered a secondary association of inferiority: an image without the substance or qualities of the original. Literary critic Fredric Jameson offers photorealism as an example of artistic simulacrum, in which a painting is created by copying a photograph that is itself a copy of the real thing. Other art forms that play with simulacra include trompe-l'œil, pop art, Italian neorealism, and French New Wave.

Philosophy
Simulacra have long been of interest to philosophers. In his Sophist, Plato speaks of two kinds of image-making. The first is a faithful reproduction, attempted to copy precisely the original."
Which art form does Fredric Jameson give as an example of artistic simulacrum?,Trompe-l'œil,Pop art,Italian neorealism,French New Wave,Photorealism,E,"A simulacrum (plural: simulacra or simulacrums, from Latin simulacrum, which means ""likeness, semblance"") is a representation or imitation of a person or thing. The word was first recorded in the English language in the late 16th century, used to describe a representation, such as a statue or a painting, especially of a god. By the late 19th century, it had gathered a secondary association of inferiority: an image without the substance or qualities of the original. Literary critic Fredric Jameson offers photorealism as an example of artistic simulacrum, in which a painting is created by copying a photograph that is itself a copy of the real thing. Other art forms that play with simulacra include trompe-l'œil, pop art, Italian neorealism, and French New Wave.

Philosophy
Simulacra have long been of interest to philosophers. In his Sophist, Plato speaks of two kinds of image-making. The first is a faithful reproduction, attempted to copy precisely the original."
"According to Plato, what are the two kinds of image-making?",Faithful reproduction and inferior reproduction,Copying and imitation,Originality and imitation,Superior reproduction and inferior reproduction,Representation and likeness,A,"A simulacrum (plural: simulacra or simulacrums, from Latin simulacrum, which means ""likeness, semblance"") is a representation or imitation of a person or thing. The word was first recorded in the English language in the late 16th century, used to describe a representation, such as a statue or a painting, especially of a god. By the late 19th century, it had gathered a secondary association of inferiority: an image without the substance or qualities of the original. Literary critic Fredric Jameson offers photorealism as an example of artistic simulacrum, in which a painting is created by copying a photograph that is itself a copy of the real thing. Other art forms that play with simulacra include trompe-l'œil, pop art, Italian neorealism, and French New Wave.

Philosophy
Simulacra have long been of interest to philosophers. In his Sophist, Plato speaks of two kinds of image-making. The first is a faithful reproduction, attempted to copy precisely the original."
What is an input kludge in computer programming?,A type of failure where simple user input is not handled,A security hole caused by mishandling user input,A method to detect user input problems through monkey testing,A technique to validate user input algorithms,A pattern for handling user input in software,A,"In computer programming, an input kludge is a type of failure in software (an anti-pattern) where simple user input is not handled. For example, if a computer program accepts free text input from the user, an ad hoc algorithm will mishandle many combinations of legal and illegal input strings. Input kludges are usually difficult for a programmer to detect in a unit test, but very easy for the end user to find. The evidence exists that the end user can easily crash software that fails to correctly handle user input.  Indeed, the buffer overflow security hole is an example of the problems caused.
To remedy input kludges, one may use input validation algorithms to handle user input. A monkey test can be used to detect an input kludge problem. A common first test to discover this problem is to roll one's hand across the computer keyboard or to 'mash' the keyboard to produce a large junk input, but such an action often lacks reproducibility."
Why are input kludges difficult to detect in unit tests?,They often require complex input strings,They are usually hidden in the code,They are not handled by the programmer,They are only found by end users,They are not considered a software failure,B,"In computer programming, an input kludge is a type of failure in software (an anti-pattern) where simple user input is not handled. For example, if a computer program accepts free text input from the user, an ad hoc algorithm will mishandle many combinations of legal and illegal input strings. Input kludges are usually difficult for a programmer to detect in a unit test, but very easy for the end user to find. The evidence exists that the end user can easily crash software that fails to correctly handle user input.  Indeed, the buffer overflow security hole is an example of the problems caused.
To remedy input kludges, one may use input validation algorithms to handle user input. A monkey test can be used to detect an input kludge problem. A common first test to discover this problem is to roll one's hand across the computer keyboard or to 'mash' the keyboard to produce a large junk input, but such an action often lacks reproducibility."
What is a common test to detect an input kludge problem?,Entering random text into the program,Using a monkey test to simulate user input,Typing a specific sequence of keys on the keyboard,Sending junk input to the program,Rolling one's hand across the computer keyboard,E,"In computer programming, an input kludge is a type of failure in software (an anti-pattern) where simple user input is not handled. For example, if a computer program accepts free text input from the user, an ad hoc algorithm will mishandle many combinations of legal and illegal input strings. Input kludges are usually difficult for a programmer to detect in a unit test, but very easy for the end user to find. The evidence exists that the end user can easily crash software that fails to correctly handle user input.  Indeed, the buffer overflow security hole is an example of the problems caused.
To remedy input kludges, one may use input validation algorithms to handle user input. A monkey test can be used to detect an input kludge problem. A common first test to discover this problem is to roll one's hand across the computer keyboard or to 'mash' the keyboard to produce a large junk input, but such an action often lacks reproducibility."
How can input kludges be remedied?,By using input validation algorithms,By performing monkey tests,By handling user input with reproducible actions,By avoiding buffer overflow security holes,By using ad hoc algorithms,A,"In computer programming, an input kludge is a type of failure in software (an anti-pattern) where simple user input is not handled. For example, if a computer program accepts free text input from the user, an ad hoc algorithm will mishandle many combinations of legal and illegal input strings. Input kludges are usually difficult for a programmer to detect in a unit test, but very easy for the end user to find. The evidence exists that the end user can easily crash software that fails to correctly handle user input.  Indeed, the buffer overflow security hole is an example of the problems caused.
To remedy input kludges, one may use input validation algorithms to handle user input. A monkey test can be used to detect an input kludge problem. A common first test to discover this problem is to roll one's hand across the computer keyboard or to 'mash' the keyboard to produce a large junk input, but such an action often lacks reproducibility."
What is the evidence that end users can easily crash software with input kludges?,The existence of buffer overflow security holes,The difficulty for programmers to detect input kludges,The use of ad hoc algorithms in user input handling,The ability to produce junk input with a keyboard,The mishandling of legal and illegal input strings,E,"In computer programming, an input kludge is a type of failure in software (an anti-pattern) where simple user input is not handled. For example, if a computer program accepts free text input from the user, an ad hoc algorithm will mishandle many combinations of legal and illegal input strings. Input kludges are usually difficult for a programmer to detect in a unit test, but very easy for the end user to find. The evidence exists that the end user can easily crash software that fails to correctly handle user input.  Indeed, the buffer overflow security hole is an example of the problems caused.
To remedy input kludges, one may use input validation algorithms to handle user input. A monkey test can be used to detect an input kludge problem. A common first test to discover this problem is to roll one's hand across the computer keyboard or to 'mash' the keyboard to produce a large junk input, but such an action often lacks reproducibility."
In what year was the Electrical Engineering program created at MIT?,1882,1902,1975,2000,2010,A,"The MIT Electrical Engineering and Computer Science Department is an engineering department of the Massachusetts Institute of Technology in Cambridge, Massachusetts. It is regarded as one of the most prestigious in the world, and offers degrees of Master of Science, Master of Engineering, Doctor of Philosophy, and Doctor of Science.

History
The curriculum for the electrical engineering program was created in 1882, and was the first such program in the country. It was initially taught by the physics faculty.  In 1902, the Institute set up a separate Electrical Engineering department. The department was renamed to Electrical Engineering and Computer Science in 1975, to highlight the new addition of computer science to the program.

Academics
Current faculty
Professors
Silvio Micali
Harold Abelson
Anant Agarwal
Akintunde I. Akinwande
Dimitri A. Antoniadis
Arvind
Arthur B."
Who was the first professor of the Electrical Engineering program at MIT?,Silvio Micali,Harold Abelson,Anant Agarwal,Akintunde I. Akinwande,Dimitri A. Antoniadis,B,"The MIT Electrical Engineering and Computer Science Department is an engineering department of the Massachusetts Institute of Technology in Cambridge, Massachusetts. It is regarded as one of the most prestigious in the world, and offers degrees of Master of Science, Master of Engineering, Doctor of Philosophy, and Doctor of Science.

History
The curriculum for the electrical engineering program was created in 1882, and was the first such program in the country. It was initially taught by the physics faculty.  In 1902, the Institute set up a separate Electrical Engineering department. The department was renamed to Electrical Engineering and Computer Science in 1975, to highlight the new addition of computer science to the program.

Academics
Current faculty
Professors
Silvio Micali
Harold Abelson
Anant Agarwal
Akintunde I. Akinwande
Dimitri A. Antoniadis
Arvind
Arthur B."
What degrees does the MIT Electrical Engineering and Computer Science Department offer?,Master of Science,Master of Engineering,Doctor of Philosophy,Doctor of Science,All of the above,E,"The MIT Electrical Engineering and Computer Science Department is an engineering department of the Massachusetts Institute of Technology in Cambridge, Massachusetts. It is regarded as one of the most prestigious in the world, and offers degrees of Master of Science, Master of Engineering, Doctor of Philosophy, and Doctor of Science.

History
The curriculum for the electrical engineering program was created in 1882, and was the first such program in the country. It was initially taught by the physics faculty.  In 1902, the Institute set up a separate Electrical Engineering department. The department was renamed to Electrical Engineering and Computer Science in 1975, to highlight the new addition of computer science to the program.

Academics
Current faculty
Professors
Silvio Micali
Harold Abelson
Anant Agarwal
Akintunde I. Akinwande
Dimitri A. Antoniadis
Arvind
Arthur B."
What addition to the program led to the renaming of the department in 1975?,Physics,Chemistry,Mathematics,Computer Science,Biology,D,"The MIT Electrical Engineering and Computer Science Department is an engineering department of the Massachusetts Institute of Technology in Cambridge, Massachusetts. It is regarded as one of the most prestigious in the world, and offers degrees of Master of Science, Master of Engineering, Doctor of Philosophy, and Doctor of Science.

History
The curriculum for the electrical engineering program was created in 1882, and was the first such program in the country. It was initially taught by the physics faculty.  In 1902, the Institute set up a separate Electrical Engineering department. The department was renamed to Electrical Engineering and Computer Science in 1975, to highlight the new addition of computer science to the program.

Academics
Current faculty
Professors
Silvio Micali
Harold Abelson
Anant Agarwal
Akintunde I. Akinwande
Dimitri A. Antoniadis
Arvind
Arthur B."
Who is a current professor in the Electrical Engineering and Computer Science department?,Silvio Micali,Harold Abelson,Anant Agarwal,Akintunde I. Akinwande,Dimitri A. Antoniadis,A,"The MIT Electrical Engineering and Computer Science Department is an engineering department of the Massachusetts Institute of Technology in Cambridge, Massachusetts. It is regarded as one of the most prestigious in the world, and offers degrees of Master of Science, Master of Engineering, Doctor of Philosophy, and Doctor of Science.

History
The curriculum for the electrical engineering program was created in 1882, and was the first such program in the country. It was initially taught by the physics faculty.  In 1902, the Institute set up a separate Electrical Engineering department. The department was renamed to Electrical Engineering and Computer Science in 1975, to highlight the new addition of computer science to the program.

Academics
Current faculty
Professors
Silvio Micali
Harold Abelson
Anant Agarwal
Akintunde I. Akinwande
Dimitri A. Antoniadis
Arvind
Arthur B."
What is the adele ring?,A central object of class field theory,A branch of algebraic number theory,The restricted product of all the completions of the global field,A self-dual topological ring,None of the above,C,"In mathematics, the adele ring of a global field (also adelic ring, ring of adeles or ring of adèles) is a central object of class field theory, a branch of algebraic number theory. It is the restricted product of all the completions of the global field and is an example of a self-dual topological ring.
An adele derives from a particular kind of idele. ""Idele"" derives from the French ""idèle"" and was coined by the French mathematician Claude Chevalley. The word stands for 'ideal element' (abbreviated: id.el.). Adele (French: ""adèle"") stands for 'additive idele' (that is, additive ideal element).
The ring of adeles allows one to describe the Artin reciprocity law, which is a generalisation of quadratic reciprocity, and other reciprocity laws over finite fields. In addition, it is a classical theorem from Weil that 
  
    G
    G
  -bundles on an algebraic curve over a finite field can be described in terms of adeles for a reductive group 
  
    G
    G
  . Adeles are also connected with the adelic algebraic groups and adelic curves.
The study of geometry of numbers over the ring of adeles of a number field is called adelic geometry.

Definition
Let 
  
    K
    K
   be a global field (a finite extension of 
  
    
      Q
    
    \mathbf {Q}
   or the function field of a curve 
  
    
      
        X
        
          /
        
        
          
            F
            
              
                q
              
            
          
        
      
    
    {\displaystyle X/\mathbf {F_{\mathit {q}}} }
   over a finite field)."
"Who coined the term ""idele""?",Claude Chevalley,Carl Friedrich Gauss,Pierre-Simon Laplace,Leonhard Euler,None of the above,A,"In mathematics, the adele ring of a global field (also adelic ring, ring of adeles or ring of adèles) is a central object of class field theory, a branch of algebraic number theory. It is the restricted product of all the completions of the global field and is an example of a self-dual topological ring.
An adele derives from a particular kind of idele. ""Idele"" derives from the French ""idèle"" and was coined by the French mathematician Claude Chevalley. The word stands for 'ideal element' (abbreviated: id.el.). Adele (French: ""adèle"") stands for 'additive idele' (that is, additive ideal element).
The ring of adeles allows one to describe the Artin reciprocity law, which is a generalisation of quadratic reciprocity, and other reciprocity laws over finite fields. In addition, it is a classical theorem from Weil that 
  
    G
    G
  -bundles on an algebraic curve over a finite field can be described in terms of adeles for a reductive group 
  
    G
    G
  . Adeles are also connected with the adelic algebraic groups and adelic curves.
The study of geometry of numbers over the ring of adeles of a number field is called adelic geometry.

Definition
Let 
  
    K
    K
   be a global field (a finite extension of 
  
    
      Q
    
    \mathbf {Q}
   or the function field of a curve 
  
    
      
        X
        
          /
        
        
          
            F
            
              
                q
              
            
          
        
      
    
    {\displaystyle X/\mathbf {F_{\mathit {q}}} }
   over a finite field)."
"What does the term ""adele"" stand for?",Ideal element,Additive idele,Adele ring,Global field,None of the above,B,"In mathematics, the adele ring of a global field (also adelic ring, ring of adeles or ring of adèles) is a central object of class field theory, a branch of algebraic number theory. It is the restricted product of all the completions of the global field and is an example of a self-dual topological ring.
An adele derives from a particular kind of idele. ""Idele"" derives from the French ""idèle"" and was coined by the French mathematician Claude Chevalley. The word stands for 'ideal element' (abbreviated: id.el.). Adele (French: ""adèle"") stands for 'additive idele' (that is, additive ideal element).
The ring of adeles allows one to describe the Artin reciprocity law, which is a generalisation of quadratic reciprocity, and other reciprocity laws over finite fields. In addition, it is a classical theorem from Weil that 
  
    G
    G
  -bundles on an algebraic curve over a finite field can be described in terms of adeles for a reductive group 
  
    G
    G
  . Adeles are also connected with the adelic algebraic groups and adelic curves.
The study of geometry of numbers over the ring of adeles of a number field is called adelic geometry.

Definition
Let 
  
    K
    K
   be a global field (a finite extension of 
  
    
      Q
    
    \mathbf {Q}
   or the function field of a curve 
  
    
      
        X
        
          /
        
        
          
            F
            
              
                q
              
            
          
        
      
    
    {\displaystyle X/\mathbf {F_{\mathit {q}}} }
   over a finite field)."
What can be described in terms of adeles for a reductive group G?,Artin reciprocity law,Quadratic reciprocity,Adelic algebraic groups,Adelic curves,None of the above,C,"In mathematics, the adele ring of a global field (also adelic ring, ring of adeles or ring of adèles) is a central object of class field theory, a branch of algebraic number theory. It is the restricted product of all the completions of the global field and is an example of a self-dual topological ring.
An adele derives from a particular kind of idele. ""Idele"" derives from the French ""idèle"" and was coined by the French mathematician Claude Chevalley. The word stands for 'ideal element' (abbreviated: id.el.). Adele (French: ""adèle"") stands for 'additive idele' (that is, additive ideal element).
The ring of adeles allows one to describe the Artin reciprocity law, which is a generalisation of quadratic reciprocity, and other reciprocity laws over finite fields. In addition, it is a classical theorem from Weil that 
  
    G
    G
  -bundles on an algebraic curve over a finite field can be described in terms of adeles for a reductive group 
  
    G
    G
  . Adeles are also connected with the adelic algebraic groups and adelic curves.
The study of geometry of numbers over the ring of adeles of a number field is called adelic geometry.

Definition
Let 
  
    K
    K
   be a global field (a finite extension of 
  
    
      Q
    
    \mathbf {Q}
   or the function field of a curve 
  
    
      
        X
        
          /
        
        
          
            F
            
              
                q
              
            
          
        
      
    
    {\displaystyle X/\mathbf {F_{\mathit {q}}} }
   over a finite field)."
What is the study of geometry of numbers over the ring of adeles called?,Algebraic number theory,Class field theory,Adelic geometry,Quadratic reciprocity,None of the above,C,"In mathematics, the adele ring of a global field (also adelic ring, ring of adeles or ring of adèles) is a central object of class field theory, a branch of algebraic number theory. It is the restricted product of all the completions of the global field and is an example of a self-dual topological ring.
An adele derives from a particular kind of idele. ""Idele"" derives from the French ""idèle"" and was coined by the French mathematician Claude Chevalley. The word stands for 'ideal element' (abbreviated: id.el.). Adele (French: ""adèle"") stands for 'additive idele' (that is, additive ideal element).
The ring of adeles allows one to describe the Artin reciprocity law, which is a generalisation of quadratic reciprocity, and other reciprocity laws over finite fields. In addition, it is a classical theorem from Weil that 
  
    G
    G
  -bundles on an algebraic curve over a finite field can be described in terms of adeles for a reductive group 
  
    G
    G
  . Adeles are also connected with the adelic algebraic groups and adelic curves.
The study of geometry of numbers over the ring of adeles of a number field is called adelic geometry.

Definition
Let 
  
    K
    K
   be a global field (a finite extension of 
  
    
      Q
    
    \mathbf {Q}
   or the function field of a curve 
  
    
      
        X
        
          /
        
        
          
            F
            
              
                q
              
            
          
        
      
    
    {\displaystyle X/\mathbf {F_{\mathit {q}}} }
   over a finite field)."
What is the shape of a pistol boiler firebox?,Rectangular,Triangular,Circular,Oval,Hexagonal,C,"A pistol boiler is a design of steam boiler used in light steam tractors and overtype steam wagons. It is noted for the unusual shape of the firebox, a circular design intended to be self-supporting without the use of firebox stays.

The name ""pistol boiler"" derives from the smooth curve of the outer firebox flowing into the boiler barrel and a supposed resemblance to the stock of an early 19th-century pistol.

Need for a self-supporting firebox
The locomotive boiler had become well established since Stephenson's day; although the cost and complexity of its firebox remained a drawback, particularly for small boilers. If the top crown sheet of the inner firebox was made flat, so as to maintain a constant water depth above it, this required complex and expensive girder stays to support it. These stays were also a safety-critical part of the boiler, and many past boiler explosions had been caused by their failure. This was especially so for boilers that were likely to be used at all carelessly or by crews who were less skilled or well-trained.
Clearly the market for small steam traction engines could make use of a novel boiler design that avoided these problems. Following the lead of the London & Birmingham Railway's Bury locomotives, some small portable engines were already using cylindrical stayless fireboxes. These combined a cylindrical vertical drum with a domed top, both shapes that could support themselves well under pressure."
"Why is the name ""pistol boiler"" used for this design?",Because it resembles a gun barrel,Because it has a smooth curve like a pistol stock,Because it is small and portable like a pistol,Because it is used in light steam tractors,Because it was invented by a gunsmith,B,"A pistol boiler is a design of steam boiler used in light steam tractors and overtype steam wagons. It is noted for the unusual shape of the firebox, a circular design intended to be self-supporting without the use of firebox stays.

The name ""pistol boiler"" derives from the smooth curve of the outer firebox flowing into the boiler barrel and a supposed resemblance to the stock of an early 19th-century pistol.

Need for a self-supporting firebox
The locomotive boiler had become well established since Stephenson's day; although the cost and complexity of its firebox remained a drawback, particularly for small boilers. If the top crown sheet of the inner firebox was made flat, so as to maintain a constant water depth above it, this required complex and expensive girder stays to support it. These stays were also a safety-critical part of the boiler, and many past boiler explosions had been caused by their failure. This was especially so for boilers that were likely to be used at all carelessly or by crews who were less skilled or well-trained.
Clearly the market for small steam traction engines could make use of a novel boiler design that avoided these problems. Following the lead of the London & Birmingham Railway's Bury locomotives, some small portable engines were already using cylindrical stayless fireboxes. These combined a cylindrical vertical drum with a domed top, both shapes that could support themselves well under pressure."
What was the main problem with the firebox of a traditional locomotive boiler?,It was too small for the boiler barrel,It required complex and expensive girder stays,It was made of a weak material,It was difficult to maintain a constant water depth,It was prone to corrosion,B,"A pistol boiler is a design of steam boiler used in light steam tractors and overtype steam wagons. It is noted for the unusual shape of the firebox, a circular design intended to be self-supporting without the use of firebox stays.

The name ""pistol boiler"" derives from the smooth curve of the outer firebox flowing into the boiler barrel and a supposed resemblance to the stock of an early 19th-century pistol.

Need for a self-supporting firebox
The locomotive boiler had become well established since Stephenson's day; although the cost and complexity of its firebox remained a drawback, particularly for small boilers. If the top crown sheet of the inner firebox was made flat, so as to maintain a constant water depth above it, this required complex and expensive girder stays to support it. These stays were also a safety-critical part of the boiler, and many past boiler explosions had been caused by their failure. This was especially so for boilers that were likely to be used at all carelessly or by crews who were less skilled or well-trained.
Clearly the market for small steam traction engines could make use of a novel boiler design that avoided these problems. Following the lead of the London & Birmingham Railway's Bury locomotives, some small portable engines were already using cylindrical stayless fireboxes. These combined a cylindrical vertical drum with a domed top, both shapes that could support themselves well under pressure."
What kind of boilers were already using cylindrical stayless fireboxes?,Light steam tractors,Portable engines,Overtype steam wagons,Stephenson locomotives,Bury locomotives,B,"A pistol boiler is a design of steam boiler used in light steam tractors and overtype steam wagons. It is noted for the unusual shape of the firebox, a circular design intended to be self-supporting without the use of firebox stays.

The name ""pistol boiler"" derives from the smooth curve of the outer firebox flowing into the boiler barrel and a supposed resemblance to the stock of an early 19th-century pistol.

Need for a self-supporting firebox
The locomotive boiler had become well established since Stephenson's day; although the cost and complexity of its firebox remained a drawback, particularly for small boilers. If the top crown sheet of the inner firebox was made flat, so as to maintain a constant water depth above it, this required complex and expensive girder stays to support it. These stays were also a safety-critical part of the boiler, and many past boiler explosions had been caused by their failure. This was especially so for boilers that were likely to be used at all carelessly or by crews who were less skilled or well-trained.
Clearly the market for small steam traction engines could make use of a novel boiler design that avoided these problems. Following the lead of the London & Birmingham Railway's Bury locomotives, some small portable engines were already using cylindrical stayless fireboxes. These combined a cylindrical vertical drum with a domed top, both shapes that could support themselves well under pressure."
What shapes were combined in the cylindrical stayless firebox design?,Cylinder and triangle,Cube and pyramid,Sphere and cone,Vertical drum and dome,Hexagon and rectangle,D,"A pistol boiler is a design of steam boiler used in light steam tractors and overtype steam wagons. It is noted for the unusual shape of the firebox, a circular design intended to be self-supporting without the use of firebox stays.

The name ""pistol boiler"" derives from the smooth curve of the outer firebox flowing into the boiler barrel and a supposed resemblance to the stock of an early 19th-century pistol.

Need for a self-supporting firebox
The locomotive boiler had become well established since Stephenson's day; although the cost and complexity of its firebox remained a drawback, particularly for small boilers. If the top crown sheet of the inner firebox was made flat, so as to maintain a constant water depth above it, this required complex and expensive girder stays to support it. These stays were also a safety-critical part of the boiler, and many past boiler explosions had been caused by their failure. This was especially so for boilers that were likely to be used at all carelessly or by crews who were less skilled or well-trained.
Clearly the market for small steam traction engines could make use of a novel boiler design that avoided these problems. Following the lead of the London & Birmingham Railway's Bury locomotives, some small portable engines were already using cylindrical stayless fireboxes. These combined a cylindrical vertical drum with a domed top, both shapes that could support themselves well under pressure."
What is sexual dimorphism?,A component of morphological variation in biological populations,A measure of the social organization and behavior in Primates,A statistical measure to assess probability distributions,A function that accounts for partial characteristics of a distribution,A measure that incorporates an inferential support,A,"Although the subject of sexual dimorphism is not in itself controversial, the measures by which it is assessed differ widely. Most of the measures are used on the assumption that a random variable is considered so that probability distributions should be taken into account. In this review, a series of sexual dimorphism measures are discussed concerning both their definition and the probability law on which they are based. Most of them are sample functions, or statistics, which account for only partial characteristics, for example the mean or expected value, of the distribution involved. Further, the most widely used measure fails to incorporate an inferential support.

Introduction
It is widely known that sexual dimorphism is an important component of the morphological variation in biological populations (see, e.g., Klein and Cruz-Uribe, 1983; Oxnard, 1987; Kelley, 1993). In higher Primates, sexual dimorphism is also related to some aspects of the social organization and behavior (Alexander et al., 1979; Clutton-Brock, 1985). Thus, it has been observed that the most dimorphic species tend to polygyny and a social organization based on male dominance, whereas in the less dimorphic species, monogamy and family groups are more common."
What is the relationship between sexual dimorphism and social organization?,The most dimorphic species tend to polygyny and male dominance,Sexual dimorphism is not related to social organization,The less dimorphic species tend to polygyny and male dominance,Sexual dimorphism is related to monogamy and family groups,There is no relationship between sexual dimorphism and social organization,A,"Although the subject of sexual dimorphism is not in itself controversial, the measures by which it is assessed differ widely. Most of the measures are used on the assumption that a random variable is considered so that probability distributions should be taken into account. In this review, a series of sexual dimorphism measures are discussed concerning both their definition and the probability law on which they are based. Most of them are sample functions, or statistics, which account for only partial characteristics, for example the mean or expected value, of the distribution involved. Further, the most widely used measure fails to incorporate an inferential support.

Introduction
It is widely known that sexual dimorphism is an important component of the morphological variation in biological populations (see, e.g., Klein and Cruz-Uribe, 1983; Oxnard, 1987; Kelley, 1993). In higher Primates, sexual dimorphism is also related to some aspects of the social organization and behavior (Alexander et al., 1979; Clutton-Brock, 1985). Thus, it has been observed that the most dimorphic species tend to polygyny and a social organization based on male dominance, whereas in the less dimorphic species, monogamy and family groups are more common."
What are some measures used to assess sexual dimorphism?,Sample functions that account for partial characteristics of the distribution,Measures based on the mean or expected value of the distribution involved,Measures that incorporate inferential support,Measures that consider probability distributions,All of the above,E,"Although the subject of sexual dimorphism is not in itself controversial, the measures by which it is assessed differ widely. Most of the measures are used on the assumption that a random variable is considered so that probability distributions should be taken into account. In this review, a series of sexual dimorphism measures are discussed concerning both their definition and the probability law on which they are based. Most of them are sample functions, or statistics, which account for only partial characteristics, for example the mean or expected value, of the distribution involved. Further, the most widely used measure fails to incorporate an inferential support.

Introduction
It is widely known that sexual dimorphism is an important component of the morphological variation in biological populations (see, e.g., Klein and Cruz-Uribe, 1983; Oxnard, 1987; Kelley, 1993). In higher Primates, sexual dimorphism is also related to some aspects of the social organization and behavior (Alexander et al., 1979; Clutton-Brock, 1985). Thus, it has been observed that the most dimorphic species tend to polygyny and a social organization based on male dominance, whereas in the less dimorphic species, monogamy and family groups are more common."
What is a common limitation of the most widely used measure of sexual dimorphism?,It fails to incorporate an inferential support,It only accounts for partial characteristics of the distribution,It is not based on probability distributions,It is not applicable to higher Primates,There are no limitations of the most widely used measure,A,"Although the subject of sexual dimorphism is not in itself controversial, the measures by which it is assessed differ widely. Most of the measures are used on the assumption that a random variable is considered so that probability distributions should be taken into account. In this review, a series of sexual dimorphism measures are discussed concerning both their definition and the probability law on which they are based. Most of them are sample functions, or statistics, which account for only partial characteristics, for example the mean or expected value, of the distribution involved. Further, the most widely used measure fails to incorporate an inferential support.

Introduction
It is widely known that sexual dimorphism is an important component of the morphological variation in biological populations (see, e.g., Klein and Cruz-Uribe, 1983; Oxnard, 1987; Kelley, 1993). In higher Primates, sexual dimorphism is also related to some aspects of the social organization and behavior (Alexander et al., 1979; Clutton-Brock, 1985). Thus, it has been observed that the most dimorphic species tend to polygyny and a social organization based on male dominance, whereas in the less dimorphic species, monogamy and family groups are more common."
Which of the following references provide information on sexual dimorphism?,"Kelley, 1993","Oxnard, 1987","Clutton-Brock, 1985","Alexander et al., 1979",All of the above,E,"Although the subject of sexual dimorphism is not in itself controversial, the measures by which it is assessed differ widely. Most of the measures are used on the assumption that a random variable is considered so that probability distributions should be taken into account. In this review, a series of sexual dimorphism measures are discussed concerning both their definition and the probability law on which they are based. Most of them are sample functions, or statistics, which account for only partial characteristics, for example the mean or expected value, of the distribution involved. Further, the most widely used measure fails to incorporate an inferential support.

Introduction
It is widely known that sexual dimorphism is an important component of the morphological variation in biological populations (see, e.g., Klein and Cruz-Uribe, 1983; Oxnard, 1987; Kelley, 1993). In higher Primates, sexual dimorphism is also related to some aspects of the social organization and behavior (Alexander et al., 1979; Clutton-Brock, 1985). Thus, it has been observed that the most dimorphic species tend to polygyny and a social organization based on male dominance, whereas in the less dimorphic species, monogamy and family groups are more common."
What does MPU stand for in MPU-401 interface?,MIDI Processing Unit,MIDI Playback Unit,MIDI Production Unit,MIDI Power Unit,MIDI Performance Unit,A,"The MPU-401, where MPU stands for MIDI Processing Unit, is an important but now obsolete interface for connecting MIDI-equipped electronic music hardware to personal computers. It was designed by Roland Corporation, which also co-authored the MIDI standard.

Design
Released around 1984, the original MPU-401 was an external breakout box providing MIDI IN/MIDI OUT/MIDI THRU/TAPE IN/TAPE OUT/MIDI SYNC connectors, for use with a separately-sold interface card/cartridge (""MPU-401 interface kit"") inserted into a computer system. For this setup, the following ""interface kits"" were made:

MIF-APL: For the Apple II.
MIF-C64: For the Commodore 64.
MIF-FM7: For the Fujitsu FM7.
MIF-IPC: For the IBM PC/IBM XT. It turned out not to work reliably with 286 and faster processors. Early versions of the actual PCB had IF-MIDI/IBM as a silk screen.
MIF-IPC-A: For the IBM AT, works with PC and XT as well.
Xanadu MUSICOM IFM-PC: For the IBM PC / IBM XT / IBM AT. This was a third party MIDI card, incorporating the MIF-IPC(-A) and additional functionality that was coupled with the OEM Roland MPU-401 BOB. It also had a mini audio jack on the PCB.
MIF-MSX: For the MSX.
MIF-PC8: For the NEC PC-88.
MIF-PC98: For the NEC PC-98.
MIF-X1: For the Sharp X1.
MIF-V64: For the Commodore 64.In 2014 hobbyists built clones of the MIF-IPC-A card for PCs.

Variants
Later, Roland would put most of the electronics originally found in the breakout box onto the interface card itself, thus reducing the size of the breakout box."
When was the original MPU-401 released?,1974,1984,1994,2004,2014,B,"The MPU-401, where MPU stands for MIDI Processing Unit, is an important but now obsolete interface for connecting MIDI-equipped electronic music hardware to personal computers. It was designed by Roland Corporation, which also co-authored the MIDI standard.

Design
Released around 1984, the original MPU-401 was an external breakout box providing MIDI IN/MIDI OUT/MIDI THRU/TAPE IN/TAPE OUT/MIDI SYNC connectors, for use with a separately-sold interface card/cartridge (""MPU-401 interface kit"") inserted into a computer system. For this setup, the following ""interface kits"" were made:

MIF-APL: For the Apple II.
MIF-C64: For the Commodore 64.
MIF-FM7: For the Fujitsu FM7.
MIF-IPC: For the IBM PC/IBM XT. It turned out not to work reliably with 286 and faster processors. Early versions of the actual PCB had IF-MIDI/IBM as a silk screen.
MIF-IPC-A: For the IBM AT, works with PC and XT as well.
Xanadu MUSICOM IFM-PC: For the IBM PC / IBM XT / IBM AT. This was a third party MIDI card, incorporating the MIF-IPC(-A) and additional functionality that was coupled with the OEM Roland MPU-401 BOB. It also had a mini audio jack on the PCB.
MIF-MSX: For the MSX.
MIF-PC8: For the NEC PC-88.
MIF-PC98: For the NEC PC-98.
MIF-X1: For the Sharp X1.
MIF-V64: For the Commodore 64.In 2014 hobbyists built clones of the MIF-IPC-A card for PCs.

Variants
Later, Roland would put most of the electronics originally found in the breakout box onto the interface card itself, thus reducing the size of the breakout box."
Which company designed the MPU-401 interface?,Apple,Commodore,Fujitsu,Roland,NEC,D,"The MPU-401, where MPU stands for MIDI Processing Unit, is an important but now obsolete interface for connecting MIDI-equipped electronic music hardware to personal computers. It was designed by Roland Corporation, which also co-authored the MIDI standard.

Design
Released around 1984, the original MPU-401 was an external breakout box providing MIDI IN/MIDI OUT/MIDI THRU/TAPE IN/TAPE OUT/MIDI SYNC connectors, for use with a separately-sold interface card/cartridge (""MPU-401 interface kit"") inserted into a computer system. For this setup, the following ""interface kits"" were made:

MIF-APL: For the Apple II.
MIF-C64: For the Commodore 64.
MIF-FM7: For the Fujitsu FM7.
MIF-IPC: For the IBM PC/IBM XT. It turned out not to work reliably with 286 and faster processors. Early versions of the actual PCB had IF-MIDI/IBM as a silk screen.
MIF-IPC-A: For the IBM AT, works with PC and XT as well.
Xanadu MUSICOM IFM-PC: For the IBM PC / IBM XT / IBM AT. This was a third party MIDI card, incorporating the MIF-IPC(-A) and additional functionality that was coupled with the OEM Roland MPU-401 BOB. It also had a mini audio jack on the PCB.
MIF-MSX: For the MSX.
MIF-PC8: For the NEC PC-88.
MIF-PC98: For the NEC PC-98.
MIF-X1: For the Sharp X1.
MIF-V64: For the Commodore 64.In 2014 hobbyists built clones of the MIF-IPC-A card for PCs.

Variants
Later, Roland would put most of the electronics originally found in the breakout box onto the interface card itself, thus reducing the size of the breakout box."
What additional functionality did the Xanadu MUSICOM IFM-PC MIDI card have?,MIDI IN/MIDI OUT/MIDI THRU/TAPE IN/TAPE OUT/MIDI SYNC connectors,Mini audio jack on the PCB,Breakout box,Smaller size,OEM Roland MPU-401 BOB,B,"The MPU-401, where MPU stands for MIDI Processing Unit, is an important but now obsolete interface for connecting MIDI-equipped electronic music hardware to personal computers. It was designed by Roland Corporation, which also co-authored the MIDI standard.

Design
Released around 1984, the original MPU-401 was an external breakout box providing MIDI IN/MIDI OUT/MIDI THRU/TAPE IN/TAPE OUT/MIDI SYNC connectors, for use with a separately-sold interface card/cartridge (""MPU-401 interface kit"") inserted into a computer system. For this setup, the following ""interface kits"" were made:

MIF-APL: For the Apple II.
MIF-C64: For the Commodore 64.
MIF-FM7: For the Fujitsu FM7.
MIF-IPC: For the IBM PC/IBM XT. It turned out not to work reliably with 286 and faster processors. Early versions of the actual PCB had IF-MIDI/IBM as a silk screen.
MIF-IPC-A: For the IBM AT, works with PC and XT as well.
Xanadu MUSICOM IFM-PC: For the IBM PC / IBM XT / IBM AT. This was a third party MIDI card, incorporating the MIF-IPC(-A) and additional functionality that was coupled with the OEM Roland MPU-401 BOB. It also had a mini audio jack on the PCB.
MIF-MSX: For the MSX.
MIF-PC8: For the NEC PC-88.
MIF-PC98: For the NEC PC-98.
MIF-X1: For the Sharp X1.
MIF-V64: For the Commodore 64.In 2014 hobbyists built clones of the MIF-IPC-A card for PCs.

Variants
Later, Roland would put most of the electronics originally found in the breakout box onto the interface card itself, thus reducing the size of the breakout box."
What happened to the breakout box in later variants of the MPU-401 interface?,It was eliminated,It was made larger,It was made smaller,It was redesigned,It was replaced with a third party MIDI card,A,"The MPU-401, where MPU stands for MIDI Processing Unit, is an important but now obsolete interface for connecting MIDI-equipped electronic music hardware to personal computers. It was designed by Roland Corporation, which also co-authored the MIDI standard.

Design
Released around 1984, the original MPU-401 was an external breakout box providing MIDI IN/MIDI OUT/MIDI THRU/TAPE IN/TAPE OUT/MIDI SYNC connectors, for use with a separately-sold interface card/cartridge (""MPU-401 interface kit"") inserted into a computer system. For this setup, the following ""interface kits"" were made:

MIF-APL: For the Apple II.
MIF-C64: For the Commodore 64.
MIF-FM7: For the Fujitsu FM7.
MIF-IPC: For the IBM PC/IBM XT. It turned out not to work reliably with 286 and faster processors. Early versions of the actual PCB had IF-MIDI/IBM as a silk screen.
MIF-IPC-A: For the IBM AT, works with PC and XT as well.
Xanadu MUSICOM IFM-PC: For the IBM PC / IBM XT / IBM AT. This was a third party MIDI card, incorporating the MIF-IPC(-A) and additional functionality that was coupled with the OEM Roland MPU-401 BOB. It also had a mini audio jack on the PCB.
MIF-MSX: For the MSX.
MIF-PC8: For the NEC PC-88.
MIF-PC98: For the NEC PC-98.
MIF-X1: For the Sharp X1.
MIF-V64: For the Commodore 64.In 2014 hobbyists built clones of the MIF-IPC-A card for PCs.

Variants
Later, Roland would put most of the electronics originally found in the breakout box onto the interface card itself, thus reducing the size of the breakout box."
What is architecture?,The process and product of designing and constructing buildings.,The study of cultures and their influence on design.,The application of scientific knowledge to develop practical inventions.,The expression of human creativity through physical structures.,The study of fine arts and their impact on culture.,A,"The following outline is an overview and topical guide to architecture:Architecture – the process and the product of designing and constructing buildings. Architectural works with a certain indefinable combination of design quality and external circumstances may become cultural symbols and / or be considered works of art.

What type of thing is architecture?
Architecture can be described as all of the following:

Academic discipline – focused study in one academic field or profession. A discipline incorporates expertise, people, projects, communities, challenges, studies, inquiry, and research areas that are strongly associated with the given discipline.
Buildings – buildings and similar structures, the product of architecture, are referred to as architecture.
One of the arts – as an art form, architecture is an outlet of human expression, that is usually influenced by culture and which in turn helps to change culture. Architecture is a physical manifestation of the internal human creative impulse.
Fine art – in Western European academic traditions, fine art is art developed primarily for aesthetics, distinguishing it from applied art that also has to serve some practical function. The word ""fine"" here does not so much denote the quality of the artwork in question, but the purity of the discipline according to traditional Western European canons.
Science – systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe. A science is a branch of science, or a discipline of science. It's a way of pursuing knowledge, not only the knowledge itself.
Applied science – branch of science that applies existing scientific knowledge to develop more practical applications, such as technology or inventions.

Definitions of architecture
Architecture is variously defined in conflicting ways, highlighting the difficulty of describing the scope of the subject precisely:
A general term to describe buildings and other physical structures – although not all buildings are generally considered to be architecture, and infrastructure (bridges, roads etc.) is civil engineering, not architecture.
The art and science, or the action and process, of designing and constructing buildings.
The design activity of the architect, the profession of designing buildings.
A building designed by an architect, the end product of architectural design.
A building whose design transcends mere function, a unifying or coherent form or structure.
The expression of thought in building.
A group or body of buildings in a particular style.
A particular style or way of designing buildings.Some key quotations on the subject of architecture:
Vitruvius: defined the essential qualities of architecture as ""firmness, commodity and delight"".
Johann Wolfgang von Goethe: ""I call architecture frozen music"".
Walter Gropius: ""Architecture begins where engineering ends"".
Le Corbusier: ""A house is a machine for living in"".
Louis Sullivan: ""..."
What is the role of architecture as an academic discipline?,To study the impact of architecture on the environment.,To promote the preservation of historical buildings.,To research and develop new construction materials.,To provide aesthetic and functional design solutions for buildings.,To analyze the social and cultural implications of architectural design.,D,"The following outline is an overview and topical guide to architecture:Architecture – the process and the product of designing and constructing buildings. Architectural works with a certain indefinable combination of design quality and external circumstances may become cultural symbols and / or be considered works of art.

What type of thing is architecture?
Architecture can be described as all of the following:

Academic discipline – focused study in one academic field or profession. A discipline incorporates expertise, people, projects, communities, challenges, studies, inquiry, and research areas that are strongly associated with the given discipline.
Buildings – buildings and similar structures, the product of architecture, are referred to as architecture.
One of the arts – as an art form, architecture is an outlet of human expression, that is usually influenced by culture and which in turn helps to change culture. Architecture is a physical manifestation of the internal human creative impulse.
Fine art – in Western European academic traditions, fine art is art developed primarily for aesthetics, distinguishing it from applied art that also has to serve some practical function. The word ""fine"" here does not so much denote the quality of the artwork in question, but the purity of the discipline according to traditional Western European canons.
Science – systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe. A science is a branch of science, or a discipline of science. It's a way of pursuing knowledge, not only the knowledge itself.
Applied science – branch of science that applies existing scientific knowledge to develop more practical applications, such as technology or inventions.

Definitions of architecture
Architecture is variously defined in conflicting ways, highlighting the difficulty of describing the scope of the subject precisely:
A general term to describe buildings and other physical structures – although not all buildings are generally considered to be architecture, and infrastructure (bridges, roads etc.) is civil engineering, not architecture.
The art and science, or the action and process, of designing and constructing buildings.
The design activity of the architect, the profession of designing buildings.
A building designed by an architect, the end product of architectural design.
A building whose design transcends mere function, a unifying or coherent form or structure.
The expression of thought in building.
A group or body of buildings in a particular style.
A particular style or way of designing buildings.Some key quotations on the subject of architecture:
Vitruvius: defined the essential qualities of architecture as ""firmness, commodity and delight"".
Johann Wolfgang von Goethe: ""I call architecture frozen music"".
Walter Gropius: ""Architecture begins where engineering ends"".
Le Corbusier: ""A house is a machine for living in"".
Louis Sullivan: ""..."
What distinguishes architecture as a fine art?,Its focus on practical functionality rather than aesthetic appeal.,Its adherence to traditional Western European canons of purity.,Its incorporation of scientific principles in the design process.,Its ability to create cultural symbols and influence society.,Its emphasis on the expression of internal human creativity.,E,"The following outline is an overview and topical guide to architecture:Architecture – the process and the product of designing and constructing buildings. Architectural works with a certain indefinable combination of design quality and external circumstances may become cultural symbols and / or be considered works of art.

What type of thing is architecture?
Architecture can be described as all of the following:

Academic discipline – focused study in one academic field or profession. A discipline incorporates expertise, people, projects, communities, challenges, studies, inquiry, and research areas that are strongly associated with the given discipline.
Buildings – buildings and similar structures, the product of architecture, are referred to as architecture.
One of the arts – as an art form, architecture is an outlet of human expression, that is usually influenced by culture and which in turn helps to change culture. Architecture is a physical manifestation of the internal human creative impulse.
Fine art – in Western European academic traditions, fine art is art developed primarily for aesthetics, distinguishing it from applied art that also has to serve some practical function. The word ""fine"" here does not so much denote the quality of the artwork in question, but the purity of the discipline according to traditional Western European canons.
Science – systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe. A science is a branch of science, or a discipline of science. It's a way of pursuing knowledge, not only the knowledge itself.
Applied science – branch of science that applies existing scientific knowledge to develop more practical applications, such as technology or inventions.

Definitions of architecture
Architecture is variously defined in conflicting ways, highlighting the difficulty of describing the scope of the subject precisely:
A general term to describe buildings and other physical structures – although not all buildings are generally considered to be architecture, and infrastructure (bridges, roads etc.) is civil engineering, not architecture.
The art and science, or the action and process, of designing and constructing buildings.
The design activity of the architect, the profession of designing buildings.
A building designed by an architect, the end product of architectural design.
A building whose design transcends mere function, a unifying or coherent form or structure.
The expression of thought in building.
A group or body of buildings in a particular style.
A particular style or way of designing buildings.Some key quotations on the subject of architecture:
Vitruvius: defined the essential qualities of architecture as ""firmness, commodity and delight"".
Johann Wolfgang von Goethe: ""I call architecture frozen music"".
Walter Gropius: ""Architecture begins where engineering ends"".
Le Corbusier: ""A house is a machine for living in"".
Louis Sullivan: ""..."
How is architecture related to science?,Architecture is a branch of applied science that focuses on practical applications.,Architecture is a form of art that draws inspiration from scientific knowledge.,Architecture incorporates scientific principles in the design and construction process.,Architecture uses scientific methods to analyze and predict building performance.,Architecture is a discipline that seeks to understand the universe through inquiry and research.,C,"The following outline is an overview and topical guide to architecture:Architecture – the process and the product of designing and constructing buildings. Architectural works with a certain indefinable combination of design quality and external circumstances may become cultural symbols and / or be considered works of art.

What type of thing is architecture?
Architecture can be described as all of the following:

Academic discipline – focused study in one academic field or profession. A discipline incorporates expertise, people, projects, communities, challenges, studies, inquiry, and research areas that are strongly associated with the given discipline.
Buildings – buildings and similar structures, the product of architecture, are referred to as architecture.
One of the arts – as an art form, architecture is an outlet of human expression, that is usually influenced by culture and which in turn helps to change culture. Architecture is a physical manifestation of the internal human creative impulse.
Fine art – in Western European academic traditions, fine art is art developed primarily for aesthetics, distinguishing it from applied art that also has to serve some practical function. The word ""fine"" here does not so much denote the quality of the artwork in question, but the purity of the discipline according to traditional Western European canons.
Science – systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe. A science is a branch of science, or a discipline of science. It's a way of pursuing knowledge, not only the knowledge itself.
Applied science – branch of science that applies existing scientific knowledge to develop more practical applications, such as technology or inventions.

Definitions of architecture
Architecture is variously defined in conflicting ways, highlighting the difficulty of describing the scope of the subject precisely:
A general term to describe buildings and other physical structures – although not all buildings are generally considered to be architecture, and infrastructure (bridges, roads etc.) is civil engineering, not architecture.
The art and science, or the action and process, of designing and constructing buildings.
The design activity of the architect, the profession of designing buildings.
A building designed by an architect, the end product of architectural design.
A building whose design transcends mere function, a unifying or coherent form or structure.
The expression of thought in building.
A group or body of buildings in a particular style.
A particular style or way of designing buildings.Some key quotations on the subject of architecture:
Vitruvius: defined the essential qualities of architecture as ""firmness, commodity and delight"".
Johann Wolfgang von Goethe: ""I call architecture frozen music"".
Walter Gropius: ""Architecture begins where engineering ends"".
Le Corbusier: ""A house is a machine for living in"".
Louis Sullivan: ""..."
What defines a building as architecture?,Its ability to serve a practical function and meet the needs of its occupants.,Its adherence to a particular architectural style or design philosophy.,Its status as a cultural symbol and work of art.,Its transcendence of mere functionality to create a unified and coherent form.,Its expression of thought and ideas through physical construction.,D,"The following outline is an overview and topical guide to architecture:Architecture – the process and the product of designing and constructing buildings. Architectural works with a certain indefinable combination of design quality and external circumstances may become cultural symbols and / or be considered works of art.

What type of thing is architecture?
Architecture can be described as all of the following:

Academic discipline – focused study in one academic field or profession. A discipline incorporates expertise, people, projects, communities, challenges, studies, inquiry, and research areas that are strongly associated with the given discipline.
Buildings – buildings and similar structures, the product of architecture, are referred to as architecture.
One of the arts – as an art form, architecture is an outlet of human expression, that is usually influenced by culture and which in turn helps to change culture. Architecture is a physical manifestation of the internal human creative impulse.
Fine art – in Western European academic traditions, fine art is art developed primarily for aesthetics, distinguishing it from applied art that also has to serve some practical function. The word ""fine"" here does not so much denote the quality of the artwork in question, but the purity of the discipline according to traditional Western European canons.
Science – systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe. A science is a branch of science, or a discipline of science. It's a way of pursuing knowledge, not only the knowledge itself.
Applied science – branch of science that applies existing scientific knowledge to develop more practical applications, such as technology or inventions.

Definitions of architecture
Architecture is variously defined in conflicting ways, highlighting the difficulty of describing the scope of the subject precisely:
A general term to describe buildings and other physical structures – although not all buildings are generally considered to be architecture, and infrastructure (bridges, roads etc.) is civil engineering, not architecture.
The art and science, or the action and process, of designing and constructing buildings.
The design activity of the architect, the profession of designing buildings.
A building designed by an architect, the end product of architectural design.
A building whose design transcends mere function, a unifying or coherent form or structure.
The expression of thought in building.
A group or body of buildings in a particular style.
A particular style or way of designing buildings.Some key quotations on the subject of architecture:
Vitruvius: defined the essential qualities of architecture as ""firmness, commodity and delight"".
Johann Wolfgang von Goethe: ""I call architecture frozen music"".
Walter Gropius: ""Architecture begins where engineering ends"".
Le Corbusier: ""A house is a machine for living in"".
Louis Sullivan: ""..."
What are the three branches of decision theory?,"Prescriptive decision theory, descriptive decision theory, empirical decision theory","Normative decision theory, descriptive decision theory, behavioral decision theory","Normative decision theory, prescriptive decision theory, descriptive decision theory","Normative decision theory, empirical decision theory, behavioral decision theory","Prescriptive decision theory, empirical decision theory, behavioral decision theory",C,"Decision theory (or the theory of choice; not to be confused with choice theory) is a branch of applied probability theory and analytic philosophy concerned with the theory of making decisions based on assigning probabilities to various factors and assigning numerical consequences to the outcome.There are three branches of decision theory:

Normative decision theory: Concerned with the identification of optimal decisions, where optimality is often determined by considering an ideal decision-maker who is able to calculate with perfect accuracy and is in some sense fully rational.
Prescriptive decision theory: Concerned with describing observed behaviors through the use of conceptual models, under the assumption that those making the decisions are behaving under some consistent rules.
Descriptive decision theory: Analyzes how individuals actually make the decisions that they do.Decision theory is a broad field from management sciences and is an interdisciplinary topic, studied by management scientists, medical researchers, mathematicians, data scientists, psychologists, biologists, social scientists, philosophers and computer scientists.
Empirical applications of this theory are usually done with the help of statistical and discrete mathematical approaches from computer science.

Normative and descriptive
Normative decision theory is concerned with identification of optimal decisions where optimality is often determined by considering an ideal decision maker who is able to calculate with perfect accuracy and is in some sense fully rational. The practical application of this prescriptive approach (how people ought to make decisions) is called decision analysis and is aimed at finding tools, methodologies, and software (decision support systems) to help people make better decisions.In contrast, descriptive decision theory is concerned with describing observed behaviors often under the assumption that those making decisions are behaving under some consistent rules. These rules may, for instance, have a procedural framework (e.g. Amos Tversky's elimination by aspects model) or an axiomatic framework (e.g. stochastic transitivity axioms), reconciling the Von Neumann-Morgenstern axioms with behavioral violations of the expected utility hypothesis, or they may explicitly give a functional form for time-inconsistent utility functions (e.g. Laibson's quasi-hyperbolic discounting).Prescriptive decision theory is concerned with predictions about behavior that positive decision theory produces to allow for further tests of the kind of decision-making that occurs in practice. In recent decades, there has also been increasing interest in ""behavioral decision theory"", contributing to a re-evaluation of what useful decision-making requires.

Types of decisions
Choice under uncertainty
The area of choice under uncertainty represents the heart of decision theory."
Which branch of decision theory is concerned with describing observed behaviors?,Normative decision theory,Prescriptive decision theory,Descriptive decision theory,Empirical decision theory,Behavioral decision theory,C,"Decision theory (or the theory of choice; not to be confused with choice theory) is a branch of applied probability theory and analytic philosophy concerned with the theory of making decisions based on assigning probabilities to various factors and assigning numerical consequences to the outcome.There are three branches of decision theory:

Normative decision theory: Concerned with the identification of optimal decisions, where optimality is often determined by considering an ideal decision-maker who is able to calculate with perfect accuracy and is in some sense fully rational.
Prescriptive decision theory: Concerned with describing observed behaviors through the use of conceptual models, under the assumption that those making the decisions are behaving under some consistent rules.
Descriptive decision theory: Analyzes how individuals actually make the decisions that they do.Decision theory is a broad field from management sciences and is an interdisciplinary topic, studied by management scientists, medical researchers, mathematicians, data scientists, psychologists, biologists, social scientists, philosophers and computer scientists.
Empirical applications of this theory are usually done with the help of statistical and discrete mathematical approaches from computer science.

Normative and descriptive
Normative decision theory is concerned with identification of optimal decisions where optimality is often determined by considering an ideal decision maker who is able to calculate with perfect accuracy and is in some sense fully rational. The practical application of this prescriptive approach (how people ought to make decisions) is called decision analysis and is aimed at finding tools, methodologies, and software (decision support systems) to help people make better decisions.In contrast, descriptive decision theory is concerned with describing observed behaviors often under the assumption that those making decisions are behaving under some consistent rules. These rules may, for instance, have a procedural framework (e.g. Amos Tversky's elimination by aspects model) or an axiomatic framework (e.g. stochastic transitivity axioms), reconciling the Von Neumann-Morgenstern axioms with behavioral violations of the expected utility hypothesis, or they may explicitly give a functional form for time-inconsistent utility functions (e.g. Laibson's quasi-hyperbolic discounting).Prescriptive decision theory is concerned with predictions about behavior that positive decision theory produces to allow for further tests of the kind of decision-making that occurs in practice. In recent decades, there has also been increasing interest in ""behavioral decision theory"", contributing to a re-evaluation of what useful decision-making requires.

Types of decisions
Choice under uncertainty
The area of choice under uncertainty represents the heart of decision theory."
Which branch of decision theory is concerned with identifying optimal decisions?,Normative decision theory,Prescriptive decision theory,Descriptive decision theory,Empirical decision theory,Behavioral decision theory,A,"Decision theory (or the theory of choice; not to be confused with choice theory) is a branch of applied probability theory and analytic philosophy concerned with the theory of making decisions based on assigning probabilities to various factors and assigning numerical consequences to the outcome.There are three branches of decision theory:

Normative decision theory: Concerned with the identification of optimal decisions, where optimality is often determined by considering an ideal decision-maker who is able to calculate with perfect accuracy and is in some sense fully rational.
Prescriptive decision theory: Concerned with describing observed behaviors through the use of conceptual models, under the assumption that those making the decisions are behaving under some consistent rules.
Descriptive decision theory: Analyzes how individuals actually make the decisions that they do.Decision theory is a broad field from management sciences and is an interdisciplinary topic, studied by management scientists, medical researchers, mathematicians, data scientists, psychologists, biologists, social scientists, philosophers and computer scientists.
Empirical applications of this theory are usually done with the help of statistical and discrete mathematical approaches from computer science.

Normative and descriptive
Normative decision theory is concerned with identification of optimal decisions where optimality is often determined by considering an ideal decision maker who is able to calculate with perfect accuracy and is in some sense fully rational. The practical application of this prescriptive approach (how people ought to make decisions) is called decision analysis and is aimed at finding tools, methodologies, and software (decision support systems) to help people make better decisions.In contrast, descriptive decision theory is concerned with describing observed behaviors often under the assumption that those making decisions are behaving under some consistent rules. These rules may, for instance, have a procedural framework (e.g. Amos Tversky's elimination by aspects model) or an axiomatic framework (e.g. stochastic transitivity axioms), reconciling the Von Neumann-Morgenstern axioms with behavioral violations of the expected utility hypothesis, or they may explicitly give a functional form for time-inconsistent utility functions (e.g. Laibson's quasi-hyperbolic discounting).Prescriptive decision theory is concerned with predictions about behavior that positive decision theory produces to allow for further tests of the kind of decision-making that occurs in practice. In recent decades, there has also been increasing interest in ""behavioral decision theory"", contributing to a re-evaluation of what useful decision-making requires.

Types of decisions
Choice under uncertainty
The area of choice under uncertainty represents the heart of decision theory."
What is the practical application of normative decision theory?,Decision analysis,Behavioral decision theory,Prescriptive decision theory,Empirical decision theory,Descriptive decision theory,A,"Decision theory (or the theory of choice; not to be confused with choice theory) is a branch of applied probability theory and analytic philosophy concerned with the theory of making decisions based on assigning probabilities to various factors and assigning numerical consequences to the outcome.There are three branches of decision theory:

Normative decision theory: Concerned with the identification of optimal decisions, where optimality is often determined by considering an ideal decision-maker who is able to calculate with perfect accuracy and is in some sense fully rational.
Prescriptive decision theory: Concerned with describing observed behaviors through the use of conceptual models, under the assumption that those making the decisions are behaving under some consistent rules.
Descriptive decision theory: Analyzes how individuals actually make the decisions that they do.Decision theory is a broad field from management sciences and is an interdisciplinary topic, studied by management scientists, medical researchers, mathematicians, data scientists, psychologists, biologists, social scientists, philosophers and computer scientists.
Empirical applications of this theory are usually done with the help of statistical and discrete mathematical approaches from computer science.

Normative and descriptive
Normative decision theory is concerned with identification of optimal decisions where optimality is often determined by considering an ideal decision maker who is able to calculate with perfect accuracy and is in some sense fully rational. The practical application of this prescriptive approach (how people ought to make decisions) is called decision analysis and is aimed at finding tools, methodologies, and software (decision support systems) to help people make better decisions.In contrast, descriptive decision theory is concerned with describing observed behaviors often under the assumption that those making decisions are behaving under some consistent rules. These rules may, for instance, have a procedural framework (e.g. Amos Tversky's elimination by aspects model) or an axiomatic framework (e.g. stochastic transitivity axioms), reconciling the Von Neumann-Morgenstern axioms with behavioral violations of the expected utility hypothesis, or they may explicitly give a functional form for time-inconsistent utility functions (e.g. Laibson's quasi-hyperbolic discounting).Prescriptive decision theory is concerned with predictions about behavior that positive decision theory produces to allow for further tests of the kind of decision-making that occurs in practice. In recent decades, there has also been increasing interest in ""behavioral decision theory"", contributing to a re-evaluation of what useful decision-making requires.

Types of decisions
Choice under uncertainty
The area of choice under uncertainty represents the heart of decision theory."
Which branch of decision theory is concerned with predictions about behavior?,Normative decision theory,Prescriptive decision theory,Descriptive decision theory,Empirical decision theory,Behavioral decision theory,D,"Decision theory (or the theory of choice; not to be confused with choice theory) is a branch of applied probability theory and analytic philosophy concerned with the theory of making decisions based on assigning probabilities to various factors and assigning numerical consequences to the outcome.There are three branches of decision theory:

Normative decision theory: Concerned with the identification of optimal decisions, where optimality is often determined by considering an ideal decision-maker who is able to calculate with perfect accuracy and is in some sense fully rational.
Prescriptive decision theory: Concerned with describing observed behaviors through the use of conceptual models, under the assumption that those making the decisions are behaving under some consistent rules.
Descriptive decision theory: Analyzes how individuals actually make the decisions that they do.Decision theory is a broad field from management sciences and is an interdisciplinary topic, studied by management scientists, medical researchers, mathematicians, data scientists, psychologists, biologists, social scientists, philosophers and computer scientists.
Empirical applications of this theory are usually done with the help of statistical and discrete mathematical approaches from computer science.

Normative and descriptive
Normative decision theory is concerned with identification of optimal decisions where optimality is often determined by considering an ideal decision maker who is able to calculate with perfect accuracy and is in some sense fully rational. The practical application of this prescriptive approach (how people ought to make decisions) is called decision analysis and is aimed at finding tools, methodologies, and software (decision support systems) to help people make better decisions.In contrast, descriptive decision theory is concerned with describing observed behaviors often under the assumption that those making decisions are behaving under some consistent rules. These rules may, for instance, have a procedural framework (e.g. Amos Tversky's elimination by aspects model) or an axiomatic framework (e.g. stochastic transitivity axioms), reconciling the Von Neumann-Morgenstern axioms with behavioral violations of the expected utility hypothesis, or they may explicitly give a functional form for time-inconsistent utility functions (e.g. Laibson's quasi-hyperbolic discounting).Prescriptive decision theory is concerned with predictions about behavior that positive decision theory produces to allow for further tests of the kind of decision-making that occurs in practice. In recent decades, there has also been increasing interest in ""behavioral decision theory"", contributing to a re-evaluation of what useful decision-making requires.

Types of decisions
Choice under uncertainty
The area of choice under uncertainty represents the heart of decision theory."
"When was the TUM School of Computation, Information and Technology (CIT) established?",2021,2022,2019,2020,2018,B,"The TUM School of Computation, Information and Technology (CIT) is a school of the Technical University of Munich, established in 2022 by the merger of three former departments. As of 2022, it is structured into the Department of Mathematics, the Department of Computer Engineering, the Department of Computer Science, and the Department of Electrical Engineering.

Department of Mathematics
The Department of Mathematics (MATH) is located at the Garching campus.

History
Mathematics was taught from the beginning at the Polytechnische Schule in München and the later Technische Hochschule München. Otto Hesse was the department's first professor for calculus, analytical geometry and analytical mechanics. Over the years, several institutes for mathematics were formed.
In 1974, the Institute of Geometry was merged with the Institute of Mathematics to form the Department of Mathematics, and informatics, which had been part of the Institute of Mathematics, became a separate department.

Research Groups
As of 2022, the research groups at the department are:
Algebra
Analysis
Analysis and Modelling
Applied Numerical Analysis, Optimization and Data Analysis
Biostatistics
Discrete Optimization
Dynamic Systems
Geometry and Topology
Mathematical Finance
Mathematical Optimization
Mathematical Physics
Mathematical Modeling of Biological Systems
Numerical Mathematics
Numerical Methods for Plasma Physics
Optimal Control
Probability Theory
Scientific Computing
Statistics

Department of Computer Science
The Department of Computer Science (CS) is located at the Garching campus.

History
The first courses in computer science at the Technical University of Munich were offered in 1967 at the Department of Mathematics, when Friedrich L. Bauer introduced a two-semester lecture titled Information Processing. In 1968, Klaus Samelson started offering a second lecture cycle titled Introduction to Informatics. By 1992, the computer science department had separated from the Department of Mathematics to form an independent Department of Informatics.In 2002, the department relocated from its old campus in the Munich city center to the new building on the Garching campus.In 2017, the Department celebrated 50 Years of Informatics Munich with a series of lectures and ceremonies, together with the Ludwig Maximilian University of Munich and the Bundeswehr University Munich.

Chairs
As of 2022, the department consists of the following chairs:
AI in Healthcare and Medicine
Algorithmic Game Theory
Algorithms and Complexity
Application and Middleware Systems
Augmented Reality
Bioinformatics
Computational Imaging and AI in Medicine
Computational Molecular Medicine
Computer Aided Medical Procedures
Computer Graphics and Visualization
Computer Vision and AI
Cyber Trust
Data Analytics and Machine Learning
Data Science and Engineering
Database Systems
Decision Science & Systems
Dynamic Vision and Learning
Efficient Algorithms
Engineering Software for Decentralized Systems
Ethics in Systems Design and Machine Learning
Formal Languages, Compiler & Software Construction
Formal Methods for Software Reliability
Hardware-aware Algorithms and Software for HPC
Information Systems & Business Process Management
Law and Security of Digitization
Legal Tech
Logic and Verification
Machine Learning of 3D Scene Geometry
Physics-based Simulation
Quantum Computing
Scientific Computing
Software & Systems Engineering
Software Engineering
Software Engineering for Business Information Systems
Theoretical Computer Science
Theoretical Foundations of AI
Visual Computing

Notable people
Seven faculty members of the Department of Informatics have been awarded the Gottfried Wilhelm Leibniz Prize, one of the highest endowed research prizes in Germany with a maximum of €2.5 million per award:

2020 – Thomas Neumann
2016 – Daniel Cremers
2008 – Susanne Albers
1997 – Ernst Mayr
1995 – Gerd Hirzinger
1994 – Manfred Broy
1991 – Karl-Heinz HoffmannFriedrich L."
"Which departments were merged to form the TUM School of Computation, Information and Technology (CIT)?","Department of Mathematics, Department of Computer Engineering, Department of Electrical Engineering","Department of Mathematics, Department of Computer Engineering, Department of Computer Science","Department of Mathematics, Department of Computer Science, Department of Electrical Engineering","Department of Computer Engineering, Department of Computer Science, Department of Electrical Engineering","Department of Mathematics, Department of Computer Engineering, Department of Informatics",C,"The TUM School of Computation, Information and Technology (CIT) is a school of the Technical University of Munich, established in 2022 by the merger of three former departments. As of 2022, it is structured into the Department of Mathematics, the Department of Computer Engineering, the Department of Computer Science, and the Department of Electrical Engineering.

Department of Mathematics
The Department of Mathematics (MATH) is located at the Garching campus.

History
Mathematics was taught from the beginning at the Polytechnische Schule in München and the later Technische Hochschule München. Otto Hesse was the department's first professor for calculus, analytical geometry and analytical mechanics. Over the years, several institutes for mathematics were formed.
In 1974, the Institute of Geometry was merged with the Institute of Mathematics to form the Department of Mathematics, and informatics, which had been part of the Institute of Mathematics, became a separate department.

Research Groups
As of 2022, the research groups at the department are:
Algebra
Analysis
Analysis and Modelling
Applied Numerical Analysis, Optimization and Data Analysis
Biostatistics
Discrete Optimization
Dynamic Systems
Geometry and Topology
Mathematical Finance
Mathematical Optimization
Mathematical Physics
Mathematical Modeling of Biological Systems
Numerical Mathematics
Numerical Methods for Plasma Physics
Optimal Control
Probability Theory
Scientific Computing
Statistics

Department of Computer Science
The Department of Computer Science (CS) is located at the Garching campus.

History
The first courses in computer science at the Technical University of Munich were offered in 1967 at the Department of Mathematics, when Friedrich L. Bauer introduced a two-semester lecture titled Information Processing. In 1968, Klaus Samelson started offering a second lecture cycle titled Introduction to Informatics. By 1992, the computer science department had separated from the Department of Mathematics to form an independent Department of Informatics.In 2002, the department relocated from its old campus in the Munich city center to the new building on the Garching campus.In 2017, the Department celebrated 50 Years of Informatics Munich with a series of lectures and ceremonies, together with the Ludwig Maximilian University of Munich and the Bundeswehr University Munich.

Chairs
As of 2022, the department consists of the following chairs:
AI in Healthcare and Medicine
Algorithmic Game Theory
Algorithms and Complexity
Application and Middleware Systems
Augmented Reality
Bioinformatics
Computational Imaging and AI in Medicine
Computational Molecular Medicine
Computer Aided Medical Procedures
Computer Graphics and Visualization
Computer Vision and AI
Cyber Trust
Data Analytics and Machine Learning
Data Science and Engineering
Database Systems
Decision Science & Systems
Dynamic Vision and Learning
Efficient Algorithms
Engineering Software for Decentralized Systems
Ethics in Systems Design and Machine Learning
Formal Languages, Compiler & Software Construction
Formal Methods for Software Reliability
Hardware-aware Algorithms and Software for HPC
Information Systems & Business Process Management
Law and Security of Digitization
Legal Tech
Logic and Verification
Machine Learning of 3D Scene Geometry
Physics-based Simulation
Quantum Computing
Scientific Computing
Software & Systems Engineering
Software Engineering
Software Engineering for Business Information Systems
Theoretical Computer Science
Theoretical Foundations of AI
Visual Computing

Notable people
Seven faculty members of the Department of Informatics have been awarded the Gottfried Wilhelm Leibniz Prize, one of the highest endowed research prizes in Germany with a maximum of €2.5 million per award:

2020 – Thomas Neumann
2016 – Daniel Cremers
2008 – Susanne Albers
1997 – Ernst Mayr
1995 – Gerd Hirzinger
1994 – Manfred Broy
1991 – Karl-Heinz HoffmannFriedrich L."
Where is the Department of Mathematics located?,Munich city center,Garching campus,Berlin,Hamburg,Frankfurt,B,"The TUM School of Computation, Information and Technology (CIT) is a school of the Technical University of Munich, established in 2022 by the merger of three former departments. As of 2022, it is structured into the Department of Mathematics, the Department of Computer Engineering, the Department of Computer Science, and the Department of Electrical Engineering.

Department of Mathematics
The Department of Mathematics (MATH) is located at the Garching campus.

History
Mathematics was taught from the beginning at the Polytechnische Schule in München and the later Technische Hochschule München. Otto Hesse was the department's first professor for calculus, analytical geometry and analytical mechanics. Over the years, several institutes for mathematics were formed.
In 1974, the Institute of Geometry was merged with the Institute of Mathematics to form the Department of Mathematics, and informatics, which had been part of the Institute of Mathematics, became a separate department.

Research Groups
As of 2022, the research groups at the department are:
Algebra
Analysis
Analysis and Modelling
Applied Numerical Analysis, Optimization and Data Analysis
Biostatistics
Discrete Optimization
Dynamic Systems
Geometry and Topology
Mathematical Finance
Mathematical Optimization
Mathematical Physics
Mathematical Modeling of Biological Systems
Numerical Mathematics
Numerical Methods for Plasma Physics
Optimal Control
Probability Theory
Scientific Computing
Statistics

Department of Computer Science
The Department of Computer Science (CS) is located at the Garching campus.

History
The first courses in computer science at the Technical University of Munich were offered in 1967 at the Department of Mathematics, when Friedrich L. Bauer introduced a two-semester lecture titled Information Processing. In 1968, Klaus Samelson started offering a second lecture cycle titled Introduction to Informatics. By 1992, the computer science department had separated from the Department of Mathematics to form an independent Department of Informatics.In 2002, the department relocated from its old campus in the Munich city center to the new building on the Garching campus.In 2017, the Department celebrated 50 Years of Informatics Munich with a series of lectures and ceremonies, together with the Ludwig Maximilian University of Munich and the Bundeswehr University Munich.

Chairs
As of 2022, the department consists of the following chairs:
AI in Healthcare and Medicine
Algorithmic Game Theory
Algorithms and Complexity
Application and Middleware Systems
Augmented Reality
Bioinformatics
Computational Imaging and AI in Medicine
Computational Molecular Medicine
Computer Aided Medical Procedures
Computer Graphics and Visualization
Computer Vision and AI
Cyber Trust
Data Analytics and Machine Learning
Data Science and Engineering
Database Systems
Decision Science & Systems
Dynamic Vision and Learning
Efficient Algorithms
Engineering Software for Decentralized Systems
Ethics in Systems Design and Machine Learning
Formal Languages, Compiler & Software Construction
Formal Methods for Software Reliability
Hardware-aware Algorithms and Software for HPC
Information Systems & Business Process Management
Law and Security of Digitization
Legal Tech
Logic and Verification
Machine Learning of 3D Scene Geometry
Physics-based Simulation
Quantum Computing
Scientific Computing
Software & Systems Engineering
Software Engineering
Software Engineering for Business Information Systems
Theoretical Computer Science
Theoretical Foundations of AI
Visual Computing

Notable people
Seven faculty members of the Department of Informatics have been awarded the Gottfried Wilhelm Leibniz Prize, one of the highest endowed research prizes in Germany with a maximum of €2.5 million per award:

2020 – Thomas Neumann
2016 – Daniel Cremers
2008 – Susanne Albers
1997 – Ernst Mayr
1995 – Gerd Hirzinger
1994 – Manfred Broy
1991 – Karl-Heinz HoffmannFriedrich L."
"Who was the first professor for calculus, analytical geometry, and analytical mechanics at the Department of Mathematics?",Otto Hesse,Friedrich L. Bauer,Klaus Samelson,Thomas Neumann,Daniel Cremers,A,"The TUM School of Computation, Information and Technology (CIT) is a school of the Technical University of Munich, established in 2022 by the merger of three former departments. As of 2022, it is structured into the Department of Mathematics, the Department of Computer Engineering, the Department of Computer Science, and the Department of Electrical Engineering.

Department of Mathematics
The Department of Mathematics (MATH) is located at the Garching campus.

History
Mathematics was taught from the beginning at the Polytechnische Schule in München and the later Technische Hochschule München. Otto Hesse was the department's first professor for calculus, analytical geometry and analytical mechanics. Over the years, several institutes for mathematics were formed.
In 1974, the Institute of Geometry was merged with the Institute of Mathematics to form the Department of Mathematics, and informatics, which had been part of the Institute of Mathematics, became a separate department.

Research Groups
As of 2022, the research groups at the department are:
Algebra
Analysis
Analysis and Modelling
Applied Numerical Analysis, Optimization and Data Analysis
Biostatistics
Discrete Optimization
Dynamic Systems
Geometry and Topology
Mathematical Finance
Mathematical Optimization
Mathematical Physics
Mathematical Modeling of Biological Systems
Numerical Mathematics
Numerical Methods for Plasma Physics
Optimal Control
Probability Theory
Scientific Computing
Statistics

Department of Computer Science
The Department of Computer Science (CS) is located at the Garching campus.

History
The first courses in computer science at the Technical University of Munich were offered in 1967 at the Department of Mathematics, when Friedrich L. Bauer introduced a two-semester lecture titled Information Processing. In 1968, Klaus Samelson started offering a second lecture cycle titled Introduction to Informatics. By 1992, the computer science department had separated from the Department of Mathematics to form an independent Department of Informatics.In 2002, the department relocated from its old campus in the Munich city center to the new building on the Garching campus.In 2017, the Department celebrated 50 Years of Informatics Munich with a series of lectures and ceremonies, together with the Ludwig Maximilian University of Munich and the Bundeswehr University Munich.

Chairs
As of 2022, the department consists of the following chairs:
AI in Healthcare and Medicine
Algorithmic Game Theory
Algorithms and Complexity
Application and Middleware Systems
Augmented Reality
Bioinformatics
Computational Imaging and AI in Medicine
Computational Molecular Medicine
Computer Aided Medical Procedures
Computer Graphics and Visualization
Computer Vision and AI
Cyber Trust
Data Analytics and Machine Learning
Data Science and Engineering
Database Systems
Decision Science & Systems
Dynamic Vision and Learning
Efficient Algorithms
Engineering Software for Decentralized Systems
Ethics in Systems Design and Machine Learning
Formal Languages, Compiler & Software Construction
Formal Methods for Software Reliability
Hardware-aware Algorithms and Software for HPC
Information Systems & Business Process Management
Law and Security of Digitization
Legal Tech
Logic and Verification
Machine Learning of 3D Scene Geometry
Physics-based Simulation
Quantum Computing
Scientific Computing
Software & Systems Engineering
Software Engineering
Software Engineering for Business Information Systems
Theoretical Computer Science
Theoretical Foundations of AI
Visual Computing

Notable people
Seven faculty members of the Department of Informatics have been awarded the Gottfried Wilhelm Leibniz Prize, one of the highest endowed research prizes in Germany with a maximum of €2.5 million per award:

2020 – Thomas Neumann
2016 – Daniel Cremers
2008 – Susanne Albers
1997 – Ernst Mayr
1995 – Gerd Hirzinger
1994 – Manfred Broy
1991 – Karl-Heinz HoffmannFriedrich L."
"Which research group focuses on Applied Numerical Analysis, Optimization, and Data Analysis?",Algebra,Analysis,Analysis and Modelling,"Applied Numerical Analysis, Optimization, and Data Analysis",Biostatistics,D,"The TUM School of Computation, Information and Technology (CIT) is a school of the Technical University of Munich, established in 2022 by the merger of three former departments. As of 2022, it is structured into the Department of Mathematics, the Department of Computer Engineering, the Department of Computer Science, and the Department of Electrical Engineering.

Department of Mathematics
The Department of Mathematics (MATH) is located at the Garching campus.

History
Mathematics was taught from the beginning at the Polytechnische Schule in München and the later Technische Hochschule München. Otto Hesse was the department's first professor for calculus, analytical geometry and analytical mechanics. Over the years, several institutes for mathematics were formed.
In 1974, the Institute of Geometry was merged with the Institute of Mathematics to form the Department of Mathematics, and informatics, which had been part of the Institute of Mathematics, became a separate department.

Research Groups
As of 2022, the research groups at the department are:
Algebra
Analysis
Analysis and Modelling
Applied Numerical Analysis, Optimization and Data Analysis
Biostatistics
Discrete Optimization
Dynamic Systems
Geometry and Topology
Mathematical Finance
Mathematical Optimization
Mathematical Physics
Mathematical Modeling of Biological Systems
Numerical Mathematics
Numerical Methods for Plasma Physics
Optimal Control
Probability Theory
Scientific Computing
Statistics

Department of Computer Science
The Department of Computer Science (CS) is located at the Garching campus.

History
The first courses in computer science at the Technical University of Munich were offered in 1967 at the Department of Mathematics, when Friedrich L. Bauer introduced a two-semester lecture titled Information Processing. In 1968, Klaus Samelson started offering a second lecture cycle titled Introduction to Informatics. By 1992, the computer science department had separated from the Department of Mathematics to form an independent Department of Informatics.In 2002, the department relocated from its old campus in the Munich city center to the new building on the Garching campus.In 2017, the Department celebrated 50 Years of Informatics Munich with a series of lectures and ceremonies, together with the Ludwig Maximilian University of Munich and the Bundeswehr University Munich.

Chairs
As of 2022, the department consists of the following chairs:
AI in Healthcare and Medicine
Algorithmic Game Theory
Algorithms and Complexity
Application and Middleware Systems
Augmented Reality
Bioinformatics
Computational Imaging and AI in Medicine
Computational Molecular Medicine
Computer Aided Medical Procedures
Computer Graphics and Visualization
Computer Vision and AI
Cyber Trust
Data Analytics and Machine Learning
Data Science and Engineering
Database Systems
Decision Science & Systems
Dynamic Vision and Learning
Efficient Algorithms
Engineering Software for Decentralized Systems
Ethics in Systems Design and Machine Learning
Formal Languages, Compiler & Software Construction
Formal Methods for Software Reliability
Hardware-aware Algorithms and Software for HPC
Information Systems & Business Process Management
Law and Security of Digitization
Legal Tech
Logic and Verification
Machine Learning of 3D Scene Geometry
Physics-based Simulation
Quantum Computing
Scientific Computing
Software & Systems Engineering
Software Engineering
Software Engineering for Business Information Systems
Theoretical Computer Science
Theoretical Foundations of AI
Visual Computing

Notable people
Seven faculty members of the Department of Informatics have been awarded the Gottfried Wilhelm Leibniz Prize, one of the highest endowed research prizes in Germany with a maximum of €2.5 million per award:

2020 – Thomas Neumann
2016 – Daniel Cremers
2008 – Susanne Albers
1997 – Ernst Mayr
1995 – Gerd Hirzinger
1994 – Manfred Broy
1991 – Karl-Heinz HoffmannFriedrich L."
What is software?,A set of computer programs and associated documentation and data,The physical components of a computer system,Instructions that change the state of the computer,Binary values that signify processor instructions,Multiple execution units or processors performing computation together,A,"Software is a set of computer programs and associated documentation and data. This is in contrast to hardware, from which the system is built and which actually performs the work.
At the lowest programming level, executable code consists of machine language instructions supported by an individual processor—typically a central processing unit (CPU) or a graphics processing unit (GPU). Machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer—an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example, displaying some text on a computer screen, causing state changes that should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to ""jump"" to a different instruction or is interrupted by the operating system. As of 2023, most personal computers, smartphone devices, and servers have processors with multiple execution units, or multiple processors performing computation together, so computing has become a much more concurrent activity than in the past.
The majority of software is written in high-level programming languages."
What is machine language?,A set of computer programs and associated documentation and data,The physical components of a computer system,Instructions that change the state of the computer,Binary values that signify processor instructions,Multiple execution units or processors performing computation together,D,"Software is a set of computer programs and associated documentation and data. This is in contrast to hardware, from which the system is built and which actually performs the work.
At the lowest programming level, executable code consists of machine language instructions supported by an individual processor—typically a central processing unit (CPU) or a graphics processing unit (GPU). Machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer—an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example, displaying some text on a computer screen, causing state changes that should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to ""jump"" to a different instruction or is interrupted by the operating system. As of 2023, most personal computers, smartphone devices, and servers have processors with multiple execution units, or multiple processors performing computation together, so computing has become a much more concurrent activity than in the past.
The majority of software is written in high-level programming languages."
What is the role of the processor in executing instructions?,To display text on a computer screen,To change the value stored in a storage location,To execute instructions in the order they are provided,To perform input or output operations,To perform computation together with multiple processors,C,"Software is a set of computer programs and associated documentation and data. This is in contrast to hardware, from which the system is built and which actually performs the work.
At the lowest programming level, executable code consists of machine language instructions supported by an individual processor—typically a central processing unit (CPU) or a graphics processing unit (GPU). Machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer—an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example, displaying some text on a computer screen, causing state changes that should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to ""jump"" to a different instruction or is interrupted by the operating system. As of 2023, most personal computers, smartphone devices, and servers have processors with multiple execution units, or multiple processors performing computation together, so computing has become a much more concurrent activity than in the past.
The majority of software is written in high-level programming languages."
What is the majority of software written in?,Machine language,High-level programming languages,Binary values,Processor instructions,Concurrent activity,B,"Software is a set of computer programs and associated documentation and data. This is in contrast to hardware, from which the system is built and which actually performs the work.
At the lowest programming level, executable code consists of machine language instructions supported by an individual processor—typically a central processing unit (CPU) or a graphics processing unit (GPU). Machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer—an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example, displaying some text on a computer screen, causing state changes that should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to ""jump"" to a different instruction or is interrupted by the operating system. As of 2023, most personal computers, smartphone devices, and servers have processors with multiple execution units, or multiple processors performing computation together, so computing has become a much more concurrent activity than in the past.
The majority of software is written in high-level programming languages."
What has computing become in recent years?,A set of computer programs and associated documentation and data,The physical components of a computer system,Instructions that change the state of the computer,Binary values that signify processor instructions,Multiple execution units or processors performing computation together,E,"Software is a set of computer programs and associated documentation and data. This is in contrast to hardware, from which the system is built and which actually performs the work.
At the lowest programming level, executable code consists of machine language instructions supported by an individual processor—typically a central processing unit (CPU) or a graphics processing unit (GPU). Machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer—an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example, displaying some text on a computer screen, causing state changes that should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to ""jump"" to a different instruction or is interrupted by the operating system. As of 2023, most personal computers, smartphone devices, and servers have processors with multiple execution units, or multiple processors performing computation together, so computing has become a much more concurrent activity than in the past.
The majority of software is written in high-level programming languages."
What is XROMM?,A technique to analyze the movement of skeletons in living organisms,A research method to study the flight of birds,A technology to create 3D printers,A technique to study the movement of robotic systems,A tool to measure underwater currents,A,"X-ray Reconstruction of Moving Morphology or XROMM is a scientific research technique. Scientists use it to create 3D images and videos of moving skeletal systems in living organisms.In XROMM, radio-opaque bone markers are implanted inside a living organism, which allows the X-ray video system to calculate accurate bone marker coordinates as the organism moves.
XROMM was invented at Brown University.XROMM can be used to model such movements as birds in flight, humans running, frogs jumping, and a toad swallowing its prey.

Original description
Brainerd, E.L., S.M. Gatesy, D.B. Baier, T.L. Hedrick, K.A. Metzger, J. Crisco, and S.L."
How does XROMM work?,By using implanted bone markers and X-ray video system,By analyzing DNA sequences,By capturing high-speed photographs,By conducting experiments in zero gravity,By using ultrasound technology,A,"X-ray Reconstruction of Moving Morphology or XROMM is a scientific research technique. Scientists use it to create 3D images and videos of moving skeletal systems in living organisms.In XROMM, radio-opaque bone markers are implanted inside a living organism, which allows the X-ray video system to calculate accurate bone marker coordinates as the organism moves.
XROMM was invented at Brown University.XROMM can be used to model such movements as birds in flight, humans running, frogs jumping, and a toad swallowing its prey.

Original description
Brainerd, E.L., S.M. Gatesy, D.B. Baier, T.L. Hedrick, K.A. Metzger, J. Crisco, and S.L."
Where was XROMM invented?,Stanford University,Harvard University,Brown University,MIT,Caltech,C,"X-ray Reconstruction of Moving Morphology or XROMM is a scientific research technique. Scientists use it to create 3D images and videos of moving skeletal systems in living organisms.In XROMM, radio-opaque bone markers are implanted inside a living organism, which allows the X-ray video system to calculate accurate bone marker coordinates as the organism moves.
XROMM was invented at Brown University.XROMM can be used to model such movements as birds in flight, humans running, frogs jumping, and a toad swallowing its prey.

Original description
Brainerd, E.L., S.M. Gatesy, D.B. Baier, T.L. Hedrick, K.A. Metzger, J. Crisco, and S.L."
What movements can XROMM model?,Birds in flight,Humans running,Frogs jumping,A toad swallowing its prey,All of the above,E,"X-ray Reconstruction of Moving Morphology or XROMM is a scientific research technique. Scientists use it to create 3D images and videos of moving skeletal systems in living organisms.In XROMM, radio-opaque bone markers are implanted inside a living organism, which allows the X-ray video system to calculate accurate bone marker coordinates as the organism moves.
XROMM was invented at Brown University.XROMM can be used to model such movements as birds in flight, humans running, frogs jumping, and a toad swallowing its prey.

Original description
Brainerd, E.L., S.M. Gatesy, D.B. Baier, T.L. Hedrick, K.A. Metzger, J. Crisco, and S.L."
What is the main application of XROMM?,Understanding skeletal development in embryos,Designing better sports equipment,Studying the behavior of marine animals,Modeling the movement of robots,Creating realistic animations in movies,B,"X-ray Reconstruction of Moving Morphology or XROMM is a scientific research technique. Scientists use it to create 3D images and videos of moving skeletal systems in living organisms.In XROMM, radio-opaque bone markers are implanted inside a living organism, which allows the X-ray video system to calculate accurate bone marker coordinates as the organism moves.
XROMM was invented at Brown University.XROMM can be used to model such movements as birds in flight, humans running, frogs jumping, and a toad swallowing its prey.

Original description
Brainerd, E.L., S.M. Gatesy, D.B. Baier, T.L. Hedrick, K.A. Metzger, J. Crisco, and S.L."
When was the Betongtavlen award first issued?,1971,1961,1951,1981,1991,B,"Betongtavlen (""The Concrete Tablet"") is a Norwegian architecture and civil engineering award issued by the National Associations of Norwegian Architects and the Norwegian Concrete Association. The award is issues to a structure ""where concrete is used in an environmentally, esthetically and technically excellent way"". The award was first issued in 1961 for Bakkehaugen Church and has as of 2011 been awarded 53 times. The award is not necessarily awarded every year, and up to four structures have been awarded in a year. Structures awarded prices include office buildings, campus buildings, ski jumps, houses, hotels, bridges, tunnels, dams, oil platforms, industrial facilities, viewpoints and cultural institutions. Prizes are not necessarily awarded immediately after the structure was completed—for instance, Elgeseter Bridge was completed in 1951 but awarded the prize in 2006.

List of awards
The following is a list of the awards, including the year it was awarded, the structure, the credited architects and engineering firms or people, the type of structure and the municipality in which it is located.

See also
List of engineering awards


== References ==."
How many times has the Betongtavlen award been awarded...in a year?,1,2,3,4,5,D,"Betongtavlen (""The Concrete Tablet"") is a Norwegian architecture and civil engineering award issued by the National Associations of Norwegian Architects and the Norwegian Concrete Association. The award is issues to a structure ""where concrete is used in an environmentally, esthetically and technically excellent way"". The award was first issued in 1961 for Bakkehaugen Church and has as of 2011 been awarded 53 times. The award is not necessarily awarded every year, and up to four structures have been awarded in a year. Structures awarded prices include office buildings, campus buildings, ski jumps, houses, hotels, bridges, tunnels, dams, oil platforms, industrial facilities, viewpoints and cultural institutions. Prizes are not necessarily awarded immediately after the structure was completed—for instance, Elgeseter Bridge was completed in 1951 but awarded the prize in 2006.

List of awards
The following is a list of the awards, including the year it was awarded, the structure, the credited architects and engineering firms or people, the type of structure and the municipality in which it is located.

See also
List of engineering awards


== References ==."
What types of structures have been awarded the Betongtavlen prize?,Hospitals,Schools,Stadiums,Museums,Bridges,E,"Betongtavlen (""The Concrete Tablet"") is a Norwegian architecture and civil engineering award issued by the National Associations of Norwegian Architects and the Norwegian Concrete Association. The award is issues to a structure ""where concrete is used in an environmentally, esthetically and technically excellent way"". The award was first issued in 1961 for Bakkehaugen Church and has as of 2011 been awarded 53 times. The award is not necessarily awarded every year, and up to four structures have been awarded in a year. Structures awarded prices include office buildings, campus buildings, ski jumps, houses, hotels, bridges, tunnels, dams, oil platforms, industrial facilities, viewpoints and cultural institutions. Prizes are not necessarily awarded immediately after the structure was completed—for instance, Elgeseter Bridge was completed in 1951 but awarded the prize in 2006.

List of awards
The following is a list of the awards, including the year it was awarded, the structure, the credited architects and engineering firms or people, the type of structure and the municipality in which it is located.

See also
List of engineering awards


== References ==."
Which structure was awarded the prize many years after it was completed?,Bakkehaugen Church,Elgeseter Bridge,Ski jumps,Oil platforms,Industrial facilities,B,"Betongtavlen (""The Concrete Tablet"") is a Norwegian architecture and civil engineering award issued by the National Associations of Norwegian Architects and the Norwegian Concrete Association. The award is issues to a structure ""where concrete is used in an environmentally, esthetically and technically excellent way"". The award was first issued in 1961 for Bakkehaugen Church and has as of 2011 been awarded 53 times. The award is not necessarily awarded every year, and up to four structures have been awarded in a year. Structures awarded prices include office buildings, campus buildings, ski jumps, houses, hotels, bridges, tunnels, dams, oil platforms, industrial facilities, viewpoints and cultural institutions. Prizes are not necessarily awarded immediately after the structure was completed—for instance, Elgeseter Bridge was completed in 1951 but awarded the prize in 2006.

List of awards
The following is a list of the awards, including the year it was awarded, the structure, the credited architects and engineering firms or people, the type of structure and the municipality in which it is located.

See also
List of engineering awards


== References ==."
What is the Betongtavlen award issued for?,"Structures that use concrete in an environmentally, esthetically and technically excellent way",Structures that are made entirely of concrete,Structures that are located in municipalities,Structures that have won other architecture awards,Structures that have been completed recently,A,"Betongtavlen (""The Concrete Tablet"") is a Norwegian architecture and civil engineering award issued by the National Associations of Norwegian Architects and the Norwegian Concrete Association. The award is issues to a structure ""where concrete is used in an environmentally, esthetically and technically excellent way"". The award was first issued in 1961 for Bakkehaugen Church and has as of 2011 been awarded 53 times. The award is not necessarily awarded every year, and up to four structures have been awarded in a year. Structures awarded prices include office buildings, campus buildings, ski jumps, houses, hotels, bridges, tunnels, dams, oil platforms, industrial facilities, viewpoints and cultural institutions. Prizes are not necessarily awarded immediately after the structure was completed—for instance, Elgeseter Bridge was completed in 1951 but awarded the prize in 2006.

List of awards
The following is a list of the awards, including the year it was awarded, the structure, the credited architects and engineering firms or people, the type of structure and the municipality in which it is located.

See also
List of engineering awards


== References ==."
What is ocean fertilization?,A process of removing carbon dioxide from the atmosphere by adding plant nutrients to the ocean,A type of technology for removing pollutants from the ocean by introducing plant nutrients,A method of increasing marine food production by adding carbon dioxide to the ocean,A process of increasing ocean acidity by introducing plant nutrients to the deep ocean,A technique for reducing ocean temperature by adding phytoplankton to the surface,A,"Ocean fertilization or ocean nourishment is a type of technology for carbon dioxide removal from the ocean based on the purposeful introduction of plant nutrients to the upper ocean to increase marine food production and to remove carbon dioxide from the atmosphere. Ocean nutrient fertilization, for example iron fertilization, could stimulate photosynthesis in phytoplankton. The phytoplankton would convert the ocean's dissolved carbon dioxide into carbohydrate, some of which would sink into the deeper ocean before oxidizing. More than a dozen open-sea experiments confirmed that adding iron to the ocean increases photosynthesis in phytoplankton by up to 30 times.This is one of the more well-researched carbon dioxide removal (CDR) approaches, however this approach would only sequester carbon on a timescale of 10-100 years dependent on ocean mixing times. While surface ocean acidity may decrease as a result of nutrient fertilization, when the sinking organic matter remineralizes, deep ocean acidity will increase. A 2021 report on CDR indicates that there is medium-high confidence that the technique could be efficient and scalable at low cost, with medium environmental risks. One of the key risks of nutrient fertilization is nutrient robbing, a process by which excess nutrients used in one location for enhanced primary productivity, as in a fertilization context, are then unavailable for normal productivity downstream."
What is the main purpose of ocean fertilization?,To reduce surface ocean acidity,To increase marine food production,To remove pollutants from the atmosphere,To decrease ocean mixing times,To increase deep ocean acidity,B,"Ocean fertilization or ocean nourishment is a type of technology for carbon dioxide removal from the ocean based on the purposeful introduction of plant nutrients to the upper ocean to increase marine food production and to remove carbon dioxide from the atmosphere. Ocean nutrient fertilization, for example iron fertilization, could stimulate photosynthesis in phytoplankton. The phytoplankton would convert the ocean's dissolved carbon dioxide into carbohydrate, some of which would sink into the deeper ocean before oxidizing. More than a dozen open-sea experiments confirmed that adding iron to the ocean increases photosynthesis in phytoplankton by up to 30 times.This is one of the more well-researched carbon dioxide removal (CDR) approaches, however this approach would only sequester carbon on a timescale of 10-100 years dependent on ocean mixing times. While surface ocean acidity may decrease as a result of nutrient fertilization, when the sinking organic matter remineralizes, deep ocean acidity will increase. A 2021 report on CDR indicates that there is medium-high confidence that the technique could be efficient and scalable at low cost, with medium environmental risks. One of the key risks of nutrient fertilization is nutrient robbing, a process by which excess nutrients used in one location for enhanced primary productivity, as in a fertilization context, are then unavailable for normal productivity downstream."
Which nutrient is commonly used in ocean fertilization experiments?,Iron,Phosphorus,Nitrogen,Calcium,Potassium,A,"Ocean fertilization or ocean nourishment is a type of technology for carbon dioxide removal from the ocean based on the purposeful introduction of plant nutrients to the upper ocean to increase marine food production and to remove carbon dioxide from the atmosphere. Ocean nutrient fertilization, for example iron fertilization, could stimulate photosynthesis in phytoplankton. The phytoplankton would convert the ocean's dissolved carbon dioxide into carbohydrate, some of which would sink into the deeper ocean before oxidizing. More than a dozen open-sea experiments confirmed that adding iron to the ocean increases photosynthesis in phytoplankton by up to 30 times.This is one of the more well-researched carbon dioxide removal (CDR) approaches, however this approach would only sequester carbon on a timescale of 10-100 years dependent on ocean mixing times. While surface ocean acidity may decrease as a result of nutrient fertilization, when the sinking organic matter remineralizes, deep ocean acidity will increase. A 2021 report on CDR indicates that there is medium-high confidence that the technique could be efficient and scalable at low cost, with medium environmental risks. One of the key risks of nutrient fertilization is nutrient robbing, a process by which excess nutrients used in one location for enhanced primary productivity, as in a fertilization context, are then unavailable for normal productivity downstream."
How much can iron fertilization increase photosynthesis in phytoplankton?,2 times,10 times,30 times,50 times,100 times,C,"Ocean fertilization or ocean nourishment is a type of technology for carbon dioxide removal from the ocean based on the purposeful introduction of plant nutrients to the upper ocean to increase marine food production and to remove carbon dioxide from the atmosphere. Ocean nutrient fertilization, for example iron fertilization, could stimulate photosynthesis in phytoplankton. The phytoplankton would convert the ocean's dissolved carbon dioxide into carbohydrate, some of which would sink into the deeper ocean before oxidizing. More than a dozen open-sea experiments confirmed that adding iron to the ocean increases photosynthesis in phytoplankton by up to 30 times.This is one of the more well-researched carbon dioxide removal (CDR) approaches, however this approach would only sequester carbon on a timescale of 10-100 years dependent on ocean mixing times. While surface ocean acidity may decrease as a result of nutrient fertilization, when the sinking organic matter remineralizes, deep ocean acidity will increase. A 2021 report on CDR indicates that there is medium-high confidence that the technique could be efficient and scalable at low cost, with medium environmental risks. One of the key risks of nutrient fertilization is nutrient robbing, a process by which excess nutrients used in one location for enhanced primary productivity, as in a fertilization context, are then unavailable for normal productivity downstream."
What is one of the risks associated with nutrient fertilization?,Decreased deep ocean acidity,Increased nutrient availability downstream,Enhanced primary productivity,Reduced carbon sequestration,Lower marine food production,B,"Ocean fertilization or ocean nourishment is a type of technology for carbon dioxide removal from the ocean based on the purposeful introduction of plant nutrients to the upper ocean to increase marine food production and to remove carbon dioxide from the atmosphere. Ocean nutrient fertilization, for example iron fertilization, could stimulate photosynthesis in phytoplankton. The phytoplankton would convert the ocean's dissolved carbon dioxide into carbohydrate, some of which would sink into the deeper ocean before oxidizing. More than a dozen open-sea experiments confirmed that adding iron to the ocean increases photosynthesis in phytoplankton by up to 30 times.This is one of the more well-researched carbon dioxide removal (CDR) approaches, however this approach would only sequester carbon on a timescale of 10-100 years dependent on ocean mixing times. While surface ocean acidity may decrease as a result of nutrient fertilization, when the sinking organic matter remineralizes, deep ocean acidity will increase. A 2021 report on CDR indicates that there is medium-high confidence that the technique could be efficient and scalable at low cost, with medium environmental risks. One of the key risks of nutrient fertilization is nutrient robbing, a process by which excess nutrients used in one location for enhanced primary productivity, as in a fertilization context, are then unavailable for normal productivity downstream."
What is civil engineering?,A profession that deals with the construction of buildings,A profession that deals with the design of physical and naturally built environment,A profession that deals with the maintenance of public works,A profession that deals with the design and construction of military infrastructure,A profession that deals with the design and construction of computer software,B,"Civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including public works such as roads, bridges, canals, dams, airports, sewage systems, pipelines, structural components of buildings, and railways.Civil engineering is traditionally broken into a number of sub-disciplines. It is considered the second-oldest engineering discipline after military engineering, and it is defined to distinguish non-military engineering from military engineering. Civil engineering can take place in the public sector from municipal public works departments through to federal government agencies, and in the private sector from locally based firms to global Fortune 500 companies.

History
Civil engineering as a discipline
Civil engineering is the application of physical and scientific principles for solving the problems of society, and its history is intricately linked to advances in the understanding of physics and mathematics throughout history. Because civil engineering is a broad profession, including several specialized sub-disciplines, its history is linked to knowledge of structures, materials science, geography, geology, soils, hydrology, environmental science, mechanics, project management, and other fields.Throughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. Knowledge was retained in guilds and seldom supplanted by advances. Structures, roads, and infrastructure that existed were repetitive, and increases in scale were incremental.One of the earliest examples of a scientific approach to physical and mathematical problems applicable to civil engineering is the work of Archimedes in the 3rd century BC, including Archimedes' principle, which underpins our understanding of buoyancy, and practical solutions such as Archimedes' screw. Brahmagupta, an Indian mathematician, used arithmetic in the 7th century AD, based on Hindu-Arabic numerals, for excavation (volume) computations.

Civil engineering profession
Engineering has been an aspect of life since the beginnings of human existence."
What is the second-oldest engineering discipline?,Mechanical engineering,Electrical engineering,Civil engineering,Chemical engineering,Biomedical engineering,C,"Civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including public works such as roads, bridges, canals, dams, airports, sewage systems, pipelines, structural components of buildings, and railways.Civil engineering is traditionally broken into a number of sub-disciplines. It is considered the second-oldest engineering discipline after military engineering, and it is defined to distinguish non-military engineering from military engineering. Civil engineering can take place in the public sector from municipal public works departments through to federal government agencies, and in the private sector from locally based firms to global Fortune 500 companies.

History
Civil engineering as a discipline
Civil engineering is the application of physical and scientific principles for solving the problems of society, and its history is intricately linked to advances in the understanding of physics and mathematics throughout history. Because civil engineering is a broad profession, including several specialized sub-disciplines, its history is linked to knowledge of structures, materials science, geography, geology, soils, hydrology, environmental science, mechanics, project management, and other fields.Throughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. Knowledge was retained in guilds and seldom supplanted by advances. Structures, roads, and infrastructure that existed were repetitive, and increases in scale were incremental.One of the earliest examples of a scientific approach to physical and mathematical problems applicable to civil engineering is the work of Archimedes in the 3rd century BC, including Archimedes' principle, which underpins our understanding of buoyancy, and practical solutions such as Archimedes' screw. Brahmagupta, an Indian mathematician, used arithmetic in the 7th century AD, based on Hindu-Arabic numerals, for excavation (volume) computations.

Civil engineering profession
Engineering has been an aspect of life since the beginnings of human existence."
Where can civil engineering take place?,Only in the public sector,Only in the private sector,Both in the public and private sector,Only in the federal government agencies,Only in global Fortune 500 companies,C,"Civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including public works such as roads, bridges, canals, dams, airports, sewage systems, pipelines, structural components of buildings, and railways.Civil engineering is traditionally broken into a number of sub-disciplines. It is considered the second-oldest engineering discipline after military engineering, and it is defined to distinguish non-military engineering from military engineering. Civil engineering can take place in the public sector from municipal public works departments through to federal government agencies, and in the private sector from locally based firms to global Fortune 500 companies.

History
Civil engineering as a discipline
Civil engineering is the application of physical and scientific principles for solving the problems of society, and its history is intricately linked to advances in the understanding of physics and mathematics throughout history. Because civil engineering is a broad profession, including several specialized sub-disciplines, its history is linked to knowledge of structures, materials science, geography, geology, soils, hydrology, environmental science, mechanics, project management, and other fields.Throughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. Knowledge was retained in guilds and seldom supplanted by advances. Structures, roads, and infrastructure that existed were repetitive, and increases in scale were incremental.One of the earliest examples of a scientific approach to physical and mathematical problems applicable to civil engineering is the work of Archimedes in the 3rd century BC, including Archimedes' principle, which underpins our understanding of buoyancy, and practical solutions such as Archimedes' screw. Brahmagupta, an Indian mathematician, used arithmetic in the 7th century AD, based on Hindu-Arabic numerals, for excavation (volume) computations.

Civil engineering profession
Engineering has been an aspect of life since the beginnings of human existence."
What fields is civil engineering linked to?,Physics and mathematics,Chemistry and biology,Geography and geology,Computer science and programming,Psychology and sociology,A,"Civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including public works such as roads, bridges, canals, dams, airports, sewage systems, pipelines, structural components of buildings, and railways.Civil engineering is traditionally broken into a number of sub-disciplines. It is considered the second-oldest engineering discipline after military engineering, and it is defined to distinguish non-military engineering from military engineering. Civil engineering can take place in the public sector from municipal public works departments through to federal government agencies, and in the private sector from locally based firms to global Fortune 500 companies.

History
Civil engineering as a discipline
Civil engineering is the application of physical and scientific principles for solving the problems of society, and its history is intricately linked to advances in the understanding of physics and mathematics throughout history. Because civil engineering is a broad profession, including several specialized sub-disciplines, its history is linked to knowledge of structures, materials science, geography, geology, soils, hydrology, environmental science, mechanics, project management, and other fields.Throughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. Knowledge was retained in guilds and seldom supplanted by advances. Structures, roads, and infrastructure that existed were repetitive, and increases in scale were incremental.One of the earliest examples of a scientific approach to physical and mathematical problems applicable to civil engineering is the work of Archimedes in the 3rd century BC, including Archimedes' principle, which underpins our understanding of buoyancy, and practical solutions such as Archimedes' screw. Brahmagupta, an Indian mathematician, used arithmetic in the 7th century AD, based on Hindu-Arabic numerals, for excavation (volume) computations.

Civil engineering profession
Engineering has been an aspect of life since the beginnings of human existence."
When did engineering begin?,In the 21st century,In the 19th century,In the 17th century,Since the beginning of human existence,In the 20th century,D,"Civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including public works such as roads, bridges, canals, dams, airports, sewage systems, pipelines, structural components of buildings, and railways.Civil engineering is traditionally broken into a number of sub-disciplines. It is considered the second-oldest engineering discipline after military engineering, and it is defined to distinguish non-military engineering from military engineering. Civil engineering can take place in the public sector from municipal public works departments through to federal government agencies, and in the private sector from locally based firms to global Fortune 500 companies.

History
Civil engineering as a discipline
Civil engineering is the application of physical and scientific principles for solving the problems of society, and its history is intricately linked to advances in the understanding of physics and mathematics throughout history. Because civil engineering is a broad profession, including several specialized sub-disciplines, its history is linked to knowledge of structures, materials science, geography, geology, soils, hydrology, environmental science, mechanics, project management, and other fields.Throughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. Knowledge was retained in guilds and seldom supplanted by advances. Structures, roads, and infrastructure that existed were repetitive, and increases in scale were incremental.One of the earliest examples of a scientific approach to physical and mathematical problems applicable to civil engineering is the work of Archimedes in the 3rd century BC, including Archimedes' principle, which underpins our understanding of buoyancy, and practical solutions such as Archimedes' screw. Brahmagupta, an Indian mathematician, used arithmetic in the 7th century AD, based on Hindu-Arabic numerals, for excavation (volume) computations.

Civil engineering profession
Engineering has been an aspect of life since the beginnings of human existence."
What is the definition of runway incursion?,"Any occurrence at an aerodrome involving the incorrect presence of an aircraft, vehicle or person on the protected area of a surface designated for the landing and take off of aircraft.","An incident involving only a single aircraft, where it makes an inappropriate exit from the runway.","The most frequent type of landing accident, slightly ahead of runway incursion.",A type of excursion where the aircraft is unable to stop before the end of the runway.,A runway's current status due to meteorological conditions and air safety.,A,"Runway safety is concerned with reducing harm that could occur on an aircraft runway. Safety means avoiding incorrect presence (incursion) of aircraft, inappropriate exits (excursion) and use of the wrong runway due to confusion. The runway condition is a runway's current status due to meteorological conditions and air safety.

Definitions of runway accidents
Several terms fall under the flight safety topic of runway safety, including incursion, excursion, and confusion.

Runway incursion
Runway incursion involves an aircraft, and a second aircraft, vehicle, or person. It is defined by ICAO and the U.S. FAA as ""Any occurrence at an aerodrome involving the incorrect presence of an aircraft, vehicle or person on the protected area of a surface designated for the landing and take off of aircraft.""

Runway excursion
Runway excursion is an incident involving only a single aircraft, where it makes an inappropriate exit from the runway.  This can happen because of pilot error, poor weather, or a fault with the aircraft. A runway overrun is a type of excursion where the aircraft is unable to stop before the end of the runway.
Runway excursion is the most frequent type of landing accident, slightly ahead of runway incursion."
What is the definition of runway excursion?,"Any occurrence at an aerodrome involving the incorrect presence of an aircraft, vehicle or person on the protected area of a surface designated for the landing and take off of aircraft.","An incident involving only a single aircraft, where it makes an inappropriate exit from the runway.","The most frequent type of landing accident, slightly ahead of runway incursion.",A type of excursion where the aircraft is unable to stop before the end of the runway.,A runway's current status due to meteorological conditions and air safety.,B,"Runway safety is concerned with reducing harm that could occur on an aircraft runway. Safety means avoiding incorrect presence (incursion) of aircraft, inappropriate exits (excursion) and use of the wrong runway due to confusion. The runway condition is a runway's current status due to meteorological conditions and air safety.

Definitions of runway accidents
Several terms fall under the flight safety topic of runway safety, including incursion, excursion, and confusion.

Runway incursion
Runway incursion involves an aircraft, and a second aircraft, vehicle, or person. It is defined by ICAO and the U.S. FAA as ""Any occurrence at an aerodrome involving the incorrect presence of an aircraft, vehicle or person on the protected area of a surface designated for the landing and take off of aircraft.""

Runway excursion
Runway excursion is an incident involving only a single aircraft, where it makes an inappropriate exit from the runway.  This can happen because of pilot error, poor weather, or a fault with the aircraft. A runway overrun is a type of excursion where the aircraft is unable to stop before the end of the runway.
Runway excursion is the most frequent type of landing accident, slightly ahead of runway incursion."
What is a runway overrun?,"Any occurrence at an aerodrome involving the incorrect presence of an aircraft, vehicle or person on the protected area of a surface designated for the landing and take off of aircraft.","An incident involving only a single aircraft, where it makes an inappropriate exit from the runway.","The most frequent type of landing accident, slightly ahead of runway incursion.",A type of excursion where the aircraft is unable to stop before the end of the runway.,A runway's current status due to meteorological conditions and air safety.,D,"Runway safety is concerned with reducing harm that could occur on an aircraft runway. Safety means avoiding incorrect presence (incursion) of aircraft, inappropriate exits (excursion) and use of the wrong runway due to confusion. The runway condition is a runway's current status due to meteorological conditions and air safety.

Definitions of runway accidents
Several terms fall under the flight safety topic of runway safety, including incursion, excursion, and confusion.

Runway incursion
Runway incursion involves an aircraft, and a second aircraft, vehicle, or person. It is defined by ICAO and the U.S. FAA as ""Any occurrence at an aerodrome involving the incorrect presence of an aircraft, vehicle or person on the protected area of a surface designated for the landing and take off of aircraft.""

Runway excursion
Runway excursion is an incident involving only a single aircraft, where it makes an inappropriate exit from the runway.  This can happen because of pilot error, poor weather, or a fault with the aircraft. A runway overrun is a type of excursion where the aircraft is unable to stop before the end of the runway.
Runway excursion is the most frequent type of landing accident, slightly ahead of runway incursion."
What is the most frequent type of landing accident?,"Any occurrence at an aerodrome involving the incorrect presence of an aircraft, vehicle or person on the protected area of a surface designated for the landing and take off of aircraft.","An incident involving only a single aircraft, where it makes an inappropriate exit from the runway.","The most frequent type of landing accident, slightly ahead of runway incursion.",A type of excursion where the aircraft is unable to stop before the end of the runway.,A runway's current status due to meteorological conditions and air safety.,C,"Runway safety is concerned with reducing harm that could occur on an aircraft runway. Safety means avoiding incorrect presence (incursion) of aircraft, inappropriate exits (excursion) and use of the wrong runway due to confusion. The runway condition is a runway's current status due to meteorological conditions and air safety.

Definitions of runway accidents
Several terms fall under the flight safety topic of runway safety, including incursion, excursion, and confusion.

Runway incursion
Runway incursion involves an aircraft, and a second aircraft, vehicle, or person. It is defined by ICAO and the U.S. FAA as ""Any occurrence at an aerodrome involving the incorrect presence of an aircraft, vehicle or person on the protected area of a surface designated for the landing and take off of aircraft.""

Runway excursion
Runway excursion is an incident involving only a single aircraft, where it makes an inappropriate exit from the runway.  This can happen because of pilot error, poor weather, or a fault with the aircraft. A runway overrun is a type of excursion where the aircraft is unable to stop before the end of the runway.
Runway excursion is the most frequent type of landing accident, slightly ahead of runway incursion."
What does runway condition refer to?,"Any occurrence at an aerodrome involving the incorrect presence of an aircraft, vehicle or person on the protected area of a surface designated for the landing and take off of aircraft.","An incident involving only a single aircraft, where it makes an inappropriate exit from the runway.","The most frequent type of landing accident, slightly ahead of runway incursion.",A type of excursion where the aircraft is unable to stop before the end of the runway.,A runway's current status due to meteorological conditions and air safety.,E,"Runway safety is concerned with reducing harm that could occur on an aircraft runway. Safety means avoiding incorrect presence (incursion) of aircraft, inappropriate exits (excursion) and use of the wrong runway due to confusion. The runway condition is a runway's current status due to meteorological conditions and air safety.

Definitions of runway accidents
Several terms fall under the flight safety topic of runway safety, including incursion, excursion, and confusion.

Runway incursion
Runway incursion involves an aircraft, and a second aircraft, vehicle, or person. It is defined by ICAO and the U.S. FAA as ""Any occurrence at an aerodrome involving the incorrect presence of an aircraft, vehicle or person on the protected area of a surface designated for the landing and take off of aircraft.""

Runway excursion
Runway excursion is an incident involving only a single aircraft, where it makes an inappropriate exit from the runway.  This can happen because of pilot error, poor weather, or a fault with the aircraft. A runway overrun is a type of excursion where the aircraft is unable to stop before the end of the runway.
Runway excursion is the most frequent type of landing accident, slightly ahead of runway incursion."
What is an electric machine?,A device that converts mechanical energy to electrical energy,A device that converts electrical energy to mechanical power,A device that produces electric power on Earth,A device that changes the voltage level of an alternating current,A device that consumes approximately 60% of all electric power produced,A,"In electrical engineering, electric machine is a general term for machines using electromagnetic forces, such as electric motors, electric generators,  and others. They are electromechanical energy converters: an electric motor converts electricity to mechanical power while an electric generator converts mechanical power to electricity. The moving parts in a machine can be rotating (rotating machines) or linear (linear machines).  Besides motors and generators, a third category often included is transformers, which although they do not have any moving parts are also energy converters, changing the voltage level of an alternating current.Electric machines, in the form of generators, produce virtually all electric power on Earth, and in the form of electric motors consume approximately 60% of all electric power produced. Electric machines were developed beginning in the mid 19th century and since that time have been a ubiquitous component of the  infrastructure. Developing more efficient electric machine technology is crucial to any global conservation, green energy, or alternative energy strategy.

Generator
An electric generator is a device that converts mechanical energy to electrical energy. A generator forces electrons to flow through an external electrical circuit."
What are the two main types of electric machines based on the movement of their parts?,Rotating machines and linear machines,Mechanical machines and electrical machines,Transformer machines and generator machines,Motor machines and converter machines,Electric machines and mechanical machines,A,"In electrical engineering, electric machine is a general term for machines using electromagnetic forces, such as electric motors, electric generators,  and others. They are electromechanical energy converters: an electric motor converts electricity to mechanical power while an electric generator converts mechanical power to electricity. The moving parts in a machine can be rotating (rotating machines) or linear (linear machines).  Besides motors and generators, a third category often included is transformers, which although they do not have any moving parts are also energy converters, changing the voltage level of an alternating current.Electric machines, in the form of generators, produce virtually all electric power on Earth, and in the form of electric motors consume approximately 60% of all electric power produced. Electric machines were developed beginning in the mid 19th century and since that time have been a ubiquitous component of the  infrastructure. Developing more efficient electric machine technology is crucial to any global conservation, green energy, or alternative energy strategy.

Generator
An electric generator is a device that converts mechanical energy to electrical energy. A generator forces electrons to flow through an external electrical circuit."
What is the role of transformers in the category of electric machines?,They convert mechanical power to electricity,They convert electricity to mechanical power,They convert the voltage level of an alternating current,They produce electric power on Earth,They consume approximately 60% of all electric power produced,C,"In electrical engineering, electric machine is a general term for machines using electromagnetic forces, such as electric motors, electric generators,  and others. They are electromechanical energy converters: an electric motor converts electricity to mechanical power while an electric generator converts mechanical power to electricity. The moving parts in a machine can be rotating (rotating machines) or linear (linear machines).  Besides motors and generators, a third category often included is transformers, which although they do not have any moving parts are also energy converters, changing the voltage level of an alternating current.Electric machines, in the form of generators, produce virtually all electric power on Earth, and in the form of electric motors consume approximately 60% of all electric power produced. Electric machines were developed beginning in the mid 19th century and since that time have been a ubiquitous component of the  infrastructure. Developing more efficient electric machine technology is crucial to any global conservation, green energy, or alternative energy strategy.

Generator
An electric generator is a device that converts mechanical energy to electrical energy. A generator forces electrons to flow through an external electrical circuit."
What is the importance of efficient electric machine technology?,It helps in global conservation efforts,It promotes green energy strategies,It supports alternative energy strategies,All of the above,None of the above,D,"In electrical engineering, electric machine is a general term for machines using electromagnetic forces, such as electric motors, electric generators,  and others. They are electromechanical energy converters: an electric motor converts electricity to mechanical power while an electric generator converts mechanical power to electricity. The moving parts in a machine can be rotating (rotating machines) or linear (linear machines).  Besides motors and generators, a third category often included is transformers, which although they do not have any moving parts are also energy converters, changing the voltage level of an alternating current.Electric machines, in the form of generators, produce virtually all electric power on Earth, and in the form of electric motors consume approximately 60% of all electric power produced. Electric machines were developed beginning in the mid 19th century and since that time have been a ubiquitous component of the  infrastructure. Developing more efficient electric machine technology is crucial to any global conservation, green energy, or alternative energy strategy.

Generator
An electric generator is a device that converts mechanical energy to electrical energy. A generator forces electrons to flow through an external electrical circuit."
When were electric machines developed?,In the mid 19th century,In the late 19th century,In the early 20th century,In the late 20th century,In the 21st century,A,"In electrical engineering, electric machine is a general term for machines using electromagnetic forces, such as electric motors, electric generators,  and others. They are electromechanical energy converters: an electric motor converts electricity to mechanical power while an electric generator converts mechanical power to electricity. The moving parts in a machine can be rotating (rotating machines) or linear (linear machines).  Besides motors and generators, a third category often included is transformers, which although they do not have any moving parts are also energy converters, changing the voltage level of an alternating current.Electric machines, in the form of generators, produce virtually all electric power on Earth, and in the form of electric motors consume approximately 60% of all electric power produced. Electric machines were developed beginning in the mid 19th century and since that time have been a ubiquitous component of the  infrastructure. Developing more efficient electric machine technology is crucial to any global conservation, green energy, or alternative energy strategy.

Generator
An electric generator is a device that converts mechanical energy to electrical energy. A generator forces electrons to flow through an external electrical circuit."
What is specific impulse?,A measure of how efficiently a reaction mass engine creates thrust,A measurement of the amount of propellant needed for a given delta-v,A measure of the actual exhaust velocity of a rocket engine,A measurement of the mass of external air accelerated by a jet engine,A measurement of the specific fuel consumption of a propulsion system,A,"Specific impulse (usually abbreviated Isp) is a measure of how efficiently a reaction mass engine (a rocket using propellant or a jet engine using fuel) creates thrust. For engines whose reaction mass is only the fuel they carry, specific impulse is exactly proportional to the effective exhaust gas velocity.
A propulsion system with a higher specific impulse uses the mass of the propellant more efficiently. In the case of a rocket, this means less propellant needed for a given delta-v, so that the vehicle attached to the engine can more efficiently gain altitude and velocity.
In an atmospheric context, specific impulse can include the contribution to impulse provided by the mass of external air that is accelerated by the engine in some way, such as by an internal turbofan or heating by fuel combustion participation then thrust expansion or by external propeller. Jet engines breathe external air for both combustion and bypass, and therefore have a much higher specific impulse than rocket engines. The specific impulse in terms of propellant mass spent has units of distance per time, which is a notional velocity called the effective exhaust velocity. This is higher than the actual exhaust velocity because the mass of the combustion air is not being accounted for. Actual and effective exhaust velocity are the same in rocket engines operating in a vacuum.
Specific impulse is inversely proportional to specific fuel consumption (SFC) by the relationship Isp = 1/(go·SFC) for SFC in kg/(N·s) and Isp = 3600/SFC for SFC in lb/(lbf·hr).

General considerations
The amount of propellant can be measured either in units of mass or weight."
How does a propulsion system with a higher specific impulse use propellant more efficiently?,By requiring less propellant for a given delta-v,By increasing the actual exhaust velocity of the engine,By decreasing the specific fuel consumption of the system,By accelerating external air in addition to the reaction mass,By using a combination of rocket and jet engines,A,"Specific impulse (usually abbreviated Isp) is a measure of how efficiently a reaction mass engine (a rocket using propellant or a jet engine using fuel) creates thrust. For engines whose reaction mass is only the fuel they carry, specific impulse is exactly proportional to the effective exhaust gas velocity.
A propulsion system with a higher specific impulse uses the mass of the propellant more efficiently. In the case of a rocket, this means less propellant needed for a given delta-v, so that the vehicle attached to the engine can more efficiently gain altitude and velocity.
In an atmospheric context, specific impulse can include the contribution to impulse provided by the mass of external air that is accelerated by the engine in some way, such as by an internal turbofan or heating by fuel combustion participation then thrust expansion or by external propeller. Jet engines breathe external air for both combustion and bypass, and therefore have a much higher specific impulse than rocket engines. The specific impulse in terms of propellant mass spent has units of distance per time, which is a notional velocity called the effective exhaust velocity. This is higher than the actual exhaust velocity because the mass of the combustion air is not being accounted for. Actual and effective exhaust velocity are the same in rocket engines operating in a vacuum.
Specific impulse is inversely proportional to specific fuel consumption (SFC) by the relationship Isp = 1/(go·SFC) for SFC in kg/(N·s) and Isp = 3600/SFC for SFC in lb/(lbf·hr).

General considerations
The amount of propellant can be measured either in units of mass or weight."
"In an atmospheric context, what can be included in the specific impulse?",The contribution of combustion air from a turbofan engine,The thrust expansion of a rocket engine,The mass of external air accelerated by a jet engine,The heating of external air by fuel combustion,The bypass of external air in a jet engine,C,"Specific impulse (usually abbreviated Isp) is a measure of how efficiently a reaction mass engine (a rocket using propellant or a jet engine using fuel) creates thrust. For engines whose reaction mass is only the fuel they carry, specific impulse is exactly proportional to the effective exhaust gas velocity.
A propulsion system with a higher specific impulse uses the mass of the propellant more efficiently. In the case of a rocket, this means less propellant needed for a given delta-v, so that the vehicle attached to the engine can more efficiently gain altitude and velocity.
In an atmospheric context, specific impulse can include the contribution to impulse provided by the mass of external air that is accelerated by the engine in some way, such as by an internal turbofan or heating by fuel combustion participation then thrust expansion or by external propeller. Jet engines breathe external air for both combustion and bypass, and therefore have a much higher specific impulse than rocket engines. The specific impulse in terms of propellant mass spent has units of distance per time, which is a notional velocity called the effective exhaust velocity. This is higher than the actual exhaust velocity because the mass of the combustion air is not being accounted for. Actual and effective exhaust velocity are the same in rocket engines operating in a vacuum.
Specific impulse is inversely proportional to specific fuel consumption (SFC) by the relationship Isp = 1/(go·SFC) for SFC in kg/(N·s) and Isp = 3600/SFC for SFC in lb/(lbf·hr).

General considerations
The amount of propellant can be measured either in units of mass or weight."
How does specific impulse relate to specific fuel consumption?,Specific impulse is inversely proportional to specific fuel consumption,Specific impulse is directly proportional to specific fuel consumption,Specific impulse has no relation to specific fuel consumption,Specific impulse is equal to specific fuel consumption,Specific impulse is the square root of specific fuel consumption,A,"Specific impulse (usually abbreviated Isp) is a measure of how efficiently a reaction mass engine (a rocket using propellant or a jet engine using fuel) creates thrust. For engines whose reaction mass is only the fuel they carry, specific impulse is exactly proportional to the effective exhaust gas velocity.
A propulsion system with a higher specific impulse uses the mass of the propellant more efficiently. In the case of a rocket, this means less propellant needed for a given delta-v, so that the vehicle attached to the engine can more efficiently gain altitude and velocity.
In an atmospheric context, specific impulse can include the contribution to impulse provided by the mass of external air that is accelerated by the engine in some way, such as by an internal turbofan or heating by fuel combustion participation then thrust expansion or by external propeller. Jet engines breathe external air for both combustion and bypass, and therefore have a much higher specific impulse than rocket engines. The specific impulse in terms of propellant mass spent has units of distance per time, which is a notional velocity called the effective exhaust velocity. This is higher than the actual exhaust velocity because the mass of the combustion air is not being accounted for. Actual and effective exhaust velocity are the same in rocket engines operating in a vacuum.
Specific impulse is inversely proportional to specific fuel consumption (SFC) by the relationship Isp = 1/(go·SFC) for SFC in kg/(N·s) and Isp = 3600/SFC for SFC in lb/(lbf·hr).

General considerations
The amount of propellant can be measured either in units of mass or weight."
How can the amount of propellant be measured?,In units of mass,In units of weight,Both in units of mass and weight,By calculating the specific fuel consumption,By measuring the effective exhaust velocity,C,"Specific impulse (usually abbreviated Isp) is a measure of how efficiently a reaction mass engine (a rocket using propellant or a jet engine using fuel) creates thrust. For engines whose reaction mass is only the fuel they carry, specific impulse is exactly proportional to the effective exhaust gas velocity.
A propulsion system with a higher specific impulse uses the mass of the propellant more efficiently. In the case of a rocket, this means less propellant needed for a given delta-v, so that the vehicle attached to the engine can more efficiently gain altitude and velocity.
In an atmospheric context, specific impulse can include the contribution to impulse provided by the mass of external air that is accelerated by the engine in some way, such as by an internal turbofan or heating by fuel combustion participation then thrust expansion or by external propeller. Jet engines breathe external air for both combustion and bypass, and therefore have a much higher specific impulse than rocket engines. The specific impulse in terms of propellant mass spent has units of distance per time, which is a notional velocity called the effective exhaust velocity. This is higher than the actual exhaust velocity because the mass of the combustion air is not being accounted for. Actual and effective exhaust velocity are the same in rocket engines operating in a vacuum.
Specific impulse is inversely proportional to specific fuel consumption (SFC) by the relationship Isp = 1/(go·SFC) for SFC in kg/(N·s) and Isp = 3600/SFC for SFC in lb/(lbf·hr).

General considerations
The amount of propellant can be measured either in units of mass or weight."
What is forensic colorimetry?,The examination of specimen color for purposes of forensic investigation,The examination of specimen texture for purposes of forensic investigation,The examination of specimen shape for purposes of forensic investigation,The examination of specimen size for purposes of forensic investigation,The examination of specimen smell for purposes of forensic investigation,A,"Forensic colorimetry, or forensic color analysis, is the examination of specimen color for purposes of forensic investigation. Typical specimens involved in color analyses include pigments, dyes, or other objects that are distinguishable by their intrinsic color. Analyses may be conducted by-eye or by computational methods, both by matching specimen colors to a standardised chart or database.

Methodology
There are numerous quantitative and qualitative methods that an investigator may employ to analyse specimen color.

Visual analysis
Color analysis can be conducted by-eye, usually matching specimen color with reference charts. This methodology has been used extensively in soil analysis, often using the Munsell color system as the reference. While this is a traditionally used method, it is considerably subjective, relying on the ability of the naked eye to match colors.

Spectrophotometry
Spectrophotometers are devices that measure the light absorbed or transmitted by colored objects. More recent developments have led to advances in handheld spectrophotometers, which allow comparisons between sample color and database colors without the need for multiple pieces of equipment. These devices are most practical for analysing colors on a flat, homogeneous solid surface.

Digital photography with post-processing
Where a heterogeneous specimen, such as a soil or powder, requires quantitative analysis, digital photography may be used."
Which method of color analysis relies on the ability of the naked eye to match colors?,Visual analysis,Spectrophotometry,Digital photography with post-processing,By computational methods,Qualitative methods,A,"Forensic colorimetry, or forensic color analysis, is the examination of specimen color for purposes of forensic investigation. Typical specimens involved in color analyses include pigments, dyes, or other objects that are distinguishable by their intrinsic color. Analyses may be conducted by-eye or by computational methods, both by matching specimen colors to a standardised chart or database.

Methodology
There are numerous quantitative and qualitative methods that an investigator may employ to analyse specimen color.

Visual analysis
Color analysis can be conducted by-eye, usually matching specimen color with reference charts. This methodology has been used extensively in soil analysis, often using the Munsell color system as the reference. While this is a traditionally used method, it is considerably subjective, relying on the ability of the naked eye to match colors.

Spectrophotometry
Spectrophotometers are devices that measure the light absorbed or transmitted by colored objects. More recent developments have led to advances in handheld spectrophotometers, which allow comparisons between sample color and database colors without the need for multiple pieces of equipment. These devices are most practical for analysing colors on a flat, homogeneous solid surface.

Digital photography with post-processing
Where a heterogeneous specimen, such as a soil or powder, requires quantitative analysis, digital photography may be used."
What is the purpose of using spectrophotometers in color analysis?,To measure the light absorbed or transmitted by colored objects,To match specimen colors to a standardized chart or database,To conduct visual analysis,"To analyze colors on a flat, homogeneous solid surface",To conduct quantitative analysis of heterogeneous specimens,A,"Forensic colorimetry, or forensic color analysis, is the examination of specimen color for purposes of forensic investigation. Typical specimens involved in color analyses include pigments, dyes, or other objects that are distinguishable by their intrinsic color. Analyses may be conducted by-eye or by computational methods, both by matching specimen colors to a standardised chart or database.

Methodology
There are numerous quantitative and qualitative methods that an investigator may employ to analyse specimen color.

Visual analysis
Color analysis can be conducted by-eye, usually matching specimen color with reference charts. This methodology has been used extensively in soil analysis, often using the Munsell color system as the reference. While this is a traditionally used method, it is considerably subjective, relying on the ability of the naked eye to match colors.

Spectrophotometry
Spectrophotometers are devices that measure the light absorbed or transmitted by colored objects. More recent developments have led to advances in handheld spectrophotometers, which allow comparisons between sample color and database colors without the need for multiple pieces of equipment. These devices are most practical for analysing colors on a flat, homogeneous solid surface.

Digital photography with post-processing
Where a heterogeneous specimen, such as a soil or powder, requires quantitative analysis, digital photography may be used."
What is the advantage of using handheld spectrophotometers?,They allow comparisons between sample color and database colors without the need for multiple pieces of equipment,They provide more accurate results than visual analysis,They are suitable for analyzing colors on irregular surfaces,They can measure the smell of a specimen,They can conduct qualitative analysis,A,"Forensic colorimetry, or forensic color analysis, is the examination of specimen color for purposes of forensic investigation. Typical specimens involved in color analyses include pigments, dyes, or other objects that are distinguishable by their intrinsic color. Analyses may be conducted by-eye or by computational methods, both by matching specimen colors to a standardised chart or database.

Methodology
There are numerous quantitative and qualitative methods that an investigator may employ to analyse specimen color.

Visual analysis
Color analysis can be conducted by-eye, usually matching specimen color with reference charts. This methodology has been used extensively in soil analysis, often using the Munsell color system as the reference. While this is a traditionally used method, it is considerably subjective, relying on the ability of the naked eye to match colors.

Spectrophotometry
Spectrophotometers are devices that measure the light absorbed or transmitted by colored objects. More recent developments have led to advances in handheld spectrophotometers, which allow comparisons between sample color and database colors without the need for multiple pieces of equipment. These devices are most practical for analysing colors on a flat, homogeneous solid surface.

Digital photography with post-processing
Where a heterogeneous specimen, such as a soil or powder, requires quantitative analysis, digital photography may be used."
When is digital photography with post-processing used in color analysis?,When conducting visual analysis of soil or powder specimens,When comparing sample color to database colors,"When analyzing colors on a flat, homogeneous solid surface",When conducting quantitative analysis of heterogeneous specimens,When measuring the light absorbed or transmitted by colored objects,D,"Forensic colorimetry, or forensic color analysis, is the examination of specimen color for purposes of forensic investigation. Typical specimens involved in color analyses include pigments, dyes, or other objects that are distinguishable by their intrinsic color. Analyses may be conducted by-eye or by computational methods, both by matching specimen colors to a standardised chart or database.

Methodology
There are numerous quantitative and qualitative methods that an investigator may employ to analyse specimen color.

Visual analysis
Color analysis can be conducted by-eye, usually matching specimen color with reference charts. This methodology has been used extensively in soil analysis, often using the Munsell color system as the reference. While this is a traditionally used method, it is considerably subjective, relying on the ability of the naked eye to match colors.

Spectrophotometry
Spectrophotometers are devices that measure the light absorbed or transmitted by colored objects. More recent developments have led to advances in handheld spectrophotometers, which allow comparisons between sample color and database colors without the need for multiple pieces of equipment. These devices are most practical for analysing colors on a flat, homogeneous solid surface.

Digital photography with post-processing
Where a heterogeneous specimen, such as a soil or powder, requires quantitative analysis, digital photography may be used."
What is tablescaping?,An activity involving the setting of dining tables for social events,A form of art that involves decorating flat surfaces in a room,A type of competition that dates back to the 1930s and 1940s,A permanent installation that changes with the seasons,A decorative treatment for any flat surface in any room,A,"Table-setting, or tablescaping, is an activity involving the setting of sometimes elaborate dining tables in artful, decorative or themed ways for social events, and in a variety of categories for competitions and exhibitions.Tablescaping can also refer to any decorative treatment for any flat surface in any room; these are often more permanent installations that will only change with the seasons or with a change of decor in the room. 
In the United States and Australia there are formal tablesetting competitions and exhibitions that date back to the 1930s and 1940s.

History
Early dining tables were purely functional; the term ""setting the table"" originated in the middle ages to describe setting a board on two trestles to provide a temporary surface on which to set food. Diners supplied their own knife and spoon and food was often eaten off a slice of bread set directly on the table. A medieval table in a wealthy household might be covered in a cloth that was used as a common napkin rather than having a decorative purpose. According to Claudia Quigley Murphy, even among the wealthy a table would be set only with a salt cellar, cups, and sometimes stands for dishes that were being delivered to the table by cooks. When plates were introduced, they were often shared among two or more diners. By the late 1600s forks were in common use; this utensil meant fewer drips and greasy fingers to wipe, which made practical the use of decorative tablecloths and napkins."
"What was the origin of the term ""setting the table""?",It originated in the middle ages to describe setting a board on two trestles,It originated in the 1930s and 1940s as part of tablesetting competitions,It originated in the United States and Australia as a form of art,It originated in wealthy households as a way to display decorative napkins,It originated in the late 1600s with the introduction of forks,A,"Table-setting, or tablescaping, is an activity involving the setting of sometimes elaborate dining tables in artful, decorative or themed ways for social events, and in a variety of categories for competitions and exhibitions.Tablescaping can also refer to any decorative treatment for any flat surface in any room; these are often more permanent installations that will only change with the seasons or with a change of decor in the room. 
In the United States and Australia there are formal tablesetting competitions and exhibitions that date back to the 1930s and 1940s.

History
Early dining tables were purely functional; the term ""setting the table"" originated in the middle ages to describe setting a board on two trestles to provide a temporary surface on which to set food. Diners supplied their own knife and spoon and food was often eaten off a slice of bread set directly on the table. A medieval table in a wealthy household might be covered in a cloth that was used as a common napkin rather than having a decorative purpose. According to Claudia Quigley Murphy, even among the wealthy a table would be set only with a salt cellar, cups, and sometimes stands for dishes that were being delivered to the table by cooks. When plates were introduced, they were often shared among two or more diners. By the late 1600s forks were in common use; this utensil meant fewer drips and greasy fingers to wipe, which made practical the use of decorative tablecloths and napkins."
What were early dining tables primarily used for?,To display decorative napkins,To provide a temporary surface for setting food,To showcase elaborate table settings,To hold cutlery and utensils,To serve as a centerpiece for social events,B,"Table-setting, or tablescaping, is an activity involving the setting of sometimes elaborate dining tables in artful, decorative or themed ways for social events, and in a variety of categories for competitions and exhibitions.Tablescaping can also refer to any decorative treatment for any flat surface in any room; these are often more permanent installations that will only change with the seasons or with a change of decor in the room. 
In the United States and Australia there are formal tablesetting competitions and exhibitions that date back to the 1930s and 1940s.

History
Early dining tables were purely functional; the term ""setting the table"" originated in the middle ages to describe setting a board on two trestles to provide a temporary surface on which to set food. Diners supplied their own knife and spoon and food was often eaten off a slice of bread set directly on the table. A medieval table in a wealthy household might be covered in a cloth that was used as a common napkin rather than having a decorative purpose. According to Claudia Quigley Murphy, even among the wealthy a table would be set only with a salt cellar, cups, and sometimes stands for dishes that were being delivered to the table by cooks. When plates were introduced, they were often shared among two or more diners. By the late 1600s forks were in common use; this utensil meant fewer drips and greasy fingers to wipe, which made practical the use of decorative tablecloths and napkins."
What utensil was introduced in the late 1600s?,Spoons,Knives,Forks,Cups,Salt cellars,C,"Table-setting, or tablescaping, is an activity involving the setting of sometimes elaborate dining tables in artful, decorative or themed ways for social events, and in a variety of categories for competitions and exhibitions.Tablescaping can also refer to any decorative treatment for any flat surface in any room; these are often more permanent installations that will only change with the seasons or with a change of decor in the room. 
In the United States and Australia there are formal tablesetting competitions and exhibitions that date back to the 1930s and 1940s.

History
Early dining tables were purely functional; the term ""setting the table"" originated in the middle ages to describe setting a board on two trestles to provide a temporary surface on which to set food. Diners supplied their own knife and spoon and food was often eaten off a slice of bread set directly on the table. A medieval table in a wealthy household might be covered in a cloth that was used as a common napkin rather than having a decorative purpose. According to Claudia Quigley Murphy, even among the wealthy a table would be set only with a salt cellar, cups, and sometimes stands for dishes that were being delivered to the table by cooks. When plates were introduced, they were often shared among two or more diners. By the late 1600s forks were in common use; this utensil meant fewer drips and greasy fingers to wipe, which made practical the use of decorative tablecloths and napkins."
What made the use of decorative tablecloths and napkins more practical?,The introduction of forks,The use of fewer drips and greasy fingers to wipe,The availability of decorative tableware,The increase in wealthy households,The popularity of tablesetting competitions,B,"Table-setting, or tablescaping, is an activity involving the setting of sometimes elaborate dining tables in artful, decorative or themed ways for social events, and in a variety of categories for competitions and exhibitions.Tablescaping can also refer to any decorative treatment for any flat surface in any room; these are often more permanent installations that will only change with the seasons or with a change of decor in the room. 
In the United States and Australia there are formal tablesetting competitions and exhibitions that date back to the 1930s and 1940s.

History
Early dining tables were purely functional; the term ""setting the table"" originated in the middle ages to describe setting a board on two trestles to provide a temporary surface on which to set food. Diners supplied their own knife and spoon and food was often eaten off a slice of bread set directly on the table. A medieval table in a wealthy household might be covered in a cloth that was used as a common napkin rather than having a decorative purpose. According to Claudia Quigley Murphy, even among the wealthy a table would be set only with a salt cellar, cups, and sometimes stands for dishes that were being delivered to the table by cooks. When plates were introduced, they were often shared among two or more diners. By the late 1600s forks were in common use; this utensil meant fewer drips and greasy fingers to wipe, which made practical the use of decorative tablecloths and napkins."
Who invented conjugate coding?,Stephen Wiesner,James Park,Alexander Holevo,Charles H. Bennett,R. P. Poplavskii,A,"This is a timeline of quantum computing.

1960s
1968
Stephen Wiesner invented conjugate coding (published in ACM SIGACT News 15(1):78–88).

1970s
1970
James Park articulated the no-cloning theorem.

1973
Alexander Holevo published a paper showing that n qubits can carry more than n classical bits of information, but at most n classical bits are accessible (a result known as ""Holevo's theorem"" or ""Holevo's bound"").
Charles H. Bennett showed that computation can be done reversibly.

1975
R. P. Poplavskii published ""Thermodynamical models of information processing"" (in Russian) which showed the computational infeasibility of simulating quantum systems on classical computers, due to the superposition principle.

1976
Polish mathematical physicist Roman Stanisław Ingarden published the paper ""Quantum Information Theory"" in Reports on Mathematical Physics, vol. 10, 43–72, 1976 (The paper was submitted in 1975). It is one of the first attempts at creating a quantum information theory, showing that Shannon information theory cannot directly be generalized to the quantum case, but rather that it is possible to construct a quantum information theory, which is a generalization of Shannon's theory, within the formalism of a generalized quantum mechanics of open systems and a generalized concept of observables (the so-called semi-observables).

1980s
1980
Paul Benioff described the first quantum mechanical model of a computer. In this work, Benioff showed that a computer could operate under the laws of quantum mechanics by describing a Schrödinger equation description of Turing machines, laying a foundation for further work in quantum computing."
Who articulated the no-cloning theorem?,Stephen Wiesner,James Park,Alexander Holevo,Charles H. Bennett,R. P. Poplavskii,B,"This is a timeline of quantum computing.

1960s
1968
Stephen Wiesner invented conjugate coding (published in ACM SIGACT News 15(1):78–88).

1970s
1970
James Park articulated the no-cloning theorem.

1973
Alexander Holevo published a paper showing that n qubits can carry more than n classical bits of information, but at most n classical bits are accessible (a result known as ""Holevo's theorem"" or ""Holevo's bound"").
Charles H. Bennett showed that computation can be done reversibly.

1975
R. P. Poplavskii published ""Thermodynamical models of information processing"" (in Russian) which showed the computational infeasibility of simulating quantum systems on classical computers, due to the superposition principle.

1976
Polish mathematical physicist Roman Stanisław Ingarden published the paper ""Quantum Information Theory"" in Reports on Mathematical Physics, vol. 10, 43–72, 1976 (The paper was submitted in 1975). It is one of the first attempts at creating a quantum information theory, showing that Shannon information theory cannot directly be generalized to the quantum case, but rather that it is possible to construct a quantum information theory, which is a generalization of Shannon's theory, within the formalism of a generalized quantum mechanics of open systems and a generalized concept of observables (the so-called semi-observables).

1980s
1980
Paul Benioff described the first quantum mechanical model of a computer. In this work, Benioff showed that a computer could operate under the laws of quantum mechanics by describing a Schrödinger equation description of Turing machines, laying a foundation for further work in quantum computing."
Who showed that n qubits can carry more than n classical bits of information?,Stephen Wiesner,James Park,Alexander Holevo,Charles H. Bennett,R. P. Poplavskii,C,"This is a timeline of quantum computing.

1960s
1968
Stephen Wiesner invented conjugate coding (published in ACM SIGACT News 15(1):78–88).

1970s
1970
James Park articulated the no-cloning theorem.

1973
Alexander Holevo published a paper showing that n qubits can carry more than n classical bits of information, but at most n classical bits are accessible (a result known as ""Holevo's theorem"" or ""Holevo's bound"").
Charles H. Bennett showed that computation can be done reversibly.

1975
R. P. Poplavskii published ""Thermodynamical models of information processing"" (in Russian) which showed the computational infeasibility of simulating quantum systems on classical computers, due to the superposition principle.

1976
Polish mathematical physicist Roman Stanisław Ingarden published the paper ""Quantum Information Theory"" in Reports on Mathematical Physics, vol. 10, 43–72, 1976 (The paper was submitted in 1975). It is one of the first attempts at creating a quantum information theory, showing that Shannon information theory cannot directly be generalized to the quantum case, but rather that it is possible to construct a quantum information theory, which is a generalization of Shannon's theory, within the formalism of a generalized quantum mechanics of open systems and a generalized concept of observables (the so-called semi-observables).

1980s
1980
Paul Benioff described the first quantum mechanical model of a computer. In this work, Benioff showed that a computer could operate under the laws of quantum mechanics by describing a Schrödinger equation description of Turing machines, laying a foundation for further work in quantum computing."
Who showed that computation can be done reversibly?,Stephen Wiesner,James Park,Alexander Holevo,Charles H. Bennett,R. P. Poplavskii,D,"This is a timeline of quantum computing.

1960s
1968
Stephen Wiesner invented conjugate coding (published in ACM SIGACT News 15(1):78–88).

1970s
1970
James Park articulated the no-cloning theorem.

1973
Alexander Holevo published a paper showing that n qubits can carry more than n classical bits of information, but at most n classical bits are accessible (a result known as ""Holevo's theorem"" or ""Holevo's bound"").
Charles H. Bennett showed that computation can be done reversibly.

1975
R. P. Poplavskii published ""Thermodynamical models of information processing"" (in Russian) which showed the computational infeasibility of simulating quantum systems on classical computers, due to the superposition principle.

1976
Polish mathematical physicist Roman Stanisław Ingarden published the paper ""Quantum Information Theory"" in Reports on Mathematical Physics, vol. 10, 43–72, 1976 (The paper was submitted in 1975). It is one of the first attempts at creating a quantum information theory, showing that Shannon information theory cannot directly be generalized to the quantum case, but rather that it is possible to construct a quantum information theory, which is a generalization of Shannon's theory, within the formalism of a generalized quantum mechanics of open systems and a generalized concept of observables (the so-called semi-observables).

1980s
1980
Paul Benioff described the first quantum mechanical model of a computer. In this work, Benioff showed that a computer could operate under the laws of quantum mechanics by describing a Schrödinger equation description of Turing machines, laying a foundation for further work in quantum computing."
Who described the first quantum mechanical model of a computer?,Stephen Wiesner,James Park,Alexander Holevo,Charles H. Bennett,R. P. Poplavskii,E,"This is a timeline of quantum computing.

1960s
1968
Stephen Wiesner invented conjugate coding (published in ACM SIGACT News 15(1):78–88).

1970s
1970
James Park articulated the no-cloning theorem.

1973
Alexander Holevo published a paper showing that n qubits can carry more than n classical bits of information, but at most n classical bits are accessible (a result known as ""Holevo's theorem"" or ""Holevo's bound"").
Charles H. Bennett showed that computation can be done reversibly.

1975
R. P. Poplavskii published ""Thermodynamical models of information processing"" (in Russian) which showed the computational infeasibility of simulating quantum systems on classical computers, due to the superposition principle.

1976
Polish mathematical physicist Roman Stanisław Ingarden published the paper ""Quantum Information Theory"" in Reports on Mathematical Physics, vol. 10, 43–72, 1976 (The paper was submitted in 1975). It is one of the first attempts at creating a quantum information theory, showing that Shannon information theory cannot directly be generalized to the quantum case, but rather that it is possible to construct a quantum information theory, which is a generalization of Shannon's theory, within the formalism of a generalized quantum mechanics of open systems and a generalized concept of observables (the so-called semi-observables).

1980s
1980
Paul Benioff described the first quantum mechanical model of a computer. In this work, Benioff showed that a computer could operate under the laws of quantum mechanics by describing a Schrödinger equation description of Turing machines, laying a foundation for further work in quantum computing."
What is the purpose of a shallow foundation?,To transfer structural load to a subsurface layer,To distribute the pressure to the soil,To support light structures only,To reduce the width of the entire foundation,To make the foundation more technical,B,"A shallow foundation is a type of building foundation that transfers structural load to the earth very near to the surface, rather than to a subsurface layer or a range of depths, as does a deep foundation. Customarily, a shallow foundation is considered as such when the width of the entire foundation is greater than its depth. In comparison to deep foundations, shallow foundations are less technical, thus making them more economical and the most widely used for relatively light structures.

Types of shallow foundation
Footings are always wider than the members that they support. Structural loads from a column or wall are usually greater than 1000kPa, while the soil's bearing capacity is commonly less than that (typically less than 400kPa). By possessing a larger bearing area, the foundation distributes the pressure to the soil, decreasing the bearing pressure to within allowable values. A structure is not limited to one footing. Multiple types of footings may be used in a construction project.

Wall footing
Also called strip footing, this footing is a continuous strip that supports structural and non-structural load bearing walls."
Why are shallow foundations more economical?,Because they are deeper than deep foundations,Because they have a larger bearing area,Because they can support heavier loads,Because they are less technical,Because they are more widely used,D,"A shallow foundation is a type of building foundation that transfers structural load to the earth very near to the surface, rather than to a subsurface layer or a range of depths, as does a deep foundation. Customarily, a shallow foundation is considered as such when the width of the entire foundation is greater than its depth. In comparison to deep foundations, shallow foundations are less technical, thus making them more economical and the most widely used for relatively light structures.

Types of shallow foundation
Footings are always wider than the members that they support. Structural loads from a column or wall are usually greater than 1000kPa, while the soil's bearing capacity is commonly less than that (typically less than 400kPa). By possessing a larger bearing area, the foundation distributes the pressure to the soil, decreasing the bearing pressure to within allowable values. A structure is not limited to one footing. Multiple types of footings may be used in a construction project.

Wall footing
Also called strip footing, this footing is a continuous strip that supports structural and non-structural load bearing walls."
What is the difference between shallow and deep foundations?,Shallow foundations have a smaller bearing area,Deep foundations are less economical,Shallow foundations are more technical,Deep foundations are used for light structures only,Shallow foundations transfer load near the surface,E,"A shallow foundation is a type of building foundation that transfers structural load to the earth very near to the surface, rather than to a subsurface layer or a range of depths, as does a deep foundation. Customarily, a shallow foundation is considered as such when the width of the entire foundation is greater than its depth. In comparison to deep foundations, shallow foundations are less technical, thus making them more economical and the most widely used for relatively light structures.

Types of shallow foundation
Footings are always wider than the members that they support. Structural loads from a column or wall are usually greater than 1000kPa, while the soil's bearing capacity is commonly less than that (typically less than 400kPa). By possessing a larger bearing area, the foundation distributes the pressure to the soil, decreasing the bearing pressure to within allowable values. A structure is not limited to one footing. Multiple types of footings may be used in a construction project.

Wall footing
Also called strip footing, this footing is a continuous strip that supports structural and non-structural load bearing walls."
What is the purpose of a wall footing?,To support non-structural load bearing walls,To reduce the width of the entire foundation,To transfer structural load to a subsurface layer,To distribute the pressure to the soil,To make the foundation more technical,A,"A shallow foundation is a type of building foundation that transfers structural load to the earth very near to the surface, rather than to a subsurface layer or a range of depths, as does a deep foundation. Customarily, a shallow foundation is considered as such when the width of the entire foundation is greater than its depth. In comparison to deep foundations, shallow foundations are less technical, thus making them more economical and the most widely used for relatively light structures.

Types of shallow foundation
Footings are always wider than the members that they support. Structural loads from a column or wall are usually greater than 1000kPa, while the soil's bearing capacity is commonly less than that (typically less than 400kPa). By possessing a larger bearing area, the foundation distributes the pressure to the soil, decreasing the bearing pressure to within allowable values. A structure is not limited to one footing. Multiple types of footings may be used in a construction project.

Wall footing
Also called strip footing, this footing is a continuous strip that supports structural and non-structural load bearing walls."
What is another name for a wall footing?,Strip footing,Deep foundation,Shallow foundation,Structural footing,Non-structural footing,A,"A shallow foundation is a type of building foundation that transfers structural load to the earth very near to the surface, rather than to a subsurface layer or a range of depths, as does a deep foundation. Customarily, a shallow foundation is considered as such when the width of the entire foundation is greater than its depth. In comparison to deep foundations, shallow foundations are less technical, thus making them more economical and the most widely used for relatively light structures.

Types of shallow foundation
Footings are always wider than the members that they support. Structural loads from a column or wall are usually greater than 1000kPa, while the soil's bearing capacity is commonly less than that (typically less than 400kPa). By possessing a larger bearing area, the foundation distributes the pressure to the soil, decreasing the bearing pressure to within allowable values. A structure is not limited to one footing. Multiple types of footings may be used in a construction project.

Wall footing
Also called strip footing, this footing is a continuous strip that supports structural and non-structural load bearing walls."
What is a shellworld?,A planet or planetoid turned into concentric matryoshka doll-like layers,A megastructure consisting of multiple layers of shells suspended above each other by orbital rings,An inflated canopy holding high pressure air around an otherwise airless world,Completely hollow shell worlds created by contained gas alone,None of the above,A,"A shellworld is any of several types of hypothetical megastructures:

A planet or a planetoid turned into series of concentric matryoshka doll-like layers supported by massive pillars. A shellworld of this type features prominently in Iain M. Banks' novel Matter.
A megastructure consisting of multiple layers of shells suspended above each other by orbital rings supported by hypothetical mass stream technology. This type of shellworld can be theoretically suspended above any type of stellar body, including planets, gas giants, stars and black holes. The most massive type of shellworld could be built around supermassive black holes at the center of galaxies.
An inflated canopy holding high pressure air around an otherwise airless world to create a breathable atmosphere. The pressure of the contained air supports the weight of the shell.
Completely hollow shell worlds can also be created on a planetary or larger scale by contained gas alone, also called bubbleworlds or gravitational balloons, as long as the outward pressure from the contained gas balances the gravitational contraction of the entire structure, resulting in no net force on the shell. The scale is limited only by the mass of gas enclosed; the shell can be made of any mundane material."
Who wrote the novel Matter?,Iain M. Banks,J.K. Rowling,Stephen King,George R.R. Martin,None of the above,A,"A shellworld is any of several types of hypothetical megastructures:

A planet or a planetoid turned into series of concentric matryoshka doll-like layers supported by massive pillars. A shellworld of this type features prominently in Iain M. Banks' novel Matter.
A megastructure consisting of multiple layers of shells suspended above each other by orbital rings supported by hypothetical mass stream technology. This type of shellworld can be theoretically suspended above any type of stellar body, including planets, gas giants, stars and black holes. The most massive type of shellworld could be built around supermassive black holes at the center of galaxies.
An inflated canopy holding high pressure air around an otherwise airless world to create a breathable atmosphere. The pressure of the contained air supports the weight of the shell.
Completely hollow shell worlds can also be created on a planetary or larger scale by contained gas alone, also called bubbleworlds or gravitational balloons, as long as the outward pressure from the contained gas balances the gravitational contraction of the entire structure, resulting in no net force on the shell. The scale is limited only by the mass of gas enclosed; the shell can be made of any mundane material."
Which type of shellworld can be suspended above any type of stellar body?,A planet or planetoid turned into concentric matryoshka doll-like layers,A megastructure consisting of multiple layers of shells suspended above each other by orbital rings,An inflated canopy holding high pressure air around an otherwise airless world,Completely hollow shell worlds created by contained gas alone,None of the above,B,"A shellworld is any of several types of hypothetical megastructures:

A planet or a planetoid turned into series of concentric matryoshka doll-like layers supported by massive pillars. A shellworld of this type features prominently in Iain M. Banks' novel Matter.
A megastructure consisting of multiple layers of shells suspended above each other by orbital rings supported by hypothetical mass stream technology. This type of shellworld can be theoretically suspended above any type of stellar body, including planets, gas giants, stars and black holes. The most massive type of shellworld could be built around supermassive black holes at the center of galaxies.
An inflated canopy holding high pressure air around an otherwise airless world to create a breathable atmosphere. The pressure of the contained air supports the weight of the shell.
Completely hollow shell worlds can also be created on a planetary or larger scale by contained gas alone, also called bubbleworlds or gravitational balloons, as long as the outward pressure from the contained gas balances the gravitational contraction of the entire structure, resulting in no net force on the shell. The scale is limited only by the mass of gas enclosed; the shell can be made of any mundane material."
How is an inflated canopy shellworld created?,By turning a planet or planetoid into concentric matryoshka doll-like layers,By using orbital rings supported by hypothetical mass stream technology,By containing high pressure air around an airless world,By creating a completely hollow shell world using contained gas,None of the above,C,"A shellworld is any of several types of hypothetical megastructures:

A planet or a planetoid turned into series of concentric matryoshka doll-like layers supported by massive pillars. A shellworld of this type features prominently in Iain M. Banks' novel Matter.
A megastructure consisting of multiple layers of shells suspended above each other by orbital rings supported by hypothetical mass stream technology. This type of shellworld can be theoretically suspended above any type of stellar body, including planets, gas giants, stars and black holes. The most massive type of shellworld could be built around supermassive black holes at the center of galaxies.
An inflated canopy holding high pressure air around an otherwise airless world to create a breathable atmosphere. The pressure of the contained air supports the weight of the shell.
Completely hollow shell worlds can also be created on a planetary or larger scale by contained gas alone, also called bubbleworlds or gravitational balloons, as long as the outward pressure from the contained gas balances the gravitational contraction of the entire structure, resulting in no net force on the shell. The scale is limited only by the mass of gas enclosed; the shell can be made of any mundane material."
What is the condition for a completely hollow shellworld?,The mass of the enclosed gas must balance the gravitational contraction of the entire structure,The shell must be made of any mundane material,The shell can be made on a planetary or larger scale,The scale is limited only by the mass of gas enclosed,None of the above,A,"A shellworld is any of several types of hypothetical megastructures:

A planet or a planetoid turned into series of concentric matryoshka doll-like layers supported by massive pillars. A shellworld of this type features prominently in Iain M. Banks' novel Matter.
A megastructure consisting of multiple layers of shells suspended above each other by orbital rings supported by hypothetical mass stream technology. This type of shellworld can be theoretically suspended above any type of stellar body, including planets, gas giants, stars and black holes. The most massive type of shellworld could be built around supermassive black holes at the center of galaxies.
An inflated canopy holding high pressure air around an otherwise airless world to create a breathable atmosphere. The pressure of the contained air supports the weight of the shell.
Completely hollow shell worlds can also be created on a planetary or larger scale by contained gas alone, also called bubbleworlds or gravitational balloons, as long as the outward pressure from the contained gas balances the gravitational contraction of the entire structure, resulting in no net force on the shell. The scale is limited only by the mass of gas enclosed; the shell can be made of any mundane material."
What is the main headquarters location of Pennant International Group Plc?,"Birmingham, UK","Cheltenham, UK","London, UK","Manchester, UK","Glasgow, UK",B,"Pennant International Group Plc comprises a number of individual companies which provide engineering products and services to wide range of markets in the UK, Canada, Australasia and the US. Its head office is in Cheltenham, UK.
The Company operates through three segments:
Training Systems, both hardware and software based, in the defence sector,
Data Services, to the defence, rail, power and government sectors,
Software, including the Omega suite of software sold into the Canadian and Australian defence sectors.

GenFly
Origins
The GenFly Generic Flying Controls Trainer is an aircraft maintenance training rig developed in the late twentieth century for the Royal Air Force, for use in training airframe mechanics and technicians, particularly in maintenance activities involving flying controls and aircraft hydraulics.

Service
The RAF took delivery of four GenFly units to RAF Cosford, UK. The training manuals were written to RAF aircraft maintenance documentation standards, in order to familiarise students with the format. All four rigs were assigned tail numbers from the RAF military aircraft register, to allow even greater realism in simulating aircraft documentation, while avoiding the possibility of confusion from fabricated tail numbers coinciding with those of real airframes. The tail numbers are: ZJ695, ZJ696, ZJ697 and ZJ698. The device entered service in 2001.

References
External links
Pennant International web site."
In which sectors does Pennant International Group Plc provide its engineering products and services?,Pharmaceutical and healthcare,Automotive and manufacturing,"Defence, rail, power, and government",Technology and telecommunications,Financial and banking,C,"Pennant International Group Plc comprises a number of individual companies which provide engineering products and services to wide range of markets in the UK, Canada, Australasia and the US. Its head office is in Cheltenham, UK.
The Company operates through three segments:
Training Systems, both hardware and software based, in the defence sector,
Data Services, to the defence, rail, power and government sectors,
Software, including the Omega suite of software sold into the Canadian and Australian defence sectors.

GenFly
Origins
The GenFly Generic Flying Controls Trainer is an aircraft maintenance training rig developed in the late twentieth century for the Royal Air Force, for use in training airframe mechanics and technicians, particularly in maintenance activities involving flying controls and aircraft hydraulics.

Service
The RAF took delivery of four GenFly units to RAF Cosford, UK. The training manuals were written to RAF aircraft maintenance documentation standards, in order to familiarise students with the format. All four rigs were assigned tail numbers from the RAF military aircraft register, to allow even greater realism in simulating aircraft documentation, while avoiding the possibility of confusion from fabricated tail numbers coinciding with those of real airframes. The tail numbers are: ZJ695, ZJ696, ZJ697 and ZJ698. The device entered service in 2001.

References
External links
Pennant International web site."
What is the purpose of the GenFly Generic Flying Controls Trainer?,To train pilots in flying controls and aircraft hydraulics,To provide maintenance training for airframe mechanics and technicians,To simulate aircraft documentation for the Royal Air Force,To develop software for the defence sectors in Canada and Australia,To provide data services to the government sector,B,"Pennant International Group Plc comprises a number of individual companies which provide engineering products and services to wide range of markets in the UK, Canada, Australasia and the US. Its head office is in Cheltenham, UK.
The Company operates through three segments:
Training Systems, both hardware and software based, in the defence sector,
Data Services, to the defence, rail, power and government sectors,
Software, including the Omega suite of software sold into the Canadian and Australian defence sectors.

GenFly
Origins
The GenFly Generic Flying Controls Trainer is an aircraft maintenance training rig developed in the late twentieth century for the Royal Air Force, for use in training airframe mechanics and technicians, particularly in maintenance activities involving flying controls and aircraft hydraulics.

Service
The RAF took delivery of four GenFly units to RAF Cosford, UK. The training manuals were written to RAF aircraft maintenance documentation standards, in order to familiarise students with the format. All four rigs were assigned tail numbers from the RAF military aircraft register, to allow even greater realism in simulating aircraft documentation, while avoiding the possibility of confusion from fabricated tail numbers coinciding with those of real airframes. The tail numbers are: ZJ695, ZJ696, ZJ697 and ZJ698. The device entered service in 2001.

References
External links
Pennant International web site."
When did the GenFly device enter service?,1990,2001,2010,2015,2020,B,"Pennant International Group Plc comprises a number of individual companies which provide engineering products and services to wide range of markets in the UK, Canada, Australasia and the US. Its head office is in Cheltenham, UK.
The Company operates through three segments:
Training Systems, both hardware and software based, in the defence sector,
Data Services, to the defence, rail, power and government sectors,
Software, including the Omega suite of software sold into the Canadian and Australian defence sectors.

GenFly
Origins
The GenFly Generic Flying Controls Trainer is an aircraft maintenance training rig developed in the late twentieth century for the Royal Air Force, for use in training airframe mechanics and technicians, particularly in maintenance activities involving flying controls and aircraft hydraulics.

Service
The RAF took delivery of four GenFly units to RAF Cosford, UK. The training manuals were written to RAF aircraft maintenance documentation standards, in order to familiarise students with the format. All four rigs were assigned tail numbers from the RAF military aircraft register, to allow even greater realism in simulating aircraft documentation, while avoiding the possibility of confusion from fabricated tail numbers coinciding with those of real airframes. The tail numbers are: ZJ695, ZJ696, ZJ697 and ZJ698. The device entered service in 2001.

References
External links
Pennant International web site."
How many GenFly units did the RAF take delivery of?,Two,Three,Four,Five,Six,C,"Pennant International Group Plc comprises a number of individual companies which provide engineering products and services to wide range of markets in the UK, Canada, Australasia and the US. Its head office is in Cheltenham, UK.
The Company operates through three segments:
Training Systems, both hardware and software based, in the defence sector,
Data Services, to the defence, rail, power and government sectors,
Software, including the Omega suite of software sold into the Canadian and Australian defence sectors.

GenFly
Origins
The GenFly Generic Flying Controls Trainer is an aircraft maintenance training rig developed in the late twentieth century for the Royal Air Force, for use in training airframe mechanics and technicians, particularly in maintenance activities involving flying controls and aircraft hydraulics.

Service
The RAF took delivery of four GenFly units to RAF Cosford, UK. The training manuals were written to RAF aircraft maintenance documentation standards, in order to familiarise students with the format. All four rigs were assigned tail numbers from the RAF military aircraft register, to allow even greater realism in simulating aircraft documentation, while avoiding the possibility of confusion from fabricated tail numbers coinciding with those of real airframes. The tail numbers are: ZJ695, ZJ696, ZJ697 and ZJ698. The device entered service in 2001.

References
External links
Pennant International web site."
What is an endless runway?,An aircraft runway that loops around to form a circle,An aircraft runway that is extremely long,An aircraft runway that can never be used for take-offs and landings,An aircraft runway that is made entirely of circular tracks,An aircraft runway that is designed for testing cars,A,"An endless runway is an aircraft runway which loops around to form a shape such as a circle.  There were experiments with this concept in the 1960s and it is now being researched at the Nederlands Lucht- en Ruimtevaartcentrum – the national aerospace laboratory of the Netherlands.

History
With the birth of aviation, the thought of taking a car to the office became old-style and soon people started to invent ways to take the airplane to the office. In an article in ""Popular Science"" from 1919, a circular runway was designed on top of some skyscrapers in New York, where one of the buildings would serve the purpose of aircraft parking. A design was made for a circular runway on the roof of Kings Cross railway station in London.
In the 1960s, the idea was taken up by a marine officer, who actually performed a number of take-offs and landings with aircraft at a circular track designed for testing cars. This work stimulated a large number of patents with runways shaped as circles or ovals and often straight tracks attached to it for taking off and landing.
In the 2010s, the idea was studied by a European consortium led by an engineer at the Royal Netherlands Aerospace Centre.  Their plans for a pilot included a smaller circular runway for delivery drones.

Safety issues
Increased runway width. Instrument Landing System can be used for the straight approach but will require modification or a new system to cope with the curving runway."
When did experiments with the concept of an endless runway take place?,In the 1910s,In the 1960s,In the 2010s,In the 1980s,In the 2000s,B,"An endless runway is an aircraft runway which loops around to form a shape such as a circle.  There were experiments with this concept in the 1960s and it is now being researched at the Nederlands Lucht- en Ruimtevaartcentrum – the national aerospace laboratory of the Netherlands.

History
With the birth of aviation, the thought of taking a car to the office became old-style and soon people started to invent ways to take the airplane to the office. In an article in ""Popular Science"" from 1919, a circular runway was designed on top of some skyscrapers in New York, where one of the buildings would serve the purpose of aircraft parking. A design was made for a circular runway on the roof of Kings Cross railway station in London.
In the 1960s, the idea was taken up by a marine officer, who actually performed a number of take-offs and landings with aircraft at a circular track designed for testing cars. This work stimulated a large number of patents with runways shaped as circles or ovals and often straight tracks attached to it for taking off and landing.
In the 2010s, the idea was studied by a European consortium led by an engineer at the Royal Netherlands Aerospace Centre.  Their plans for a pilot included a smaller circular runway for delivery drones.

Safety issues
Increased runway width. Instrument Landing System can be used for the straight approach but will require modification or a new system to cope with the curving runway."
What was the purpose of the circular runway designed on top of skyscrapers in New York?,To provide parking space for airplanes,To provide a landing area for helicopters,To allow airplanes to take off and land directly on the roofs of buildings,To serve as a recreational area for pilots,To test the feasibility of circular runways,C,"An endless runway is an aircraft runway which loops around to form a shape such as a circle.  There were experiments with this concept in the 1960s and it is now being researched at the Nederlands Lucht- en Ruimtevaartcentrum – the national aerospace laboratory of the Netherlands.

History
With the birth of aviation, the thought of taking a car to the office became old-style and soon people started to invent ways to take the airplane to the office. In an article in ""Popular Science"" from 1919, a circular runway was designed on top of some skyscrapers in New York, where one of the buildings would serve the purpose of aircraft parking. A design was made for a circular runway on the roof of Kings Cross railway station in London.
In the 1960s, the idea was taken up by a marine officer, who actually performed a number of take-offs and landings with aircraft at a circular track designed for testing cars. This work stimulated a large number of patents with runways shaped as circles or ovals and often straight tracks attached to it for taking off and landing.
In the 2010s, the idea was studied by a European consortium led by an engineer at the Royal Netherlands Aerospace Centre.  Their plans for a pilot included a smaller circular runway for delivery drones.

Safety issues
Increased runway width. Instrument Landing System can be used for the straight approach but will require modification or a new system to cope with the curving runway."
Who led the European consortium that studied the idea of an endless runway?,An engineer at the Royal Netherlands Aerospace Centre,An engineer at the Nederlands Lucht- en Ruimtevaartcentrum,A marine officer,An engineer at the Kings Cross railway station,A delivery drone pilot,A,"An endless runway is an aircraft runway which loops around to form a shape such as a circle.  There were experiments with this concept in the 1960s and it is now being researched at the Nederlands Lucht- en Ruimtevaartcentrum – the national aerospace laboratory of the Netherlands.

History
With the birth of aviation, the thought of taking a car to the office became old-style and soon people started to invent ways to take the airplane to the office. In an article in ""Popular Science"" from 1919, a circular runway was designed on top of some skyscrapers in New York, where one of the buildings would serve the purpose of aircraft parking. A design was made for a circular runway on the roof of Kings Cross railway station in London.
In the 1960s, the idea was taken up by a marine officer, who actually performed a number of take-offs and landings with aircraft at a circular track designed for testing cars. This work stimulated a large number of patents with runways shaped as circles or ovals and often straight tracks attached to it for taking off and landing.
In the 2010s, the idea was studied by a European consortium led by an engineer at the Royal Netherlands Aerospace Centre.  Their plans for a pilot included a smaller circular runway for delivery drones.

Safety issues
Increased runway width. Instrument Landing System can be used for the straight approach but will require modification or a new system to cope with the curving runway."
What modification or new system would be required for the Instrument Landing System to work with a curving runway?,No modification or new system is required,A modification or new system is not possible,A new system needs to be developed,The curving runway cannot be used with the Instrument Landing System,The curving runway requires a modified version of the Instrument Landing System,C,"An endless runway is an aircraft runway which loops around to form a shape such as a circle.  There were experiments with this concept in the 1960s and it is now being researched at the Nederlands Lucht- en Ruimtevaartcentrum – the national aerospace laboratory of the Netherlands.

History
With the birth of aviation, the thought of taking a car to the office became old-style and soon people started to invent ways to take the airplane to the office. In an article in ""Popular Science"" from 1919, a circular runway was designed on top of some skyscrapers in New York, where one of the buildings would serve the purpose of aircraft parking. A design was made for a circular runway on the roof of Kings Cross railway station in London.
In the 1960s, the idea was taken up by a marine officer, who actually performed a number of take-offs and landings with aircraft at a circular track designed for testing cars. This work stimulated a large number of patents with runways shaped as circles or ovals and often straight tracks attached to it for taking off and landing.
In the 2010s, the idea was studied by a European consortium led by an engineer at the Royal Netherlands Aerospace Centre.  Their plans for a pilot included a smaller circular runway for delivery drones.

Safety issues
Increased runway width. Instrument Landing System can be used for the straight approach but will require modification or a new system to cope with the curving runway."
What is the purpose of error analysis for the Global Positioning System (GPS)?,To determine the distance between GPS satellites and the receiver,To understand how GPS works and the magnitude of expected errors,To correct for residual errors in the GPS system,To compute the GPS receiver position based on satellite data,To estimate the numerical error in GPS measurements,B,"The error analysis for the Global Positioning System is important for understanding how GPS works, and for knowing what magnitude of error should be expected. The GPS makes corrections for receiver clock errors and other effects but there are still residual errors which are not corrected. GPS receiver position is computed based on data received from the satellites. Errors depend on geometric dilution of precision and the sources listed in the table below.

Overview
User equivalent range errors (UERE) are shown in the table. There is also a numerical error with an estimated value, 
  
    
       
      
        σ
        
          n
          u
          m
        
      
    
    \ \sigma _{num}
  , of about 1 meter (3 ft 3 in). The standard deviations, 
  
    
       
      
        σ
        
          R
        
      
    
    \ \sigma _{R}
  , for the coarse/acquisition (C/A) and precise codes are also shown in the table. These standard deviations are computed by taking the square root of the sum of the squares of the individual components (i.e., RSS for root sum squares)."
What are user equivalent range errors (UERE) in GPS?,Errors caused by clock errors in the GPS receiver,Errors that are corrected by the GPS system,Errors in the GPS receiver position computation,Errors in the range measurement between GPS satellites and the receiver,Errors in the numerical estimation of GPS measurements,D,"The error analysis for the Global Positioning System is important for understanding how GPS works, and for knowing what magnitude of error should be expected. The GPS makes corrections for receiver clock errors and other effects but there are still residual errors which are not corrected. GPS receiver position is computed based on data received from the satellites. Errors depend on geometric dilution of precision and the sources listed in the table below.

Overview
User equivalent range errors (UERE) are shown in the table. There is also a numerical error with an estimated value, 
  
    
       
      
        σ
        
          n
          u
          m
        
      
    
    \ \sigma _{num}
  , of about 1 meter (3 ft 3 in). The standard deviations, 
  
    
       
      
        σ
        
          R
        
      
    
    \ \sigma _{R}
  , for the coarse/acquisition (C/A) and precise codes are also shown in the table. These standard deviations are computed by taking the square root of the sum of the squares of the individual components (i.e., RSS for root sum squares)."
What is the estimated numerical error in GPS measurements?,1 meter (3 ft 3 in),3 meters (9 ft 10 in),10 meters (32 ft 10 in),100 meters (328 ft),No numerical error is present in GPS measurements,A,"The error analysis for the Global Positioning System is important for understanding how GPS works, and for knowing what magnitude of error should be expected. The GPS makes corrections for receiver clock errors and other effects but there are still residual errors which are not corrected. GPS receiver position is computed based on data received from the satellites. Errors depend on geometric dilution of precision and the sources listed in the table below.

Overview
User equivalent range errors (UERE) are shown in the table. There is also a numerical error with an estimated value, 
  
    
       
      
        σ
        
          n
          u
          m
        
      
    
    \ \sigma _{num}
  , of about 1 meter (3 ft 3 in). The standard deviations, 
  
    
       
      
        σ
        
          R
        
      
    
    \ \sigma _{R}
  , for the coarse/acquisition (C/A) and precise codes are also shown in the table. These standard deviations are computed by taking the square root of the sum of the squares of the individual components (i.e., RSS for root sum squares)."
How are the standard deviations (σR) for the coarse/acquisition (C/A) and precise codes computed?,By summing the individual components of the GPS system,By taking the square root of the sum of the squares of the individual components,By dividing the sum of the individual components by the number of components,By multiplying the individual components of the GPS system,By subtracting the individual components of the GPS system from each other,B,"The error analysis for the Global Positioning System is important for understanding how GPS works, and for knowing what magnitude of error should be expected. The GPS makes corrections for receiver clock errors and other effects but there are still residual errors which are not corrected. GPS receiver position is computed based on data received from the satellites. Errors depend on geometric dilution of precision and the sources listed in the table below.

Overview
User equivalent range errors (UERE) are shown in the table. There is also a numerical error with an estimated value, 
  
    
       
      
        σ
        
          n
          u
          m
        
      
    
    \ \sigma _{num}
  , of about 1 meter (3 ft 3 in). The standard deviations, 
  
    
       
      
        σ
        
          R
        
      
    
    \ \sigma _{R}
  , for the coarse/acquisition (C/A) and precise codes are also shown in the table. These standard deviations are computed by taking the square root of the sum of the squares of the individual components (i.e., RSS for root sum squares)."
What factors contribute to the residual errors in GPS measurements?,Geometric dilution of precision,Receiver clock errors,Other effects not corrected by the GPS system,Numerical errors in GPS measurements,All of the above,E,"The error analysis for the Global Positioning System is important for understanding how GPS works, and for knowing what magnitude of error should be expected. The GPS makes corrections for receiver clock errors and other effects but there are still residual errors which are not corrected. GPS receiver position is computed based on data received from the satellites. Errors depend on geometric dilution of precision and the sources listed in the table below.

Overview
User equivalent range errors (UERE) are shown in the table. There is also a numerical error with an estimated value, 
  
    
       
      
        σ
        
          n
          u
          m
        
      
    
    \ \sigma _{num}
  , of about 1 meter (3 ft 3 in). The standard deviations, 
  
    
       
      
        σ
        
          R
        
      
    
    \ \sigma _{R}
  , for the coarse/acquisition (C/A) and precise codes are also shown in the table. These standard deviations are computed by taking the square root of the sum of the squares of the individual components (i.e., RSS for root sum squares)."
What is the definition of ecological engineering according to Mitsch and Jorgensen?,Utilizing ecological science and theory,Applying engineering design methods,Designing societal services that benefit society and nature,Using natural energy sources to manipulate environmental systems,Conserving non-renewable energy sources,C,"Ecological engineering uses ecology and engineering to predict, design, construct or restore, and manage ecosystems that integrate ""human society with its natural environment for the benefit of both"".

Origins, key concepts, definitions, and applications
Ecological engineering emerged as a new idea in the early 1960s, but its definition has taken several decades to refine, its implementation is still undergoing adjustment, and its broader recognition as a new paradigm is relatively recent. Ecological engineering was introduced by Howard Odum and others as utilizing natural energy sources as the predominant input to manipulate and control environmental systems. The origins of ecological engineering are in Odum's work with ecological modeling and ecosystem simulation to capture holistic macro-patterns of energy and material flows affecting the efficient use of resources.
Mitsch and Jorgensen summarized five basic concepts that differentiate ecological engineering from other approaches to addressing problems to benefit society and nature: 1) it is based on the self-designing capacity of ecosystems; 2) it can be the field (or acid) test of ecological theories; 3) it relies on system approaches; 4) it conserves non-renewable energy sources; and 5) it supports ecosystem and biological conservation.
Mitsch and Jorgensen were the first to define ecological engineering as designing societal services such that they benefit society and nature, and later noted the design should be systems based, sustainable, and integrate society with its natural environment.
Bergen et al. defined ecological engineering as: 1) utilizing ecological science and theory; 2) applying to all types of ecosystems; 3) adapting engineering design methods; and 4) acknowledging a guiding value system.
Barrett (1999) offers a more literal definition of the term: ""the design, construction, operation and management (that is, engineering) of landscape/aquatic structures and associated plant and animal communities (that is, ecosystems) to benefit humanity and, often, nature."" Barrett continues: ""other terms with equivalent or similar meanings include ecotechnology and two terms most often used in the erosion control field: soil bioengineering and biotechnical engineering.  However, ecological engineering should not be confused with 'biotechnology' when describing genetic engineering at the cellular level, or 'bioengineering' meaning construction of artificial body parts.""
The applications in ecological engineering can be classified into 3 spatial scales: 1) mesocosms (~0.1 to hundreds of meters); 2) ecosystems (~one to tens of km); and 3) regional systems (>tens of km). The complexity of the design likely increases with the spatial scale. Applications are increasing in breadth and depth, and likely impacting the field's definition, as more opportunities to design and use ecosystems as interfaces between society and nature are explored."
Who introduced the concept of ecological engineering?,Howard Odum,Mitsch and Jorgensen,Barrett,Bergen et al.,"No one, it emerged naturally",A,"Ecological engineering uses ecology and engineering to predict, design, construct or restore, and manage ecosystems that integrate ""human society with its natural environment for the benefit of both"".

Origins, key concepts, definitions, and applications
Ecological engineering emerged as a new idea in the early 1960s, but its definition has taken several decades to refine, its implementation is still undergoing adjustment, and its broader recognition as a new paradigm is relatively recent. Ecological engineering was introduced by Howard Odum and others as utilizing natural energy sources as the predominant input to manipulate and control environmental systems. The origins of ecological engineering are in Odum's work with ecological modeling and ecosystem simulation to capture holistic macro-patterns of energy and material flows affecting the efficient use of resources.
Mitsch and Jorgensen summarized five basic concepts that differentiate ecological engineering from other approaches to addressing problems to benefit society and nature: 1) it is based on the self-designing capacity of ecosystems; 2) it can be the field (or acid) test of ecological theories; 3) it relies on system approaches; 4) it conserves non-renewable energy sources; and 5) it supports ecosystem and biological conservation.
Mitsch and Jorgensen were the first to define ecological engineering as designing societal services such that they benefit society and nature, and later noted the design should be systems based, sustainable, and integrate society with its natural environment.
Bergen et al. defined ecological engineering as: 1) utilizing ecological science and theory; 2) applying to all types of ecosystems; 3) adapting engineering design methods; and 4) acknowledging a guiding value system.
Barrett (1999) offers a more literal definition of the term: ""the design, construction, operation and management (that is, engineering) of landscape/aquatic structures and associated plant and animal communities (that is, ecosystems) to benefit humanity and, often, nature."" Barrett continues: ""other terms with equivalent or similar meanings include ecotechnology and two terms most often used in the erosion control field: soil bioengineering and biotechnical engineering.  However, ecological engineering should not be confused with 'biotechnology' when describing genetic engineering at the cellular level, or 'bioengineering' meaning construction of artificial body parts.""
The applications in ecological engineering can be classified into 3 spatial scales: 1) mesocosms (~0.1 to hundreds of meters); 2) ecosystems (~one to tens of km); and 3) regional systems (>tens of km). The complexity of the design likely increases with the spatial scale. Applications are increasing in breadth and depth, and likely impacting the field's definition, as more opportunities to design and use ecosystems as interfaces between society and nature are explored."
What are the three spatial scales of applications in ecological engineering?,"Microcosms, ecosystems, and global systems","Mesocosms, ecosystems, and regional systems","Local systems, national systems, and global systems","Mesocosms, ecosystems, and global systems","Microcosms, local systems, and regional systems",B,"Ecological engineering uses ecology and engineering to predict, design, construct or restore, and manage ecosystems that integrate ""human society with its natural environment for the benefit of both"".

Origins, key concepts, definitions, and applications
Ecological engineering emerged as a new idea in the early 1960s, but its definition has taken several decades to refine, its implementation is still undergoing adjustment, and its broader recognition as a new paradigm is relatively recent. Ecological engineering was introduced by Howard Odum and others as utilizing natural energy sources as the predominant input to manipulate and control environmental systems. The origins of ecological engineering are in Odum's work with ecological modeling and ecosystem simulation to capture holistic macro-patterns of energy and material flows affecting the efficient use of resources.
Mitsch and Jorgensen summarized five basic concepts that differentiate ecological engineering from other approaches to addressing problems to benefit society and nature: 1) it is based on the self-designing capacity of ecosystems; 2) it can be the field (or acid) test of ecological theories; 3) it relies on system approaches; 4) it conserves non-renewable energy sources; and 5) it supports ecosystem and biological conservation.
Mitsch and Jorgensen were the first to define ecological engineering as designing societal services such that they benefit society and nature, and later noted the design should be systems based, sustainable, and integrate society with its natural environment.
Bergen et al. defined ecological engineering as: 1) utilizing ecological science and theory; 2) applying to all types of ecosystems; 3) adapting engineering design methods; and 4) acknowledging a guiding value system.
Barrett (1999) offers a more literal definition of the term: ""the design, construction, operation and management (that is, engineering) of landscape/aquatic structures and associated plant and animal communities (that is, ecosystems) to benefit humanity and, often, nature."" Barrett continues: ""other terms with equivalent or similar meanings include ecotechnology and two terms most often used in the erosion control field: soil bioengineering and biotechnical engineering.  However, ecological engineering should not be confused with 'biotechnology' when describing genetic engineering at the cellular level, or 'bioengineering' meaning construction of artificial body parts.""
The applications in ecological engineering can be classified into 3 spatial scales: 1) mesocosms (~0.1 to hundreds of meters); 2) ecosystems (~one to tens of km); and 3) regional systems (>tens of km). The complexity of the design likely increases with the spatial scale. Applications are increasing in breadth and depth, and likely impacting the field's definition, as more opportunities to design and use ecosystems as interfaces between society and nature are explored."
What are the key concepts that differentiate ecological engineering?,Self-designing capacity of ecosystems,System approaches,Conserving non-renewable energy sources,Supporting ecosystem and biological conservation,All of the above,E,"Ecological engineering uses ecology and engineering to predict, design, construct or restore, and manage ecosystems that integrate ""human society with its natural environment for the benefit of both"".

Origins, key concepts, definitions, and applications
Ecological engineering emerged as a new idea in the early 1960s, but its definition has taken several decades to refine, its implementation is still undergoing adjustment, and its broader recognition as a new paradigm is relatively recent. Ecological engineering was introduced by Howard Odum and others as utilizing natural energy sources as the predominant input to manipulate and control environmental systems. The origins of ecological engineering are in Odum's work with ecological modeling and ecosystem simulation to capture holistic macro-patterns of energy and material flows affecting the efficient use of resources.
Mitsch and Jorgensen summarized five basic concepts that differentiate ecological engineering from other approaches to addressing problems to benefit society and nature: 1) it is based on the self-designing capacity of ecosystems; 2) it can be the field (or acid) test of ecological theories; 3) it relies on system approaches; 4) it conserves non-renewable energy sources; and 5) it supports ecosystem and biological conservation.
Mitsch and Jorgensen were the first to define ecological engineering as designing societal services such that they benefit society and nature, and later noted the design should be systems based, sustainable, and integrate society with its natural environment.
Bergen et al. defined ecological engineering as: 1) utilizing ecological science and theory; 2) applying to all types of ecosystems; 3) adapting engineering design methods; and 4) acknowledging a guiding value system.
Barrett (1999) offers a more literal definition of the term: ""the design, construction, operation and management (that is, engineering) of landscape/aquatic structures and associated plant and animal communities (that is, ecosystems) to benefit humanity and, often, nature."" Barrett continues: ""other terms with equivalent or similar meanings include ecotechnology and two terms most often used in the erosion control field: soil bioengineering and biotechnical engineering.  However, ecological engineering should not be confused with 'biotechnology' when describing genetic engineering at the cellular level, or 'bioengineering' meaning construction of artificial body parts.""
The applications in ecological engineering can be classified into 3 spatial scales: 1) mesocosms (~0.1 to hundreds of meters); 2) ecosystems (~one to tens of km); and 3) regional systems (>tens of km). The complexity of the design likely increases with the spatial scale. Applications are increasing in breadth and depth, and likely impacting the field's definition, as more opportunities to design and use ecosystems as interfaces between society and nature are explored."
What does ecological engineering aim to integrate?,Human society with its natural environment,Ecological science with engineering principles,Biology with technology,Ecosystems with renewable energy sources,All of the above,A,"Ecological engineering uses ecology and engineering to predict, design, construct or restore, and manage ecosystems that integrate ""human society with its natural environment for the benefit of both"".

Origins, key concepts, definitions, and applications
Ecological engineering emerged as a new idea in the early 1960s, but its definition has taken several decades to refine, its implementation is still undergoing adjustment, and its broader recognition as a new paradigm is relatively recent. Ecological engineering was introduced by Howard Odum and others as utilizing natural energy sources as the predominant input to manipulate and control environmental systems. The origins of ecological engineering are in Odum's work with ecological modeling and ecosystem simulation to capture holistic macro-patterns of energy and material flows affecting the efficient use of resources.
Mitsch and Jorgensen summarized five basic concepts that differentiate ecological engineering from other approaches to addressing problems to benefit society and nature: 1) it is based on the self-designing capacity of ecosystems; 2) it can be the field (or acid) test of ecological theories; 3) it relies on system approaches; 4) it conserves non-renewable energy sources; and 5) it supports ecosystem and biological conservation.
Mitsch and Jorgensen were the first to define ecological engineering as designing societal services such that they benefit society and nature, and later noted the design should be systems based, sustainable, and integrate society with its natural environment.
Bergen et al. defined ecological engineering as: 1) utilizing ecological science and theory; 2) applying to all types of ecosystems; 3) adapting engineering design methods; and 4) acknowledging a guiding value system.
Barrett (1999) offers a more literal definition of the term: ""the design, construction, operation and management (that is, engineering) of landscape/aquatic structures and associated plant and animal communities (that is, ecosystems) to benefit humanity and, often, nature."" Barrett continues: ""other terms with equivalent or similar meanings include ecotechnology and two terms most often used in the erosion control field: soil bioengineering and biotechnical engineering.  However, ecological engineering should not be confused with 'biotechnology' when describing genetic engineering at the cellular level, or 'bioengineering' meaning construction of artificial body parts.""
The applications in ecological engineering can be classified into 3 spatial scales: 1) mesocosms (~0.1 to hundreds of meters); 2) ecosystems (~one to tens of km); and 3) regional systems (>tens of km). The complexity of the design likely increases with the spatial scale. Applications are increasing in breadth and depth, and likely impacting the field's definition, as more opportunities to design and use ecosystems as interfaces between society and nature are explored."
What does the National Electrical Manufacturers Association (NEMA) define standards for?,Electrical equipment in hazardous areas,Electrical enclosures used in industrial applications,NEMA connectors for electrical equipment,NEMA 250- Enclosures for Electrical Equipment,UL enclosure ratings,B,"The National Electrical Manufacturers Association (NEMA) defines standards used in North America for various grades of electrical enclosures typically used in industrial applications.  Each is rated to protect against personal access to hazardous parts, and additional type-dependent designated environmental conditions.  A typical NEMA enclosure might be rated to provide protection against environmental hazards such as water, dust, oil or coolant or atmospheres containing corrosive agents such as acetylene or gasoline.  A full list of NEMA enclosure types is available for download from the NEMA website.

Enclosure types
Below is a list of NEMA enclosure types; these types are further defined in NEMA 250- Enclosures for Electrical Equipment.  Each type specifies characteristics of an enclosure, but not, for example, a specific enclosure size. Note that higher numbers do not include the lower-numbered tests.  For example, types 3, 4 and 6 are intended for outdoor use, but type 5 is not.
A NEMA enclosure rating does not mean that it also meets the same UL enclosure rating.NFPA is National Fire Protection Association, and NEC is National Electrical Code (U.S.A.)

See also
Electrical equipment in hazardous areas
NEMA connector – Another common, but mostly unrelated, set of standards from NEMA


== References ==."
What are some environmental hazards that a NEMA enclosure might protect against?,Water and dust only,Oil and coolant only,Atmospheres containing corrosive agents only,"Water, dust, oil or coolant, and atmospheres containing corrosive agents",None of the above,D,"The National Electrical Manufacturers Association (NEMA) defines standards used in North America for various grades of electrical enclosures typically used in industrial applications.  Each is rated to protect against personal access to hazardous parts, and additional type-dependent designated environmental conditions.  A typical NEMA enclosure might be rated to provide protection against environmental hazards such as water, dust, oil or coolant or atmospheres containing corrosive agents such as acetylene or gasoline.  A full list of NEMA enclosure types is available for download from the NEMA website.

Enclosure types
Below is a list of NEMA enclosure types; these types are further defined in NEMA 250- Enclosures for Electrical Equipment.  Each type specifies characteristics of an enclosure, but not, for example, a specific enclosure size. Note that higher numbers do not include the lower-numbered tests.  For example, types 3, 4 and 6 are intended for outdoor use, but type 5 is not.
A NEMA enclosure rating does not mean that it also meets the same UL enclosure rating.NFPA is National Fire Protection Association, and NEC is National Electrical Code (U.S.A.)

See also
Electrical equipment in hazardous areas
NEMA connector – Another common, but mostly unrelated, set of standards from NEMA


== References ==."
Are higher NEMA enclosure types inclusive of lower-numbered tests?,"Yes, they include all lower-numbered tests","No, they do not include all lower-numbered tests",It depends on the specific enclosure size,Higher NEMA enclosure types are intended for outdoor use,None of the above,B,"The National Electrical Manufacturers Association (NEMA) defines standards used in North America for various grades of electrical enclosures typically used in industrial applications.  Each is rated to protect against personal access to hazardous parts, and additional type-dependent designated environmental conditions.  A typical NEMA enclosure might be rated to provide protection against environmental hazards such as water, dust, oil or coolant or atmospheres containing corrosive agents such as acetylene or gasoline.  A full list of NEMA enclosure types is available for download from the NEMA website.

Enclosure types
Below is a list of NEMA enclosure types; these types are further defined in NEMA 250- Enclosures for Electrical Equipment.  Each type specifies characteristics of an enclosure, but not, for example, a specific enclosure size. Note that higher numbers do not include the lower-numbered tests.  For example, types 3, 4 and 6 are intended for outdoor use, but type 5 is not.
A NEMA enclosure rating does not mean that it also meets the same UL enclosure rating.NFPA is National Fire Protection Association, and NEC is National Electrical Code (U.S.A.)

See also
Electrical equipment in hazardous areas
NEMA connector – Another common, but mostly unrelated, set of standards from NEMA


== References ==."
What does a NEMA enclosure rating not necessarily indicate?,It does not indicate protection against personal access to hazardous parts,It does not indicate additional type-dependent designated environmental conditions,It does not indicate compliance with UL enclosure ratings,It does not indicate compliance with NFPA and NEC standards,None of the above,C,"The National Electrical Manufacturers Association (NEMA) defines standards used in North America for various grades of electrical enclosures typically used in industrial applications.  Each is rated to protect against personal access to hazardous parts, and additional type-dependent designated environmental conditions.  A typical NEMA enclosure might be rated to provide protection against environmental hazards such as water, dust, oil or coolant or atmospheres containing corrosive agents such as acetylene or gasoline.  A full list of NEMA enclosure types is available for download from the NEMA website.

Enclosure types
Below is a list of NEMA enclosure types; these types are further defined in NEMA 250- Enclosures for Electrical Equipment.  Each type specifies characteristics of an enclosure, but not, for example, a specific enclosure size. Note that higher numbers do not include the lower-numbered tests.  For example, types 3, 4 and 6 are intended for outdoor use, but type 5 is not.
A NEMA enclosure rating does not mean that it also meets the same UL enclosure rating.NFPA is National Fire Protection Association, and NEC is National Electrical Code (U.S.A.)

See also
Electrical equipment in hazardous areas
NEMA connector – Another common, but mostly unrelated, set of standards from NEMA


== References ==."
Which organization is responsible for the NEMA standards?,UL (Underwriters Laboratories),NFPA (National Fire Protection Association),NEC (National Electrical Code),NEMA (National Electrical Manufacturers Association),None of the above,D,"The National Electrical Manufacturers Association (NEMA) defines standards used in North America for various grades of electrical enclosures typically used in industrial applications.  Each is rated to protect against personal access to hazardous parts, and additional type-dependent designated environmental conditions.  A typical NEMA enclosure might be rated to provide protection against environmental hazards such as water, dust, oil or coolant or atmospheres containing corrosive agents such as acetylene or gasoline.  A full list of NEMA enclosure types is available for download from the NEMA website.

Enclosure types
Below is a list of NEMA enclosure types; these types are further defined in NEMA 250- Enclosures for Electrical Equipment.  Each type specifies characteristics of an enclosure, but not, for example, a specific enclosure size. Note that higher numbers do not include the lower-numbered tests.  For example, types 3, 4 and 6 are intended for outdoor use, but type 5 is not.
A NEMA enclosure rating does not mean that it also meets the same UL enclosure rating.NFPA is National Fire Protection Association, and NEC is National Electrical Code (U.S.A.)

See also
Electrical equipment in hazardous areas
NEMA connector – Another common, but mostly unrelated, set of standards from NEMA


== References ==."
What is the main objective of cost engineering?,To manage project cost,To estimate project cost,To control project cost,To forecast project cost,To appraise project cost,A,"Cost engineering is ""the engineering practice devoted to the management of project cost, involving such activities as estimating, cost control, cost forecasting, investment appraisal and risk analysis"". ""Cost Engineers budget, plan and monitor investment projects. They seek the optimum balance between cost, quality and time requirements.""Skills and knowledge of cost engineers are similar to those of quantity surveyors.  In many industries, cost engineering is synonymous with project controls.  As the title ""engineer"" has legal requirements in many jurisdictions (i.e. Texas, Canada), the cost engineering discipline is often renamed to project controls.A cost engineer is ""an engineer whose judgment and experience are utilized in the application of scientific principles and techniques to problems of estimation; cost control; business planning and management science; profitability analysis; project management; and planning and scheduling"".

Overview
One key objective of cost engineering is to arrive at accurate cost estimates and schedules, and to avoid cost overruns and schedule slips. Cost engineering goes beyond preparing cost estimates and schedules by helping manage resources and supporting assessment and decision-making."
What are the activities involved in cost engineering?,Estimating and cost control,Cost forecasting and risk analysis,Investment appraisal and quantity surveying,Project controls and risk analysis,Project controls and quantity surveying,B,"Cost engineering is ""the engineering practice devoted to the management of project cost, involving such activities as estimating, cost control, cost forecasting, investment appraisal and risk analysis"". ""Cost Engineers budget, plan and monitor investment projects. They seek the optimum balance between cost, quality and time requirements.""Skills and knowledge of cost engineers are similar to those of quantity surveyors.  In many industries, cost engineering is synonymous with project controls.  As the title ""engineer"" has legal requirements in many jurisdictions (i.e. Texas, Canada), the cost engineering discipline is often renamed to project controls.A cost engineer is ""an engineer whose judgment and experience are utilized in the application of scientific principles and techniques to problems of estimation; cost control; business planning and management science; profitability analysis; project management; and planning and scheduling"".

Overview
One key objective of cost engineering is to arrive at accurate cost estimates and schedules, and to avoid cost overruns and schedule slips. Cost engineering goes beyond preparing cost estimates and schedules by helping manage resources and supporting assessment and decision-making."
What skills and knowledge do cost engineers possess?,Similar to quantity surveyors,Similar to project managers,Similar to investment analysts,Similar to risk analysts,Similar to business planners,A,"Cost engineering is ""the engineering practice devoted to the management of project cost, involving such activities as estimating, cost control, cost forecasting, investment appraisal and risk analysis"". ""Cost Engineers budget, plan and monitor investment projects. They seek the optimum balance between cost, quality and time requirements.""Skills and knowledge of cost engineers are similar to those of quantity surveyors.  In many industries, cost engineering is synonymous with project controls.  As the title ""engineer"" has legal requirements in many jurisdictions (i.e. Texas, Canada), the cost engineering discipline is often renamed to project controls.A cost engineer is ""an engineer whose judgment and experience are utilized in the application of scientific principles and techniques to problems of estimation; cost control; business planning and management science; profitability analysis; project management; and planning and scheduling"".

Overview
One key objective of cost engineering is to arrive at accurate cost estimates and schedules, and to avoid cost overruns and schedule slips. Cost engineering goes beyond preparing cost estimates and schedules by helping manage resources and supporting assessment and decision-making."
What is another term used interchangeably with cost engineering in many industries?,Project appraisal,Project estimation,Project management,Project controls,Project scheduling,D,"Cost engineering is ""the engineering practice devoted to the management of project cost, involving such activities as estimating, cost control, cost forecasting, investment appraisal and risk analysis"". ""Cost Engineers budget, plan and monitor investment projects. They seek the optimum balance between cost, quality and time requirements.""Skills and knowledge of cost engineers are similar to those of quantity surveyors.  In many industries, cost engineering is synonymous with project controls.  As the title ""engineer"" has legal requirements in many jurisdictions (i.e. Texas, Canada), the cost engineering discipline is often renamed to project controls.A cost engineer is ""an engineer whose judgment and experience are utilized in the application of scientific principles and techniques to problems of estimation; cost control; business planning and management science; profitability analysis; project management; and planning and scheduling"".

Overview
One key objective of cost engineering is to arrive at accurate cost estimates and schedules, and to avoid cost overruns and schedule slips. Cost engineering goes beyond preparing cost estimates and schedules by helping manage resources and supporting assessment and decision-making."
What is the role of a cost engineer?,To prepare cost estimates and schedules,To manage resources and support decision-making,To analyze profitability and manage projects,To plan and schedule projects,To control cost overruns and schedule slips,B,"Cost engineering is ""the engineering practice devoted to the management of project cost, involving such activities as estimating, cost control, cost forecasting, investment appraisal and risk analysis"". ""Cost Engineers budget, plan and monitor investment projects. They seek the optimum balance between cost, quality and time requirements.""Skills and knowledge of cost engineers are similar to those of quantity surveyors.  In many industries, cost engineering is synonymous with project controls.  As the title ""engineer"" has legal requirements in many jurisdictions (i.e. Texas, Canada), the cost engineering discipline is often renamed to project controls.A cost engineer is ""an engineer whose judgment and experience are utilized in the application of scientific principles and techniques to problems of estimation; cost control; business planning and management science; profitability analysis; project management; and planning and scheduling"".

Overview
One key objective of cost engineering is to arrive at accurate cost estimates and schedules, and to avoid cost overruns and schedule slips. Cost engineering goes beyond preparing cost estimates and schedules by helping manage resources and supporting assessment and decision-making."
What is the dimension of the space of dual quaternions?,4,6,8,10,12,C,"In mathematics, the dual quaternions are an 8-dimensional real algebra isomorphic to the tensor product of the quaternions and the dual numbers.  Thus, they may be constructed in the same way as the quaternions, except using dual numbers instead of real numbers as coefficients.  A dual quaternion can be represented in the form A + εB, where A and B are ordinary quaternions and ε is the dual unit, which satisfies ε2 = 0 and commutes with every element of the algebra.  
Unlike quaternions, the dual quaternions do not form a division algebra.
In mechanics, the dual quaternions are applied as a number system to represent rigid transformations in three dimensions.   Since the space of dual quaternions is 8-dimensional and a rigid transformation has six real degrees of freedom, three for translations and three for rotations, dual quaternions obeying two algebraic constraints are used in this application.
Similar to the way that rotations in 3D space can be represented by quaternions of unit length, rigid motions in 3D space can be represented by dual quaternions of unit length. This fact is used in theoretical kinematics (see McCarthy), and in applications to 3D computer graphics, robotics and computer vision. Polynomials with coefficients given by (non-zero real norm) dual quaternions have also been used in the context of mechanical linkages design.

History
W."
Which algebra are the dual quaternions isomorphic to?,Quaternion algebra,Complex numbers,Real numbers,Dual numbers,Tensor product,E,"In mathematics, the dual quaternions are an 8-dimensional real algebra isomorphic to the tensor product of the quaternions and the dual numbers.  Thus, they may be constructed in the same way as the quaternions, except using dual numbers instead of real numbers as coefficients.  A dual quaternion can be represented in the form A + εB, where A and B are ordinary quaternions and ε is the dual unit, which satisfies ε2 = 0 and commutes with every element of the algebra.  
Unlike quaternions, the dual quaternions do not form a division algebra.
In mechanics, the dual quaternions are applied as a number system to represent rigid transformations in three dimensions.   Since the space of dual quaternions is 8-dimensional and a rigid transformation has six real degrees of freedom, three for translations and three for rotations, dual quaternions obeying two algebraic constraints are used in this application.
Similar to the way that rotations in 3D space can be represented by quaternions of unit length, rigid motions in 3D space can be represented by dual quaternions of unit length. This fact is used in theoretical kinematics (see McCarthy), and in applications to 3D computer graphics, robotics and computer vision. Polynomials with coefficients given by (non-zero real norm) dual quaternions have also been used in the context of mechanical linkages design.

History
W."
What is the form of a dual quaternion?,A + εB,A - εB,A + B,A - B,A * B,A,"In mathematics, the dual quaternions are an 8-dimensional real algebra isomorphic to the tensor product of the quaternions and the dual numbers.  Thus, they may be constructed in the same way as the quaternions, except using dual numbers instead of real numbers as coefficients.  A dual quaternion can be represented in the form A + εB, where A and B are ordinary quaternions and ε is the dual unit, which satisfies ε2 = 0 and commutes with every element of the algebra.  
Unlike quaternions, the dual quaternions do not form a division algebra.
In mechanics, the dual quaternions are applied as a number system to represent rigid transformations in three dimensions.   Since the space of dual quaternions is 8-dimensional and a rigid transformation has six real degrees of freedom, three for translations and three for rotations, dual quaternions obeying two algebraic constraints are used in this application.
Similar to the way that rotations in 3D space can be represented by quaternions of unit length, rigid motions in 3D space can be represented by dual quaternions of unit length. This fact is used in theoretical kinematics (see McCarthy), and in applications to 3D computer graphics, robotics and computer vision. Polynomials with coefficients given by (non-zero real norm) dual quaternions have also been used in the context of mechanical linkages design.

History
W."
What is the dual unit in the dual quaternion algebra?,ε,i,j,k,1,A,"In mathematics, the dual quaternions are an 8-dimensional real algebra isomorphic to the tensor product of the quaternions and the dual numbers.  Thus, they may be constructed in the same way as the quaternions, except using dual numbers instead of real numbers as coefficients.  A dual quaternion can be represented in the form A + εB, where A and B are ordinary quaternions and ε is the dual unit, which satisfies ε2 = 0 and commutes with every element of the algebra.  
Unlike quaternions, the dual quaternions do not form a division algebra.
In mechanics, the dual quaternions are applied as a number system to represent rigid transformations in three dimensions.   Since the space of dual quaternions is 8-dimensional and a rigid transformation has six real degrees of freedom, three for translations and three for rotations, dual quaternions obeying two algebraic constraints are used in this application.
Similar to the way that rotations in 3D space can be represented by quaternions of unit length, rigid motions in 3D space can be represented by dual quaternions of unit length. This fact is used in theoretical kinematics (see McCarthy), and in applications to 3D computer graphics, robotics and computer vision. Polynomials with coefficients given by (non-zero real norm) dual quaternions have also been used in the context of mechanical linkages design.

History
W."
Do dual quaternions form a division algebra?,Yes,No,Sometimes,Not sure,Maybe,B,"In mathematics, the dual quaternions are an 8-dimensional real algebra isomorphic to the tensor product of the quaternions and the dual numbers.  Thus, they may be constructed in the same way as the quaternions, except using dual numbers instead of real numbers as coefficients.  A dual quaternion can be represented in the form A + εB, where A and B are ordinary quaternions and ε is the dual unit, which satisfies ε2 = 0 and commutes with every element of the algebra.  
Unlike quaternions, the dual quaternions do not form a division algebra.
In mechanics, the dual quaternions are applied as a number system to represent rigid transformations in three dimensions.   Since the space of dual quaternions is 8-dimensional and a rigid transformation has six real degrees of freedom, three for translations and three for rotations, dual quaternions obeying two algebraic constraints are used in this application.
Similar to the way that rotations in 3D space can be represented by quaternions of unit length, rigid motions in 3D space can be represented by dual quaternions of unit length. This fact is used in theoretical kinematics (see McCarthy), and in applications to 3D computer graphics, robotics and computer vision. Polynomials with coefficients given by (non-zero real norm) dual quaternions have also been used in the context of mechanical linkages design.

History
W."
What is the role of a modern watchmaker?,Creating customized watches from scratch,Repairing watches using factory-made spare parts,Designing and manufacturing all the parts of a watch by hand,Specializing in making and repairing clocks,Crafting replacement parts for older watches,B,"A watchmaker is an artisan who makes and repairs watches. Since a majority of watches are now factory-made, most modern watchmakers only repair watches. However, originally they were master craftsmen who built watches, including all their parts, by hand. Modern watchmakers, when required to repair older watches, for which replacement parts may not be available, must have fabrication skills, and can typically manufacture replacements for many of the parts found in a watch. The term clockmaker refers to an equivalent occupation specializing in clocks.
Most practising professional watchmakers service current or recent production watches.  They seldom fabricate replacement parts. Instead they obtain and fit factory spare parts applicable to the watch brand being serviced."
What skills are required for a watchmaker to repair older watches?,Knowledge of different watch brands,Ability to fabricate replacement parts,Expertise in manufacturing all parts of a watch by hand,Experience in designing customized watches,Specialization in clock repair,B,"A watchmaker is an artisan who makes and repairs watches. Since a majority of watches are now factory-made, most modern watchmakers only repair watches. However, originally they were master craftsmen who built watches, including all their parts, by hand. Modern watchmakers, when required to repair older watches, for which replacement parts may not be available, must have fabrication skills, and can typically manufacture replacements for many of the parts found in a watch. The term clockmaker refers to an equivalent occupation specializing in clocks.
Most practising professional watchmakers service current or recent production watches.  They seldom fabricate replacement parts. Instead they obtain and fit factory spare parts applicable to the watch brand being serviced."
What is the main difference between a watchmaker and a clockmaker?,"Watchmakers specialize in repairing watches, while clockmakers specialize in repairing clocks","Watchmakers create customized watches, while clockmakers create customized clocks","Watchmakers fabricate replacement parts, while clockmakers do not","Watchmakers use factory-made spare parts, while clockmakers manufacture all the parts by hand","Watchmakers service current watches, while clockmakers service older watches",A,"A watchmaker is an artisan who makes and repairs watches. Since a majority of watches are now factory-made, most modern watchmakers only repair watches. However, originally they were master craftsmen who built watches, including all their parts, by hand. Modern watchmakers, when required to repair older watches, for which replacement parts may not be available, must have fabrication skills, and can typically manufacture replacements for many of the parts found in a watch. The term clockmaker refers to an equivalent occupation specializing in clocks.
Most practising professional watchmakers service current or recent production watches.  They seldom fabricate replacement parts. Instead they obtain and fit factory spare parts applicable to the watch brand being serviced."
What do most practising professional watchmakers do?,Manufacture replacement parts for current watches,Repair watches using factory-made spare parts,Design and build customized watches,Specialize in clock repair,Create handmade watches from scratch,B,"A watchmaker is an artisan who makes and repairs watches. Since a majority of watches are now factory-made, most modern watchmakers only repair watches. However, originally they were master craftsmen who built watches, including all their parts, by hand. Modern watchmakers, when required to repair older watches, for which replacement parts may not be available, must have fabrication skills, and can typically manufacture replacements for many of the parts found in a watch. The term clockmaker refers to an equivalent occupation specializing in clocks.
Most practising professional watchmakers service current or recent production watches.  They seldom fabricate replacement parts. Instead they obtain and fit factory spare parts applicable to the watch brand being serviced."
What types of watches do modern watchmakers mainly service?,Customized watches built from scratch,Watches with rare and expensive parts,Older watches for which replacement parts are not available,Watches of a specific brand,Clocks that require expert repair,D,"A watchmaker is an artisan who makes and repairs watches. Since a majority of watches are now factory-made, most modern watchmakers only repair watches. However, originally they were master craftsmen who built watches, including all their parts, by hand. Modern watchmakers, when required to repair older watches, for which replacement parts may not be available, must have fabrication skills, and can typically manufacture replacements for many of the parts found in a watch. The term clockmaker refers to an equivalent occupation specializing in clocks.
Most practising professional watchmakers service current or recent production watches.  They seldom fabricate replacement parts. Instead they obtain and fit factory spare parts applicable to the watch brand being serviced."
What is the mainstream wiring system in Hong Kong?,BS 1365,BS 546,BS 3676,BS 1363,BS 5321,D,"This article exists to give readers an insight into electrical wiring in Hong Kong.

Overview
BS 1363 is the mainstream wiring system in Hong Kong. In old buildings, the BS 546 system is also common.
Due to its proximity to mainland China, electrical products from there are present in Hong Kong, especially those as a result of cross-border purchase carried out by mainland Chinese immigrants. Nevertheless, even if the product meets the safety requirements in China, they are not necessarily confirmed to BS 1363 standard, thus may have plug and socket compatibility issues when using them in Hong Kong.
Before the enforcement of the Electrical Products (Safety) Regulation, many types of plugs could be found in Hong Kong. Using this old equipment sometimes leads to plug and socket mating problems. Nowadays, virtually all plugs fitted in equipment sold in Hong Kong are BS 1363 compatible.
Many overseas citizens who bring equipment in from their home countries do not use the British wiring system. Citizens from the United Kingdom and countries that use British wiring systems normally do not encounter plug-and-socket issues in Hong Kong.

Case specific analysis
Mainland China two-pin plugs
The 2-pin plugs (with round or flat pins) in mainland China are compatible with the British converter plugs sold in Hong Kong. For shavers and electric toothbrushes that take less than 1A of current, British shaver adaptors can be also used.

Mainland China three-pin plugs
Mainland China three-pin plugs cannot be converted with a British standard adaptors, which are commonly sold in Hong Kong."
Which wiring system is common in old buildings in Hong Kong?,BS 1365,BS 546,BS 3676,BS 1363,BS 5321,B,"This article exists to give readers an insight into electrical wiring in Hong Kong.

Overview
BS 1363 is the mainstream wiring system in Hong Kong. In old buildings, the BS 546 system is also common.
Due to its proximity to mainland China, electrical products from there are present in Hong Kong, especially those as a result of cross-border purchase carried out by mainland Chinese immigrants. Nevertheless, even if the product meets the safety requirements in China, they are not necessarily confirmed to BS 1363 standard, thus may have plug and socket compatibility issues when using them in Hong Kong.
Before the enforcement of the Electrical Products (Safety) Regulation, many types of plugs could be found in Hong Kong. Using this old equipment sometimes leads to plug and socket mating problems. Nowadays, virtually all plugs fitted in equipment sold in Hong Kong are BS 1363 compatible.
Many overseas citizens who bring equipment in from their home countries do not use the British wiring system. Citizens from the United Kingdom and countries that use British wiring systems normally do not encounter plug-and-socket issues in Hong Kong.

Case specific analysis
Mainland China two-pin plugs
The 2-pin plugs (with round or flat pins) in mainland China are compatible with the British converter plugs sold in Hong Kong. For shavers and electric toothbrushes that take less than 1A of current, British shaver adaptors can be also used.

Mainland China three-pin plugs
Mainland China three-pin plugs cannot be converted with a British standard adaptors, which are commonly sold in Hong Kong."
What are the plug and socket compatibility issues in Hong Kong?,Issues with mainland Chinese products,Issues with British products,Issues with overseas products,Issues with old equipment,Issues with new equipment,D,"This article exists to give readers an insight into electrical wiring in Hong Kong.

Overview
BS 1363 is the mainstream wiring system in Hong Kong. In old buildings, the BS 546 system is also common.
Due to its proximity to mainland China, electrical products from there are present in Hong Kong, especially those as a result of cross-border purchase carried out by mainland Chinese immigrants. Nevertheless, even if the product meets the safety requirements in China, they are not necessarily confirmed to BS 1363 standard, thus may have plug and socket compatibility issues when using them in Hong Kong.
Before the enforcement of the Electrical Products (Safety) Regulation, many types of plugs could be found in Hong Kong. Using this old equipment sometimes leads to plug and socket mating problems. Nowadays, virtually all plugs fitted in equipment sold in Hong Kong are BS 1363 compatible.
Many overseas citizens who bring equipment in from their home countries do not use the British wiring system. Citizens from the United Kingdom and countries that use British wiring systems normally do not encounter plug-and-socket issues in Hong Kong.

Case specific analysis
Mainland China two-pin plugs
The 2-pin plugs (with round or flat pins) in mainland China are compatible with the British converter plugs sold in Hong Kong. For shavers and electric toothbrushes that take less than 1A of current, British shaver adaptors can be also used.

Mainland China three-pin plugs
Mainland China three-pin plugs cannot be converted with a British standard adaptors, which are commonly sold in Hong Kong."
Which plugs are compatible with the British converter plugs sold in Hong Kong?,Mainland China two-pin plugs,Mainland China three-pin plugs,British shaver adaptors,British standard adaptors,British converter plugs,A,"This article exists to give readers an insight into electrical wiring in Hong Kong.

Overview
BS 1363 is the mainstream wiring system in Hong Kong. In old buildings, the BS 546 system is also common.
Due to its proximity to mainland China, electrical products from there are present in Hong Kong, especially those as a result of cross-border purchase carried out by mainland Chinese immigrants. Nevertheless, even if the product meets the safety requirements in China, they are not necessarily confirmed to BS 1363 standard, thus may have plug and socket compatibility issues when using them in Hong Kong.
Before the enforcement of the Electrical Products (Safety) Regulation, many types of plugs could be found in Hong Kong. Using this old equipment sometimes leads to plug and socket mating problems. Nowadays, virtually all plugs fitted in equipment sold in Hong Kong are BS 1363 compatible.
Many overseas citizens who bring equipment in from their home countries do not use the British wiring system. Citizens from the United Kingdom and countries that use British wiring systems normally do not encounter plug-and-socket issues in Hong Kong.

Case specific analysis
Mainland China two-pin plugs
The 2-pin plugs (with round or flat pins) in mainland China are compatible with the British converter plugs sold in Hong Kong. For shavers and electric toothbrushes that take less than 1A of current, British shaver adaptors can be also used.

Mainland China three-pin plugs
Mainland China three-pin plugs cannot be converted with a British standard adaptors, which are commonly sold in Hong Kong."
Which group of citizens normally do not encounter plug-and-socket issues in Hong Kong?,Citizens from the United Kingdom,Citizens from countries with British wiring systems,Citizens from mainland China,Citizens from overseas countries,Citizens from Hong Kong,B,"This article exists to give readers an insight into electrical wiring in Hong Kong.

Overview
BS 1363 is the mainstream wiring system in Hong Kong. In old buildings, the BS 546 system is also common.
Due to its proximity to mainland China, electrical products from there are present in Hong Kong, especially those as a result of cross-border purchase carried out by mainland Chinese immigrants. Nevertheless, even if the product meets the safety requirements in China, they are not necessarily confirmed to BS 1363 standard, thus may have plug and socket compatibility issues when using them in Hong Kong.
Before the enforcement of the Electrical Products (Safety) Regulation, many types of plugs could be found in Hong Kong. Using this old equipment sometimes leads to plug and socket mating problems. Nowadays, virtually all plugs fitted in equipment sold in Hong Kong are BS 1363 compatible.
Many overseas citizens who bring equipment in from their home countries do not use the British wiring system. Citizens from the United Kingdom and countries that use British wiring systems normally do not encounter plug-and-socket issues in Hong Kong.

Case specific analysis
Mainland China two-pin plugs
The 2-pin plugs (with round or flat pins) in mainland China are compatible with the British converter plugs sold in Hong Kong. For shavers and electric toothbrushes that take less than 1A of current, British shaver adaptors can be also used.

Mainland China three-pin plugs
Mainland China three-pin plugs cannot be converted with a British standard adaptors, which are commonly sold in Hong Kong."
What is Infrastructure as code (IaC)?,The process of managing and provisioning computer data centers through physical hardware configuration,The process of managing and provisioning computer data centers through machine-readable definition files,The process of managing and provisioning computer data centers through interactive configuration tools,The process of managing and provisioning computer data centers through version control system,The process of managing and provisioning computer data centers through declarative definitions,B,"Infrastructure as code (IaC) is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools.
The IT infrastructure managed by this process comprises both physical equipment, such as bare-metal servers, as well as virtual machines, and associated configuration resources.
The definitions may be in a version control system.
The code in the definition files may use either scripts or declarative definitions, rather than maintaining the code through manual processes, but IaC more often employs declarative approaches.

Overview
IaC grew as a response to the difficulty posed by utility computing and second-generation web frameworks. In 2006, the launch of Amazon Web Services’ Elastic Compute Cloud and the 1.0 version of Ruby on Rails just months before created widespread scaling problems in the enterprise that were previously experienced only at large, multi-national companies. With new tools emerging to handle this ever-growing field, the idea of IaC was born. The thought of modeling infrastructure with code, and then having the ability to design, implement, and deploy application infrastructure with known software best practices appealed to both software developers and IT infrastructure administrators. The ability to treat infrastructure as code and use the same tools as any other software project would allow developers to rapidly deploy applications.

Advantages
The value of IaC can be broken down into three measurable categories: cost, speed, and risk. Cost reduction aims at helping not only the enterprise financially, but also in terms of people and effort, meaning that by removing the manual component, people are able to refocus their efforts on other enterprise tasks. Infrastructure automation enables speed through faster execution when configuring your infrastructure and aims at providing visibility to help other teams across the enterprise work quickly and more efficiently."
What does IaC manage?,"Only physical equipment, such as bare-metal servers",Only virtual machines,Both physical and virtual infrastructure,Only configuration resources,Only version control system,C,"Infrastructure as code (IaC) is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools.
The IT infrastructure managed by this process comprises both physical equipment, such as bare-metal servers, as well as virtual machines, and associated configuration resources.
The definitions may be in a version control system.
The code in the definition files may use either scripts or declarative definitions, rather than maintaining the code through manual processes, but IaC more often employs declarative approaches.

Overview
IaC grew as a response to the difficulty posed by utility computing and second-generation web frameworks. In 2006, the launch of Amazon Web Services’ Elastic Compute Cloud and the 1.0 version of Ruby on Rails just months before created widespread scaling problems in the enterprise that were previously experienced only at large, multi-national companies. With new tools emerging to handle this ever-growing field, the idea of IaC was born. The thought of modeling infrastructure with code, and then having the ability to design, implement, and deploy application infrastructure with known software best practices appealed to both software developers and IT infrastructure administrators. The ability to treat infrastructure as code and use the same tools as any other software project would allow developers to rapidly deploy applications.

Advantages
The value of IaC can be broken down into three measurable categories: cost, speed, and risk. Cost reduction aims at helping not only the enterprise financially, but also in terms of people and effort, meaning that by removing the manual component, people are able to refocus their efforts on other enterprise tasks. Infrastructure automation enables speed through faster execution when configuring your infrastructure and aims at providing visibility to help other teams across the enterprise work quickly and more efficiently."
Which approach does IaC more often employ?,Scripts,Declarative definitions,Physical hardware configuration,Interactive configuration tools,Version control system,B,"Infrastructure as code (IaC) is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools.
The IT infrastructure managed by this process comprises both physical equipment, such as bare-metal servers, as well as virtual machines, and associated configuration resources.
The definitions may be in a version control system.
The code in the definition files may use either scripts or declarative definitions, rather than maintaining the code through manual processes, but IaC more often employs declarative approaches.

Overview
IaC grew as a response to the difficulty posed by utility computing and second-generation web frameworks. In 2006, the launch of Amazon Web Services’ Elastic Compute Cloud and the 1.0 version of Ruby on Rails just months before created widespread scaling problems in the enterprise that were previously experienced only at large, multi-national companies. With new tools emerging to handle this ever-growing field, the idea of IaC was born. The thought of modeling infrastructure with code, and then having the ability to design, implement, and deploy application infrastructure with known software best practices appealed to both software developers and IT infrastructure administrators. The ability to treat infrastructure as code and use the same tools as any other software project would allow developers to rapidly deploy applications.

Advantages
The value of IaC can be broken down into three measurable categories: cost, speed, and risk. Cost reduction aims at helping not only the enterprise financially, but also in terms of people and effort, meaning that by removing the manual component, people are able to refocus their efforts on other enterprise tasks. Infrastructure automation enables speed through faster execution when configuring your infrastructure and aims at providing visibility to help other teams across the enterprise work quickly and more efficiently."
What are the advantages of IaC?,Cost reduction only,Speed improvement only,Risk reduction only,Cost reduction and speed improvement,Cost reduction and risk reduction,D,"Infrastructure as code (IaC) is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools.
The IT infrastructure managed by this process comprises both physical equipment, such as bare-metal servers, as well as virtual machines, and associated configuration resources.
The definitions may be in a version control system.
The code in the definition files may use either scripts or declarative definitions, rather than maintaining the code through manual processes, but IaC more often employs declarative approaches.

Overview
IaC grew as a response to the difficulty posed by utility computing and second-generation web frameworks. In 2006, the launch of Amazon Web Services’ Elastic Compute Cloud and the 1.0 version of Ruby on Rails just months before created widespread scaling problems in the enterprise that were previously experienced only at large, multi-national companies. With new tools emerging to handle this ever-growing field, the idea of IaC was born. The thought of modeling infrastructure with code, and then having the ability to design, implement, and deploy application infrastructure with known software best practices appealed to both software developers and IT infrastructure administrators. The ability to treat infrastructure as code and use the same tools as any other software project would allow developers to rapidly deploy applications.

Advantages
The value of IaC can be broken down into three measurable categories: cost, speed, and risk. Cost reduction aims at helping not only the enterprise financially, but also in terms of people and effort, meaning that by removing the manual component, people are able to refocus their efforts on other enterprise tasks. Infrastructure automation enables speed through faster execution when configuring your infrastructure and aims at providing visibility to help other teams across the enterprise work quickly and more efficiently."
Why was IaC born?,To handle the difficulty posed by utility computing and second-generation web frameworks,To manage large multi-national companies,To automate infrastructure configuration,To provide visibility to other teams across the enterprise,To rapidly deploy applications,A,"Infrastructure as code (IaC) is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools.
The IT infrastructure managed by this process comprises both physical equipment, such as bare-metal servers, as well as virtual machines, and associated configuration resources.
The definitions may be in a version control system.
The code in the definition files may use either scripts or declarative definitions, rather than maintaining the code through manual processes, but IaC more often employs declarative approaches.

Overview
IaC grew as a response to the difficulty posed by utility computing and second-generation web frameworks. In 2006, the launch of Amazon Web Services’ Elastic Compute Cloud and the 1.0 version of Ruby on Rails just months before created widespread scaling problems in the enterprise that were previously experienced only at large, multi-national companies. With new tools emerging to handle this ever-growing field, the idea of IaC was born. The thought of modeling infrastructure with code, and then having the ability to design, implement, and deploy application infrastructure with known software best practices appealed to both software developers and IT infrastructure administrators. The ability to treat infrastructure as code and use the same tools as any other software project would allow developers to rapidly deploy applications.

Advantages
The value of IaC can be broken down into three measurable categories: cost, speed, and risk. Cost reduction aims at helping not only the enterprise financially, but also in terms of people and effort, meaning that by removing the manual component, people are able to refocus their efforts on other enterprise tasks. Infrastructure automation enables speed through faster execution when configuring your infrastructure and aims at providing visibility to help other teams across the enterprise work quickly and more efficiently."
What is the purpose of braking action reports in aviation?,To assess the quality of the runway surface,To determine the crosswind limits for an aircraft,To measure the stopping distances on a runway,To report the level of friction on a runway,To evaluate the performance of the aircraft during landing,D,"Braking action in aviation is a description of how easily an aircraft can stop after landing on a runway. Either pilots or airport management can report the braking action according to the U.S. Federal Aviation Administration.When reporting braking action, any of the following terms may be used: Good; Medium; Poor; Nil - bad or no braking action. If an air traffic controller receives a braking action report worse than good, a Pilot Report (PIREP) must be completed and an advisory must be included in the Automatic Terminal Information Service (""Braking Action Advisories are in effect"").  As of October 2019, the FAA has used mu values to describe braking conditions

Europe
In Europe this differs from the above reference. Braking action reports in Europe are an indication/declaration of reduced friction on a runway due to runway contamination (see Landing performance, under the Runway Surface section) which may impact an aircraft's crosswind limits. 
European reports have nothing to do with stopping distances on a runway, though they should alert pilots that stopping distances will also be affected."
Which organization is responsible for reporting braking action in the United States?,Air Traffic Control,Federal Aviation Administration,Pilots,Airport Management,Automatic Terminal Information Service,B,"Braking action in aviation is a description of how easily an aircraft can stop after landing on a runway. Either pilots or airport management can report the braking action according to the U.S. Federal Aviation Administration.When reporting braking action, any of the following terms may be used: Good; Medium; Poor; Nil - bad or no braking action. If an air traffic controller receives a braking action report worse than good, a Pilot Report (PIREP) must be completed and an advisory must be included in the Automatic Terminal Information Service (""Braking Action Advisories are in effect"").  As of October 2019, the FAA has used mu values to describe braking conditions

Europe
In Europe this differs from the above reference. Braking action reports in Europe are an indication/declaration of reduced friction on a runway due to runway contamination (see Landing performance, under the Runway Surface section) which may impact an aircraft's crosswind limits. 
European reports have nothing to do with stopping distances on a runway, though they should alert pilots that stopping distances will also be affected."
What are the possible terms used to report braking action according to the U.S. Federal Aviation Administration?,Excellent; High; Low; None,Good; Medium; Poor; Nil,Safe; Risky; Unsafe; None,Strong; Moderate; Weak; None,Effective; Average; Ineffective; None,B,"Braking action in aviation is a description of how easily an aircraft can stop after landing on a runway. Either pilots or airport management can report the braking action according to the U.S. Federal Aviation Administration.When reporting braking action, any of the following terms may be used: Good; Medium; Poor; Nil - bad or no braking action. If an air traffic controller receives a braking action report worse than good, a Pilot Report (PIREP) must be completed and an advisory must be included in the Automatic Terminal Information Service (""Braking Action Advisories are in effect"").  As of October 2019, the FAA has used mu values to describe braking conditions

Europe
In Europe this differs from the above reference. Braking action reports in Europe are an indication/declaration of reduced friction on a runway due to runway contamination (see Landing performance, under the Runway Surface section) which may impact an aircraft's crosswind limits. 
European reports have nothing to do with stopping distances on a runway, though they should alert pilots that stopping distances will also be affected."
"What action must be taken by an air traffic controller if they receive a braking action report worse than ""Good""?",They must complete a Pilot Report (PIREP),They must update the Automatic Terminal Information Service,They must notify the airport management,They must suspend all aircraft operations,They must request a runway inspection,A,"Braking action in aviation is a description of how easily an aircraft can stop after landing on a runway. Either pilots or airport management can report the braking action according to the U.S. Federal Aviation Administration.When reporting braking action, any of the following terms may be used: Good; Medium; Poor; Nil - bad or no braking action. If an air traffic controller receives a braking action report worse than good, a Pilot Report (PIREP) must be completed and an advisory must be included in the Automatic Terminal Information Service (""Braking Action Advisories are in effect"").  As of October 2019, the FAA has used mu values to describe braking conditions

Europe
In Europe this differs from the above reference. Braking action reports in Europe are an indication/declaration of reduced friction on a runway due to runway contamination (see Landing performance, under the Runway Surface section) which may impact an aircraft's crosswind limits. 
European reports have nothing to do with stopping distances on a runway, though they should alert pilots that stopping distances will also be affected."
"In Europe, what do braking action reports indicate?",The stopping distances on a runway,The quality of the runway surface,The crosswind limits for an aircraft,The performance of the aircraft during landing,The reduced friction on a runway due to contamination,E,"Braking action in aviation is a description of how easily an aircraft can stop after landing on a runway. Either pilots or airport management can report the braking action according to the U.S. Federal Aviation Administration.When reporting braking action, any of the following terms may be used: Good; Medium; Poor; Nil - bad or no braking action. If an air traffic controller receives a braking action report worse than good, a Pilot Report (PIREP) must be completed and an advisory must be included in the Automatic Terminal Information Service (""Braking Action Advisories are in effect"").  As of October 2019, the FAA has used mu values to describe braking conditions

Europe
In Europe this differs from the above reference. Braking action reports in Europe are an indication/declaration of reduced friction on a runway due to runway contamination (see Landing performance, under the Runway Surface section) which may impact an aircraft's crosswind limits. 
European reports have nothing to do with stopping distances on a runway, though they should alert pilots that stopping distances will also be affected."
What is grammar systems theory?,The study of formal languages in computer science.,The study of systems of finite collections of formal grammars generating a formal language.,The study of artificial intelligence agents.,The study of decentralized systems.,The study of distributed systems.,B,"Grammar systems theory is a field of theoretical computer science that studies systems of finite collections of formal grammars generating a formal language. Each grammar works on a string, a so-called sequential form that represents an environment. Grammar systems can thus be used as a formalization of decentralized or distributed systems of agents in artificial intelligence.Let 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   be a simple reactive agent moving on the table and trying not to fall down from the table with two reactions, t for turning and ƒ for moving forward. The set of possible behaviors of 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   can then be described as formal language

  
    
      
        
          
            L
            
              A
            
          
        
        =
        {
        (
        
          f
          
            m
          
        
        
          t
          
            n
          
        
        
          f
          
            r
          
        
        
          )
          
            +
          
        
        :
        1
        ≤
        m
        ≤
        k
        ;
        1
        ≤
        n
        ≤
        ℓ
        ;
        1
        ≤
        r
        ≤
        k
        }
        ,
      
    
    {\displaystyle \mathbb {L_{A}} =\{(f^{m}t^{n}f^{r})^{+}:1\leq m\leq k;1\leq n\leq \ell ;1\leq r\leq k\},}
  where ƒ can be done maximally k times and t can be done maximally ℓ times considering the dimensions of the table.
 
Let 
  
    
      
        
          
            G
            
              A
            
          
        
      
    
    {\displaystyle \mathbb {G_{A}} }
   be a formal grammar which generates language 
  
    
      
        
          
            L
            
              A
            
          
        
      
    
    {\displaystyle \mathbb {L_{A}} }
  . The behavior of 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   is then described by this grammar. Suppose the 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   has a subsumption architecture; each component of this architecture can be then represented as a formal grammar, too, and the final behavior of the agent is then described by this system of grammars.The schema on the right describes such a system of grammars which shares a common string representing an environment. The shared sequential form is sequentially rewritten by each grammar, which can represent either a component or generally an agent.
If grammars communicate together and work on a shared sequential form, it is called a Cooperating Distributed (DC) grammar system."
What is the set of possible behaviors of agent A?,A formal language.,A formal grammar.,A reactive agent.,A decentralized system.,A distributed system.,A,"Grammar systems theory is a field of theoretical computer science that studies systems of finite collections of formal grammars generating a formal language. Each grammar works on a string, a so-called sequential form that represents an environment. Grammar systems can thus be used as a formalization of decentralized or distributed systems of agents in artificial intelligence.Let 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   be a simple reactive agent moving on the table and trying not to fall down from the table with two reactions, t for turning and ƒ for moving forward. The set of possible behaviors of 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   can then be described as formal language

  
    
      
        
          
            L
            
              A
            
          
        
        =
        {
        (
        
          f
          
            m
          
        
        
          t
          
            n
          
        
        
          f
          
            r
          
        
        
          )
          
            +
          
        
        :
        1
        ≤
        m
        ≤
        k
        ;
        1
        ≤
        n
        ≤
        ℓ
        ;
        1
        ≤
        r
        ≤
        k
        }
        ,
      
    
    {\displaystyle \mathbb {L_{A}} =\{(f^{m}t^{n}f^{r})^{+}:1\leq m\leq k;1\leq n\leq \ell ;1\leq r\leq k\},}
  where ƒ can be done maximally k times and t can be done maximally ℓ times considering the dimensions of the table.
 
Let 
  
    
      
        
          
            G
            
              A
            
          
        
      
    
    {\displaystyle \mathbb {G_{A}} }
   be a formal grammar which generates language 
  
    
      
        
          
            L
            
              A
            
          
        
      
    
    {\displaystyle \mathbb {L_{A}} }
  . The behavior of 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   is then described by this grammar. Suppose the 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   has a subsumption architecture; each component of this architecture can be then represented as a formal grammar, too, and the final behavior of the agent is then described by this system of grammars.The schema on the right describes such a system of grammars which shares a common string representing an environment. The shared sequential form is sequentially rewritten by each grammar, which can represent either a component or generally an agent.
If grammars communicate together and work on a shared sequential form, it is called a Cooperating Distributed (DC) grammar system."
What does ƒ and t represent in the behavior of agent A?,"Turning and moving forward, respectively.","Moving forward and turning, respectively.","Turning and moving backward, respectively.","Moving backward and turning, respectively.","Moving forward and turning twice, respectively.",A,"Grammar systems theory is a field of theoretical computer science that studies systems of finite collections of formal grammars generating a formal language. Each grammar works on a string, a so-called sequential form that represents an environment. Grammar systems can thus be used as a formalization of decentralized or distributed systems of agents in artificial intelligence.Let 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   be a simple reactive agent moving on the table and trying not to fall down from the table with two reactions, t for turning and ƒ for moving forward. The set of possible behaviors of 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   can then be described as formal language

  
    
      
        
          
            L
            
              A
            
          
        
        =
        {
        (
        
          f
          
            m
          
        
        
          t
          
            n
          
        
        
          f
          
            r
          
        
        
          )
          
            +
          
        
        :
        1
        ≤
        m
        ≤
        k
        ;
        1
        ≤
        n
        ≤
        ℓ
        ;
        1
        ≤
        r
        ≤
        k
        }
        ,
      
    
    {\displaystyle \mathbb {L_{A}} =\{(f^{m}t^{n}f^{r})^{+}:1\leq m\leq k;1\leq n\leq \ell ;1\leq r\leq k\},}
  where ƒ can be done maximally k times and t can be done maximally ℓ times considering the dimensions of the table.
 
Let 
  
    
      
        
          
            G
            
              A
            
          
        
      
    
    {\displaystyle \mathbb {G_{A}} }
   be a formal grammar which generates language 
  
    
      
        
          
            L
            
              A
            
          
        
      
    
    {\displaystyle \mathbb {L_{A}} }
  . The behavior of 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   is then described by this grammar. Suppose the 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   has a subsumption architecture; each component of this architecture can be then represented as a formal grammar, too, and the final behavior of the agent is then described by this system of grammars.The schema on the right describes such a system of grammars which shares a common string representing an environment. The shared sequential form is sequentially rewritten by each grammar, which can represent either a component or generally an agent.
If grammars communicate together and work on a shared sequential form, it is called a Cooperating Distributed (DC) grammar system."
What is the purpose of the grammar system in the description of the behavior of agent A?,To generate a formal language.,To represent a subsumption architecture.,To communicate and cooperate with other grammars.,To describe the final behavior of the agent.,To rewrite the shared sequential form.,D,"Grammar systems theory is a field of theoretical computer science that studies systems of finite collections of formal grammars generating a formal language. Each grammar works on a string, a so-called sequential form that represents an environment. Grammar systems can thus be used as a formalization of decentralized or distributed systems of agents in artificial intelligence.Let 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   be a simple reactive agent moving on the table and trying not to fall down from the table with two reactions, t for turning and ƒ for moving forward. The set of possible behaviors of 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   can then be described as formal language

  
    
      
        
          
            L
            
              A
            
          
        
        =
        {
        (
        
          f
          
            m
          
        
        
          t
          
            n
          
        
        
          f
          
            r
          
        
        
          )
          
            +
          
        
        :
        1
        ≤
        m
        ≤
        k
        ;
        1
        ≤
        n
        ≤
        ℓ
        ;
        1
        ≤
        r
        ≤
        k
        }
        ,
      
    
    {\displaystyle \mathbb {L_{A}} =\{(f^{m}t^{n}f^{r})^{+}:1\leq m\leq k;1\leq n\leq \ell ;1\leq r\leq k\},}
  where ƒ can be done maximally k times and t can be done maximally ℓ times considering the dimensions of the table.
 
Let 
  
    
      
        
          
            G
            
              A
            
          
        
      
    
    {\displaystyle \mathbb {G_{A}} }
   be a formal grammar which generates language 
  
    
      
        
          
            L
            
              A
            
          
        
      
    
    {\displaystyle \mathbb {L_{A}} }
  . The behavior of 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   is then described by this grammar. Suppose the 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   has a subsumption architecture; each component of this architecture can be then represented as a formal grammar, too, and the final behavior of the agent is then described by this system of grammars.The schema on the right describes such a system of grammars which shares a common string representing an environment. The shared sequential form is sequentially rewritten by each grammar, which can represent either a component or generally an agent.
If grammars communicate together and work on a shared sequential form, it is called a Cooperating Distributed (DC) grammar system."
What is a Cooperating Distributed (DC) grammar system?,A system of formal grammars working on a shared sequential form.,A system of formal grammars that do not communicate with each other.,A system of formal grammars that work independently.,A system of formal grammars that generate a formal language.,A system of formal grammars that represent an environment.,A,"Grammar systems theory is a field of theoretical computer science that studies systems of finite collections of formal grammars generating a formal language. Each grammar works on a string, a so-called sequential form that represents an environment. Grammar systems can thus be used as a formalization of decentralized or distributed systems of agents in artificial intelligence.Let 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   be a simple reactive agent moving on the table and trying not to fall down from the table with two reactions, t for turning and ƒ for moving forward. The set of possible behaviors of 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   can then be described as formal language

  
    
      
        
          
            L
            
              A
            
          
        
        =
        {
        (
        
          f
          
            m
          
        
        
          t
          
            n
          
        
        
          f
          
            r
          
        
        
          )
          
            +
          
        
        :
        1
        ≤
        m
        ≤
        k
        ;
        1
        ≤
        n
        ≤
        ℓ
        ;
        1
        ≤
        r
        ≤
        k
        }
        ,
      
    
    {\displaystyle \mathbb {L_{A}} =\{(f^{m}t^{n}f^{r})^{+}:1\leq m\leq k;1\leq n\leq \ell ;1\leq r\leq k\},}
  where ƒ can be done maximally k times and t can be done maximally ℓ times considering the dimensions of the table.
 
Let 
  
    
      
        
          
            G
            
              A
            
          
        
      
    
    {\displaystyle \mathbb {G_{A}} }
   be a formal grammar which generates language 
  
    
      
        
          
            L
            
              A
            
          
        
      
    
    {\displaystyle \mathbb {L_{A}} }
  . The behavior of 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   is then described by this grammar. Suppose the 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   has a subsumption architecture; each component of this architecture can be then represented as a formal grammar, too, and the final behavior of the agent is then described by this system of grammars.The schema on the right describes such a system of grammars which shares a common string representing an environment. The shared sequential form is sequentially rewritten by each grammar, which can represent either a component or generally an agent.
If grammars communicate together and work on a shared sequential form, it is called a Cooperating Distributed (DC) grammar system."
What is the time value of money?,It is the concept that money has greater value when received now rather than later,It is the concept that money has greater value when received later rather than now,It is the concept that money has the same value regardless of when it is received,It is the concept that money has no value,It is the concept that money has different values depending on the circumstances,A,"The time value of money is the widely accepted conjecture that there is greater benefit to receiving a sum of money now rather than an identical sum later. It may be seen as an implication of the later-developed concept of time preference.
The time value of money is among the factors considered when weighing the opportunity costs of spending rather than saving or investing money.  As such, it is among the reasons why interest is paid or earned: interest, whether it is on a bank deposit or debt, compensates the depositor or lender for the loss of their use of their money. Investors are willing to forgo spending their money now only if they expect a favorable net return on their investment in the future, such that the increased value to be available later is sufficiently high to offset both the preference to spending money now and inflation (if present); see required rate of return.

History
The Talmud (~500 CE) recognizes the time value of money. In Tractate Makkos page 3a the Talmud discusses a case where witnesses falsely claimed that the term of a loan was 30 days when it was actually 10 years. The false witnesses must pay the difference of the value of the loan ""in a situation where he would be required to give the money back (within) thirty days..., and that same sum in a situation where he would be required to give the money back (within) 10 years...The difference is the sum that the testimony of the (false) witnesses sought to have the borrower lose; therefore, it is the sum that they must pay.""The notion was later described by Martín de Azpilcueta (1491–1586) of the School of Salamanca.

Calculations
Time value of money problems involve the net value of cash flows at different points in time.
In a typical case, the variables might be: a balance (the real or nominal value of a debt or a financial asset in terms of monetary units), a periodic rate of interest, the number of periods, and a series of cash flows. (In the case of a debt, cash flows are payments against principal and interest; in the case of a financial asset, these are contributions to or withdrawals from the balance.)  More generally, the cash flows may not be periodic but may be specified individually."
Why is the time value of money considered when weighing spending and saving?,To determine the opportunity costs of spending money,To determine the opportunity costs of saving money,To determine the opportunity costs of investing money,To determine the opportunity costs of not spending money,To determine the opportunity costs of not saving money,A,"The time value of money is the widely accepted conjecture that there is greater benefit to receiving a sum of money now rather than an identical sum later. It may be seen as an implication of the later-developed concept of time preference.
The time value of money is among the factors considered when weighing the opportunity costs of spending rather than saving or investing money.  As such, it is among the reasons why interest is paid or earned: interest, whether it is on a bank deposit or debt, compensates the depositor or lender for the loss of their use of their money. Investors are willing to forgo spending their money now only if they expect a favorable net return on their investment in the future, such that the increased value to be available later is sufficiently high to offset both the preference to spending money now and inflation (if present); see required rate of return.

History
The Talmud (~500 CE) recognizes the time value of money. In Tractate Makkos page 3a the Talmud discusses a case where witnesses falsely claimed that the term of a loan was 30 days when it was actually 10 years. The false witnesses must pay the difference of the value of the loan ""in a situation where he would be required to give the money back (within) thirty days..., and that same sum in a situation where he would be required to give the money back (within) 10 years...The difference is the sum that the testimony of the (false) witnesses sought to have the borrower lose; therefore, it is the sum that they must pay.""The notion was later described by Martín de Azpilcueta (1491–1586) of the School of Salamanca.

Calculations
Time value of money problems involve the net value of cash flows at different points in time.
In a typical case, the variables might be: a balance (the real or nominal value of a debt or a financial asset in terms of monetary units), a periodic rate of interest, the number of periods, and a series of cash flows. (In the case of a debt, cash flows are payments against principal and interest; in the case of a financial asset, these are contributions to or withdrawals from the balance.)  More generally, the cash flows may not be periodic but may be specified individually."
What is one of the reasons interest is paid or earned?,To compensate for the loss of use of money,To encourage people to spend money,To discourage people from saving money,To stabilize the economy,To increase the value of money,A,"The time value of money is the widely accepted conjecture that there is greater benefit to receiving a sum of money now rather than an identical sum later. It may be seen as an implication of the later-developed concept of time preference.
The time value of money is among the factors considered when weighing the opportunity costs of spending rather than saving or investing money.  As such, it is among the reasons why interest is paid or earned: interest, whether it is on a bank deposit or debt, compensates the depositor or lender for the loss of their use of their money. Investors are willing to forgo spending their money now only if they expect a favorable net return on their investment in the future, such that the increased value to be available later is sufficiently high to offset both the preference to spending money now and inflation (if present); see required rate of return.

History
The Talmud (~500 CE) recognizes the time value of money. In Tractate Makkos page 3a the Talmud discusses a case where witnesses falsely claimed that the term of a loan was 30 days when it was actually 10 years. The false witnesses must pay the difference of the value of the loan ""in a situation where he would be required to give the money back (within) thirty days..., and that same sum in a situation where he would be required to give the money back (within) 10 years...The difference is the sum that the testimony of the (false) witnesses sought to have the borrower lose; therefore, it is the sum that they must pay.""The notion was later described by Martín de Azpilcueta (1491–1586) of the School of Salamanca.

Calculations
Time value of money problems involve the net value of cash flows at different points in time.
In a typical case, the variables might be: a balance (the real or nominal value of a debt or a financial asset in terms of monetary units), a periodic rate of interest, the number of periods, and a series of cash flows. (In the case of a debt, cash flows are payments against principal and interest; in the case of a financial asset, these are contributions to or withdrawals from the balance.)  More generally, the cash flows may not be periodic but may be specified individually."
Who recognized the time value of money in the Talmud?,Martín de Azpilcueta,The School of Salamanca,Tractate Makkos,The false witnesses,The borrowers,C,"The time value of money is the widely accepted conjecture that there is greater benefit to receiving a sum of money now rather than an identical sum later. It may be seen as an implication of the later-developed concept of time preference.
The time value of money is among the factors considered when weighing the opportunity costs of spending rather than saving or investing money.  As such, it is among the reasons why interest is paid or earned: interest, whether it is on a bank deposit or debt, compensates the depositor or lender for the loss of their use of their money. Investors are willing to forgo spending their money now only if they expect a favorable net return on their investment in the future, such that the increased value to be available later is sufficiently high to offset both the preference to spending money now and inflation (if present); see required rate of return.

History
The Talmud (~500 CE) recognizes the time value of money. In Tractate Makkos page 3a the Talmud discusses a case where witnesses falsely claimed that the term of a loan was 30 days when it was actually 10 years. The false witnesses must pay the difference of the value of the loan ""in a situation where he would be required to give the money back (within) thirty days..., and that same sum in a situation where he would be required to give the money back (within) 10 years...The difference is the sum that the testimony of the (false) witnesses sought to have the borrower lose; therefore, it is the sum that they must pay.""The notion was later described by Martín de Azpilcueta (1491–1586) of the School of Salamanca.

Calculations
Time value of money problems involve the net value of cash flows at different points in time.
In a typical case, the variables might be: a balance (the real or nominal value of a debt or a financial asset in terms of monetary units), a periodic rate of interest, the number of periods, and a series of cash flows. (In the case of a debt, cash flows are payments against principal and interest; in the case of a financial asset, these are contributions to or withdrawals from the balance.)  More generally, the cash flows may not be periodic but may be specified individually."
What variables are involved in time value of money problems?,"Balance, periodic rate of interest, number of periods, series of cash flows","Balance, periodic rate of interest, number of periods, individual cash flows","Balance, interest rate, time period, net cash flows","Debt, financial asset, cash flows, interest rate","Debt, financial asset, cash flows, time period",B,"The time value of money is the widely accepted conjecture that there is greater benefit to receiving a sum of money now rather than an identical sum later. It may be seen as an implication of the later-developed concept of time preference.
The time value of money is among the factors considered when weighing the opportunity costs of spending rather than saving or investing money.  As such, it is among the reasons why interest is paid or earned: interest, whether it is on a bank deposit or debt, compensates the depositor or lender for the loss of their use of their money. Investors are willing to forgo spending their money now only if they expect a favorable net return on their investment in the future, such that the increased value to be available later is sufficiently high to offset both the preference to spending money now and inflation (if present); see required rate of return.

History
The Talmud (~500 CE) recognizes the time value of money. In Tractate Makkos page 3a the Talmud discusses a case where witnesses falsely claimed that the term of a loan was 30 days when it was actually 10 years. The false witnesses must pay the difference of the value of the loan ""in a situation where he would be required to give the money back (within) thirty days..., and that same sum in a situation where he would be required to give the money back (within) 10 years...The difference is the sum that the testimony of the (false) witnesses sought to have the borrower lose; therefore, it is the sum that they must pay.""The notion was later described by Martín de Azpilcueta (1491–1586) of the School of Salamanca.

Calculations
Time value of money problems involve the net value of cash flows at different points in time.
In a typical case, the variables might be: a balance (the real or nominal value of a debt or a financial asset in terms of monetary units), a periodic rate of interest, the number of periods, and a series of cash flows. (In the case of a debt, cash flows are payments against principal and interest; in the case of a financial asset, these are contributions to or withdrawals from the balance.)  More generally, the cash flows may not be periodic but may be specified individually."
What is the phenomenon behind the theory of Brownian motors?,Diffusion of particles in a fluid,Electromagnetic forces,Gravity,Nuclear reactions,Friction between solid surfaces,A,"Brownian motors are nanoscale or molecular machines that use chemical reactions to generate directed motion in space. The theory behind Brownian motors relies on the phenomenon of Brownian motion, random motion of particles suspended in a fluid (a liquid or a gas) resulting from their collision with the fast-moving molecules in the fluid.On the nanoscale (1-100 nm), viscosity dominates inertia, and the extremely high degree of thermal noise in the environment makes conventional directed motion all but impossible, because the forces impelling these motors in the desired direction are minuscule when compared to the random forces exerted by the environment. Brownian motors operate specifically to utilise this high level of random noise to achieve directed motion, and as such are only viable on the nanoscale.The concept of Brownian motors is a recent one, having only been coined in 1995 by Peter Hänggi, but the existence of such motors in nature may have existed for a very long time and help to explain crucial cellular processes that require movement at the nanoscale, such as protein synthesis and muscular contraction. If this is the case, Brownian motors may have implications for the foundations of life itself.In more recent times, humans have attempted to apply this knowledge of natural Brownian motors to solve human problems. The applications of Brownian motors are most obvious in nanorobotics due to its inherent reliance on directed motion.

History
20th century
The term “Brownian motor” was originally invented by Swiss theoretical physicist Peter Hänggi in 1995. The Brownian motor, like the phenomenon of Brownian motion that underpinned its underlying theory, was also named after 19th century Scottish botanist Robert Brown, who, while looking through a microscope at pollen of the plant Clarkia pulchella immersed in water, famously described the random motion of pollen particles in water in 1827. In 1905, almost eighty years later, theoretical physicist Albert Einstein published a paper where he modeled the motion of the pollen as being moved by individual water molecules, and this was verified experimentally by Jean Perrin in 1908, who was awarded the Nobel Prize in Physics in 1926 ""for his work on the discontinuous structure of matter""."
Why is directed motion difficult to achieve on the nanoscale?,The forces impelling the motors are too strong,The motors are too large,The motors are affected by electromagnetic fields,Viscosity dominates inertia,The random forces are too weak,D,"Brownian motors are nanoscale or molecular machines that use chemical reactions to generate directed motion in space. The theory behind Brownian motors relies on the phenomenon of Brownian motion, random motion of particles suspended in a fluid (a liquid or a gas) resulting from their collision with the fast-moving molecules in the fluid.On the nanoscale (1-100 nm), viscosity dominates inertia, and the extremely high degree of thermal noise in the environment makes conventional directed motion all but impossible, because the forces impelling these motors in the desired direction are minuscule when compared to the random forces exerted by the environment. Brownian motors operate specifically to utilise this high level of random noise to achieve directed motion, and as such are only viable on the nanoscale.The concept of Brownian motors is a recent one, having only been coined in 1995 by Peter Hänggi, but the existence of such motors in nature may have existed for a very long time and help to explain crucial cellular processes that require movement at the nanoscale, such as protein synthesis and muscular contraction. If this is the case, Brownian motors may have implications for the foundations of life itself.In more recent times, humans have attempted to apply this knowledge of natural Brownian motors to solve human problems. The applications of Brownian motors are most obvious in nanorobotics due to its inherent reliance on directed motion.

History
20th century
The term “Brownian motor” was originally invented by Swiss theoretical physicist Peter Hänggi in 1995. The Brownian motor, like the phenomenon of Brownian motion that underpinned its underlying theory, was also named after 19th century Scottish botanist Robert Brown, who, while looking through a microscope at pollen of the plant Clarkia pulchella immersed in water, famously described the random motion of pollen particles in water in 1827. In 1905, almost eighty years later, theoretical physicist Albert Einstein published a paper where he modeled the motion of the pollen as being moved by individual water molecules, and this was verified experimentally by Jean Perrin in 1908, who was awarded the Nobel Prize in Physics in 1926 ""for his work on the discontinuous structure of matter""."
"When was the term ""Brownian motor"" coined?",1827,1905,1926,1995,2008,D,"Brownian motors are nanoscale or molecular machines that use chemical reactions to generate directed motion in space. The theory behind Brownian motors relies on the phenomenon of Brownian motion, random motion of particles suspended in a fluid (a liquid or a gas) resulting from their collision with the fast-moving molecules in the fluid.On the nanoscale (1-100 nm), viscosity dominates inertia, and the extremely high degree of thermal noise in the environment makes conventional directed motion all but impossible, because the forces impelling these motors in the desired direction are minuscule when compared to the random forces exerted by the environment. Brownian motors operate specifically to utilise this high level of random noise to achieve directed motion, and as such are only viable on the nanoscale.The concept of Brownian motors is a recent one, having only been coined in 1995 by Peter Hänggi, but the existence of such motors in nature may have existed for a very long time and help to explain crucial cellular processes that require movement at the nanoscale, such as protein synthesis and muscular contraction. If this is the case, Brownian motors may have implications for the foundations of life itself.In more recent times, humans have attempted to apply this knowledge of natural Brownian motors to solve human problems. The applications of Brownian motors are most obvious in nanorobotics due to its inherent reliance on directed motion.

History
20th century
The term “Brownian motor” was originally invented by Swiss theoretical physicist Peter Hänggi in 1995. The Brownian motor, like the phenomenon of Brownian motion that underpinned its underlying theory, was also named after 19th century Scottish botanist Robert Brown, who, while looking through a microscope at pollen of the plant Clarkia pulchella immersed in water, famously described the random motion of pollen particles in water in 1827. In 1905, almost eighty years later, theoretical physicist Albert Einstein published a paper where he modeled the motion of the pollen as being moved by individual water molecules, and this was verified experimentally by Jean Perrin in 1908, who was awarded the Nobel Prize in Physics in 1926 ""for his work on the discontinuous structure of matter""."
Who described the random motion of pollen particles in water?,Peter Hänggi,Robert Brown,Albert Einstein,Jean Perrin,Swiss theoretical physicist,B,"Brownian motors are nanoscale or molecular machines that use chemical reactions to generate directed motion in space. The theory behind Brownian motors relies on the phenomenon of Brownian motion, random motion of particles suspended in a fluid (a liquid or a gas) resulting from their collision with the fast-moving molecules in the fluid.On the nanoscale (1-100 nm), viscosity dominates inertia, and the extremely high degree of thermal noise in the environment makes conventional directed motion all but impossible, because the forces impelling these motors in the desired direction are minuscule when compared to the random forces exerted by the environment. Brownian motors operate specifically to utilise this high level of random noise to achieve directed motion, and as such are only viable on the nanoscale.The concept of Brownian motors is a recent one, having only been coined in 1995 by Peter Hänggi, but the existence of such motors in nature may have existed for a very long time and help to explain crucial cellular processes that require movement at the nanoscale, such as protein synthesis and muscular contraction. If this is the case, Brownian motors may have implications for the foundations of life itself.In more recent times, humans have attempted to apply this knowledge of natural Brownian motors to solve human problems. The applications of Brownian motors are most obvious in nanorobotics due to its inherent reliance on directed motion.

History
20th century
The term “Brownian motor” was originally invented by Swiss theoretical physicist Peter Hänggi in 1995. The Brownian motor, like the phenomenon of Brownian motion that underpinned its underlying theory, was also named after 19th century Scottish botanist Robert Brown, who, while looking through a microscope at pollen of the plant Clarkia pulchella immersed in water, famously described the random motion of pollen particles in water in 1827. In 1905, almost eighty years later, theoretical physicist Albert Einstein published a paper where he modeled the motion of the pollen as being moved by individual water molecules, and this was verified experimentally by Jean Perrin in 1908, who was awarded the Nobel Prize in Physics in 1926 ""for his work on the discontinuous structure of matter""."
What are the implications of Brownian motors?,They can explain protein synthesis and muscular contraction,They can generate electricity,They can be used in space exploration,They can control weather patterns,They can cure diseases,A,"Brownian motors are nanoscale or molecular machines that use chemical reactions to generate directed motion in space. The theory behind Brownian motors relies on the phenomenon of Brownian motion, random motion of particles suspended in a fluid (a liquid or a gas) resulting from their collision with the fast-moving molecules in the fluid.On the nanoscale (1-100 nm), viscosity dominates inertia, and the extremely high degree of thermal noise in the environment makes conventional directed motion all but impossible, because the forces impelling these motors in the desired direction are minuscule when compared to the random forces exerted by the environment. Brownian motors operate specifically to utilise this high level of random noise to achieve directed motion, and as such are only viable on the nanoscale.The concept of Brownian motors is a recent one, having only been coined in 1995 by Peter Hänggi, but the existence of such motors in nature may have existed for a very long time and help to explain crucial cellular processes that require movement at the nanoscale, such as protein synthesis and muscular contraction. If this is the case, Brownian motors may have implications for the foundations of life itself.In more recent times, humans have attempted to apply this knowledge of natural Brownian motors to solve human problems. The applications of Brownian motors are most obvious in nanorobotics due to its inherent reliance on directed motion.

History
20th century
The term “Brownian motor” was originally invented by Swiss theoretical physicist Peter Hänggi in 1995. The Brownian motor, like the phenomenon of Brownian motion that underpinned its underlying theory, was also named after 19th century Scottish botanist Robert Brown, who, while looking through a microscope at pollen of the plant Clarkia pulchella immersed in water, famously described the random motion of pollen particles in water in 1827. In 1905, almost eighty years later, theoretical physicist Albert Einstein published a paper where he modeled the motion of the pollen as being moved by individual water molecules, and this was verified experimentally by Jean Perrin in 1908, who was awarded the Nobel Prize in Physics in 1926 ""for his work on the discontinuous structure of matter""."
What is the Coulomb barrier?,A barrier that prevents two nuclei from getting close enough to undergo a nuclear reaction,The energy barrier due to electrostatic interaction that two nuclei need to overcome to bind together,The energy barrier due to electrostatic interaction that prevents two nuclei from colliding,A barrier that prevents the fusion of two nuclei,The energy barrier due to quantum mechanical tunnelling,A,"The Coulomb barrier, named after Coulomb's law, which is in turn named after physicist Charles-Augustin de Coulomb, is the energy barrier due to electrostatic interaction that two nuclei need to overcome so they can get close enough to undergo a  nuclear reaction.

Potential energy barrier
This energy barrier is given by the electric potential energy:

  
    
      
        
          U
          
            coul
          
        
        =
        
          
            
              
                q
                
                  1
                
              
              
              
                q
                
                  2
                
              
            
            
              4
              π
              
                ϵ
                
                  0
                
              
              r
            
          
        
      
    
    {\displaystyle U_{\text{coul}}={{q_{1}\,q_{2}} \over 4\pi \epsilon _{0}r}}
  where

ε0 is the permittivity of free space;
q1, q2 are the charges of the interacting particles;
r is the interaction radius.A positive value of U is due to a repulsive force, so interacting particles are at higher energy levels as they get closer. A negative potential energy indicates a bound state (due to an attractive force).
The Coulomb barrier increases with the atomic numbers (i.e. the number of protons) of the colliding nuclei:

  
    
      
        
          U
          
            coul
          
        
        =
        
          
            
              
                Z
                
                  1
                
              
              
              
                Z
                
                  2
                
              
              
              
                e
                
                  2
                
              
            
            
              4
              π
              
                ϵ
                
                  0
                
              
              r
            
          
        
      
    
    {\displaystyle U_{\text{coul}}={{Z_{1}\,Z_{2}\,e^{2}} \over 4\pi \epsilon _{0}r}}
  where e is the elementary charge, and Zi the corresponding atomic numbers.
To overcome this barrier, nuclei have to collide at high velocities, so their kinetic energies drive them close enough for the strong interaction to take place and bind them together.
According to the kinetic theory of gases, the temperature of a gas is just a measure of the average kinetic energy of the particles in that gas. For classical ideal gases the velocity distribution of the gas particles is given by Maxwell–Boltzmann. From this distribution, the fraction of particles with a velocity high enough to overcome the Coulomb barrier can be determined.
In practice, temperatures needed to overcome the Coulomb barrier turned out to be smaller than expected due to quantum mechanical tunnelling, as established by Gamow. The consideration of barrier-penetration through tunnelling and the speed distribution gives rise to a limited range of conditions where fusion can take place, known as the Gamow window.
The absence of the Coulomb barrier enabled the discovery of the neutron by James Chadwick in 1932.


== References ==."
What is the formula for the Coulomb barrier?,Ucoul = (q1 * q2) / (4 * pi * epsilon0 * r),Ucoul = (Z1 * Z2 * e^2) / (4 * pi * epsilon0 * r),Ucoul = (q1 * q2) / (4 * pi * epsilon0 * r^2),Ucoul = (Z1 * Z2 * e^2) / (4 * pi * epsilon0 * r^2),Ucoul = (q1 * q2 * e^2) / (4 * pi * epsilon0 * r),B,"The Coulomb barrier, named after Coulomb's law, which is in turn named after physicist Charles-Augustin de Coulomb, is the energy barrier due to electrostatic interaction that two nuclei need to overcome so they can get close enough to undergo a  nuclear reaction.

Potential energy barrier
This energy barrier is given by the electric potential energy:

  
    
      
        
          U
          
            coul
          
        
        =
        
          
            
              
                q
                
                  1
                
              
              
              
                q
                
                  2
                
              
            
            
              4
              π
              
                ϵ
                
                  0
                
              
              r
            
          
        
      
    
    {\displaystyle U_{\text{coul}}={{q_{1}\,q_{2}} \over 4\pi \epsilon _{0}r}}
  where

ε0 is the permittivity of free space;
q1, q2 are the charges of the interacting particles;
r is the interaction radius.A positive value of U is due to a repulsive force, so interacting particles are at higher energy levels as they get closer. A negative potential energy indicates a bound state (due to an attractive force).
The Coulomb barrier increases with the atomic numbers (i.e. the number of protons) of the colliding nuclei:

  
    
      
        
          U
          
            coul
          
        
        =
        
          
            
              
                Z
                
                  1
                
              
              
              
                Z
                
                  2
                
              
              
              
                e
                
                  2
                
              
            
            
              4
              π
              
                ϵ
                
                  0
                
              
              r
            
          
        
      
    
    {\displaystyle U_{\text{coul}}={{Z_{1}\,Z_{2}\,e^{2}} \over 4\pi \epsilon _{0}r}}
  where e is the elementary charge, and Zi the corresponding atomic numbers.
To overcome this barrier, nuclei have to collide at high velocities, so their kinetic energies drive them close enough for the strong interaction to take place and bind them together.
According to the kinetic theory of gases, the temperature of a gas is just a measure of the average kinetic energy of the particles in that gas. For classical ideal gases the velocity distribution of the gas particles is given by Maxwell–Boltzmann. From this distribution, the fraction of particles with a velocity high enough to overcome the Coulomb barrier can be determined.
In practice, temperatures needed to overcome the Coulomb barrier turned out to be smaller than expected due to quantum mechanical tunnelling, as established by Gamow. The consideration of barrier-penetration through tunnelling and the speed distribution gives rise to a limited range of conditions where fusion can take place, known as the Gamow window.
The absence of the Coulomb barrier enabled the discovery of the neutron by James Chadwick in 1932.


== References ==."
What causes the Coulomb barrier to increase?,An increase in the atomic numbers of the colliding nuclei,An increase in the interaction radius,An increase in the kinetic energy of the colliding nuclei,An increase in the electric potential energy,An increase in the permittivity of free space,A,"The Coulomb barrier, named after Coulomb's law, which is in turn named after physicist Charles-Augustin de Coulomb, is the energy barrier due to electrostatic interaction that two nuclei need to overcome so they can get close enough to undergo a  nuclear reaction.

Potential energy barrier
This energy barrier is given by the electric potential energy:

  
    
      
        
          U
          
            coul
          
        
        =
        
          
            
              
                q
                
                  1
                
              
              
              
                q
                
                  2
                
              
            
            
              4
              π
              
                ϵ
                
                  0
                
              
              r
            
          
        
      
    
    {\displaystyle U_{\text{coul}}={{q_{1}\,q_{2}} \over 4\pi \epsilon _{0}r}}
  where

ε0 is the permittivity of free space;
q1, q2 are the charges of the interacting particles;
r is the interaction radius.A positive value of U is due to a repulsive force, so interacting particles are at higher energy levels as they get closer. A negative potential energy indicates a bound state (due to an attractive force).
The Coulomb barrier increases with the atomic numbers (i.e. the number of protons) of the colliding nuclei:

  
    
      
        
          U
          
            coul
          
        
        =
        
          
            
              
                Z
                
                  1
                
              
              
              
                Z
                
                  2
                
              
              
              
                e
                
                  2
                
              
            
            
              4
              π
              
                ϵ
                
                  0
                
              
              r
            
          
        
      
    
    {\displaystyle U_{\text{coul}}={{Z_{1}\,Z_{2}\,e^{2}} \over 4\pi \epsilon _{0}r}}
  where e is the elementary charge, and Zi the corresponding atomic numbers.
To overcome this barrier, nuclei have to collide at high velocities, so their kinetic energies drive them close enough for the strong interaction to take place and bind them together.
According to the kinetic theory of gases, the temperature of a gas is just a measure of the average kinetic energy of the particles in that gas. For classical ideal gases the velocity distribution of the gas particles is given by Maxwell–Boltzmann. From this distribution, the fraction of particles with a velocity high enough to overcome the Coulomb barrier can be determined.
In practice, temperatures needed to overcome the Coulomb barrier turned out to be smaller than expected due to quantum mechanical tunnelling, as established by Gamow. The consideration of barrier-penetration through tunnelling and the speed distribution gives rise to a limited range of conditions where fusion can take place, known as the Gamow window.
The absence of the Coulomb barrier enabled the discovery of the neutron by James Chadwick in 1932.


== References ==."
How do nuclei overcome the Coulomb barrier?,By undergoing a nuclear reaction,By colliding at low velocities,By colliding at high velocities,By increasing their atomic numbers,By increasing their interaction radius,C,"The Coulomb barrier, named after Coulomb's law, which is in turn named after physicist Charles-Augustin de Coulomb, is the energy barrier due to electrostatic interaction that two nuclei need to overcome so they can get close enough to undergo a  nuclear reaction.

Potential energy barrier
This energy barrier is given by the electric potential energy:

  
    
      
        
          U
          
            coul
          
        
        =
        
          
            
              
                q
                
                  1
                
              
              
              
                q
                
                  2
                
              
            
            
              4
              π
              
                ϵ
                
                  0
                
              
              r
            
          
        
      
    
    {\displaystyle U_{\text{coul}}={{q_{1}\,q_{2}} \over 4\pi \epsilon _{0}r}}
  where

ε0 is the permittivity of free space;
q1, q2 are the charges of the interacting particles;
r is the interaction radius.A positive value of U is due to a repulsive force, so interacting particles are at higher energy levels as they get closer. A negative potential energy indicates a bound state (due to an attractive force).
The Coulomb barrier increases with the atomic numbers (i.e. the number of protons) of the colliding nuclei:

  
    
      
        
          U
          
            coul
          
        
        =
        
          
            
              
                Z
                
                  1
                
              
              
              
                Z
                
                  2
                
              
              
              
                e
                
                  2
                
              
            
            
              4
              π
              
                ϵ
                
                  0
                
              
              r
            
          
        
      
    
    {\displaystyle U_{\text{coul}}={{Z_{1}\,Z_{2}\,e^{2}} \over 4\pi \epsilon _{0}r}}
  where e is the elementary charge, and Zi the corresponding atomic numbers.
To overcome this barrier, nuclei have to collide at high velocities, so their kinetic energies drive them close enough for the strong interaction to take place and bind them together.
According to the kinetic theory of gases, the temperature of a gas is just a measure of the average kinetic energy of the particles in that gas. For classical ideal gases the velocity distribution of the gas particles is given by Maxwell–Boltzmann. From this distribution, the fraction of particles with a velocity high enough to overcome the Coulomb barrier can be determined.
In practice, temperatures needed to overcome the Coulomb barrier turned out to be smaller than expected due to quantum mechanical tunnelling, as established by Gamow. The consideration of barrier-penetration through tunnelling and the speed distribution gives rise to a limited range of conditions where fusion can take place, known as the Gamow window.
The absence of the Coulomb barrier enabled the discovery of the neutron by James Chadwick in 1932.


== References ==."
What is the Gamow window?,The limited range of conditions where fusion can take place,The temperature needed to overcome the Coulomb barrier,The fraction of particles with a velocity high enough to overcome the Coulomb barrier,The absence of the Coulomb barrier,The discovery of the neutron by James Chadwick,A,"The Coulomb barrier, named after Coulomb's law, which is in turn named after physicist Charles-Augustin de Coulomb, is the energy barrier due to electrostatic interaction that two nuclei need to overcome so they can get close enough to undergo a  nuclear reaction.

Potential energy barrier
This energy barrier is given by the electric potential energy:

  
    
      
        
          U
          
            coul
          
        
        =
        
          
            
              
                q
                
                  1
                
              
              
              
                q
                
                  2
                
              
            
            
              4
              π
              
                ϵ
                
                  0
                
              
              r
            
          
        
      
    
    {\displaystyle U_{\text{coul}}={{q_{1}\,q_{2}} \over 4\pi \epsilon _{0}r}}
  where

ε0 is the permittivity of free space;
q1, q2 are the charges of the interacting particles;
r is the interaction radius.A positive value of U is due to a repulsive force, so interacting particles are at higher energy levels as they get closer. A negative potential energy indicates a bound state (due to an attractive force).
The Coulomb barrier increases with the atomic numbers (i.e. the number of protons) of the colliding nuclei:

  
    
      
        
          U
          
            coul
          
        
        =
        
          
            
              
                Z
                
                  1
                
              
              
              
                Z
                
                  2
                
              
              
              
                e
                
                  2
                
              
            
            
              4
              π
              
                ϵ
                
                  0
                
              
              r
            
          
        
      
    
    {\displaystyle U_{\text{coul}}={{Z_{1}\,Z_{2}\,e^{2}} \over 4\pi \epsilon _{0}r}}
  where e is the elementary charge, and Zi the corresponding atomic numbers.
To overcome this barrier, nuclei have to collide at high velocities, so their kinetic energies drive them close enough for the strong interaction to take place and bind them together.
According to the kinetic theory of gases, the temperature of a gas is just a measure of the average kinetic energy of the particles in that gas. For classical ideal gases the velocity distribution of the gas particles is given by Maxwell–Boltzmann. From this distribution, the fraction of particles with a velocity high enough to overcome the Coulomb barrier can be determined.
In practice, temperatures needed to overcome the Coulomb barrier turned out to be smaller than expected due to quantum mechanical tunnelling, as established by Gamow. The consideration of barrier-penetration through tunnelling and the speed distribution gives rise to a limited range of conditions where fusion can take place, known as the Gamow window.
The absence of the Coulomb barrier enabled the discovery of the neutron by James Chadwick in 1932.


== References ==."
What is GMSL?,A video distribution technology used in cars,A wireless communication technology,A data storage technology,A satellite navigation technology,A virtual reality technology,A,"Gigabit Multimedia Serial Link, commonly referred to as GMSL, is a serial link technology that is used for video distribution in cars. It was developed by Maxim Integrated. Maxim Integrated was acquired by Analog Devices in 2021.  
GMSL is an asymmetric, full duplex SerDes technology - which means that it transports data at a high rate in the downlink (or forward) direction, while simultaneously transporting a lower data rate in the uplink (or reverse) direction . It transports power, bidirectional control data, Ethernet, bidirectional audio and multiple streams of unidirectional video simultaneously over a single coaxial cable or shielded twisted pair cable. A GMSL serializer receives video from a standard digital video interface such as HDMI, DisplayPort, Camera Serial Interface (CSI-2) or Display Serial Interface (DSI) over a cable up to 15 meter in length. The data is received by a deserializer that outputs it on another standard digital video interface."
Who developed GMSL?,Analog Devices,Maxim Integrated,HDMI,DisplayPort,Camera Serial Interface (CSI-2),B,"Gigabit Multimedia Serial Link, commonly referred to as GMSL, is a serial link technology that is used for video distribution in cars. It was developed by Maxim Integrated. Maxim Integrated was acquired by Analog Devices in 2021.  
GMSL is an asymmetric, full duplex SerDes technology - which means that it transports data at a high rate in the downlink (or forward) direction, while simultaneously transporting a lower data rate in the uplink (or reverse) direction . It transports power, bidirectional control data, Ethernet, bidirectional audio and multiple streams of unidirectional video simultaneously over a single coaxial cable or shielded twisted pair cable. A GMSL serializer receives video from a standard digital video interface such as HDMI, DisplayPort, Camera Serial Interface (CSI-2) or Display Serial Interface (DSI) over a cable up to 15 meter in length. The data is received by a deserializer that outputs it on another standard digital video interface."
What does GMSL transport?,Power and control data only,Video and audio only,Ethernet and audio only,"Data, audio, video, power, and control data",Video and power only,D,"Gigabit Multimedia Serial Link, commonly referred to as GMSL, is a serial link technology that is used for video distribution in cars. It was developed by Maxim Integrated. Maxim Integrated was acquired by Analog Devices in 2021.  
GMSL is an asymmetric, full duplex SerDes technology - which means that it transports data at a high rate in the downlink (or forward) direction, while simultaneously transporting a lower data rate in the uplink (or reverse) direction . It transports power, bidirectional control data, Ethernet, bidirectional audio and multiple streams of unidirectional video simultaneously over a single coaxial cable or shielded twisted pair cable. A GMSL serializer receives video from a standard digital video interface such as HDMI, DisplayPort, Camera Serial Interface (CSI-2) or Display Serial Interface (DSI) over a cable up to 15 meter in length. The data is received by a deserializer that outputs it on another standard digital video interface."
Which digital video interfaces can be used with GMSL?,USB and Thunderbolt,Bluetooth and Wi-Fi,HDMI and DisplayPort,VGA and DVI,Ethernet and USB,C,"Gigabit Multimedia Serial Link, commonly referred to as GMSL, is a serial link technology that is used for video distribution in cars. It was developed by Maxim Integrated. Maxim Integrated was acquired by Analog Devices in 2021.  
GMSL is an asymmetric, full duplex SerDes technology - which means that it transports data at a high rate in the downlink (or forward) direction, while simultaneously transporting a lower data rate in the uplink (or reverse) direction . It transports power, bidirectional control data, Ethernet, bidirectional audio and multiple streams of unidirectional video simultaneously over a single coaxial cable or shielded twisted pair cable. A GMSL serializer receives video from a standard digital video interface such as HDMI, DisplayPort, Camera Serial Interface (CSI-2) or Display Serial Interface (DSI) over a cable up to 15 meter in length. The data is received by a deserializer that outputs it on another standard digital video interface."
What is the maximum cable length for GMSL?,5 meters,10 meters,15 meters,20 meters,25 meters,C,"Gigabit Multimedia Serial Link, commonly referred to as GMSL, is a serial link technology that is used for video distribution in cars. It was developed by Maxim Integrated. Maxim Integrated was acquired by Analog Devices in 2021.  
GMSL is an asymmetric, full duplex SerDes technology - which means that it transports data at a high rate in the downlink (or forward) direction, while simultaneously transporting a lower data rate in the uplink (or reverse) direction . It transports power, bidirectional control data, Ethernet, bidirectional audio and multiple streams of unidirectional video simultaneously over a single coaxial cable or shielded twisted pair cable. A GMSL serializer receives video from a standard digital video interface such as HDMI, DisplayPort, Camera Serial Interface (CSI-2) or Display Serial Interface (DSI) over a cable up to 15 meter in length. The data is received by a deserializer that outputs it on another standard digital video interface."
What is a morphic word?,An infinite sequence of symbols constructed from a particular class of endomorphism of a free monoid.,A sequence of letters that is generated by a fixed point of a prolongable k-uniform morphism.,A word that is the limit of a sequence of letters that begins with a specific letter and is generated by applying a specific endomorphism repeatedly.,A word that is the image of a pure morphic word under a coding.,A word that is the result of applying a deterministic context-free Lindenmayer system to a given word.,A,"In mathematics and computer science, a morphic word or substitutive word is an infinite sequence of symbols which is constructed from a particular class of endomorphism of a free monoid.
Every automatic sequence is morphic.

Definition
Let f be an endomorphism of the free monoid A∗ on an alphabet A with the property that there is a letter a such that f(a) = as for a non-empty string s: we say that f is prolongable at a.  The word

  
    
      a
      s
      f
      (
      s
      )
      f
      (
      f
      (
      s
      )
      )
      ⋯
      
        f
        
          (
          n
          )
        
      
      (
      s
      )
      ⋯
       
    
    asf(s)f(f(s))\cdots f^{(n)}(s)\cdots \
  is a pure morphic or pure substitutive word.  Note that it is the limit of the sequence a, f(a), f(f(a)), f(f(f(a))), ...
It is clearly a fixed point of the endomorphism f: the unique such sequence beginning with the letter a. In general, a morphic word is the image of a pure morphic word under a coding, that is, a morphism that maps letter to letter.If a morphic word is constructed as the fixed point of a prolongable k-uniform morphism on A∗ then the word is k-automatic.  The n-th term in such a sequence can be produced by a finite state automaton reading the digits of n in base k.

Examples
The Thue–Morse sequence is generated over {0,1} by the 2-uniform endomorphism 0 → 01, 1 → 10.
The Fibonacci word  is generated over {a,b} by the endomorphism a → ab, b → a.
The tribonacci word is generated over {a,b,c} by the endomorphism a → ab, b → ac, c → a.
The Rudin–Shapiro sequence is obtained from the fixed point of the 2-uniform morphism a → ab, b → ac, c → db, d → dc  followed by the coding a,b → 0, c,d → 1.
The regular paperfolding sequence is obtained from the fixed point of the 2-uniform morphism a → ab, b → cb, c → ad, d → cd  followed by the coding a,b → 0, c,d → 1.

D0L system
A D0L system (deterministic context-free Lindenmayer system) is given by a word w of the free monoid A∗ on an alphabet A together with a morphism σ prolongable at w.  The system generates the infinite D0L word ω = limn→∞ σn(w).  Purely morphic words are D0L words but not conversely."
What is the property of an endomorphism that makes it prolongable at a certain letter?,The endomorphism maps the letter to itself.,The endomorphism maps the letter to a non-empty string.,The endomorphism maps the letter to a string that starts with the letter itself.,The endomorphism maps the letter to a string that ends with the letter itself.,The endomorphism maps the letter to a string that contains the letter itself.,C,"In mathematics and computer science, a morphic word or substitutive word is an infinite sequence of symbols which is constructed from a particular class of endomorphism of a free monoid.
Every automatic sequence is morphic.

Definition
Let f be an endomorphism of the free monoid A∗ on an alphabet A with the property that there is a letter a such that f(a) = as for a non-empty string s: we say that f is prolongable at a.  The word

  
    
      a
      s
      f
      (
      s
      )
      f
      (
      f
      (
      s
      )
      )
      ⋯
      
        f
        
          (
          n
          )
        
      
      (
      s
      )
      ⋯
       
    
    asf(s)f(f(s))\cdots f^{(n)}(s)\cdots \
  is a pure morphic or pure substitutive word.  Note that it is the limit of the sequence a, f(a), f(f(a)), f(f(f(a))), ...
It is clearly a fixed point of the endomorphism f: the unique such sequence beginning with the letter a. In general, a morphic word is the image of a pure morphic word under a coding, that is, a morphism that maps letter to letter.If a morphic word is constructed as the fixed point of a prolongable k-uniform morphism on A∗ then the word is k-automatic.  The n-th term in such a sequence can be produced by a finite state automaton reading the digits of n in base k.

Examples
The Thue–Morse sequence is generated over {0,1} by the 2-uniform endomorphism 0 → 01, 1 → 10.
The Fibonacci word  is generated over {a,b} by the endomorphism a → ab, b → a.
The tribonacci word is generated over {a,b,c} by the endomorphism a → ab, b → ac, c → a.
The Rudin–Shapiro sequence is obtained from the fixed point of the 2-uniform morphism a → ab, b → ac, c → db, d → dc  followed by the coding a,b → 0, c,d → 1.
The regular paperfolding sequence is obtained from the fixed point of the 2-uniform morphism a → ab, b → cb, c → ad, d → cd  followed by the coding a,b → 0, c,d → 1.

D0L system
A D0L system (deterministic context-free Lindenmayer system) is given by a word w of the free monoid A∗ on an alphabet A together with a morphism σ prolongable at w.  The system generates the infinite D0L word ω = limn→∞ σn(w).  Purely morphic words are D0L words but not conversely."
What is the Thue-Morse sequence?,"An infinite sequence of letters generated over {0,1} by the 2-uniform endomorphism 0 → 01, 1 → 10.","An infinite sequence of letters generated over {a,b} by the endomorphism a → ab, b → a.","An infinite sequence of letters generated over {a,b,c} by the endomorphism a → ab, b → ac, c → a.","An infinite sequence of letters obtained from the fixed point of the 2-uniform morphism a → ab, b → ac, c → db, d → dc followed by a specific coding.","An infinite sequence of letters obtained from the fixed point of the 2-uniform morphism a → ab, b → cb, c → ad, d → cd followed by a specific coding.",A,"In mathematics and computer science, a morphic word or substitutive word is an infinite sequence of symbols which is constructed from a particular class of endomorphism of a free monoid.
Every automatic sequence is morphic.

Definition
Let f be an endomorphism of the free monoid A∗ on an alphabet A with the property that there is a letter a such that f(a) = as for a non-empty string s: we say that f is prolongable at a.  The word

  
    
      a
      s
      f
      (
      s
      )
      f
      (
      f
      (
      s
      )
      )
      ⋯
      
        f
        
          (
          n
          )
        
      
      (
      s
      )
      ⋯
       
    
    asf(s)f(f(s))\cdots f^{(n)}(s)\cdots \
  is a pure morphic or pure substitutive word.  Note that it is the limit of the sequence a, f(a), f(f(a)), f(f(f(a))), ...
It is clearly a fixed point of the endomorphism f: the unique such sequence beginning with the letter a. In general, a morphic word is the image of a pure morphic word under a coding, that is, a morphism that maps letter to letter.If a morphic word is constructed as the fixed point of a prolongable k-uniform morphism on A∗ then the word is k-automatic.  The n-th term in such a sequence can be produced by a finite state automaton reading the digits of n in base k.

Examples
The Thue–Morse sequence is generated over {0,1} by the 2-uniform endomorphism 0 → 01, 1 → 10.
The Fibonacci word  is generated over {a,b} by the endomorphism a → ab, b → a.
The tribonacci word is generated over {a,b,c} by the endomorphism a → ab, b → ac, c → a.
The Rudin–Shapiro sequence is obtained from the fixed point of the 2-uniform morphism a → ab, b → ac, c → db, d → dc  followed by the coding a,b → 0, c,d → 1.
The regular paperfolding sequence is obtained from the fixed point of the 2-uniform morphism a → ab, b → cb, c → ad, d → cd  followed by the coding a,b → 0, c,d → 1.

D0L system
A D0L system (deterministic context-free Lindenmayer system) is given by a word w of the free monoid A∗ on an alphabet A together with a morphism σ prolongable at w.  The system generates the infinite D0L word ω = limn→∞ σn(w).  Purely morphic words are D0L words but not conversely."
What is the Fibonacci word?,"An infinite sequence of letters generated over {0,1} by the 2-uniform endomorphism 0 → 01, 1 → 10.","An infinite sequence of letters generated over {a,b} by the endomorphism a → ab, b → a.","An infinite sequence of letters generated over {a,b,c} by the endomorphism a → ab, b → ac, c → a.","An infinite sequence of letters obtained from the fixed point of the 2-uniform morphism a → ab, b → ac, c → db, d → dc followed by a specific coding.","An infinite sequence of letters obtained from the fixed point of the 2-uniform morphism a → ab, b → cb, c → ad, d → cd followed by a specific coding.",B,"In mathematics and computer science, a morphic word or substitutive word is an infinite sequence of symbols which is constructed from a particular class of endomorphism of a free monoid.
Every automatic sequence is morphic.

Definition
Let f be an endomorphism of the free monoid A∗ on an alphabet A with the property that there is a letter a such that f(a) = as for a non-empty string s: we say that f is prolongable at a.  The word

  
    
      a
      s
      f
      (
      s
      )
      f
      (
      f
      (
      s
      )
      )
      ⋯
      
        f
        
          (
          n
          )
        
      
      (
      s
      )
      ⋯
       
    
    asf(s)f(f(s))\cdots f^{(n)}(s)\cdots \
  is a pure morphic or pure substitutive word.  Note that it is the limit of the sequence a, f(a), f(f(a)), f(f(f(a))), ...
It is clearly a fixed point of the endomorphism f: the unique such sequence beginning with the letter a. In general, a morphic word is the image of a pure morphic word under a coding, that is, a morphism that maps letter to letter.If a morphic word is constructed as the fixed point of a prolongable k-uniform morphism on A∗ then the word is k-automatic.  The n-th term in such a sequence can be produced by a finite state automaton reading the digits of n in base k.

Examples
The Thue–Morse sequence is generated over {0,1} by the 2-uniform endomorphism 0 → 01, 1 → 10.
The Fibonacci word  is generated over {a,b} by the endomorphism a → ab, b → a.
The tribonacci word is generated over {a,b,c} by the endomorphism a → ab, b → ac, c → a.
The Rudin–Shapiro sequence is obtained from the fixed point of the 2-uniform morphism a → ab, b → ac, c → db, d → dc  followed by the coding a,b → 0, c,d → 1.
The regular paperfolding sequence is obtained from the fixed point of the 2-uniform morphism a → ab, b → cb, c → ad, d → cd  followed by the coding a,b → 0, c,d → 1.

D0L system
A D0L system (deterministic context-free Lindenmayer system) is given by a word w of the free monoid A∗ on an alphabet A together with a morphism σ prolongable at w.  The system generates the infinite D0L word ω = limn→∞ σn(w).  Purely morphic words are D0L words but not conversely."
What is a D0L system?,A system that generates a D0L word by applying a morphism to a given word.,A system that generates a D0L word by applying a specific coding to a given word.,A system that generates a D0L word by applying a fixed point of a prolongable k-uniform morphism to a given word.,A system that generates a D0L word by applying a specific mapping to a given word.,A system that generates a D0L word by applying a finite state automaton to the digits of a number in base k.,C,"In mathematics and computer science, a morphic word or substitutive word is an infinite sequence of symbols which is constructed from a particular class of endomorphism of a free monoid.
Every automatic sequence is morphic.

Definition
Let f be an endomorphism of the free monoid A∗ on an alphabet A with the property that there is a letter a such that f(a) = as for a non-empty string s: we say that f is prolongable at a.  The word

  
    
      a
      s
      f
      (
      s
      )
      f
      (
      f
      (
      s
      )
      )
      ⋯
      
        f
        
          (
          n
          )
        
      
      (
      s
      )
      ⋯
       
    
    asf(s)f(f(s))\cdots f^{(n)}(s)\cdots \
  is a pure morphic or pure substitutive word.  Note that it is the limit of the sequence a, f(a), f(f(a)), f(f(f(a))), ...
It is clearly a fixed point of the endomorphism f: the unique such sequence beginning with the letter a. In general, a morphic word is the image of a pure morphic word under a coding, that is, a morphism that maps letter to letter.If a morphic word is constructed as the fixed point of a prolongable k-uniform morphism on A∗ then the word is k-automatic.  The n-th term in such a sequence can be produced by a finite state automaton reading the digits of n in base k.

Examples
The Thue–Morse sequence is generated over {0,1} by the 2-uniform endomorphism 0 → 01, 1 → 10.
The Fibonacci word  is generated over {a,b} by the endomorphism a → ab, b → a.
The tribonacci word is generated over {a,b,c} by the endomorphism a → ab, b → ac, c → a.
The Rudin–Shapiro sequence is obtained from the fixed point of the 2-uniform morphism a → ab, b → ac, c → db, d → dc  followed by the coding a,b → 0, c,d → 1.
The regular paperfolding sequence is obtained from the fixed point of the 2-uniform morphism a → ab, b → cb, c → ad, d → cd  followed by the coding a,b → 0, c,d → 1.

D0L system
A D0L system (deterministic context-free Lindenmayer system) is given by a word w of the free monoid A∗ on an alphabet A together with a morphism σ prolongable at w.  The system generates the infinite D0L word ω = limn→∞ σn(w).  Purely morphic words are D0L words but not conversely."
What is the purpose of the ZetaGrid project?,To explore the non-trivial roots of the Riemann zeta function,To develop distributed computing technology,To study the stability of hosting providers,To disprove the Riemann hypothesis,To verify the Riemann hypothesis,A,"ZetaGrid was at one time the largest distributed computing project, designed to explore the non-trivial roots of the Riemann zeta function, checking over one billion roots a day.
Roots of the zeta function are of particular interest in mathematics; a single root out of alignment would disprove the Riemann hypothesis, with far-reaching consequences for all of mathematics. As of June, 2023 no counterexample to the Riemann hypothesis has been found.
The project ended in November 2005 due to instability of the hosting provider. The first more than 1013 zeroes were checked. The project administrator stated that after the results were analyzed, they would be posted on the American Mathematical Society website. The official status remains unclear, however, as it was never published nor independently verified. This is likely because there was no evidence that each zero was actually computed, as there was no process implemented to check each one as it was calculated.

References
External links
Home page (Web archive)."
What would be the consequence of finding a single root out of alignment?,It would prove the Riemann hypothesis,It would disprove the Riemann hypothesis,It would end the ZetaGrid project,It would lead to the development of distributed computing technology,It would have no impact on mathematics,B,"ZetaGrid was at one time the largest distributed computing project, designed to explore the non-trivial roots of the Riemann zeta function, checking over one billion roots a day.
Roots of the zeta function are of particular interest in mathematics; a single root out of alignment would disprove the Riemann hypothesis, with far-reaching consequences for all of mathematics. As of June, 2023 no counterexample to the Riemann hypothesis has been found.
The project ended in November 2005 due to instability of the hosting provider. The first more than 1013 zeroes were checked. The project administrator stated that after the results were analyzed, they would be posted on the American Mathematical Society website. The official status remains unclear, however, as it was never published nor independently verified. This is likely because there was no evidence that each zero was actually computed, as there was no process implemented to check each one as it was calculated.

References
External links
Home page (Web archive)."
Why did the ZetaGrid project end in 2005?,Due to the completion of checking over one billion roots,Due to instability of the hosting provider,Due to the discovery of a counterexample to the Riemann hypothesis,Due to lack of interest from mathematicians,Due to budget constraints,B,"ZetaGrid was at one time the largest distributed computing project, designed to explore the non-trivial roots of the Riemann zeta function, checking over one billion roots a day.
Roots of the zeta function are of particular interest in mathematics; a single root out of alignment would disprove the Riemann hypothesis, with far-reaching consequences for all of mathematics. As of June, 2023 no counterexample to the Riemann hypothesis has been found.
The project ended in November 2005 due to instability of the hosting provider. The first more than 1013 zeroes were checked. The project administrator stated that after the results were analyzed, they would be posted on the American Mathematical Society website. The official status remains unclear, however, as it was never published nor independently verified. This is likely because there was no evidence that each zero was actually computed, as there was no process implemented to check each one as it was calculated.

References
External links
Home page (Web archive)."
How many zeroes were checked in the ZetaGrid project?,More than 1010,More than 1011,More than 1012,More than 1013,More than 1014,D,"ZetaGrid was at one time the largest distributed computing project, designed to explore the non-trivial roots of the Riemann zeta function, checking over one billion roots a day.
Roots of the zeta function are of particular interest in mathematics; a single root out of alignment would disprove the Riemann hypothesis, with far-reaching consequences for all of mathematics. As of June, 2023 no counterexample to the Riemann hypothesis has been found.
The project ended in November 2005 due to instability of the hosting provider. The first more than 1013 zeroes were checked. The project administrator stated that after the results were analyzed, they would be posted on the American Mathematical Society website. The official status remains unclear, however, as it was never published nor independently verified. This is likely because there was no evidence that each zero was actually computed, as there was no process implemented to check each one as it was calculated.

References
External links
Home page (Web archive)."
Why were the results of the ZetaGrid project not published?,The results were lost due to a technical issue,The results were inconclusive,There was no evidence that each zero was actually computed,The project administrator decided not to publish them,The American Mathematical Society rejected the publication,C,"ZetaGrid was at one time the largest distributed computing project, designed to explore the non-trivial roots of the Riemann zeta function, checking over one billion roots a day.
Roots of the zeta function are of particular interest in mathematics; a single root out of alignment would disprove the Riemann hypothesis, with far-reaching consequences for all of mathematics. As of June, 2023 no counterexample to the Riemann hypothesis has been found.
The project ended in November 2005 due to instability of the hosting provider. The first more than 1013 zeroes were checked. The project administrator stated that after the results were analyzed, they would be posted on the American Mathematical Society website. The official status remains unclear, however, as it was never published nor independently verified. This is likely because there was no evidence that each zero was actually computed, as there was no process implemented to check each one as it was calculated.

References
External links
Home page (Web archive)."
What is natural science?,A systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.,The study of nature and its phenomena based on empirical evidence.,A branch of science that focuses on academic disciplines.,An overview of academic fields and professions.,The study of physical and life sciences.,A,"The following outline is provided as an overview of and topical guide to natural science:
Natural science – a major branch of science that tries to explain, and predict, nature's phenomena based on empirical evidence. In natural science, hypothesis must be verified scientifically to be regarded as scientific theory.  Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into 2 main branches: life science, and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.

Overview
Natural science can be described as all of the following:

Branch of science – systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.
Major category of academic disciplines – an academic discipline is focused study in one academic field or profession. A discipline incorporates expertise, people, projects, communities, challenges, studies, inquiry, and research areas that are strongly associated with academic areas of study or areas of professional practice. For example, the branches of science are commonly referred to as the scientific disciplines."
What is a scientific theory?,A hypothesis that has been verified scientifically.,An explanation or prediction about nature's phenomena based on empirical evidence.,A branch of natural science that focuses on academic disciplines.,A systematic enterprise that builds and organizes knowledge.,A set of criteria and methods used for quality control in natural science.,B,"The following outline is provided as an overview of and topical guide to natural science:
Natural science – a major branch of science that tries to explain, and predict, nature's phenomena based on empirical evidence. In natural science, hypothesis must be verified scientifically to be regarded as scientific theory.  Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into 2 main branches: life science, and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.

Overview
Natural science can be described as all of the following:

Branch of science – systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.
Major category of academic disciplines – an academic discipline is focused study in one academic field or profession. A discipline incorporates expertise, people, projects, communities, challenges, studies, inquiry, and research areas that are strongly associated with academic areas of study or areas of professional practice. For example, the branches of science are commonly referred to as the scientific disciplines."
What are the criteria and methods used for quality control in natural science?,"Validity, accuracy, and social mechanisms.",Peer review and repeatability of findings.,Empirical evidence and scientific theories.,Systematic enterprise and academic disciplines.,Research areas and scientific explanations.,B,"The following outline is provided as an overview of and topical guide to natural science:
Natural science – a major branch of science that tries to explain, and predict, nature's phenomena based on empirical evidence. In natural science, hypothesis must be verified scientifically to be regarded as scientific theory.  Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into 2 main branches: life science, and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.

Overview
Natural science can be described as all of the following:

Branch of science – systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.
Major category of academic disciplines – an academic discipline is focused study in one academic field or profession. A discipline incorporates expertise, people, projects, communities, challenges, studies, inquiry, and research areas that are strongly associated with academic areas of study or areas of professional practice. For example, the branches of science are commonly referred to as the scientific disciplines."
How can natural science be broken down into branches?,By studying academic disciplines and professional practices.,By focusing on the study of nature and its phenomena.,By categorizing academic fields and professions.,By dividing it into life science and physical science.,By organizing knowledge and testable explanations.,D,"The following outline is provided as an overview of and topical guide to natural science:
Natural science – a major branch of science that tries to explain, and predict, nature's phenomena based on empirical evidence. In natural science, hypothesis must be verified scientifically to be regarded as scientific theory.  Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into 2 main branches: life science, and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.

Overview
Natural science can be described as all of the following:

Branch of science – systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.
Major category of academic disciplines – an academic discipline is focused study in one academic field or profession. A discipline incorporates expertise, people, projects, communities, challenges, studies, inquiry, and research areas that are strongly associated with academic areas of study or areas of professional practice. For example, the branches of science are commonly referred to as the scientific disciplines."
What are the sub-branches of natural science?,Academic disciplines and research areas.,"Expertise, projects, and communities.",Scientific theories and empirical evidence.,Life science and physical science.,Inquiries and challenges.,D,"The following outline is provided as an overview of and topical guide to natural science:
Natural science – a major branch of science that tries to explain, and predict, nature's phenomena based on empirical evidence. In natural science, hypothesis must be verified scientifically to be regarded as scientific theory.  Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into 2 main branches: life science, and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.

Overview
Natural science can be described as all of the following:

Branch of science – systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.
Major category of academic disciplines – an academic discipline is focused study in one academic field or profession. A discipline incorporates expertise, people, projects, communities, challenges, studies, inquiry, and research areas that are strongly associated with academic areas of study or areas of professional practice. For example, the branches of science are commonly referred to as the scientific disciplines."
What is formal linguistics?,The branch of linguistics that studies the formal aspects of natural languages,The branch of linguistics that studies the history of language,The branch of linguistics that focuses on the social aspects of language,The branch of linguistics that studies the phonetics and phonology of language,The branch of linguistics that focuses on language acquisition,A,"Formal linguistics is the branch of linguistics which uses applied mathematical methods for the analysis of natural languages. Such methods include formal languages, formal grammars and first-order logical expressions. Formal linguistics also forms the basis of computational linguistics. Since the 1980s, the term is often used to refer to Chomskyan linguistics.

Approaches
Semiotic
Methods of formal linguistics were introduced by semioticians such as Charles Sanders Peirce and Louis Hjelmslev. Building on the work of David Hilbert and Rudolf Carnap, Hjelmslev proposed the use of formal grammars to analyse, generate and explain language in his 1943 book Prolegomena to a Theory of Language. In this view, language is regarded as arising from a mathematical relationship between meaning and form.
The formal description of language was further developed by linguists including J. R."
Which methods are used in formal linguistics?,Statistical analysis and machine learning,Applied mathematical methods and formal grammars,Ethnographic observation and fieldwork,Psycholinguistic experiments and cognitive modeling,Historical analysis and comparative linguistics,B,"Formal linguistics is the branch of linguistics which uses applied mathematical methods for the analysis of natural languages. Such methods include formal languages, formal grammars and first-order logical expressions. Formal linguistics also forms the basis of computational linguistics. Since the 1980s, the term is often used to refer to Chomskyan linguistics.

Approaches
Semiotic
Methods of formal linguistics were introduced by semioticians such as Charles Sanders Peirce and Louis Hjelmslev. Building on the work of David Hilbert and Rudolf Carnap, Hjelmslev proposed the use of formal grammars to analyse, generate and explain language in his 1943 book Prolegomena to a Theory of Language. In this view, language is regarded as arising from a mathematical relationship between meaning and form.
The formal description of language was further developed by linguists including J. R."
Who proposed the use of formal grammars in linguistics?,Charles Sanders Peirce,Louis Hjelmslev,David Hilbert,Rudolf Carnap,J. R. Firth,B,"Formal linguistics is the branch of linguistics which uses applied mathematical methods for the analysis of natural languages. Such methods include formal languages, formal grammars and first-order logical expressions. Formal linguistics also forms the basis of computational linguistics. Since the 1980s, the term is often used to refer to Chomskyan linguistics.

Approaches
Semiotic
Methods of formal linguistics were introduced by semioticians such as Charles Sanders Peirce and Louis Hjelmslev. Building on the work of David Hilbert and Rudolf Carnap, Hjelmslev proposed the use of formal grammars to analyse, generate and explain language in his 1943 book Prolegomena to a Theory of Language. In this view, language is regarded as arising from a mathematical relationship between meaning and form.
The formal description of language was further developed by linguists including J. R."
What is the relationship between meaning and form in language according to formal linguistics?,There is no relationship between meaning and form,Meaning and form are determined by cultural context,Meaning and form are determined by individual speakers,Meaning and form are determined by a mathematical relationship,Meaning and form are determined by historical evolution,D,"Formal linguistics is the branch of linguistics which uses applied mathematical methods for the analysis of natural languages. Such methods include formal languages, formal grammars and first-order logical expressions. Formal linguistics also forms the basis of computational linguistics. Since the 1980s, the term is often used to refer to Chomskyan linguistics.

Approaches
Semiotic
Methods of formal linguistics were introduced by semioticians such as Charles Sanders Peirce and Louis Hjelmslev. Building on the work of David Hilbert and Rudolf Carnap, Hjelmslev proposed the use of formal grammars to analyse, generate and explain language in his 1943 book Prolegomena to a Theory of Language. In this view, language is regarded as arising from a mathematical relationship between meaning and form.
The formal description of language was further developed by linguists including J. R."
"What is the current use of the term ""formal linguistics""?",Referring to the study of language acquisition,Referring to the study of the social aspects of language,Referring to the study of the phonetics and phonology of language,Referring to Chomskyan linguistics,Referring to the historical analysis of language,D,"Formal linguistics is the branch of linguistics which uses applied mathematical methods for the analysis of natural languages. Such methods include formal languages, formal grammars and first-order logical expressions. Formal linguistics also forms the basis of computational linguistics. Since the 1980s, the term is often used to refer to Chomskyan linguistics.

Approaches
Semiotic
Methods of formal linguistics were introduced by semioticians such as Charles Sanders Peirce and Louis Hjelmslev. Building on the work of David Hilbert and Rudolf Carnap, Hjelmslev proposed the use of formal grammars to analyse, generate and explain language in his 1943 book Prolegomena to a Theory of Language. In this view, language is regarded as arising from a mathematical relationship between meaning and form.
The formal description of language was further developed by linguists including J. R."
What is quantum engineering?,The development of technology that capitalizes on the laws of classical mechanics,The development of technology that capitalizes on the laws of quantum mechanics,The development of technology that capitalizes on the laws of Newtonian mechanics,The development of technology that capitalizes on the laws of electromagnetism,The development of technology that capitalizes on the laws of thermodynamics,B,"Quantum engineering is the development of technology that capitalizes on the laws of quantum mechanics. Quantum engineering uses quantum mechanics as a toolbox for the development of quantum technologies, such as quantum sensors or quantum computers.
There are many devices available which rely on quantum mechanical effects and have revolutionized society through medicine, optical communication, high-speed internet, and high-performance computing, just to mention a few examples. Nowadays, after the first quantum revolution that brought us lasers, MRI imagers and transistors, a second wave of quantum technologies is expected to impact society in a similar way. This second quantum revolution makes use of quantum coherence and capitalizes on the great progress achieved in the last century in understanding and controlling atomic-scale systems. It is expected to help solve many of today's global challenges and has triggered several initiatives and research programs all over the globe. Quantum mechanical effects are used as a resource in novel technologies with far-reaching applications, including quantum sensors and novel imaging techniques, secure communication (quantum internet) and quantum computing.

Education programs
Quantum engineering is evolving into its own engineering discipline. The quantum industry requires a quantum-literate workforce, a missing resource at the moment."
What are some examples of quantum technologies?,X-ray machines and microscopes,Televisions and radios,Refrigerators and dishwashers,Lasers and MRI imagers,Cars and airplanes,D,"Quantum engineering is the development of technology that capitalizes on the laws of quantum mechanics. Quantum engineering uses quantum mechanics as a toolbox for the development of quantum technologies, such as quantum sensors or quantum computers.
There are many devices available which rely on quantum mechanical effects and have revolutionized society through medicine, optical communication, high-speed internet, and high-performance computing, just to mention a few examples. Nowadays, after the first quantum revolution that brought us lasers, MRI imagers and transistors, a second wave of quantum technologies is expected to impact society in a similar way. This second quantum revolution makes use of quantum coherence and capitalizes on the great progress achieved in the last century in understanding and controlling atomic-scale systems. It is expected to help solve many of today's global challenges and has triggered several initiatives and research programs all over the globe. Quantum mechanical effects are used as a resource in novel technologies with far-reaching applications, including quantum sensors and novel imaging techniques, secure communication (quantum internet) and quantum computing.

Education programs
Quantum engineering is evolving into its own engineering discipline. The quantum industry requires a quantum-literate workforce, a missing resource at the moment."
What is the second quantum revolution?,The development of quantum technologies using classical mechanics,The development of quantum technologies using quantum coherence,The development of quantum technologies using Newtonian mechanics,The development of quantum technologies using electromagnetism,The development of quantum technologies using thermodynamics,B,"Quantum engineering is the development of technology that capitalizes on the laws of quantum mechanics. Quantum engineering uses quantum mechanics as a toolbox for the development of quantum technologies, such as quantum sensors or quantum computers.
There are many devices available which rely on quantum mechanical effects and have revolutionized society through medicine, optical communication, high-speed internet, and high-performance computing, just to mention a few examples. Nowadays, after the first quantum revolution that brought us lasers, MRI imagers and transistors, a second wave of quantum technologies is expected to impact society in a similar way. This second quantum revolution makes use of quantum coherence and capitalizes on the great progress achieved in the last century in understanding and controlling atomic-scale systems. It is expected to help solve many of today's global challenges and has triggered several initiatives and research programs all over the globe. Quantum mechanical effects are used as a resource in novel technologies with far-reaching applications, including quantum sensors and novel imaging techniques, secure communication (quantum internet) and quantum computing.

Education programs
Quantum engineering is evolving into its own engineering discipline. The quantum industry requires a quantum-literate workforce, a missing resource at the moment."
What are some applications of quantum mechanical effects?,Quantum sensors and novel imaging techniques,Cooking and cleaning,Farming and gardening,Writing and reading,Driving and traveling,A,"Quantum engineering is the development of technology that capitalizes on the laws of quantum mechanics. Quantum engineering uses quantum mechanics as a toolbox for the development of quantum technologies, such as quantum sensors or quantum computers.
There are many devices available which rely on quantum mechanical effects and have revolutionized society through medicine, optical communication, high-speed internet, and high-performance computing, just to mention a few examples. Nowadays, after the first quantum revolution that brought us lasers, MRI imagers and transistors, a second wave of quantum technologies is expected to impact society in a similar way. This second quantum revolution makes use of quantum coherence and capitalizes on the great progress achieved in the last century in understanding and controlling atomic-scale systems. It is expected to help solve many of today's global challenges and has triggered several initiatives and research programs all over the globe. Quantum mechanical effects are used as a resource in novel technologies with far-reaching applications, including quantum sensors and novel imaging techniques, secure communication (quantum internet) and quantum computing.

Education programs
Quantum engineering is evolving into its own engineering discipline. The quantum industry requires a quantum-literate workforce, a missing resource at the moment."
What is needed to support the quantum industry?,A quantum-literate workforce,A quantum-illiterate workforce,A quantum-resistant workforce,A quantum-conscious workforce,A quantum-resistant workforce,A,"Quantum engineering is the development of technology that capitalizes on the laws of quantum mechanics. Quantum engineering uses quantum mechanics as a toolbox for the development of quantum technologies, such as quantum sensors or quantum computers.
There are many devices available which rely on quantum mechanical effects and have revolutionized society through medicine, optical communication, high-speed internet, and high-performance computing, just to mention a few examples. Nowadays, after the first quantum revolution that brought us lasers, MRI imagers and transistors, a second wave of quantum technologies is expected to impact society in a similar way. This second quantum revolution makes use of quantum coherence and capitalizes on the great progress achieved in the last century in understanding and controlling atomic-scale systems. It is expected to help solve many of today's global challenges and has triggered several initiatives and research programs all over the globe. Quantum mechanical effects are used as a resource in novel technologies with far-reaching applications, including quantum sensors and novel imaging techniques, secure communication (quantum internet) and quantum computing.

Education programs
Quantum engineering is evolving into its own engineering discipline. The quantum industry requires a quantum-literate workforce, a missing resource at the moment."
"What is the main subject of ""A New Kind of Science""?",The study of cellular automata,The exploration of computational systems,The nature of computation and its implications,The study of simple abstract rules,The relevance of simple programs to other fields of science,E,"A New Kind of Science is a book by Stephen Wolfram, published by his company Wolfram Research under the imprint Wolfram Media in 2002. It contains an empirical and systematic study of computational systems such as cellular automata. Wolfram calls these systems simple programs and argues that the scientific philosophy and methods appropriate for the study of simple programs are relevant to other fields of science.

Contents
Computation and its implications
The thesis of A New Kind of Science (NKS) is twofold: that the nature of computation must be explored experimentally, and that the results of these experiments have great relevance to understanding the physical world.

Simple programs
The basic subject of Wolfram's ""new kind of science"" is the study of simple abstract rules—essentially, elementary computer programs. In almost any class of a computational system, one very quickly finds instances of great complexity among its simplest cases (after a time series of multiple iterative loops, applying the same simple set of rules on itself, similar to a self-reinforcing cycle using a set of rules). This seems to be true regardless of the components of the system and the details of its setup. Systems explored in the book include, amongst others, cellular automata in one, two, and three dimensions; mobile automata; Turing machines in 1 and 2 dimensions; several varieties of substitution and network systems; recursive functions; nested recursive functions; combinators; tag systems; register machines; reversal-addition. For a program to qualify as simple, there are several requirements:

Its operation can be completely explained by a simple graphical illustration.
It can be completely explained in a few sentences of human language.
It can be implemented in a computer language using just a few lines of code.
The number of its possible variations is small enough so that all of them can be computed.Generally, simple programs tend to have a very simple abstract framework."
"According to the book, what is the nature of computation?",It must be explored experimentally,It is irrelevant to understanding the physical world,It can be completely explained in a few sentences of human language,It is only relevant to the study of simple programs,It can be implemented in a computer language using just a few lines of code,A,"A New Kind of Science is a book by Stephen Wolfram, published by his company Wolfram Research under the imprint Wolfram Media in 2002. It contains an empirical and systematic study of computational systems such as cellular automata. Wolfram calls these systems simple programs and argues that the scientific philosophy and methods appropriate for the study of simple programs are relevant to other fields of science.

Contents
Computation and its implications
The thesis of A New Kind of Science (NKS) is twofold: that the nature of computation must be explored experimentally, and that the results of these experiments have great relevance to understanding the physical world.

Simple programs
The basic subject of Wolfram's ""new kind of science"" is the study of simple abstract rules—essentially, elementary computer programs. In almost any class of a computational system, one very quickly finds instances of great complexity among its simplest cases (after a time series of multiple iterative loops, applying the same simple set of rules on itself, similar to a self-reinforcing cycle using a set of rules). This seems to be true regardless of the components of the system and the details of its setup. Systems explored in the book include, amongst others, cellular automata in one, two, and three dimensions; mobile automata; Turing machines in 1 and 2 dimensions; several varieties of substitution and network systems; recursive functions; nested recursive functions; combinators; tag systems; register machines; reversal-addition. For a program to qualify as simple, there are several requirements:

Its operation can be completely explained by a simple graphical illustration.
It can be completely explained in a few sentences of human language.
It can be implemented in a computer language using just a few lines of code.
The number of its possible variations is small enough so that all of them can be computed.Generally, simple programs tend to have a very simple abstract framework."
What is required for a program to qualify as simple?,It can be completely explained by a simple graphical illustration,It can be completely explained in a few sentences of human language,Its operation is difficult to understand,It has a large number of possible variations,It is not applicable to computational systems,A,"A New Kind of Science is a book by Stephen Wolfram, published by his company Wolfram Research under the imprint Wolfram Media in 2002. It contains an empirical and systematic study of computational systems such as cellular automata. Wolfram calls these systems simple programs and argues that the scientific philosophy and methods appropriate for the study of simple programs are relevant to other fields of science.

Contents
Computation and its implications
The thesis of A New Kind of Science (NKS) is twofold: that the nature of computation must be explored experimentally, and that the results of these experiments have great relevance to understanding the physical world.

Simple programs
The basic subject of Wolfram's ""new kind of science"" is the study of simple abstract rules—essentially, elementary computer programs. In almost any class of a computational system, one very quickly finds instances of great complexity among its simplest cases (after a time series of multiple iterative loops, applying the same simple set of rules on itself, similar to a self-reinforcing cycle using a set of rules). This seems to be true regardless of the components of the system and the details of its setup. Systems explored in the book include, amongst others, cellular automata in one, two, and three dimensions; mobile automata; Turing machines in 1 and 2 dimensions; several varieties of substitution and network systems; recursive functions; nested recursive functions; combinators; tag systems; register machines; reversal-addition. For a program to qualify as simple, there are several requirements:

Its operation can be completely explained by a simple graphical illustration.
It can be completely explained in a few sentences of human language.
It can be implemented in a computer language using just a few lines of code.
The number of its possible variations is small enough so that all of them can be computed.Generally, simple programs tend to have a very simple abstract framework."
What are some of the systems explored in the book?,"Cellular automata in one, two, and three dimensions",Mobile automata,Turing machines in 1 and 2 dimensions,Several varieties of substitution and network systems,All of the above,E,"A New Kind of Science is a book by Stephen Wolfram, published by his company Wolfram Research under the imprint Wolfram Media in 2002. It contains an empirical and systematic study of computational systems such as cellular automata. Wolfram calls these systems simple programs and argues that the scientific philosophy and methods appropriate for the study of simple programs are relevant to other fields of science.

Contents
Computation and its implications
The thesis of A New Kind of Science (NKS) is twofold: that the nature of computation must be explored experimentally, and that the results of these experiments have great relevance to understanding the physical world.

Simple programs
The basic subject of Wolfram's ""new kind of science"" is the study of simple abstract rules—essentially, elementary computer programs. In almost any class of a computational system, one very quickly finds instances of great complexity among its simplest cases (after a time series of multiple iterative loops, applying the same simple set of rules on itself, similar to a self-reinforcing cycle using a set of rules). This seems to be true regardless of the components of the system and the details of its setup. Systems explored in the book include, amongst others, cellular automata in one, two, and three dimensions; mobile automata; Turing machines in 1 and 2 dimensions; several varieties of substitution and network systems; recursive functions; nested recursive functions; combinators; tag systems; register machines; reversal-addition. For a program to qualify as simple, there are several requirements:

Its operation can be completely explained by a simple graphical illustration.
It can be completely explained in a few sentences of human language.
It can be implemented in a computer language using just a few lines of code.
The number of its possible variations is small enough so that all of them can be computed.Generally, simple programs tend to have a very simple abstract framework."
What is the main argument of the book?,The scientific philosophy and methods applicable to the study of simple programs are relevant to other fields of science,Simple programs are not relevant to understanding the physical world,The study of simple abstract rules is unnecessary,Computational systems are too complex to study,The exploration of computational systems is irrelevant,A,"A New Kind of Science is a book by Stephen Wolfram, published by his company Wolfram Research under the imprint Wolfram Media in 2002. It contains an empirical and systematic study of computational systems such as cellular automata. Wolfram calls these systems simple programs and argues that the scientific philosophy and methods appropriate for the study of simple programs are relevant to other fields of science.

Contents
Computation and its implications
The thesis of A New Kind of Science (NKS) is twofold: that the nature of computation must be explored experimentally, and that the results of these experiments have great relevance to understanding the physical world.

Simple programs
The basic subject of Wolfram's ""new kind of science"" is the study of simple abstract rules—essentially, elementary computer programs. In almost any class of a computational system, one very quickly finds instances of great complexity among its simplest cases (after a time series of multiple iterative loops, applying the same simple set of rules on itself, similar to a self-reinforcing cycle using a set of rules). This seems to be true regardless of the components of the system and the details of its setup. Systems explored in the book include, amongst others, cellular automata in one, two, and three dimensions; mobile automata; Turing machines in 1 and 2 dimensions; several varieties of substitution and network systems; recursive functions; nested recursive functions; combinators; tag systems; register machines; reversal-addition. For a program to qualify as simple, there are several requirements:

Its operation can be completely explained by a simple graphical illustration.
It can be completely explained in a few sentences of human language.
It can be implemented in a computer language using just a few lines of code.
The number of its possible variations is small enough so that all of them can be computed.Generally, simple programs tend to have a very simple abstract framework."
What is microfadeometry?,A technique that measures color changes in objects of art using intense light spots,A technique that measures the chemical composition of dyes in cultural objects,A technique that tests and analyzes light damage on cultural objects,A technique that focuses light onto an object’s surface to obtain light-stability information,A technique that uses xenon arc lamps to collect reflected light,A,"Microfadeometry is a technique that uses tiny spots of intense light to probe and measure color changes in objects of art that are particularly sensitive to light exposure.
This process is completed using a recently designed instrument known as a microfading tester. The data from the test is represented by reflectance spectra.

History
Light-fastness testing dates back to as early as 1733. In the late 19th century with early art conservation studies, Russell and Abney published Action of Light on Watercolors in 1888 sparking a concern with light and the aging of cultural materials. Microfading, as a technique, was first identified in Paul Whitmore’s lab book entry on September 21, 1994. His work on the topic was later published in 1999.

Application
Before the creation of this technique, recording light-stability information directly from the objects was nearly impossible. Through the use of microfading technology, it is now possible to obtain both information about the chemical characterization of the dyes and the associated kinetics of color change.
Benchtop microfaders and microfading testers are used to test and analyze light damage on cultural objects and potential light damage to objects in museum collections.

Theory
Using a powerful xenon arc lamp, 1 lumen of light is focused through an optical fiber onto a 300–400 μm spot of the object’s surface. The reflected light is collected by a second fiber optic."
When was microfading first identified?,1733,1888,1994,1999,None of the above,C,"Microfadeometry is a technique that uses tiny spots of intense light to probe and measure color changes in objects of art that are particularly sensitive to light exposure.
This process is completed using a recently designed instrument known as a microfading tester. The data from the test is represented by reflectance spectra.

History
Light-fastness testing dates back to as early as 1733. In the late 19th century with early art conservation studies, Russell and Abney published Action of Light on Watercolors in 1888 sparking a concern with light and the aging of cultural materials. Microfading, as a technique, was first identified in Paul Whitmore’s lab book entry on September 21, 1994. His work on the topic was later published in 1999.

Application
Before the creation of this technique, recording light-stability information directly from the objects was nearly impossible. Through the use of microfading technology, it is now possible to obtain both information about the chemical characterization of the dyes and the associated kinetics of color change.
Benchtop microfaders and microfading testers are used to test and analyze light damage on cultural objects and potential light damage to objects in museum collections.

Theory
Using a powerful xenon arc lamp, 1 lumen of light is focused through an optical fiber onto a 300–400 μm spot of the object’s surface. The reflected light is collected by a second fiber optic."
What is the purpose of microfading?,To measure color changes in objects of art,To obtain information about the chemical composition of dyes,To test and analyze light damage on cultural objects,To focus light onto an object’s surface to obtain light-stability information,To use xenon arc lamps to collect reflected light,A,"Microfadeometry is a technique that uses tiny spots of intense light to probe and measure color changes in objects of art that are particularly sensitive to light exposure.
This process is completed using a recently designed instrument known as a microfading tester. The data from the test is represented by reflectance spectra.

History
Light-fastness testing dates back to as early as 1733. In the late 19th century with early art conservation studies, Russell and Abney published Action of Light on Watercolors in 1888 sparking a concern with light and the aging of cultural materials. Microfading, as a technique, was first identified in Paul Whitmore’s lab book entry on September 21, 1994. His work on the topic was later published in 1999.

Application
Before the creation of this technique, recording light-stability information directly from the objects was nearly impossible. Through the use of microfading technology, it is now possible to obtain both information about the chemical characterization of the dyes and the associated kinetics of color change.
Benchtop microfaders and microfading testers are used to test and analyze light damage on cultural objects and potential light damage to objects in museum collections.

Theory
Using a powerful xenon arc lamp, 1 lumen of light is focused through an optical fiber onto a 300–400 μm spot of the object’s surface. The reflected light is collected by a second fiber optic."
How does microfadeometry work?,By focusing intense light spots onto an object’s surface,By analyzing the chemical composition of dyes in cultural objects,By testing and analyzing light damage on cultural objects,By collecting reflected light using xenon arc lamps,None of the above,A,"Microfadeometry is a technique that uses tiny spots of intense light to probe and measure color changes in objects of art that are particularly sensitive to light exposure.
This process is completed using a recently designed instrument known as a microfading tester. The data from the test is represented by reflectance spectra.

History
Light-fastness testing dates back to as early as 1733. In the late 19th century with early art conservation studies, Russell and Abney published Action of Light on Watercolors in 1888 sparking a concern with light and the aging of cultural materials. Microfading, as a technique, was first identified in Paul Whitmore’s lab book entry on September 21, 1994. His work on the topic was later published in 1999.

Application
Before the creation of this technique, recording light-stability information directly from the objects was nearly impossible. Through the use of microfading technology, it is now possible to obtain both information about the chemical characterization of the dyes and the associated kinetics of color change.
Benchtop microfaders and microfading testers are used to test and analyze light damage on cultural objects and potential light damage to objects in museum collections.

Theory
Using a powerful xenon arc lamp, 1 lumen of light is focused through an optical fiber onto a 300–400 μm spot of the object’s surface. The reflected light is collected by a second fiber optic."
What is the instrument used in microfadeometry?,Benchtop microfaders,Microfading testers,Xenon arc lamps,Optical fibers,None of the above,B,"Microfadeometry is a technique that uses tiny spots of intense light to probe and measure color changes in objects of art that are particularly sensitive to light exposure.
This process is completed using a recently designed instrument known as a microfading tester. The data from the test is represented by reflectance spectra.

History
Light-fastness testing dates back to as early as 1733. In the late 19th century with early art conservation studies, Russell and Abney published Action of Light on Watercolors in 1888 sparking a concern with light and the aging of cultural materials. Microfading, as a technique, was first identified in Paul Whitmore’s lab book entry on September 21, 1994. His work on the topic was later published in 1999.

Application
Before the creation of this technique, recording light-stability information directly from the objects was nearly impossible. Through the use of microfading technology, it is now possible to obtain both information about the chemical characterization of the dyes and the associated kinetics of color change.
Benchtop microfaders and microfading testers are used to test and analyze light damage on cultural objects and potential light damage to objects in museum collections.

Theory
Using a powerful xenon arc lamp, 1 lumen of light is focused through an optical fiber onto a 300–400 μm spot of the object’s surface. The reflected light is collected by a second fiber optic."
What is one of Sir Kenneth Grange's most famous design works?,The first UK parking meters,Razors for Wilkinson Sword,Typewriters for Imperial,Clothes irons for Morphy Richards,Cigarette lighters for Ronson,A,"Sir Kenneth Henry Grange, CBE, PPCSD, RDI (born 17 July 1929, London) is a British industrial designer, renowned for a wide range of designs for familiar, everyday objects.

Career
Grange's career began as a drafting assistant with the architect Jack Howe in the 1950s. His independent career started rather accidentally with commissions for exhibition stands, but by the early 1970s he was a founding-partner in Pentagram, an interdisciplinary design consultancy.Grange's career has spanned more than half a century, and many of his designs became – and are still – familiar items in the household or on the street. These designs include the first UK parking meters for Venner, kettles and food mixers for Kenwood, razors for Wilkinson Sword, cameras for Kodak, typewriters for Imperial, clothes irons for Morphy Richards, cigarette lighters for Ronson, washing machines for Bendix, pens for Parker, bus shelters, Reuters computers, and regional Royal Mail postboxes.Grange was also responsible for the aerodynamics, interior layout and exterior styling of the nose cone of British Rail's High Speed Train (known as the InterCity 125) and also involved in the design of the 1997 LTI TX1 version of the famous London taxicab. He has carried out many commissions for Japanese companies.
One quality of much of Grange's design work is that it is not based on just the styling of a product. His design concepts arise from a fundamental reassessment of the purpose, function and use of the product. He has also said that his attitude to designing any product is that he wants it to be ""a pleasure to use"". Grange was a pioneer of user-centred design, aiming to eliminate what he sees as the ""contradictions"" inherent in products that fail to embody ease-of-use.

Since retiring from Pentagram in 1997, Grange continues to work independently."
Which company did Sir Kenneth Grange design cameras for?,Venner,Kenwood,Kodak,Imperial,Ronson,C,"Sir Kenneth Henry Grange, CBE, PPCSD, RDI (born 17 July 1929, London) is a British industrial designer, renowned for a wide range of designs for familiar, everyday objects.

Career
Grange's career began as a drafting assistant with the architect Jack Howe in the 1950s. His independent career started rather accidentally with commissions for exhibition stands, but by the early 1970s he was a founding-partner in Pentagram, an interdisciplinary design consultancy.Grange's career has spanned more than half a century, and many of his designs became – and are still – familiar items in the household or on the street. These designs include the first UK parking meters for Venner, kettles and food mixers for Kenwood, razors for Wilkinson Sword, cameras for Kodak, typewriters for Imperial, clothes irons for Morphy Richards, cigarette lighters for Ronson, washing machines for Bendix, pens for Parker, bus shelters, Reuters computers, and regional Royal Mail postboxes.Grange was also responsible for the aerodynamics, interior layout and exterior styling of the nose cone of British Rail's High Speed Train (known as the InterCity 125) and also involved in the design of the 1997 LTI TX1 version of the famous London taxicab. He has carried out many commissions for Japanese companies.
One quality of much of Grange's design work is that it is not based on just the styling of a product. His design concepts arise from a fundamental reassessment of the purpose, function and use of the product. He has also said that his attitude to designing any product is that he wants it to be ""a pleasure to use"". Grange was a pioneer of user-centred design, aiming to eliminate what he sees as the ""contradictions"" inherent in products that fail to embody ease-of-use.

Since retiring from Pentagram in 1997, Grange continues to work independently."
What is one of the qualities of Sir Kenneth Grange's design work?,Based on just the styling of a product,Lack of functionality,User-centered design,Failure to embody ease-of-use,Contradictions inherent in products,C,"Sir Kenneth Henry Grange, CBE, PPCSD, RDI (born 17 July 1929, London) is a British industrial designer, renowned for a wide range of designs for familiar, everyday objects.

Career
Grange's career began as a drafting assistant with the architect Jack Howe in the 1950s. His independent career started rather accidentally with commissions for exhibition stands, but by the early 1970s he was a founding-partner in Pentagram, an interdisciplinary design consultancy.Grange's career has spanned more than half a century, and many of his designs became – and are still – familiar items in the household or on the street. These designs include the first UK parking meters for Venner, kettles and food mixers for Kenwood, razors for Wilkinson Sword, cameras for Kodak, typewriters for Imperial, clothes irons for Morphy Richards, cigarette lighters for Ronson, washing machines for Bendix, pens for Parker, bus shelters, Reuters computers, and regional Royal Mail postboxes.Grange was also responsible for the aerodynamics, interior layout and exterior styling of the nose cone of British Rail's High Speed Train (known as the InterCity 125) and also involved in the design of the 1997 LTI TX1 version of the famous London taxicab. He has carried out many commissions for Japanese companies.
One quality of much of Grange's design work is that it is not based on just the styling of a product. His design concepts arise from a fundamental reassessment of the purpose, function and use of the product. He has also said that his attitude to designing any product is that he wants it to be ""a pleasure to use"". Grange was a pioneer of user-centred design, aiming to eliminate what he sees as the ""contradictions"" inherent in products that fail to embody ease-of-use.

Since retiring from Pentagram in 1997, Grange continues to work independently."
Which company did Sir Kenneth Grange design clothes irons for?,Venner,Kenwood,Kodak,Morphy Richards,Ronson,D,"Sir Kenneth Henry Grange, CBE, PPCSD, RDI (born 17 July 1929, London) is a British industrial designer, renowned for a wide range of designs for familiar, everyday objects.

Career
Grange's career began as a drafting assistant with the architect Jack Howe in the 1950s. His independent career started rather accidentally with commissions for exhibition stands, but by the early 1970s he was a founding-partner in Pentagram, an interdisciplinary design consultancy.Grange's career has spanned more than half a century, and many of his designs became – and are still – familiar items in the household or on the street. These designs include the first UK parking meters for Venner, kettles and food mixers for Kenwood, razors for Wilkinson Sword, cameras for Kodak, typewriters for Imperial, clothes irons for Morphy Richards, cigarette lighters for Ronson, washing machines for Bendix, pens for Parker, bus shelters, Reuters computers, and regional Royal Mail postboxes.Grange was also responsible for the aerodynamics, interior layout and exterior styling of the nose cone of British Rail's High Speed Train (known as the InterCity 125) and also involved in the design of the 1997 LTI TX1 version of the famous London taxicab. He has carried out many commissions for Japanese companies.
One quality of much of Grange's design work is that it is not based on just the styling of a product. His design concepts arise from a fundamental reassessment of the purpose, function and use of the product. He has also said that his attitude to designing any product is that he wants it to be ""a pleasure to use"". Grange was a pioneer of user-centred design, aiming to eliminate what he sees as the ""contradictions"" inherent in products that fail to embody ease-of-use.

Since retiring from Pentagram in 1997, Grange continues to work independently."
When did Sir Kenneth Grange retire from Pentagram?,1997,1950,1970,1929,1999,A,"Sir Kenneth Henry Grange, CBE, PPCSD, RDI (born 17 July 1929, London) is a British industrial designer, renowned for a wide range of designs for familiar, everyday objects.

Career
Grange's career began as a drafting assistant with the architect Jack Howe in the 1950s. His independent career started rather accidentally with commissions for exhibition stands, but by the early 1970s he was a founding-partner in Pentagram, an interdisciplinary design consultancy.Grange's career has spanned more than half a century, and many of his designs became – and are still – familiar items in the household or on the street. These designs include the first UK parking meters for Venner, kettles and food mixers for Kenwood, razors for Wilkinson Sword, cameras for Kodak, typewriters for Imperial, clothes irons for Morphy Richards, cigarette lighters for Ronson, washing machines for Bendix, pens for Parker, bus shelters, Reuters computers, and regional Royal Mail postboxes.Grange was also responsible for the aerodynamics, interior layout and exterior styling of the nose cone of British Rail's High Speed Train (known as the InterCity 125) and also involved in the design of the 1997 LTI TX1 version of the famous London taxicab. He has carried out many commissions for Japanese companies.
One quality of much of Grange's design work is that it is not based on just the styling of a product. His design concepts arise from a fundamental reassessment of the purpose, function and use of the product. He has also said that his attitude to designing any product is that he wants it to be ""a pleasure to use"". Grange was a pioneer of user-centred design, aiming to eliminate what he sees as the ""contradictions"" inherent in products that fail to embody ease-of-use.

Since retiring from Pentagram in 1997, Grange continues to work independently."
What is cloud engineering?,The application of engineering disciplines to cloud computing,The development of cloud computing systems and solutions,The commercialization and standardization of cloud computing applications,"The implementation of cloud services such as ""software as a service""",The management of cloud resources to solve business problems,A,"Cloud engineering is the application of engineering disciplines to cloud computing. It brings a systematic approach to concerns of commercialization, standardization, and governance of cloud computing applications. In practice, it leverages the methods and tools of engineering in conceiving, developing, operating and maintaining cloud computing systems and solutions. It is about the process of designing the systems necessary to leverage the power and economics of cloud resources to solve business problems.

Core features
Cloud engineering is a field of engineering that focuses on cloud services, such as ""software as a service"", ""platform as a service"", and ""infrastructure as a service"". It is a multidisciplinary method encompassing contributions from diverse areas such as systems engineering, software engineering, web engineering, performance engineering, information technology engineering, security engineering, platform engineering, service engineering, risk engineering, and quality engineering. The nature of commodity-like capabilities delivered by cloud services and the inherent challenges in this business model drive the need for cloud engineering as the core discipline.
Elements of Cloud Engineering include:

Foundation: the fundamental basics, concepts, guiding principles, and taxonomy
Implementation: the building blocks and practice guides for Cloud realization
Lifecycle: the end-to-end iteration of Cloud development and delivery
Management: the design-time and run-time Cloud management from multiple perspectives

Profession
The professionals who work in the field of cloud engineering are primarily cloud architects and engineers. The key skills possessed by cloud engineering professionals are:

Know the language of business and domain knowledge
Understand the conceptual, logical and physical architecture
Master various cloud technologies, frameworks, and platforms
Implement the solutions for quality of cloud services, e.g."
What are the core features of cloud engineering?,"Focus on cloud services such as ""software as a service""",Multidisciplinary approach encompassing contributions from diverse areas,Inherent challenges in the business model of cloud services,Building blocks and practice guides for Cloud realization,End-to-end iteration of Cloud development and delivery,B,"Cloud engineering is the application of engineering disciplines to cloud computing. It brings a systematic approach to concerns of commercialization, standardization, and governance of cloud computing applications. In practice, it leverages the methods and tools of engineering in conceiving, developing, operating and maintaining cloud computing systems and solutions. It is about the process of designing the systems necessary to leverage the power and economics of cloud resources to solve business problems.

Core features
Cloud engineering is a field of engineering that focuses on cloud services, such as ""software as a service"", ""platform as a service"", and ""infrastructure as a service"". It is a multidisciplinary method encompassing contributions from diverse areas such as systems engineering, software engineering, web engineering, performance engineering, information technology engineering, security engineering, platform engineering, service engineering, risk engineering, and quality engineering. The nature of commodity-like capabilities delivered by cloud services and the inherent challenges in this business model drive the need for cloud engineering as the core discipline.
Elements of Cloud Engineering include:

Foundation: the fundamental basics, concepts, guiding principles, and taxonomy
Implementation: the building blocks and practice guides for Cloud realization
Lifecycle: the end-to-end iteration of Cloud development and delivery
Management: the design-time and run-time Cloud management from multiple perspectives

Profession
The professionals who work in the field of cloud engineering are primarily cloud architects and engineers. The key skills possessed by cloud engineering professionals are:

Know the language of business and domain knowledge
Understand the conceptual, logical and physical architecture
Master various cloud technologies, frameworks, and platforms
Implement the solutions for quality of cloud services, e.g."
What are the elements of cloud engineering?,"Foundation, Implementation, Lifecycle, Management","Fundamental basics, concepts, guiding principles, and taxonomy",Building blocks and practice guides for Cloud realization,End-to-end iteration of Cloud development and delivery,Design-time and run-time Cloud management from multiple perspectives,A,"Cloud engineering is the application of engineering disciplines to cloud computing. It brings a systematic approach to concerns of commercialization, standardization, and governance of cloud computing applications. In practice, it leverages the methods and tools of engineering in conceiving, developing, operating and maintaining cloud computing systems and solutions. It is about the process of designing the systems necessary to leverage the power and economics of cloud resources to solve business problems.

Core features
Cloud engineering is a field of engineering that focuses on cloud services, such as ""software as a service"", ""platform as a service"", and ""infrastructure as a service"". It is a multidisciplinary method encompassing contributions from diverse areas such as systems engineering, software engineering, web engineering, performance engineering, information technology engineering, security engineering, platform engineering, service engineering, risk engineering, and quality engineering. The nature of commodity-like capabilities delivered by cloud services and the inherent challenges in this business model drive the need for cloud engineering as the core discipline.
Elements of Cloud Engineering include:

Foundation: the fundamental basics, concepts, guiding principles, and taxonomy
Implementation: the building blocks and practice guides for Cloud realization
Lifecycle: the end-to-end iteration of Cloud development and delivery
Management: the design-time and run-time Cloud management from multiple perspectives

Profession
The professionals who work in the field of cloud engineering are primarily cloud architects and engineers. The key skills possessed by cloud engineering professionals are:

Know the language of business and domain knowledge
Understand the conceptual, logical and physical architecture
Master various cloud technologies, frameworks, and platforms
Implement the solutions for quality of cloud services, e.g."
What are the key skills possessed by cloud engineering professionals?,Language of business and domain knowledge,"Conceptual, logical and physical architecture","Various cloud technologies, frameworks, and platforms",Solutions for quality of cloud services,All of the above,E,"Cloud engineering is the application of engineering disciplines to cloud computing. It brings a systematic approach to concerns of commercialization, standardization, and governance of cloud computing applications. In practice, it leverages the methods and tools of engineering in conceiving, developing, operating and maintaining cloud computing systems and solutions. It is about the process of designing the systems necessary to leverage the power and economics of cloud resources to solve business problems.

Core features
Cloud engineering is a field of engineering that focuses on cloud services, such as ""software as a service"", ""platform as a service"", and ""infrastructure as a service"". It is a multidisciplinary method encompassing contributions from diverse areas such as systems engineering, software engineering, web engineering, performance engineering, information technology engineering, security engineering, platform engineering, service engineering, risk engineering, and quality engineering. The nature of commodity-like capabilities delivered by cloud services and the inherent challenges in this business model drive the need for cloud engineering as the core discipline.
Elements of Cloud Engineering include:

Foundation: the fundamental basics, concepts, guiding principles, and taxonomy
Implementation: the building blocks and practice guides for Cloud realization
Lifecycle: the end-to-end iteration of Cloud development and delivery
Management: the design-time and run-time Cloud management from multiple perspectives

Profession
The professionals who work in the field of cloud engineering are primarily cloud architects and engineers. The key skills possessed by cloud engineering professionals are:

Know the language of business and domain knowledge
Understand the conceptual, logical and physical architecture
Master various cloud technologies, frameworks, and platforms
Implement the solutions for quality of cloud services, e.g."
Why is cloud engineering important?,"It brings a systematic approach to concerns of commercialization, standardization, and governance of cloud computing applications","It leverages the methods and tools of engineering in conceiving, developing, operating and maintaining cloud computing systems and solutions",It enables businesses to leverage the power and economics of cloud resources to solve business problems,All of the above,None of the above,D,"Cloud engineering is the application of engineering disciplines to cloud computing. It brings a systematic approach to concerns of commercialization, standardization, and governance of cloud computing applications. In practice, it leverages the methods and tools of engineering in conceiving, developing, operating and maintaining cloud computing systems and solutions. It is about the process of designing the systems necessary to leverage the power and economics of cloud resources to solve business problems.

Core features
Cloud engineering is a field of engineering that focuses on cloud services, such as ""software as a service"", ""platform as a service"", and ""infrastructure as a service"". It is a multidisciplinary method encompassing contributions from diverse areas such as systems engineering, software engineering, web engineering, performance engineering, information technology engineering, security engineering, platform engineering, service engineering, risk engineering, and quality engineering. The nature of commodity-like capabilities delivered by cloud services and the inherent challenges in this business model drive the need for cloud engineering as the core discipline.
Elements of Cloud Engineering include:

Foundation: the fundamental basics, concepts, guiding principles, and taxonomy
Implementation: the building blocks and practice guides for Cloud realization
Lifecycle: the end-to-end iteration of Cloud development and delivery
Management: the design-time and run-time Cloud management from multiple perspectives

Profession
The professionals who work in the field of cloud engineering are primarily cloud architects and engineers. The key skills possessed by cloud engineering professionals are:

Know the language of business and domain knowledge
Understand the conceptual, logical and physical architecture
Master various cloud technologies, frameworks, and platforms
Implement the solutions for quality of cloud services, e.g."
What is thermal resistance?,A heat property that measures the temperature difference by which an object or material resists a heat flow,A property that measures the amount of heat generated by electrical components,A property that measures the thermal conductivity of a material,A property that measures the thermal insulance of a material,A property that measures the specific thermal resistance of a material,A,"Thermal resistance is a heat property and a measurement of a temperature difference by which an object or material resists a heat flow. Thermal resistance is the reciprocal of thermal conductance.

(Absolute) thermal resistance R in kelvins per watt (K/W) is a property of a particular component. For example, a characteristic of a heat sink.
Specific thermal resistance or thermal resistivity Rλ in kelvin–metres per watt (K⋅m/W), is a material constant.
Thermal insulance has the units square metre kelvin per watt (m2⋅K/W) in SI units or square foot degree Fahrenheit–hours per British thermal unit (ft2⋅°F⋅h/Btu) in imperial units. It is the thermal resistance of unit area of a material. In terms of insulation, it is measured by the R-value.

Absolute thermal resistance
Absolute thermal resistance is the temperature difference across a structure when a unit of heat energy flows through it in unit time.  It is the reciprocal of thermal conductance. The SI unit of absolute thermal resistance is kelvins per watt (K/W) or the equivalent degrees Celsius per watt (°C/W) – the two are the same since the intervals are equal: ΔT = 1 K = 1 °C.
The thermal resistance of materials is of great interest to electronic engineers because most electrical components generate heat and need to be cooled."
What is absolute thermal resistance?,The temperature difference across a structure when a unit of heat energy flows through it in unit time,The amount of heat generated by electrical components,The thermal conductivity of a material,The thermal insulance of a material,The specific thermal resistance of a material,A,"Thermal resistance is a heat property and a measurement of a temperature difference by which an object or material resists a heat flow. Thermal resistance is the reciprocal of thermal conductance.

(Absolute) thermal resistance R in kelvins per watt (K/W) is a property of a particular component. For example, a characteristic of a heat sink.
Specific thermal resistance or thermal resistivity Rλ in kelvin–metres per watt (K⋅m/W), is a material constant.
Thermal insulance has the units square metre kelvin per watt (m2⋅K/W) in SI units or square foot degree Fahrenheit–hours per British thermal unit (ft2⋅°F⋅h/Btu) in imperial units. It is the thermal resistance of unit area of a material. In terms of insulation, it is measured by the R-value.

Absolute thermal resistance
Absolute thermal resistance is the temperature difference across a structure when a unit of heat energy flows through it in unit time.  It is the reciprocal of thermal conductance. The SI unit of absolute thermal resistance is kelvins per watt (K/W) or the equivalent degrees Celsius per watt (°C/W) – the two are the same since the intervals are equal: ΔT = 1 K = 1 °C.
The thermal resistance of materials is of great interest to electronic engineers because most electrical components generate heat and need to be cooled."
What is the SI unit of absolute thermal resistance?,Kelvins per watt (K/W),Kelvin–metres per watt (K⋅m/W),Square metre kelvin per watt (m2⋅K/W),Square foot degree Fahrenheit–hours per British thermal unit (ft2⋅°F⋅h/Btu),Degrees Celsius per watt (°C/W),A,"Thermal resistance is a heat property and a measurement of a temperature difference by which an object or material resists a heat flow. Thermal resistance is the reciprocal of thermal conductance.

(Absolute) thermal resistance R in kelvins per watt (K/W) is a property of a particular component. For example, a characteristic of a heat sink.
Specific thermal resistance or thermal resistivity Rλ in kelvin–metres per watt (K⋅m/W), is a material constant.
Thermal insulance has the units square metre kelvin per watt (m2⋅K/W) in SI units or square foot degree Fahrenheit–hours per British thermal unit (ft2⋅°F⋅h/Btu) in imperial units. It is the thermal resistance of unit area of a material. In terms of insulation, it is measured by the R-value.

Absolute thermal resistance
Absolute thermal resistance is the temperature difference across a structure when a unit of heat energy flows through it in unit time.  It is the reciprocal of thermal conductance. The SI unit of absolute thermal resistance is kelvins per watt (K/W) or the equivalent degrees Celsius per watt (°C/W) – the two are the same since the intervals are equal: ΔT = 1 K = 1 °C.
The thermal resistance of materials is of great interest to electronic engineers because most electrical components generate heat and need to be cooled."
Why is thermal resistance important to electronic engineers?,Because it measures the temperature difference across a structure,Because it measures the amount of heat generated by electrical components,Because it measures the thermal conductivity of a material,Because it measures the thermal insulance of a material,Because most electrical components generate heat and need to be cooled,E,"Thermal resistance is a heat property and a measurement of a temperature difference by which an object or material resists a heat flow. Thermal resistance is the reciprocal of thermal conductance.

(Absolute) thermal resistance R in kelvins per watt (K/W) is a property of a particular component. For example, a characteristic of a heat sink.
Specific thermal resistance or thermal resistivity Rλ in kelvin–metres per watt (K⋅m/W), is a material constant.
Thermal insulance has the units square metre kelvin per watt (m2⋅K/W) in SI units or square foot degree Fahrenheit–hours per British thermal unit (ft2⋅°F⋅h/Btu) in imperial units. It is the thermal resistance of unit area of a material. In terms of insulation, it is measured by the R-value.

Absolute thermal resistance
Absolute thermal resistance is the temperature difference across a structure when a unit of heat energy flows through it in unit time.  It is the reciprocal of thermal conductance. The SI unit of absolute thermal resistance is kelvins per watt (K/W) or the equivalent degrees Celsius per watt (°C/W) – the two are the same since the intervals are equal: ΔT = 1 K = 1 °C.
The thermal resistance of materials is of great interest to electronic engineers because most electrical components generate heat and need to be cooled."
What is specific thermal resistance or thermal resistivity?,A heat property that measures the temperature difference by which an object or material resists a heat flow,A property that measures the amount of heat generated by electrical components,A material constant that measures the thermal conductivity of a material,A material constant that measures the thermal insulance of a material,A property that measures the specific thermal resistance of a material,E,"Thermal resistance is a heat property and a measurement of a temperature difference by which an object or material resists a heat flow. Thermal resistance is the reciprocal of thermal conductance.

(Absolute) thermal resistance R in kelvins per watt (K/W) is a property of a particular component. For example, a characteristic of a heat sink.
Specific thermal resistance or thermal resistivity Rλ in kelvin–metres per watt (K⋅m/W), is a material constant.
Thermal insulance has the units square metre kelvin per watt (m2⋅K/W) in SI units or square foot degree Fahrenheit–hours per British thermal unit (ft2⋅°F⋅h/Btu) in imperial units. It is the thermal resistance of unit area of a material. In terms of insulation, it is measured by the R-value.

Absolute thermal resistance
Absolute thermal resistance is the temperature difference across a structure when a unit of heat energy flows through it in unit time.  It is the reciprocal of thermal conductance. The SI unit of absolute thermal resistance is kelvins per watt (K/W) or the equivalent degrees Celsius per watt (°C/W) – the two are the same since the intervals are equal: ΔT = 1 K = 1 °C.
The thermal resistance of materials is of great interest to electronic engineers because most electrical components generate heat and need to be cooled."
What is entropic gravity?,A fundamental force in modern physics,A theory that describes gravity as an entropic force,A theory that describes gravity as a fundamental interaction,A principle in quantum mechanics,A principle in classical mechanics,B,"Entropic gravity, also known as emergent gravity, is a theory in modern physics that describes gravity as an entropic force—a force with macro-scale homogeneity but which is subject to quantum-level disorder—and not a fundamental interaction. The theory, based on string theory, black hole physics, and quantum information theory, describes gravity as an emergent phenomenon that springs from the quantum entanglement of small bits of spacetime information. As such, entropic gravity is said to abide by the second law of thermodynamics under which the entropy of a physical system tends to increase over time.
The theory has been controversial within the physics community but has sparked research and experiments to test its validity.

Significance
At its simplest, the theory holds that when gravity becomes vanishingly weak—levels seen only at interstellar distances—it diverges from its classically understood nature and its strength begins to decay linearly with distance from a mass.
Entropic gravity provides an underlying framework to explain Modified Newtonian Dynamics, or MOND, which holds that at a gravitational acceleration threshold of approximately 1.2×10−10 m/s2, gravitational strength begins to vary inversely linearly with distance from a mass rather than the normal inverse-square law of the distance. This is an exceedingly low threshold, measuring only 12 trillionths gravity's strength at Earth's surface; an object dropped from a height of one meter would fall for 36 hours were Earth's gravity this weak. It is also 3,000 times less than the remnant of Earth's gravitational field that exists at the point where Voyager 1 crossed the solar system's heliopause and entered interstellar space.
The theory claims to be consistent with both the macro-level observations of Newtonian gravity as well as Einstein's theory of general relativity and its gravitational distortion of spacetime. Importantly, the theory also explains (without invoking the existence of dark matter and tweaking of its new free parameters) why galactic rotation curves differ from the profile expected with visible matter.
The theory of entropic gravity posits that what has been interpreted as unobserved dark matter is the product of quantum effects that can be regarded as a form of positive dark energy that lifts the vacuum energy of space from its ground state value. A central tenet of the theory is that the positive dark energy leads to a thermal-volume law contribution to entropy that overtakes the area law of anti-de Sitter space precisely at
the cosmological horizon.
Thus this theory provides an alternative explanation for what mainstream physics currently attributes to dark matter."
What is the significance of entropic gravity?,It explains Modified Newtonian Dynamics,It disproves Einstein's theory of general relativity,It provides evidence for dark matter,It explains the behavior of galactic rotation curves,It predicts the existence of quantum black holes,D,"Entropic gravity, also known as emergent gravity, is a theory in modern physics that describes gravity as an entropic force—a force with macro-scale homogeneity but which is subject to quantum-level disorder—and not a fundamental interaction. The theory, based on string theory, black hole physics, and quantum information theory, describes gravity as an emergent phenomenon that springs from the quantum entanglement of small bits of spacetime information. As such, entropic gravity is said to abide by the second law of thermodynamics under which the entropy of a physical system tends to increase over time.
The theory has been controversial within the physics community but has sparked research and experiments to test its validity.

Significance
At its simplest, the theory holds that when gravity becomes vanishingly weak—levels seen only at interstellar distances—it diverges from its classically understood nature and its strength begins to decay linearly with distance from a mass.
Entropic gravity provides an underlying framework to explain Modified Newtonian Dynamics, or MOND, which holds that at a gravitational acceleration threshold of approximately 1.2×10−10 m/s2, gravitational strength begins to vary inversely linearly with distance from a mass rather than the normal inverse-square law of the distance. This is an exceedingly low threshold, measuring only 12 trillionths gravity's strength at Earth's surface; an object dropped from a height of one meter would fall for 36 hours were Earth's gravity this weak. It is also 3,000 times less than the remnant of Earth's gravitational field that exists at the point where Voyager 1 crossed the solar system's heliopause and entered interstellar space.
The theory claims to be consistent with both the macro-level observations of Newtonian gravity as well as Einstein's theory of general relativity and its gravitational distortion of spacetime. Importantly, the theory also explains (without invoking the existence of dark matter and tweaking of its new free parameters) why galactic rotation curves differ from the profile expected with visible matter.
The theory of entropic gravity posits that what has been interpreted as unobserved dark matter is the product of quantum effects that can be regarded as a form of positive dark energy that lifts the vacuum energy of space from its ground state value. A central tenet of the theory is that the positive dark energy leads to a thermal-volume law contribution to entropy that overtakes the area law of anti-de Sitter space precisely at
the cosmological horizon.
Thus this theory provides an alternative explanation for what mainstream physics currently attributes to dark matter."
How does entropic gravity explain the behavior of galactic rotation curves?,By invoking the existence of dark matter,By tweaking the parameters of general relativity,By considering the effects of quantum entanglement,By applying the inverse-square law of gravity,By analyzing the decay of gravitational strength,C,"Entropic gravity, also known as emergent gravity, is a theory in modern physics that describes gravity as an entropic force—a force with macro-scale homogeneity but which is subject to quantum-level disorder—and not a fundamental interaction. The theory, based on string theory, black hole physics, and quantum information theory, describes gravity as an emergent phenomenon that springs from the quantum entanglement of small bits of spacetime information. As such, entropic gravity is said to abide by the second law of thermodynamics under which the entropy of a physical system tends to increase over time.
The theory has been controversial within the physics community but has sparked research and experiments to test its validity.

Significance
At its simplest, the theory holds that when gravity becomes vanishingly weak—levels seen only at interstellar distances—it diverges from its classically understood nature and its strength begins to decay linearly with distance from a mass.
Entropic gravity provides an underlying framework to explain Modified Newtonian Dynamics, or MOND, which holds that at a gravitational acceleration threshold of approximately 1.2×10−10 m/s2, gravitational strength begins to vary inversely linearly with distance from a mass rather than the normal inverse-square law of the distance. This is an exceedingly low threshold, measuring only 12 trillionths gravity's strength at Earth's surface; an object dropped from a height of one meter would fall for 36 hours were Earth's gravity this weak. It is also 3,000 times less than the remnant of Earth's gravitational field that exists at the point where Voyager 1 crossed the solar system's heliopause and entered interstellar space.
The theory claims to be consistent with both the macro-level observations of Newtonian gravity as well as Einstein's theory of general relativity and its gravitational distortion of spacetime. Importantly, the theory also explains (without invoking the existence of dark matter and tweaking of its new free parameters) why galactic rotation curves differ from the profile expected with visible matter.
The theory of entropic gravity posits that what has been interpreted as unobserved dark matter is the product of quantum effects that can be regarded as a form of positive dark energy that lifts the vacuum energy of space from its ground state value. A central tenet of the theory is that the positive dark energy leads to a thermal-volume law contribution to entropy that overtakes the area law of anti-de Sitter space precisely at
the cosmological horizon.
Thus this theory provides an alternative explanation for what mainstream physics currently attributes to dark matter."
What does the theory of entropic gravity attribute to dark matter?,Unobserved matter in galaxies,Quantum effects that lift the vacuum energy,The decay of gravitational strength,The distortion of spacetime,The existence of black holes,B,"Entropic gravity, also known as emergent gravity, is a theory in modern physics that describes gravity as an entropic force—a force with macro-scale homogeneity but which is subject to quantum-level disorder—and not a fundamental interaction. The theory, based on string theory, black hole physics, and quantum information theory, describes gravity as an emergent phenomenon that springs from the quantum entanglement of small bits of spacetime information. As such, entropic gravity is said to abide by the second law of thermodynamics under which the entropy of a physical system tends to increase over time.
The theory has been controversial within the physics community but has sparked research and experiments to test its validity.

Significance
At its simplest, the theory holds that when gravity becomes vanishingly weak—levels seen only at interstellar distances—it diverges from its classically understood nature and its strength begins to decay linearly with distance from a mass.
Entropic gravity provides an underlying framework to explain Modified Newtonian Dynamics, or MOND, which holds that at a gravitational acceleration threshold of approximately 1.2×10−10 m/s2, gravitational strength begins to vary inversely linearly with distance from a mass rather than the normal inverse-square law of the distance. This is an exceedingly low threshold, measuring only 12 trillionths gravity's strength at Earth's surface; an object dropped from a height of one meter would fall for 36 hours were Earth's gravity this weak. It is also 3,000 times less than the remnant of Earth's gravitational field that exists at the point where Voyager 1 crossed the solar system's heliopause and entered interstellar space.
The theory claims to be consistent with both the macro-level observations of Newtonian gravity as well as Einstein's theory of general relativity and its gravitational distortion of spacetime. Importantly, the theory also explains (without invoking the existence of dark matter and tweaking of its new free parameters) why galactic rotation curves differ from the profile expected with visible matter.
The theory of entropic gravity posits that what has been interpreted as unobserved dark matter is the product of quantum effects that can be regarded as a form of positive dark energy that lifts the vacuum energy of space from its ground state value. A central tenet of the theory is that the positive dark energy leads to a thermal-volume law contribution to entropy that overtakes the area law of anti-de Sitter space precisely at
the cosmological horizon.
Thus this theory provides an alternative explanation for what mainstream physics currently attributes to dark matter."
Why is the theory of entropic gravity controversial?,It contradicts the principles of quantum mechanics,It challenges the concept of dark matter,It is not consistent with Einstein's theory of general relativity,It cannot explain the behavior of galactic rotation curves,It is based on string theory and quantum information theory,B,"Entropic gravity, also known as emergent gravity, is a theory in modern physics that describes gravity as an entropic force—a force with macro-scale homogeneity but which is subject to quantum-level disorder—and not a fundamental interaction. The theory, based on string theory, black hole physics, and quantum information theory, describes gravity as an emergent phenomenon that springs from the quantum entanglement of small bits of spacetime information. As such, entropic gravity is said to abide by the second law of thermodynamics under which the entropy of a physical system tends to increase over time.
The theory has been controversial within the physics community but has sparked research and experiments to test its validity.

Significance
At its simplest, the theory holds that when gravity becomes vanishingly weak—levels seen only at interstellar distances—it diverges from its classically understood nature and its strength begins to decay linearly with distance from a mass.
Entropic gravity provides an underlying framework to explain Modified Newtonian Dynamics, or MOND, which holds that at a gravitational acceleration threshold of approximately 1.2×10−10 m/s2, gravitational strength begins to vary inversely linearly with distance from a mass rather than the normal inverse-square law of the distance. This is an exceedingly low threshold, measuring only 12 trillionths gravity's strength at Earth's surface; an object dropped from a height of one meter would fall for 36 hours were Earth's gravity this weak. It is also 3,000 times less than the remnant of Earth's gravitational field that exists at the point where Voyager 1 crossed the solar system's heliopause and entered interstellar space.
The theory claims to be consistent with both the macro-level observations of Newtonian gravity as well as Einstein's theory of general relativity and its gravitational distortion of spacetime. Importantly, the theory also explains (without invoking the existence of dark matter and tweaking of its new free parameters) why galactic rotation curves differ from the profile expected with visible matter.
The theory of entropic gravity posits that what has been interpreted as unobserved dark matter is the product of quantum effects that can be regarded as a form of positive dark energy that lifts the vacuum energy of space from its ground state value. A central tenet of the theory is that the positive dark energy leads to a thermal-volume law contribution to entropy that overtakes the area law of anti-de Sitter space precisely at
the cosmological horizon.
Thus this theory provides an alternative explanation for what mainstream physics currently attributes to dark matter."
What is the definition of a natural product in the field of organic chemistry?,Any substance produced by life,Organic compounds isolated from natural sources,Chemicals synthesized in a laboratory,"Cosmetics, dietary supplements, and foods produced from natural sources",Chemicals with added artificial ingredients,B,"A natural product is a natural compound or substance produced by a living organism—that is, found in nature. In the broadest sense, natural products include any substance produced by life. Natural products can also be prepared by chemical synthesis (both semisynthesis and total synthesis) and have played a central role in the development of the field of organic chemistry by providing challenging synthetic targets. The term natural product has also been extended for commercial purposes to refer to cosmetics, dietary supplements, and foods produced from natural sources without added artificial ingredients.Within the field of organic chemistry, the definition of natural products is usually restricted to organic compounds isolated from natural sources that are produced by the  pathways of primary or secondary metabolism.  Within the field of medicinal chemistry, the definition is often further restricted to secondary metabolites. Secondary metabolites (or specialized metabolites) are not essential for survival, but nevertheless provide organisms that produce them an evolutionary advantage. Many secondary metabolites are cytotoxic and have been selected and optimized through evolution for use as ""chemical warfare"" agents against prey, predators, and competing organisms."
Why have natural products played a central role in the development of organic chemistry?,They are easy to synthesize,They are challenging synthetic targets,They have no commercial value,They are not produced by living organisms,They have no medicinal value,B,"A natural product is a natural compound or substance produced by a living organism—that is, found in nature. In the broadest sense, natural products include any substance produced by life. Natural products can also be prepared by chemical synthesis (both semisynthesis and total synthesis) and have played a central role in the development of the field of organic chemistry by providing challenging synthetic targets. The term natural product has also been extended for commercial purposes to refer to cosmetics, dietary supplements, and foods produced from natural sources without added artificial ingredients.Within the field of organic chemistry, the definition of natural products is usually restricted to organic compounds isolated from natural sources that are produced by the  pathways of primary or secondary metabolism.  Within the field of medicinal chemistry, the definition is often further restricted to secondary metabolites. Secondary metabolites (or specialized metabolites) are not essential for survival, but nevertheless provide organisms that produce them an evolutionary advantage. Many secondary metabolites are cytotoxic and have been selected and optimized through evolution for use as ""chemical warfare"" agents against prey, predators, and competing organisms."
What are secondary metabolites?,Organic compounds isolated from natural sources,Chemicals synthesized in a laboratory,Chemicals with added artificial ingredients,Substances produced by life,Compounds that provide an evolutionary advantage but are not essential for survival,E,"A natural product is a natural compound or substance produced by a living organism—that is, found in nature. In the broadest sense, natural products include any substance produced by life. Natural products can also be prepared by chemical synthesis (both semisynthesis and total synthesis) and have played a central role in the development of the field of organic chemistry by providing challenging synthetic targets. The term natural product has also been extended for commercial purposes to refer to cosmetics, dietary supplements, and foods produced from natural sources without added artificial ingredients.Within the field of organic chemistry, the definition of natural products is usually restricted to organic compounds isolated from natural sources that are produced by the  pathways of primary or secondary metabolism.  Within the field of medicinal chemistry, the definition is often further restricted to secondary metabolites. Secondary metabolites (or specialized metabolites) are not essential for survival, but nevertheless provide organisms that produce them an evolutionary advantage. Many secondary metabolites are cytotoxic and have been selected and optimized through evolution for use as ""chemical warfare"" agents against prey, predators, and competing organisms."
How have secondary metabolites been optimized through evolution?,By becoming cytotoxic,By being essential for survival,By having no medicinal value,By being produced by primary metabolism,By having no commercial value,A,"A natural product is a natural compound or substance produced by a living organism—that is, found in nature. In the broadest sense, natural products include any substance produced by life. Natural products can also be prepared by chemical synthesis (both semisynthesis and total synthesis) and have played a central role in the development of the field of organic chemistry by providing challenging synthetic targets. The term natural product has also been extended for commercial purposes to refer to cosmetics, dietary supplements, and foods produced from natural sources without added artificial ingredients.Within the field of organic chemistry, the definition of natural products is usually restricted to organic compounds isolated from natural sources that are produced by the  pathways of primary or secondary metabolism.  Within the field of medicinal chemistry, the definition is often further restricted to secondary metabolites. Secondary metabolites (or specialized metabolites) are not essential for survival, but nevertheless provide organisms that produce them an evolutionary advantage. Many secondary metabolites are cytotoxic and have been selected and optimized through evolution for use as ""chemical warfare"" agents against prey, predators, and competing organisms."
What are natural products often used for by organisms that produce them?,"As chemical warfare agents against prey, predators, and competing organisms",As essential compounds for survival,As commercial products with added artificial ingredients,"As cosmetics, dietary supplements, and foods",As substances produced by life,A,"A natural product is a natural compound or substance produced by a living organism—that is, found in nature. In the broadest sense, natural products include any substance produced by life. Natural products can also be prepared by chemical synthesis (both semisynthesis and total synthesis) and have played a central role in the development of the field of organic chemistry by providing challenging synthetic targets. The term natural product has also been extended for commercial purposes to refer to cosmetics, dietary supplements, and foods produced from natural sources without added artificial ingredients.Within the field of organic chemistry, the definition of natural products is usually restricted to organic compounds isolated from natural sources that are produced by the  pathways of primary or secondary metabolism.  Within the field of medicinal chemistry, the definition is often further restricted to secondary metabolites. Secondary metabolites (or specialized metabolites) are not essential for survival, but nevertheless provide organisms that produce them an evolutionary advantage. Many secondary metabolites are cytotoxic and have been selected and optimized through evolution for use as ""chemical warfare"" agents against prey, predators, and competing organisms."
What is algebraic number theory?,A branch of number theory that uses abstract algebra techniques to study prime numbers,A branch of number theory that uses abstract algebra techniques to study algebraic objects such as algebraic number fields and their rings of integers,A branch of number theory that uses abstract algebra techniques to study complex numbers,A branch of number theory that uses abstract algebra techniques to study irrational numbers,A branch of number theory that uses abstract algebra techniques to study transcendental numbers,B,"Algebraic number theory is a branch of number theory that uses the techniques of abstract algebra to study the integers, rational numbers, and their generalizations. Number-theoretic questions are expressed in terms of properties of algebraic objects such as algebraic number fields and their rings of integers, finite fields, and function fields. These properties, such as whether a ring admits unique factorization, the behavior of ideals, and the Galois groups of fields, can resolve questions of primary importance in number theory, like the existence of solutions to Diophantine equations.

History of algebraic number theory
Diophantus
The beginnings of algebraic number theory can be traced to Diophantine equations, named after the 3rd-century Alexandrian mathematician, Diophantus, who studied them and developed methods for the solution of some kinds of Diophantine equations. A typical Diophantine problem is to find two integers x and y such that their sum, and the sum of their squares, equal two given numbers A and B, respectively:

  
    
      A
      =
      x
      +
      y
       
    
    A=x+y\
  

  
    
      B
      =
      
        x
        
          2
        
      
      +
      
        y
        
          2
        
      
      .
       
    
    B=x^{2}+y^{2}.\
  Diophantine equations have been studied for thousands of years. For example, the solutions to the quadratic Diophantine equation  x2 + y2 = z2 are given by the Pythagorean triples, originally solved by the Babylonians (c. 1800 BC). Solutions to linear Diophantine equations, such as 26x + 65y = 13, may be found using the Euclidean algorithm (c. 5th century BC).Diophantus' major work was the Arithmetica, of which only a portion has survived.

Fermat
Fermat's Last Theorem was first conjectured by Pierre de Fermat in 1637, famously in the margin of a copy of Arithmetica where he claimed he had a proof that was too large to fit in the margin."
What properties of algebraic objects does algebraic number theory study?,Unique factorization and the behavior of polynomials,Unique factorization and the behavior of ideals,Unique factorization and the behavior of prime numbers,Unique factorization and the behavior of rational numbers,Unique factorization and the behavior of real numbers,B,"Algebraic number theory is a branch of number theory that uses the techniques of abstract algebra to study the integers, rational numbers, and their generalizations. Number-theoretic questions are expressed in terms of properties of algebraic objects such as algebraic number fields and their rings of integers, finite fields, and function fields. These properties, such as whether a ring admits unique factorization, the behavior of ideals, and the Galois groups of fields, can resolve questions of primary importance in number theory, like the existence of solutions to Diophantine equations.

History of algebraic number theory
Diophantus
The beginnings of algebraic number theory can be traced to Diophantine equations, named after the 3rd-century Alexandrian mathematician, Diophantus, who studied them and developed methods for the solution of some kinds of Diophantine equations. A typical Diophantine problem is to find two integers x and y such that their sum, and the sum of their squares, equal two given numbers A and B, respectively:

  
    
      A
      =
      x
      +
      y
       
    
    A=x+y\
  

  
    
      B
      =
      
        x
        
          2
        
      
      +
      
        y
        
          2
        
      
      .
       
    
    B=x^{2}+y^{2}.\
  Diophantine equations have been studied for thousands of years. For example, the solutions to the quadratic Diophantine equation  x2 + y2 = z2 are given by the Pythagorean triples, originally solved by the Babylonians (c. 1800 BC). Solutions to linear Diophantine equations, such as 26x + 65y = 13, may be found using the Euclidean algorithm (c. 5th century BC).Diophantus' major work was the Arithmetica, of which only a portion has survived.

Fermat
Fermat's Last Theorem was first conjectured by Pierre de Fermat in 1637, famously in the margin of a copy of Arithmetica where he claimed he had a proof that was too large to fit in the margin."
Who is considered the father of algebraic number theory?,Diophantus,Fermat,Pierre de Fermat,Alexandrian mathematician,Babylonians,A,"Algebraic number theory is a branch of number theory that uses the techniques of abstract algebra to study the integers, rational numbers, and their generalizations. Number-theoretic questions are expressed in terms of properties of algebraic objects such as algebraic number fields and their rings of integers, finite fields, and function fields. These properties, such as whether a ring admits unique factorization, the behavior of ideals, and the Galois groups of fields, can resolve questions of primary importance in number theory, like the existence of solutions to Diophantine equations.

History of algebraic number theory
Diophantus
The beginnings of algebraic number theory can be traced to Diophantine equations, named after the 3rd-century Alexandrian mathematician, Diophantus, who studied them and developed methods for the solution of some kinds of Diophantine equations. A typical Diophantine problem is to find two integers x and y such that their sum, and the sum of their squares, equal two given numbers A and B, respectively:

  
    
      A
      =
      x
      +
      y
       
    
    A=x+y\
  

  
    
      B
      =
      
        x
        
          2
        
      
      +
      
        y
        
          2
        
      
      .
       
    
    B=x^{2}+y^{2}.\
  Diophantine equations have been studied for thousands of years. For example, the solutions to the quadratic Diophantine equation  x2 + y2 = z2 are given by the Pythagorean triples, originally solved by the Babylonians (c. 1800 BC). Solutions to linear Diophantine equations, such as 26x + 65y = 13, may be found using the Euclidean algorithm (c. 5th century BC).Diophantus' major work was the Arithmetica, of which only a portion has survived.

Fermat
Fermat's Last Theorem was first conjectured by Pierre de Fermat in 1637, famously in the margin of a copy of Arithmetica where he claimed he had a proof that was too large to fit in the margin."
What is a Diophantine equation?,An equation named after Diophantus that involves only rational numbers,An equation named after Diophantus that involves only integers,An equation named after Diophantus that involves both rational and irrational numbers,An equation named after Diophantus that involves only prime numbers,An equation named after Diophantus that involves only real numbers,B,"Algebraic number theory is a branch of number theory that uses the techniques of abstract algebra to study the integers, rational numbers, and their generalizations. Number-theoretic questions are expressed in terms of properties of algebraic objects such as algebraic number fields and their rings of integers, finite fields, and function fields. These properties, such as whether a ring admits unique factorization, the behavior of ideals, and the Galois groups of fields, can resolve questions of primary importance in number theory, like the existence of solutions to Diophantine equations.

History of algebraic number theory
Diophantus
The beginnings of algebraic number theory can be traced to Diophantine equations, named after the 3rd-century Alexandrian mathematician, Diophantus, who studied them and developed methods for the solution of some kinds of Diophantine equations. A typical Diophantine problem is to find two integers x and y such that their sum, and the sum of their squares, equal two given numbers A and B, respectively:

  
    
      A
      =
      x
      +
      y
       
    
    A=x+y\
  

  
    
      B
      =
      
        x
        
          2
        
      
      +
      
        y
        
          2
        
      
      .
       
    
    B=x^{2}+y^{2}.\
  Diophantine equations have been studied for thousands of years. For example, the solutions to the quadratic Diophantine equation  x2 + y2 = z2 are given by the Pythagorean triples, originally solved by the Babylonians (c. 1800 BC). Solutions to linear Diophantine equations, such as 26x + 65y = 13, may be found using the Euclidean algorithm (c. 5th century BC).Diophantus' major work was the Arithmetica, of which only a portion has survived.

Fermat
Fermat's Last Theorem was first conjectured by Pierre de Fermat in 1637, famously in the margin of a copy of Arithmetica where he claimed he had a proof that was too large to fit in the margin."
What is Fermat's Last Theorem?,A theorem about prime numbers proven by Pierre de Fermat,A theorem about rational numbers proven by Pierre de Fermat,A theorem about algebraic number fields proven by Pierre de Fermat,A theorem about Diophantine equations proven by Pierre de Fermat,A theorem about complex numbers proven by Pierre de Fermat,D,"Algebraic number theory is a branch of number theory that uses the techniques of abstract algebra to study the integers, rational numbers, and their generalizations. Number-theoretic questions are expressed in terms of properties of algebraic objects such as algebraic number fields and their rings of integers, finite fields, and function fields. These properties, such as whether a ring admits unique factorization, the behavior of ideals, and the Galois groups of fields, can resolve questions of primary importance in number theory, like the existence of solutions to Diophantine equations.

History of algebraic number theory
Diophantus
The beginnings of algebraic number theory can be traced to Diophantine equations, named after the 3rd-century Alexandrian mathematician, Diophantus, who studied them and developed methods for the solution of some kinds of Diophantine equations. A typical Diophantine problem is to find two integers x and y such that their sum, and the sum of their squares, equal two given numbers A and B, respectively:

  
    
      A
      =
      x
      +
      y
       
    
    A=x+y\
  

  
    
      B
      =
      
        x
        
          2
        
      
      +
      
        y
        
          2
        
      
      .
       
    
    B=x^{2}+y^{2}.\
  Diophantine equations have been studied for thousands of years. For example, the solutions to the quadratic Diophantine equation  x2 + y2 = z2 are given by the Pythagorean triples, originally solved by the Babylonians (c. 1800 BC). Solutions to linear Diophantine equations, such as 26x + 65y = 13, may be found using the Euclidean algorithm (c. 5th century BC).Diophantus' major work was the Arithmetica, of which only a portion has survived.

Fermat
Fermat's Last Theorem was first conjectured by Pierre de Fermat in 1637, famously in the margin of a copy of Arithmetica where he claimed he had a proof that was too large to fit in the margin."
What is municipal engineering concerned with?,Designing and constructing buildings,Managing public transportation systems,Maintaining urban infrastructure,Developing agricultural practices,Promoting environmental conservation,C,"Municipal or urban engineering applies the tools of science, art and engineering in an urban environment.
Municipal engineering is concerned with municipal infrastructure.  This involves specifying, designing, constructing, and maintaining streets, sidewalks, water supply networks, sewers, street lighting, municipal solid waste management and disposal, storage depots for various bulk materials used for maintenance and public works (salt, sand, etc.), public parks and cycling infrastructure.
In the case of underground utility networks, it may also include the civil portion (conduits and access chambers) of the local distribution networks of electrical and telecommunications services.  It can also include the optimizing of garbage collection and bus service networks.  Some of these disciplines overlap with other civil engineering specialties, however municipal engineering focuses on the coordination of these infrastructure networks and services, as they are often built simultaneously (for a given street or development project), and managed by the same municipal authority.

History
Modern municipal engineering finds its origins in the 19th-century United Kingdom, following the Industrial Revolution and the growth of large industrial cities. The threat to urban populations from epidemics of waterborne diseases such as cholera and typhus led to the development of a profession devoted to ""sanitary science"" that later became ""municipal engineering"".A key figure of the so-called ""public health movement"" was Edwin Chadwick, author of the parliamentary report, published in 1842.Early British legislation included:

Burgh Police Act 1833 - powers of paving, lighting, cleansing, watching, supplying with water and improving their communities.
Municipal Corporations Act 1835
Public Health Act 1866 – formation of drainage boards
Public Health Act 1875 known at the time as the Great Public Health ActThis legislation provided local authorities with powers to undertake municipal engineering projects and to appoint borough surveyors (later known as ""municipal engineers"").In the U.K, the Association of Municipal Engineers, (subsequently named Institution of Municipal Engineers), was established in 1874 under the encouragement of the Institution of Civil Engineers, to address the issue of the application of sanitary science. By the early 20th century, Municipal Engineering had become a broad discipline embracing many of the responsibilities undertaken by local authorities, including roads, drainage, flood control, coastal engineering, public health, waste management, street cleaning, water supply, sewers, waste water treatment, crematoria, public baths, slum clearance, town planning, public housing, energy supply, parks, leisure facilities, libraries, town halls and other municipal buildings.In the UK, the development of different strands of knowledge necessary for the management of municipal infrastructure led to the emergence of separate specialised institutions, including:

For drainage: Chartered Institution of Water and Environmental Management, 1895
For town planning: Town Planning Institute 1914 ... subsequently becoming the Royal Town Planning Institute
For street lighting: Association of Public Lighting Engineers, 1934...subsequently becoming the Institution of Lighting Engineers
For highway engineering: Institution of Highways and Transportation, 1930
For public housing: Institute of Housing, 1931In 1984 the Institution of Municipal Engineers merged with the Institution of Civil Engineers.Since the 1970s, there has been a global trend toward increasing privatisation and outsourcing of municipal engineering services.In the UK in the 1990s a change in management philosophy brought the demise of the traditional organisational structure of boroughs where the three functions of town clerk, borough treasurer and borough engineer were replaced by an administrative structure with a larger number of specialised departments.In the late 1990s and early 21st century there was increasing dissatisfaction over what was perceived to be fractured and dysfunctional public services designed along narrow specialties."
Which of the following is NOT included in municipal infrastructure?,Water supply networks,Street lighting,Public parks,Airports and seaports,Sewers,D,"Municipal or urban engineering applies the tools of science, art and engineering in an urban environment.
Municipal engineering is concerned with municipal infrastructure.  This involves specifying, designing, constructing, and maintaining streets, sidewalks, water supply networks, sewers, street lighting, municipal solid waste management and disposal, storage depots for various bulk materials used for maintenance and public works (salt, sand, etc.), public parks and cycling infrastructure.
In the case of underground utility networks, it may also include the civil portion (conduits and access chambers) of the local distribution networks of electrical and telecommunications services.  It can also include the optimizing of garbage collection and bus service networks.  Some of these disciplines overlap with other civil engineering specialties, however municipal engineering focuses on the coordination of these infrastructure networks and services, as they are often built simultaneously (for a given street or development project), and managed by the same municipal authority.

History
Modern municipal engineering finds its origins in the 19th-century United Kingdom, following the Industrial Revolution and the growth of large industrial cities. The threat to urban populations from epidemics of waterborne diseases such as cholera and typhus led to the development of a profession devoted to ""sanitary science"" that later became ""municipal engineering"".A key figure of the so-called ""public health movement"" was Edwin Chadwick, author of the parliamentary report, published in 1842.Early British legislation included:

Burgh Police Act 1833 - powers of paving, lighting, cleansing, watching, supplying with water and improving their communities.
Municipal Corporations Act 1835
Public Health Act 1866 – formation of drainage boards
Public Health Act 1875 known at the time as the Great Public Health ActThis legislation provided local authorities with powers to undertake municipal engineering projects and to appoint borough surveyors (later known as ""municipal engineers"").In the U.K, the Association of Municipal Engineers, (subsequently named Institution of Municipal Engineers), was established in 1874 under the encouragement of the Institution of Civil Engineers, to address the issue of the application of sanitary science. By the early 20th century, Municipal Engineering had become a broad discipline embracing many of the responsibilities undertaken by local authorities, including roads, drainage, flood control, coastal engineering, public health, waste management, street cleaning, water supply, sewers, waste water treatment, crematoria, public baths, slum clearance, town planning, public housing, energy supply, parks, leisure facilities, libraries, town halls and other municipal buildings.In the UK, the development of different strands of knowledge necessary for the management of municipal infrastructure led to the emergence of separate specialised institutions, including:

For drainage: Chartered Institution of Water and Environmental Management, 1895
For town planning: Town Planning Institute 1914 ... subsequently becoming the Royal Town Planning Institute
For street lighting: Association of Public Lighting Engineers, 1934...subsequently becoming the Institution of Lighting Engineers
For highway engineering: Institution of Highways and Transportation, 1930
For public housing: Institute of Housing, 1931In 1984 the Institution of Municipal Engineers merged with the Institution of Civil Engineers.Since the 1970s, there has been a global trend toward increasing privatisation and outsourcing of municipal engineering services.In the UK in the 1990s a change in management philosophy brought the demise of the traditional organisational structure of boroughs where the three functions of town clerk, borough treasurer and borough engineer were replaced by an administrative structure with a larger number of specialised departments.In the late 1990s and early 21st century there was increasing dissatisfaction over what was perceived to be fractured and dysfunctional public services designed along narrow specialties."
What role did Edwin Chadwick play in the development of municipal engineering?,He invented the first sewage system,He authored a report on public health,He designed the first street lighting system,He established the first municipal engineering association,He developed the first water supply network,B,"Municipal or urban engineering applies the tools of science, art and engineering in an urban environment.
Municipal engineering is concerned with municipal infrastructure.  This involves specifying, designing, constructing, and maintaining streets, sidewalks, water supply networks, sewers, street lighting, municipal solid waste management and disposal, storage depots for various bulk materials used for maintenance and public works (salt, sand, etc.), public parks and cycling infrastructure.
In the case of underground utility networks, it may also include the civil portion (conduits and access chambers) of the local distribution networks of electrical and telecommunications services.  It can also include the optimizing of garbage collection and bus service networks.  Some of these disciplines overlap with other civil engineering specialties, however municipal engineering focuses on the coordination of these infrastructure networks and services, as they are often built simultaneously (for a given street or development project), and managed by the same municipal authority.

History
Modern municipal engineering finds its origins in the 19th-century United Kingdom, following the Industrial Revolution and the growth of large industrial cities. The threat to urban populations from epidemics of waterborne diseases such as cholera and typhus led to the development of a profession devoted to ""sanitary science"" that later became ""municipal engineering"".A key figure of the so-called ""public health movement"" was Edwin Chadwick, author of the parliamentary report, published in 1842.Early British legislation included:

Burgh Police Act 1833 - powers of paving, lighting, cleansing, watching, supplying with water and improving their communities.
Municipal Corporations Act 1835
Public Health Act 1866 – formation of drainage boards
Public Health Act 1875 known at the time as the Great Public Health ActThis legislation provided local authorities with powers to undertake municipal engineering projects and to appoint borough surveyors (later known as ""municipal engineers"").In the U.K, the Association of Municipal Engineers, (subsequently named Institution of Municipal Engineers), was established in 1874 under the encouragement of the Institution of Civil Engineers, to address the issue of the application of sanitary science. By the early 20th century, Municipal Engineering had become a broad discipline embracing many of the responsibilities undertaken by local authorities, including roads, drainage, flood control, coastal engineering, public health, waste management, street cleaning, water supply, sewers, waste water treatment, crematoria, public baths, slum clearance, town planning, public housing, energy supply, parks, leisure facilities, libraries, town halls and other municipal buildings.In the UK, the development of different strands of knowledge necessary for the management of municipal infrastructure led to the emergence of separate specialised institutions, including:

For drainage: Chartered Institution of Water and Environmental Management, 1895
For town planning: Town Planning Institute 1914 ... subsequently becoming the Royal Town Planning Institute
For street lighting: Association of Public Lighting Engineers, 1934...subsequently becoming the Institution of Lighting Engineers
For highway engineering: Institution of Highways and Transportation, 1930
For public housing: Institute of Housing, 1931In 1984 the Institution of Municipal Engineers merged with the Institution of Civil Engineers.Since the 1970s, there has been a global trend toward increasing privatisation and outsourcing of municipal engineering services.In the UK in the 1990s a change in management philosophy brought the demise of the traditional organisational structure of boroughs where the three functions of town clerk, borough treasurer and borough engineer were replaced by an administrative structure with a larger number of specialised departments.In the late 1990s and early 21st century there was increasing dissatisfaction over what was perceived to be fractured and dysfunctional public services designed along narrow specialties."
What was the purpose of the Public Health Act 1875?,To establish drainage boards,To appoint borough surveyors,To address the issue of sanitary science,To privatize municipal engineering projects,To promote town planning,C,"Municipal or urban engineering applies the tools of science, art and engineering in an urban environment.
Municipal engineering is concerned with municipal infrastructure.  This involves specifying, designing, constructing, and maintaining streets, sidewalks, water supply networks, sewers, street lighting, municipal solid waste management and disposal, storage depots for various bulk materials used for maintenance and public works (salt, sand, etc.), public parks and cycling infrastructure.
In the case of underground utility networks, it may also include the civil portion (conduits and access chambers) of the local distribution networks of electrical and telecommunications services.  It can also include the optimizing of garbage collection and bus service networks.  Some of these disciplines overlap with other civil engineering specialties, however municipal engineering focuses on the coordination of these infrastructure networks and services, as they are often built simultaneously (for a given street or development project), and managed by the same municipal authority.

History
Modern municipal engineering finds its origins in the 19th-century United Kingdom, following the Industrial Revolution and the growth of large industrial cities. The threat to urban populations from epidemics of waterborne diseases such as cholera and typhus led to the development of a profession devoted to ""sanitary science"" that later became ""municipal engineering"".A key figure of the so-called ""public health movement"" was Edwin Chadwick, author of the parliamentary report, published in 1842.Early British legislation included:

Burgh Police Act 1833 - powers of paving, lighting, cleansing, watching, supplying with water and improving their communities.
Municipal Corporations Act 1835
Public Health Act 1866 – formation of drainage boards
Public Health Act 1875 known at the time as the Great Public Health ActThis legislation provided local authorities with powers to undertake municipal engineering projects and to appoint borough surveyors (later known as ""municipal engineers"").In the U.K, the Association of Municipal Engineers, (subsequently named Institution of Municipal Engineers), was established in 1874 under the encouragement of the Institution of Civil Engineers, to address the issue of the application of sanitary science. By the early 20th century, Municipal Engineering had become a broad discipline embracing many of the responsibilities undertaken by local authorities, including roads, drainage, flood control, coastal engineering, public health, waste management, street cleaning, water supply, sewers, waste water treatment, crematoria, public baths, slum clearance, town planning, public housing, energy supply, parks, leisure facilities, libraries, town halls and other municipal buildings.In the UK, the development of different strands of knowledge necessary for the management of municipal infrastructure led to the emergence of separate specialised institutions, including:

For drainage: Chartered Institution of Water and Environmental Management, 1895
For town planning: Town Planning Institute 1914 ... subsequently becoming the Royal Town Planning Institute
For street lighting: Association of Public Lighting Engineers, 1934...subsequently becoming the Institution of Lighting Engineers
For highway engineering: Institution of Highways and Transportation, 1930
For public housing: Institute of Housing, 1931In 1984 the Institution of Municipal Engineers merged with the Institution of Civil Engineers.Since the 1970s, there has been a global trend toward increasing privatisation and outsourcing of municipal engineering services.In the UK in the 1990s a change in management philosophy brought the demise of the traditional organisational structure of boroughs where the three functions of town clerk, borough treasurer and borough engineer were replaced by an administrative structure with a larger number of specialised departments.In the late 1990s and early 21st century there was increasing dissatisfaction over what was perceived to be fractured and dysfunctional public services designed along narrow specialties."
What led to the change in management philosophy in the UK in the 1990s?,Increasing privatization of municipal engineering services,Dissatisfaction with fractured and dysfunctional public services,Emergence of separate specialized institutions,Advancements in technology for urban infrastructure,Growing demand for renewable energy sources,B,"Municipal or urban engineering applies the tools of science, art and engineering in an urban environment.
Municipal engineering is concerned with municipal infrastructure.  This involves specifying, designing, constructing, and maintaining streets, sidewalks, water supply networks, sewers, street lighting, municipal solid waste management and disposal, storage depots for various bulk materials used for maintenance and public works (salt, sand, etc.), public parks and cycling infrastructure.
In the case of underground utility networks, it may also include the civil portion (conduits and access chambers) of the local distribution networks of electrical and telecommunications services.  It can also include the optimizing of garbage collection and bus service networks.  Some of these disciplines overlap with other civil engineering specialties, however municipal engineering focuses on the coordination of these infrastructure networks and services, as they are often built simultaneously (for a given street or development project), and managed by the same municipal authority.

History
Modern municipal engineering finds its origins in the 19th-century United Kingdom, following the Industrial Revolution and the growth of large industrial cities. The threat to urban populations from epidemics of waterborne diseases such as cholera and typhus led to the development of a profession devoted to ""sanitary science"" that later became ""municipal engineering"".A key figure of the so-called ""public health movement"" was Edwin Chadwick, author of the parliamentary report, published in 1842.Early British legislation included:

Burgh Police Act 1833 - powers of paving, lighting, cleansing, watching, supplying with water and improving their communities.
Municipal Corporations Act 1835
Public Health Act 1866 – formation of drainage boards
Public Health Act 1875 known at the time as the Great Public Health ActThis legislation provided local authorities with powers to undertake municipal engineering projects and to appoint borough surveyors (later known as ""municipal engineers"").In the U.K, the Association of Municipal Engineers, (subsequently named Institution of Municipal Engineers), was established in 1874 under the encouragement of the Institution of Civil Engineers, to address the issue of the application of sanitary science. By the early 20th century, Municipal Engineering had become a broad discipline embracing many of the responsibilities undertaken by local authorities, including roads, drainage, flood control, coastal engineering, public health, waste management, street cleaning, water supply, sewers, waste water treatment, crematoria, public baths, slum clearance, town planning, public housing, energy supply, parks, leisure facilities, libraries, town halls and other municipal buildings.In the UK, the development of different strands of knowledge necessary for the management of municipal infrastructure led to the emergence of separate specialised institutions, including:

For drainage: Chartered Institution of Water and Environmental Management, 1895
For town planning: Town Planning Institute 1914 ... subsequently becoming the Royal Town Planning Institute
For street lighting: Association of Public Lighting Engineers, 1934...subsequently becoming the Institution of Lighting Engineers
For highway engineering: Institution of Highways and Transportation, 1930
For public housing: Institute of Housing, 1931In 1984 the Institution of Municipal Engineers merged with the Institution of Civil Engineers.Since the 1970s, there has been a global trend toward increasing privatisation and outsourcing of municipal engineering services.In the UK in the 1990s a change in management philosophy brought the demise of the traditional organisational structure of boroughs where the three functions of town clerk, borough treasurer and borough engineer were replaced by an administrative structure with a larger number of specialised departments.In the late 1990s and early 21st century there was increasing dissatisfaction over what was perceived to be fractured and dysfunctional public services designed along narrow specialties."
What is sports engineering?,A sub-discipline of engineering that uses math and science to develop technology for sports,A sub-discipline of engineering that focuses on improving sports equipment,A sub-discipline of engineering that studies the biomechanics of sports,A sub-discipline of engineering that specializes in materials science for sports,A sub-discipline of engineering that applies physics to sports,A,"Sports engineering is a sub-discipline of engineering that applies math and science to develop technology, equipment, and other resources as they pertain to sport. 

Sports engineering was first introduced by Issac Newton’s observation of a tennis ball. In the mid-twentieth century, Howard Head became one of the first engineers to apply engineering principles to improve sports equipment. Starting in 1999, the biannual international conference for sports engineering was established to commemorate achievements in the field. Presently, the journal entitled “Sports Engineering,” details the innovations and research projects that sports engineers are working on.The study of sports engineering requires an understanding of a variety of engineering topics including physics, mechanical engineering, materials science, and biomechanics. Many practitioners hold degrees in those topics rather than in sports engineering specifically. Specific study programs in sports engineering and technology are becoming more common at the graduate level, and also at the undergraduate level in Europe."
Who was one of the first engineers to apply engineering principles to improve sports equipment?,Issac Newton,Howard Head,Albert Einstein,Nikola Tesla,Thomas Edison,B,"Sports engineering is a sub-discipline of engineering that applies math and science to develop technology, equipment, and other resources as they pertain to sport. 

Sports engineering was first introduced by Issac Newton’s observation of a tennis ball. In the mid-twentieth century, Howard Head became one of the first engineers to apply engineering principles to improve sports equipment. Starting in 1999, the biannual international conference for sports engineering was established to commemorate achievements in the field. Presently, the journal entitled “Sports Engineering,” details the innovations and research projects that sports engineers are working on.The study of sports engineering requires an understanding of a variety of engineering topics including physics, mechanical engineering, materials science, and biomechanics. Many practitioners hold degrees in those topics rather than in sports engineering specifically. Specific study programs in sports engineering and technology are becoming more common at the graduate level, and also at the undergraduate level in Europe."
When was the biannual international conference for sports engineering established?,1999,1950,1980,2005,1975,A,"Sports engineering is a sub-discipline of engineering that applies math and science to develop technology, equipment, and other resources as they pertain to sport. 

Sports engineering was first introduced by Issac Newton’s observation of a tennis ball. In the mid-twentieth century, Howard Head became one of the first engineers to apply engineering principles to improve sports equipment. Starting in 1999, the biannual international conference for sports engineering was established to commemorate achievements in the field. Presently, the journal entitled “Sports Engineering,” details the innovations and research projects that sports engineers are working on.The study of sports engineering requires an understanding of a variety of engineering topics including physics, mechanical engineering, materials science, and biomechanics. Many practitioners hold degrees in those topics rather than in sports engineering specifically. Specific study programs in sports engineering and technology are becoming more common at the graduate level, and also at the undergraduate level in Europe."
"What does the journal ""Sports Engineering"" detail?",The history of sports engineering,The achievements of sports engineers,The future of sports engineering,The biannual international conference for sports engineering,The study programs in sports engineering,B,"Sports engineering is a sub-discipline of engineering that applies math and science to develop technology, equipment, and other resources as they pertain to sport. 

Sports engineering was first introduced by Issac Newton’s observation of a tennis ball. In the mid-twentieth century, Howard Head became one of the first engineers to apply engineering principles to improve sports equipment. Starting in 1999, the biannual international conference for sports engineering was established to commemorate achievements in the field. Presently, the journal entitled “Sports Engineering,” details the innovations and research projects that sports engineers are working on.The study of sports engineering requires an understanding of a variety of engineering topics including physics, mechanical engineering, materials science, and biomechanics. Many practitioners hold degrees in those topics rather than in sports engineering specifically. Specific study programs in sports engineering and technology are becoming more common at the graduate level, and also at the undergraduate level in Europe."
What topics are required to study sports engineering?,Physics and mathematics,Mechanical engineering and materials science,Biomechanics and materials science,Physics and biomechanics,Mechanical engineering and biomechanics,B,"Sports engineering is a sub-discipline of engineering that applies math and science to develop technology, equipment, and other resources as they pertain to sport. 

Sports engineering was first introduced by Issac Newton’s observation of a tennis ball. In the mid-twentieth century, Howard Head became one of the first engineers to apply engineering principles to improve sports equipment. Starting in 1999, the biannual international conference for sports engineering was established to commemorate achievements in the field. Presently, the journal entitled “Sports Engineering,” details the innovations and research projects that sports engineers are working on.The study of sports engineering requires an understanding of a variety of engineering topics including physics, mechanical engineering, materials science, and biomechanics. Many practitioners hold degrees in those topics rather than in sports engineering specifically. Specific study programs in sports engineering and technology are becoming more common at the graduate level, and also at the undergraduate level in Europe."
What is molecular engineering?,A field of study concerned with the design and testing of molecular properties,A field of study concerned with macroscopic systems,A field of study concerned with altering nanotechnology,A field of study concerned with trial-and-error approaches,A field of study concerned with printing electronics,A,"Molecular engineering is an emerging field of study concerned with the design and testing of molecular properties, behavior and interactions in order to assemble better materials, systems, and processes for specific functions. This approach, in which observable properties of a macroscopic system are influenced by direct alteration of a molecular structure, falls into the broader category of “bottom-up” design.

Molecular engineering is highly interdisciplinary by nature, encompassing aspects of chemical engineering, materials science, bioengineering, electrical engineering, physics, mechanical engineering, and chemistry. There is also considerable overlap with nanotechnology, in that both are concerned with the behavior of materials on the scale of nanometers or smaller. Given the highly fundamental nature of molecular interactions, there are a plethora of potential application areas, limited perhaps only by one's imagination and the laws of physics. However, some of the early successes of molecular engineering have come in the fields of immunotherapy, synthetic biology, and printable electronics (see molecular engineering applications).
Molecular engineering is a dynamic and evolving field with complex target problems; breakthroughs require sophisticated and creative engineers who are conversant across disciplines. A rational engineering methodology that is based on molecular principles is in contrast to the widespread trial-and-error approaches common throughout engineering disciplines. Rather than relying on well-described but poorly-understood empirical correlations between the makeup of a system and its properties, a molecular design approach seeks to manipulate system properties directly using an understanding of their chemical and physical origins."
Which disciplines does molecular engineering encompass?,"Chemical engineering, materials science, and bioengineering","Electrical engineering, physics, and mechanical engineering",Chemistry and nanotechnology,Physics and mechanical engineering,Immunotherapy and synthetic biology,A,"Molecular engineering is an emerging field of study concerned with the design and testing of molecular properties, behavior and interactions in order to assemble better materials, systems, and processes for specific functions. This approach, in which observable properties of a macroscopic system are influenced by direct alteration of a molecular structure, falls into the broader category of “bottom-up” design.

Molecular engineering is highly interdisciplinary by nature, encompassing aspects of chemical engineering, materials science, bioengineering, electrical engineering, physics, mechanical engineering, and chemistry. There is also considerable overlap with nanotechnology, in that both are concerned with the behavior of materials on the scale of nanometers or smaller. Given the highly fundamental nature of molecular interactions, there are a plethora of potential application areas, limited perhaps only by one's imagination and the laws of physics. However, some of the early successes of molecular engineering have come in the fields of immunotherapy, synthetic biology, and printable electronics (see molecular engineering applications).
Molecular engineering is a dynamic and evolving field with complex target problems; breakthroughs require sophisticated and creative engineers who are conversant across disciplines. A rational engineering methodology that is based on molecular principles is in contrast to the widespread trial-and-error approaches common throughout engineering disciplines. Rather than relying on well-described but poorly-understood empirical correlations between the makeup of a system and its properties, a molecular design approach seeks to manipulate system properties directly using an understanding of their chemical and physical origins."
What are some early successes of molecular engineering?,"Immunotherapy, synthetic biology, and printable electronics","Chemical engineering, materials science, and bioengineering","Electrical engineering, physics, and mechanical engineering",Chemistry and nanotechnology,Physics and mechanical engineering,A,"Molecular engineering is an emerging field of study concerned with the design and testing of molecular properties, behavior and interactions in order to assemble better materials, systems, and processes for specific functions. This approach, in which observable properties of a macroscopic system are influenced by direct alteration of a molecular structure, falls into the broader category of “bottom-up” design.

Molecular engineering is highly interdisciplinary by nature, encompassing aspects of chemical engineering, materials science, bioengineering, electrical engineering, physics, mechanical engineering, and chemistry. There is also considerable overlap with nanotechnology, in that both are concerned with the behavior of materials on the scale of nanometers or smaller. Given the highly fundamental nature of molecular interactions, there are a plethora of potential application areas, limited perhaps only by one's imagination and the laws of physics. However, some of the early successes of molecular engineering have come in the fields of immunotherapy, synthetic biology, and printable electronics (see molecular engineering applications).
Molecular engineering is a dynamic and evolving field with complex target problems; breakthroughs require sophisticated and creative engineers who are conversant across disciplines. A rational engineering methodology that is based on molecular principles is in contrast to the widespread trial-and-error approaches common throughout engineering disciplines. Rather than relying on well-described but poorly-understood empirical correlations between the makeup of a system and its properties, a molecular design approach seeks to manipulate system properties directly using an understanding of their chemical and physical origins."
What sets molecular engineering apart from other engineering disciplines?,It relies on trial-and-error approaches,It encompasses macroscopic systems,It focuses on printed electronics,It seeks to manipulate system properties directly based on chemical and physical origins,It focuses on nanotechnology,D,"Molecular engineering is an emerging field of study concerned with the design and testing of molecular properties, behavior and interactions in order to assemble better materials, systems, and processes for specific functions. This approach, in which observable properties of a macroscopic system are influenced by direct alteration of a molecular structure, falls into the broader category of “bottom-up” design.

Molecular engineering is highly interdisciplinary by nature, encompassing aspects of chemical engineering, materials science, bioengineering, electrical engineering, physics, mechanical engineering, and chemistry. There is also considerable overlap with nanotechnology, in that both are concerned with the behavior of materials on the scale of nanometers or smaller. Given the highly fundamental nature of molecular interactions, there are a plethora of potential application areas, limited perhaps only by one's imagination and the laws of physics. However, some of the early successes of molecular engineering have come in the fields of immunotherapy, synthetic biology, and printable electronics (see molecular engineering applications).
Molecular engineering is a dynamic and evolving field with complex target problems; breakthroughs require sophisticated and creative engineers who are conversant across disciplines. A rational engineering methodology that is based on molecular principles is in contrast to the widespread trial-and-error approaches common throughout engineering disciplines. Rather than relying on well-described but poorly-understood empirical correlations between the makeup of a system and its properties, a molecular design approach seeks to manipulate system properties directly using an understanding of their chemical and physical origins."
What is required for breakthroughs in molecular engineering?,Conversance across disciplines and trial-and-error approaches,Sophisticated and creative engineers conversant across disciplines,An understanding of chemical and physical origins,An understanding of empirical correlations,An understanding of macroscopic systems,B,"Molecular engineering is an emerging field of study concerned with the design and testing of molecular properties, behavior and interactions in order to assemble better materials, systems, and processes for specific functions. This approach, in which observable properties of a macroscopic system are influenced by direct alteration of a molecular structure, falls into the broader category of “bottom-up” design.

Molecular engineering is highly interdisciplinary by nature, encompassing aspects of chemical engineering, materials science, bioengineering, electrical engineering, physics, mechanical engineering, and chemistry. There is also considerable overlap with nanotechnology, in that both are concerned with the behavior of materials on the scale of nanometers or smaller. Given the highly fundamental nature of molecular interactions, there are a plethora of potential application areas, limited perhaps only by one's imagination and the laws of physics. However, some of the early successes of molecular engineering have come in the fields of immunotherapy, synthetic biology, and printable electronics (see molecular engineering applications).
Molecular engineering is a dynamic and evolving field with complex target problems; breakthroughs require sophisticated and creative engineers who are conversant across disciplines. A rational engineering methodology that is based on molecular principles is in contrast to the widespread trial-and-error approaches common throughout engineering disciplines. Rather than relying on well-described but poorly-understood empirical correlations between the makeup of a system and its properties, a molecular design approach seeks to manipulate system properties directly using an understanding of their chemical and physical origins."
What is a lottery in expected utility theory?,A continuous distribution of probability on a set of states of nature,A discrete distribution of probability on a set of states of nature,A random assortment of outcomes with no probability distribution,A deterministic outcome with no uncertainty,A distribution of probabilities on a set of states of mind,B,"In expected utility theory, a lottery is a discrete distribution of probability on a set of states of nature. The elements of a lottery correspond to the probabilities that each of the states of nature will occur, e.g. (Rain:.70, No Rain:.30). Much of the theoretical analysis of choice under uncertainty involves characterizing the available choices in terms of lotteries.
In economics, individuals are assumed to rank lotteries according to a rational system of preferences, although it is now accepted that people make irrational choices systematically.  Behavioral economics studies what happens in markets in which some of the agents display human complications and limitations.

Choice under risk
According to expected utility theory, someone chooses among lotteries by multiplying his subjective estimate of the probabilities of the possible outcomes by a utility attached to each outcome by his personal utility function. Thus, each lottery has an expected utility, a linear combination of the utilities of the outcomes in which weights are the subjective probabilities. It is also founded in the famous example, the St."
How are lotteries ranked in economics?,Based on the number of outcomes,Based on the probabilities of the outcomes,Based on the utilities attached to each outcome,Based on the subjective estimates of the probabilities,Based on the expected utility of each outcome,E,"In expected utility theory, a lottery is a discrete distribution of probability on a set of states of nature. The elements of a lottery correspond to the probabilities that each of the states of nature will occur, e.g. (Rain:.70, No Rain:.30). Much of the theoretical analysis of choice under uncertainty involves characterizing the available choices in terms of lotteries.
In economics, individuals are assumed to rank lotteries according to a rational system of preferences, although it is now accepted that people make irrational choices systematically.  Behavioral economics studies what happens in markets in which some of the agents display human complications and limitations.

Choice under risk
According to expected utility theory, someone chooses among lotteries by multiplying his subjective estimate of the probabilities of the possible outcomes by a utility attached to each outcome by his personal utility function. Thus, each lottery has an expected utility, a linear combination of the utilities of the outcomes in which weights are the subjective probabilities. It is also founded in the famous example, the St."
What is the basis of expected utility theory?,The subjective estimates of probabilities,The utilities attached to each outcome,The linear combination of utility and probabilities,The rational system of preferences,The analysis of choice under uncertainty,C,"In expected utility theory, a lottery is a discrete distribution of probability on a set of states of nature. The elements of a lottery correspond to the probabilities that each of the states of nature will occur, e.g. (Rain:.70, No Rain:.30). Much of the theoretical analysis of choice under uncertainty involves characterizing the available choices in terms of lotteries.
In economics, individuals are assumed to rank lotteries according to a rational system of preferences, although it is now accepted that people make irrational choices systematically.  Behavioral economics studies what happens in markets in which some of the agents display human complications and limitations.

Choice under risk
According to expected utility theory, someone chooses among lotteries by multiplying his subjective estimate of the probabilities of the possible outcomes by a utility attached to each outcome by his personal utility function. Thus, each lottery has an expected utility, a linear combination of the utilities of the outcomes in which weights are the subjective probabilities. It is also founded in the famous example, the St."
What does behavioral economics study?,The market behavior of rational agents,The choices individuals make in uncertain situations,The limitations and complications of human decision-making,The expected utility of different outcomes,The theory of preferences in economics,C,"In expected utility theory, a lottery is a discrete distribution of probability on a set of states of nature. The elements of a lottery correspond to the probabilities that each of the states of nature will occur, e.g. (Rain:.70, No Rain:.30). Much of the theoretical analysis of choice under uncertainty involves characterizing the available choices in terms of lotteries.
In economics, individuals are assumed to rank lotteries according to a rational system of preferences, although it is now accepted that people make irrational choices systematically.  Behavioral economics studies what happens in markets in which some of the agents display human complications and limitations.

Choice under risk
According to expected utility theory, someone chooses among lotteries by multiplying his subjective estimate of the probabilities of the possible outcomes by a utility attached to each outcome by his personal utility function. Thus, each lottery has an expected utility, a linear combination of the utilities of the outcomes in which weights are the subjective probabilities. It is also founded in the famous example, the St."
What is the expected utility of a lottery?,A subjective estimate of the probabilities of the outcomes,A linear combination of the utilities of the outcomes,A rational system of preferences in choice-making,A rational system of preferences in choice-making,A distribution of probabilities on a set of states of mind,B,"In expected utility theory, a lottery is a discrete distribution of probability on a set of states of nature. The elements of a lottery correspond to the probabilities that each of the states of nature will occur, e.g. (Rain:.70, No Rain:.30). Much of the theoretical analysis of choice under uncertainty involves characterizing the available choices in terms of lotteries.
In economics, individuals are assumed to rank lotteries according to a rational system of preferences, although it is now accepted that people make irrational choices systematically.  Behavioral economics studies what happens in markets in which some of the agents display human complications and limitations.

Choice under risk
According to expected utility theory, someone chooses among lotteries by multiplying his subjective estimate of the probabilities of the possible outcomes by a utility attached to each outcome by his personal utility function. Thus, each lottery has an expected utility, a linear combination of the utilities of the outcomes in which weights are the subjective probabilities. It is also founded in the famous example, the St."
What is the geographical feature of the Gotjawal Forest?,Flat plains,Sandy beaches,Rocky area,Marshy swamps,Gentle slopes,C,"Gotjawal Forest is a naturally formed forest located on the middle slopes of Halla Mountain, Jeju Island in South Korea. It covers the rocky area of ʻaʻā on Jeju Island off the southwestern coast of South Korea. Due to the geographical feature, the region remains largely undisturbed by people. The Gotjawal Forest is an enclave of the Southern Korea evergreen forests ecoregion, and is a favorite place of the Jeju locals.

Etymology
Traditionally, Jeju locals call any forest on rocky ground ""Gotjawal"" (곶자왈). According to the Jeju Dialect Dictionary, ""Gotjawal"" refers to an unmanned and unapproachable forest mixed with trees and bushes. However, Song Shi-tae suggested, in his Ph.D. dissertation, a new meaning for the term ""Gotjawal"" because of its direct association with ""ʻAʻā."
Which ecoregion does the Gotjawal Forest belong to?,Temperate rainforests,Tropical rainforests,Mangrove swamps,Desert dunes,Southern Korea evergreen forests,E,"Gotjawal Forest is a naturally formed forest located on the middle slopes of Halla Mountain, Jeju Island in South Korea. It covers the rocky area of ʻaʻā on Jeju Island off the southwestern coast of South Korea. Due to the geographical feature, the region remains largely undisturbed by people. The Gotjawal Forest is an enclave of the Southern Korea evergreen forests ecoregion, and is a favorite place of the Jeju locals.

Etymology
Traditionally, Jeju locals call any forest on rocky ground ""Gotjawal"" (곶자왈). According to the Jeju Dialect Dictionary, ""Gotjawal"" refers to an unmanned and unapproachable forest mixed with trees and bushes. However, Song Shi-tae suggested, in his Ph.D. dissertation, a new meaning for the term ""Gotjawal"" because of its direct association with ""ʻAʻā."
"What does the term ""Gotjawal"" refer to according to the Jeju Dialect Dictionary?",A forest with tall trees,A forest with rare plants,An unmanned and unapproachable forest mixed with trees and bushes,A forest with abundant wildlife,A forest with natural springs,C,"Gotjawal Forest is a naturally formed forest located on the middle slopes of Halla Mountain, Jeju Island in South Korea. It covers the rocky area of ʻaʻā on Jeju Island off the southwestern coast of South Korea. Due to the geographical feature, the region remains largely undisturbed by people. The Gotjawal Forest is an enclave of the Southern Korea evergreen forests ecoregion, and is a favorite place of the Jeju locals.

Etymology
Traditionally, Jeju locals call any forest on rocky ground ""Gotjawal"" (곶자왈). According to the Jeju Dialect Dictionary, ""Gotjawal"" refers to an unmanned and unapproachable forest mixed with trees and bushes. However, Song Shi-tae suggested, in his Ph.D. dissertation, a new meaning for the term ""Gotjawal"" because of its direct association with ""ʻAʻā."
"Who suggested a new meaning for the term ""Gotjawal""?",Jeju locals,Ph.D. dissertation author,Ecologists,Government officials,Tourism board,B,"Gotjawal Forest is a naturally formed forest located on the middle slopes of Halla Mountain, Jeju Island in South Korea. It covers the rocky area of ʻaʻā on Jeju Island off the southwestern coast of South Korea. Due to the geographical feature, the region remains largely undisturbed by people. The Gotjawal Forest is an enclave of the Southern Korea evergreen forests ecoregion, and is a favorite place of the Jeju locals.

Etymology
Traditionally, Jeju locals call any forest on rocky ground ""Gotjawal"" (곶자왈). According to the Jeju Dialect Dictionary, ""Gotjawal"" refers to an unmanned and unapproachable forest mixed with trees and bushes. However, Song Shi-tae suggested, in his Ph.D. dissertation, a new meaning for the term ""Gotjawal"" because of its direct association with ""ʻAʻā."
"What is the direct association of ""Gotjawal"" with?",Temperate forests,Tropical beaches,Rocky mountains,Marshy wetlands,ʻAʻā,E,"Gotjawal Forest is a naturally formed forest located on the middle slopes of Halla Mountain, Jeju Island in South Korea. It covers the rocky area of ʻaʻā on Jeju Island off the southwestern coast of South Korea. Due to the geographical feature, the region remains largely undisturbed by people. The Gotjawal Forest is an enclave of the Southern Korea evergreen forests ecoregion, and is a favorite place of the Jeju locals.

Etymology
Traditionally, Jeju locals call any forest on rocky ground ""Gotjawal"" (곶자왈). According to the Jeju Dialect Dictionary, ""Gotjawal"" refers to an unmanned and unapproachable forest mixed with trees and bushes. However, Song Shi-tae suggested, in his Ph.D. dissertation, a new meaning for the term ""Gotjawal"" because of its direct association with ""ʻAʻā."
What is the definition of geomatics according to the ISO/TC 211 series of standards?,"The discipline concerned with the collection, distribution, storage, analysis, processing, presentation of geographic data or geographic information.",The study of land surveying and remote sensing.,The management of geographic (geospatial) data.,The study of geophysics and geography.,The study of satellite navigation systems.,A,"Geomatics is defined in the ISO/TC 211 series of standards as the ""discipline concerned with the collection, distribution, storage, analysis, processing, presentation of geographic data or geographic information"". Under another definition, it consists of products, services and tools involved in the collection, integration and management of geographic (geospatial) data. It is also known as geomatic(s) engineering (geodesy and geoinformatics engineering or geospatial engineering). Surveying engineering was the widely used name for geomatic(s) engineering in the past.

History and etymology
The term was proposed in French (""géomatique"") at the end of the 1960s by scientist Bernard Dubuisson to reflect at the time recent changes in the jobs of surveyor and photogrammetrist. The term was first employed in a French Ministry of Public Works memorandum dated 1 June 1971 instituting a ""standing committee of geomatics"" in the government.The term was popularised in English by French-Canadian surveyor Michel Paradis in his The little Geodesist that could article, in 1981 and in a keynote address at the centennial congress of the Canadian Institute of Surveying (now known as the Canadian Institute of Geomatics) in April 1982. He claimed that at the end of the 20th century the needs for geographical information would reach a scope without precedent in history and that, in order to address these needs, it was necessary to integrate in a new discipline both the traditional disciplines of land surveying and the new tools and techniques of data capture, manipulation, storage and diffusion.Geomatics includes the tools and techniques used in land surveying, remote sensing, cartography, geographic information systems (GIS), global navigation satellite systems (GPS, GLONASS, Galileo, BeiDou), photogrammetry, geophysics, geography, and related forms of earth mapping. The term was originally used in Canada but has since been adopted by the International Organization for Standardization, the Royal Institution of Chartered Surveyors, and many other international authorities, although some (especially in the United States) have shown a preference for the term geospatial technology, which may be defined as synonym of ""geospatial information and communications technology"".Although many definitions of geomatics, such as the above, appear to encompass the entire discipline relating to geographic information – including geodesy, geographic information systems, remote sensing, satellite navigation, and cartography –, the term is almost exclusively restricted to the perspective of surveying and engineering toward geographic information."
"Who proposed the term ""geomatics"" in French in the 1960s?",Bernard Dubuisson,Michel Paradis,ISO/TC 211,Royal Institution of Chartered Surveyors,Canadian Institute of Geomatics,A,"Geomatics is defined in the ISO/TC 211 series of standards as the ""discipline concerned with the collection, distribution, storage, analysis, processing, presentation of geographic data or geographic information"". Under another definition, it consists of products, services and tools involved in the collection, integration and management of geographic (geospatial) data. It is also known as geomatic(s) engineering (geodesy and geoinformatics engineering or geospatial engineering). Surveying engineering was the widely used name for geomatic(s) engineering in the past.

History and etymology
The term was proposed in French (""géomatique"") at the end of the 1960s by scientist Bernard Dubuisson to reflect at the time recent changes in the jobs of surveyor and photogrammetrist. The term was first employed in a French Ministry of Public Works memorandum dated 1 June 1971 instituting a ""standing committee of geomatics"" in the government.The term was popularised in English by French-Canadian surveyor Michel Paradis in his The little Geodesist that could article, in 1981 and in a keynote address at the centennial congress of the Canadian Institute of Surveying (now known as the Canadian Institute of Geomatics) in April 1982. He claimed that at the end of the 20th century the needs for geographical information would reach a scope without precedent in history and that, in order to address these needs, it was necessary to integrate in a new discipline both the traditional disciplines of land surveying and the new tools and techniques of data capture, manipulation, storage and diffusion.Geomatics includes the tools and techniques used in land surveying, remote sensing, cartography, geographic information systems (GIS), global navigation satellite systems (GPS, GLONASS, Galileo, BeiDou), photogrammetry, geophysics, geography, and related forms of earth mapping. The term was originally used in Canada but has since been adopted by the International Organization for Standardization, the Royal Institution of Chartered Surveyors, and many other international authorities, although some (especially in the United States) have shown a preference for the term geospatial technology, which may be defined as synonym of ""geospatial information and communications technology"".Although many definitions of geomatics, such as the above, appear to encompass the entire discipline relating to geographic information – including geodesy, geographic information systems, remote sensing, satellite navigation, and cartography –, the term is almost exclusively restricted to the perspective of surveying and engineering toward geographic information."
"In which year did the term ""geomatics"" gain popularity in English?",1960,1971,1981,1982,2000,D,"Geomatics is defined in the ISO/TC 211 series of standards as the ""discipline concerned with the collection, distribution, storage, analysis, processing, presentation of geographic data or geographic information"". Under another definition, it consists of products, services and tools involved in the collection, integration and management of geographic (geospatial) data. It is also known as geomatic(s) engineering (geodesy and geoinformatics engineering or geospatial engineering). Surveying engineering was the widely used name for geomatic(s) engineering in the past.

History and etymology
The term was proposed in French (""géomatique"") at the end of the 1960s by scientist Bernard Dubuisson to reflect at the time recent changes in the jobs of surveyor and photogrammetrist. The term was first employed in a French Ministry of Public Works memorandum dated 1 June 1971 instituting a ""standing committee of geomatics"" in the government.The term was popularised in English by French-Canadian surveyor Michel Paradis in his The little Geodesist that could article, in 1981 and in a keynote address at the centennial congress of the Canadian Institute of Surveying (now known as the Canadian Institute of Geomatics) in April 1982. He claimed that at the end of the 20th century the needs for geographical information would reach a scope without precedent in history and that, in order to address these needs, it was necessary to integrate in a new discipline both the traditional disciplines of land surveying and the new tools and techniques of data capture, manipulation, storage and diffusion.Geomatics includes the tools and techniques used in land surveying, remote sensing, cartography, geographic information systems (GIS), global navigation satellite systems (GPS, GLONASS, Galileo, BeiDou), photogrammetry, geophysics, geography, and related forms of earth mapping. The term was originally used in Canada but has since been adopted by the International Organization for Standardization, the Royal Institution of Chartered Surveyors, and many other international authorities, although some (especially in the United States) have shown a preference for the term geospatial technology, which may be defined as synonym of ""geospatial information and communications technology"".Although many definitions of geomatics, such as the above, appear to encompass the entire discipline relating to geographic information – including geodesy, geographic information systems, remote sensing, satellite navigation, and cartography –, the term is almost exclusively restricted to the perspective of surveying and engineering toward geographic information."
What are some of the disciplines included in geomatics?,"Land surveying, remote sensing, cartography","Geophysics, geography, geodesy",Geographic information systems (GIS),All of the above,None of the above,D,"Geomatics is defined in the ISO/TC 211 series of standards as the ""discipline concerned with the collection, distribution, storage, analysis, processing, presentation of geographic data or geographic information"". Under another definition, it consists of products, services and tools involved in the collection, integration and management of geographic (geospatial) data. It is also known as geomatic(s) engineering (geodesy and geoinformatics engineering or geospatial engineering). Surveying engineering was the widely used name for geomatic(s) engineering in the past.

History and etymology
The term was proposed in French (""géomatique"") at the end of the 1960s by scientist Bernard Dubuisson to reflect at the time recent changes in the jobs of surveyor and photogrammetrist. The term was first employed in a French Ministry of Public Works memorandum dated 1 June 1971 instituting a ""standing committee of geomatics"" in the government.The term was popularised in English by French-Canadian surveyor Michel Paradis in his The little Geodesist that could article, in 1981 and in a keynote address at the centennial congress of the Canadian Institute of Surveying (now known as the Canadian Institute of Geomatics) in April 1982. He claimed that at the end of the 20th century the needs for geographical information would reach a scope without precedent in history and that, in order to address these needs, it was necessary to integrate in a new discipline both the traditional disciplines of land surveying and the new tools and techniques of data capture, manipulation, storage and diffusion.Geomatics includes the tools and techniques used in land surveying, remote sensing, cartography, geographic information systems (GIS), global navigation satellite systems (GPS, GLONASS, Galileo, BeiDou), photogrammetry, geophysics, geography, and related forms of earth mapping. The term was originally used in Canada but has since been adopted by the International Organization for Standardization, the Royal Institution of Chartered Surveyors, and many other international authorities, although some (especially in the United States) have shown a preference for the term geospatial technology, which may be defined as synonym of ""geospatial information and communications technology"".Although many definitions of geomatics, such as the above, appear to encompass the entire discipline relating to geographic information – including geodesy, geographic information systems, remote sensing, satellite navigation, and cartography –, the term is almost exclusively restricted to the perspective of surveying and engineering toward geographic information."
What term is sometimes used as a synonym for geospatial technology?,Geomatics,Geodesy,Remote sensing,Cartography,Geophysics,A,"Geomatics is defined in the ISO/TC 211 series of standards as the ""discipline concerned with the collection, distribution, storage, analysis, processing, presentation of geographic data or geographic information"". Under another definition, it consists of products, services and tools involved in the collection, integration and management of geographic (geospatial) data. It is also known as geomatic(s) engineering (geodesy and geoinformatics engineering or geospatial engineering). Surveying engineering was the widely used name for geomatic(s) engineering in the past.

History and etymology
The term was proposed in French (""géomatique"") at the end of the 1960s by scientist Bernard Dubuisson to reflect at the time recent changes in the jobs of surveyor and photogrammetrist. The term was first employed in a French Ministry of Public Works memorandum dated 1 June 1971 instituting a ""standing committee of geomatics"" in the government.The term was popularised in English by French-Canadian surveyor Michel Paradis in his The little Geodesist that could article, in 1981 and in a keynote address at the centennial congress of the Canadian Institute of Surveying (now known as the Canadian Institute of Geomatics) in April 1982. He claimed that at the end of the 20th century the needs for geographical information would reach a scope without precedent in history and that, in order to address these needs, it was necessary to integrate in a new discipline both the traditional disciplines of land surveying and the new tools and techniques of data capture, manipulation, storage and diffusion.Geomatics includes the tools and techniques used in land surveying, remote sensing, cartography, geographic information systems (GIS), global navigation satellite systems (GPS, GLONASS, Galileo, BeiDou), photogrammetry, geophysics, geography, and related forms of earth mapping. The term was originally used in Canada but has since been adopted by the International Organization for Standardization, the Royal Institution of Chartered Surveyors, and many other international authorities, although some (especially in the United States) have shown a preference for the term geospatial technology, which may be defined as synonym of ""geospatial information and communications technology"".Although many definitions of geomatics, such as the above, appear to encompass the entire discipline relating to geographic information – including geodesy, geographic information systems, remote sensing, satellite navigation, and cartography –, the term is almost exclusively restricted to the perspective of surveying and engineering toward geographic information."
What is the focus of theoretical computer science?,The study of algorithms and data structures,The mathematical aspects of computer science,The development of quantum mechanics,The study of neural networks,The study of information theory,B,"Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory.
It is difficult to circumscribe the theoretical areas precisely. The ACM's Special Interest Group on Algorithms and Computation Theory (SIGACT) provides the following description:
TCS covers a wide variety of topics including algorithms, data structures, computational complexity, parallel and distributed computation, probabilistic computation, quantum computation, automata theory, information theory, cryptography, program semantics and verification, algorithmic game theory, machine learning, computational biology, computational economics, computational geometry, and computational number theory and algebra. Work in this field is often distinguished by its emphasis on mathematical technique and rigor.

History
While logical inference and mathematical proof had existed previously, in 1931 Kurt Gödel proved with his incompleteness theorem that there are fundamental limitations on what statements could be proved or disproved.
Information theory was added to the field with a 1948 mathematical theory of communication by Claude Shannon. In the same decade, Donald Hebb introduced a mathematical model of learning in the brain. With mounting biological data supporting this hypothesis with some modification, the fields of neural networks and parallel distributed processing were established. In 1971, Stephen Cook and, working independently, Leonid Levin, proved that there exist practically relevant problems that are NP-complete – a landmark result in computational complexity theory.
With the development of quantum mechanics in the beginning of the 20th century came the concept that mathematical operations could be performed on an entire particle wavefunction. In other words, one could compute functions on multiple states simultaneously."
Which theorem proved the fundamental limitations on what statements could be proved or disproved?,Gödel's incompleteness theorem,Hebb's learning model,Shannon's theory of communication,Cook and Levin's result in computational complexity theory,Quantum mechanics,A,"Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory.
It is difficult to circumscribe the theoretical areas precisely. The ACM's Special Interest Group on Algorithms and Computation Theory (SIGACT) provides the following description:
TCS covers a wide variety of topics including algorithms, data structures, computational complexity, parallel and distributed computation, probabilistic computation, quantum computation, automata theory, information theory, cryptography, program semantics and verification, algorithmic game theory, machine learning, computational biology, computational economics, computational geometry, and computational number theory and algebra. Work in this field is often distinguished by its emphasis on mathematical technique and rigor.

History
While logical inference and mathematical proof had existed previously, in 1931 Kurt Gödel proved with his incompleteness theorem that there are fundamental limitations on what statements could be proved or disproved.
Information theory was added to the field with a 1948 mathematical theory of communication by Claude Shannon. In the same decade, Donald Hebb introduced a mathematical model of learning in the brain. With mounting biological data supporting this hypothesis with some modification, the fields of neural networks and parallel distributed processing were established. In 1971, Stephen Cook and, working independently, Leonid Levin, proved that there exist practically relevant problems that are NP-complete – a landmark result in computational complexity theory.
With the development of quantum mechanics in the beginning of the 20th century came the concept that mathematical operations could be performed on an entire particle wavefunction. In other words, one could compute functions on multiple states simultaneously."
Which field of study was established based on the mathematical model of learning in the brain?,Algorithmic game theory,Computational biology,Neural networks,Computational economics,Computational geometry,C,"Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory.
It is difficult to circumscribe the theoretical areas precisely. The ACM's Special Interest Group on Algorithms and Computation Theory (SIGACT) provides the following description:
TCS covers a wide variety of topics including algorithms, data structures, computational complexity, parallel and distributed computation, probabilistic computation, quantum computation, automata theory, information theory, cryptography, program semantics and verification, algorithmic game theory, machine learning, computational biology, computational economics, computational geometry, and computational number theory and algebra. Work in this field is often distinguished by its emphasis on mathematical technique and rigor.

History
While logical inference and mathematical proof had existed previously, in 1931 Kurt Gödel proved with his incompleteness theorem that there are fundamental limitations on what statements could be proved or disproved.
Information theory was added to the field with a 1948 mathematical theory of communication by Claude Shannon. In the same decade, Donald Hebb introduced a mathematical model of learning in the brain. With mounting biological data supporting this hypothesis with some modification, the fields of neural networks and parallel distributed processing were established. In 1971, Stephen Cook and, working independently, Leonid Levin, proved that there exist practically relevant problems that are NP-complete – a landmark result in computational complexity theory.
With the development of quantum mechanics in the beginning of the 20th century came the concept that mathematical operations could be performed on an entire particle wavefunction. In other words, one could compute functions on multiple states simultaneously."
What was the landmark result in computational complexity theory?,Gödel's incompleteness theorem,Hebb's learning model,Shannon's theory of communication,Cook and Levin's result,Quantum mechanics,D,"Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory.
It is difficult to circumscribe the theoretical areas precisely. The ACM's Special Interest Group on Algorithms and Computation Theory (SIGACT) provides the following description:
TCS covers a wide variety of topics including algorithms, data structures, computational complexity, parallel and distributed computation, probabilistic computation, quantum computation, automata theory, information theory, cryptography, program semantics and verification, algorithmic game theory, machine learning, computational biology, computational economics, computational geometry, and computational number theory and algebra. Work in this field is often distinguished by its emphasis on mathematical technique and rigor.

History
While logical inference and mathematical proof had existed previously, in 1931 Kurt Gödel proved with his incompleteness theorem that there are fundamental limitations on what statements could be proved or disproved.
Information theory was added to the field with a 1948 mathematical theory of communication by Claude Shannon. In the same decade, Donald Hebb introduced a mathematical model of learning in the brain. With mounting biological data supporting this hypothesis with some modification, the fields of neural networks and parallel distributed processing were established. In 1971, Stephen Cook and, working independently, Leonid Levin, proved that there exist practically relevant problems that are NP-complete – a landmark result in computational complexity theory.
With the development of quantum mechanics in the beginning of the 20th century came the concept that mathematical operations could be performed on an entire particle wavefunction. In other words, one could compute functions on multiple states simultaneously."
What concept allowed mathematical operations to be performed on an entire particle wavefunction?,Gödel's incompleteness theorem,Hebb's learning model,Shannon's theory of communication,Quantum mechanics,Neural networks,D,"Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory.
It is difficult to circumscribe the theoretical areas precisely. The ACM's Special Interest Group on Algorithms and Computation Theory (SIGACT) provides the following description:
TCS covers a wide variety of topics including algorithms, data structures, computational complexity, parallel and distributed computation, probabilistic computation, quantum computation, automata theory, information theory, cryptography, program semantics and verification, algorithmic game theory, machine learning, computational biology, computational economics, computational geometry, and computational number theory and algebra. Work in this field is often distinguished by its emphasis on mathematical technique and rigor.

History
While logical inference and mathematical proof had existed previously, in 1931 Kurt Gödel proved with his incompleteness theorem that there are fundamental limitations on what statements could be proved or disproved.
Information theory was added to the field with a 1948 mathematical theory of communication by Claude Shannon. In the same decade, Donald Hebb introduced a mathematical model of learning in the brain. With mounting biological data supporting this hypothesis with some modification, the fields of neural networks and parallel distributed processing were established. In 1971, Stephen Cook and, working independently, Leonid Levin, proved that there exist practically relevant problems that are NP-complete – a landmark result in computational complexity theory.
With the development of quantum mechanics in the beginning of the 20th century came the concept that mathematical operations could be performed on an entire particle wavefunction. In other words, one could compute functions on multiple states simultaneously."
What is the main focus of the academic discipline of history of science?,Studying the cultural impacts of scientific practices,Examining the ability to manipulate the natural world,Investigating the political impacts of scientific practices,Answering questions about the nature and function of science,Analyzing the economic impacts of scientific practices,D,"The history of science and technology (HST) is a field of history that examines the understanding of the natural world (science) and the ability to manipulate it (technology) at different points in time. This academic discipline also studies the cultural, economic, and political impacts of and contexts for scientific practices.

Academic study of history of science
History of science is an academic discipline with an international community of specialists. Main professional organizations for this field include the History of Science Society, the British Society for the History of Science, and the European Society for the History of Science.
Much of the study of the history of science has been devoted to answering questions about what science is, how it functions, and whether it exhibits large-scale patterns and trends.

History of the academic study of history of science
Histories of science were originally written by practicing and retired scientists, starting primarily with William Whewell's History of the Inductive Sciences (1837), as a way to communicate the virtues of science to the public.Auguste Comte proposed that there should be a specific discipline to deal with the history of science.The development of the distinct academic discipline of the history of science and technology did not occur until the early 20th century. Historians have suggested that this was bound to the changing role of science during the same time period.After World War I, extensive resources were put into teaching and researching the discipline, with the hopes that it would help the public better understand both Science and Technology as they came to play an exceedingly prominent role in the world.In the decades since the end of World War II, history of science became an academic discipline, with graduate schools, research institutes, public and private patronage, peer-reviewed journals, and professional societies.

Formation of academic departments
In the United States, a more formal study of the history of science as an independent discipline was initiated by George Sarton's publications, Introduction to the History of Science (1927) and the journal Isis (founded in 1912). Sarton exemplified the early 20th-century view of the history of science as the history of great men and great ideas. He shared with many of his contemporaries a Whiggish belief in history as a record of the advances and delays in the march of progress.The study of the history of science continued to be a small effort until the rise of Big Science after World War II. With the work of I."
What were the early histories of science written for?,To educate the public about the virtues of science,To document the advances and delays in the march of progress,To examine the changing role of science after World War I,"To communicate the cultural, economic, and political impacts of scientific practices",To explore the international community of specialists in the field,A,"The history of science and technology (HST) is a field of history that examines the understanding of the natural world (science) and the ability to manipulate it (technology) at different points in time. This academic discipline also studies the cultural, economic, and political impacts of and contexts for scientific practices.

Academic study of history of science
History of science is an academic discipline with an international community of specialists. Main professional organizations for this field include the History of Science Society, the British Society for the History of Science, and the European Society for the History of Science.
Much of the study of the history of science has been devoted to answering questions about what science is, how it functions, and whether it exhibits large-scale patterns and trends.

History of the academic study of history of science
Histories of science were originally written by practicing and retired scientists, starting primarily with William Whewell's History of the Inductive Sciences (1837), as a way to communicate the virtues of science to the public.Auguste Comte proposed that there should be a specific discipline to deal with the history of science.The development of the distinct academic discipline of the history of science and technology did not occur until the early 20th century. Historians have suggested that this was bound to the changing role of science during the same time period.After World War I, extensive resources were put into teaching and researching the discipline, with the hopes that it would help the public better understand both Science and Technology as they came to play an exceedingly prominent role in the world.In the decades since the end of World War II, history of science became an academic discipline, with graduate schools, research institutes, public and private patronage, peer-reviewed journals, and professional societies.

Formation of academic departments
In the United States, a more formal study of the history of science as an independent discipline was initiated by George Sarton's publications, Introduction to the History of Science (1927) and the journal Isis (founded in 1912). Sarton exemplified the early 20th-century view of the history of science as the history of great men and great ideas. He shared with many of his contemporaries a Whiggish belief in history as a record of the advances and delays in the march of progress.The study of the history of science continued to be a small effort until the rise of Big Science after World War II. With the work of I."
When did the distinct academic discipline of history of science and technology emerge?,Before World War I,During the early 20th century,After World War II,During the rise of Big Science,In the 19th century,B,"The history of science and technology (HST) is a field of history that examines the understanding of the natural world (science) and the ability to manipulate it (technology) at different points in time. This academic discipline also studies the cultural, economic, and political impacts of and contexts for scientific practices.

Academic study of history of science
History of science is an academic discipline with an international community of specialists. Main professional organizations for this field include the History of Science Society, the British Society for the History of Science, and the European Society for the History of Science.
Much of the study of the history of science has been devoted to answering questions about what science is, how it functions, and whether it exhibits large-scale patterns and trends.

History of the academic study of history of science
Histories of science were originally written by practicing and retired scientists, starting primarily with William Whewell's History of the Inductive Sciences (1837), as a way to communicate the virtues of science to the public.Auguste Comte proposed that there should be a specific discipline to deal with the history of science.The development of the distinct academic discipline of the history of science and technology did not occur until the early 20th century. Historians have suggested that this was bound to the changing role of science during the same time period.After World War I, extensive resources were put into teaching and researching the discipline, with the hopes that it would help the public better understand both Science and Technology as they came to play an exceedingly prominent role in the world.In the decades since the end of World War II, history of science became an academic discipline, with graduate schools, research institutes, public and private patronage, peer-reviewed journals, and professional societies.

Formation of academic departments
In the United States, a more formal study of the history of science as an independent discipline was initiated by George Sarton's publications, Introduction to the History of Science (1927) and the journal Isis (founded in 1912). Sarton exemplified the early 20th-century view of the history of science as the history of great men and great ideas. He shared with many of his contemporaries a Whiggish belief in history as a record of the advances and delays in the march of progress.The study of the history of science continued to be a small effort until the rise of Big Science after World War II. With the work of I."
Who initiated the formal study of the history of science as an independent discipline in the United States?,George Sarton,William Whewell,Auguste Comte,I. Bernard Cohen,Isis journal,A,"The history of science and technology (HST) is a field of history that examines the understanding of the natural world (science) and the ability to manipulate it (technology) at different points in time. This academic discipline also studies the cultural, economic, and political impacts of and contexts for scientific practices.

Academic study of history of science
History of science is an academic discipline with an international community of specialists. Main professional organizations for this field include the History of Science Society, the British Society for the History of Science, and the European Society for the History of Science.
Much of the study of the history of science has been devoted to answering questions about what science is, how it functions, and whether it exhibits large-scale patterns and trends.

History of the academic study of history of science
Histories of science were originally written by practicing and retired scientists, starting primarily with William Whewell's History of the Inductive Sciences (1837), as a way to communicate the virtues of science to the public.Auguste Comte proposed that there should be a specific discipline to deal with the history of science.The development of the distinct academic discipline of the history of science and technology did not occur until the early 20th century. Historians have suggested that this was bound to the changing role of science during the same time period.After World War I, extensive resources were put into teaching and researching the discipline, with the hopes that it would help the public better understand both Science and Technology as they came to play an exceedingly prominent role in the world.In the decades since the end of World War II, history of science became an academic discipline, with graduate schools, research institutes, public and private patronage, peer-reviewed journals, and professional societies.

Formation of academic departments
In the United States, a more formal study of the history of science as an independent discipline was initiated by George Sarton's publications, Introduction to the History of Science (1927) and the journal Isis (founded in 1912). Sarton exemplified the early 20th-century view of the history of science as the history of great men and great ideas. He shared with many of his contemporaries a Whiggish belief in history as a record of the advances and delays in the march of progress.The study of the history of science continued to be a small effort until the rise of Big Science after World War II. With the work of I."
What was the early 20th-century view of the history of science?,The history of great men and great ideas,The cultural impacts of scientific practices,The economic impacts of scientific practices,The political impacts of scientific practices,The nature and function of science,A,"The history of science and technology (HST) is a field of history that examines the understanding of the natural world (science) and the ability to manipulate it (technology) at different points in time. This academic discipline also studies the cultural, economic, and political impacts of and contexts for scientific practices.

Academic study of history of science
History of science is an academic discipline with an international community of specialists. Main professional organizations for this field include the History of Science Society, the British Society for the History of Science, and the European Society for the History of Science.
Much of the study of the history of science has been devoted to answering questions about what science is, how it functions, and whether it exhibits large-scale patterns and trends.

History of the academic study of history of science
Histories of science were originally written by practicing and retired scientists, starting primarily with William Whewell's History of the Inductive Sciences (1837), as a way to communicate the virtues of science to the public.Auguste Comte proposed that there should be a specific discipline to deal with the history of science.The development of the distinct academic discipline of the history of science and technology did not occur until the early 20th century. Historians have suggested that this was bound to the changing role of science during the same time period.After World War I, extensive resources were put into teaching and researching the discipline, with the hopes that it would help the public better understand both Science and Technology as they came to play an exceedingly prominent role in the world.In the decades since the end of World War II, history of science became an academic discipline, with graduate schools, research institutes, public and private patronage, peer-reviewed journals, and professional societies.

Formation of academic departments
In the United States, a more formal study of the history of science as an independent discipline was initiated by George Sarton's publications, Introduction to the History of Science (1927) and the journal Isis (founded in 1912). Sarton exemplified the early 20th-century view of the history of science as the history of great men and great ideas. He shared with many of his contemporaries a Whiggish belief in history as a record of the advances and delays in the march of progress.The study of the history of science continued to be a small effort until the rise of Big Science after World War II. With the work of I."
What is the Particle Physics Project Prioritization Panel (P5)?,A permanent subcommittee of the High Energy Physics Advisory Panel (HEPAP),A scientific advisory panel tasked with recommending plans for U.S. investment in particle physics research,A temporary panel that evaluates research priorities in the field globally,A panel that chairs the Department of Energy's Office of Science and the National Science Foundation,A subcommittee that convenes a year-long discussion within the particle physics community,B,"The Particle Physics Project Prioritization Panel (P5) is a scientific advisory panel tasked with recommending plans for U.S. investment in particle physics research over the next ten years, on the basis of various funding scenarios. The P5 is a temporary subcommittee of the High Energy Physics Advisory Panel (HEPAP), which serves the Department of Energy's Office of Science and the National Science Foundation. In 2014 the panel was chaired by Steven Ritz of the University of California, Santa Cruz. In 2023, the panel will be chaired by Hitoshi Murayama of the University of California, Berkeley.

2014 report
In 2013, HEPAP was asked to convene a panel (the P5) to evaluate research priorities in the context of anticipated developments in the field globally in the next 20 years. Recommendations were to be made on the basis of three funding scenarios for high-energy physics:
A constant funding level for the next three years followed by an annual 2% increase, relative to the FY2013 budget
A constant funding level for the next three years followed by an annual 3% increase, relative to the proposed FY2014 budget
An unconstrained budget

Science drivers
In May 2014, the first P5 report since 2008 was released. The 2014 report identified five ""science drivers""—goals intended to inform funding priorities—drawn from a year-long discussion within the particle physics community."
Who chaired the P5 panel in 2014?,"Steven Ritz of the University of California, Santa Cruz","Hitoshi Murayama of the University of California, Berkeley",The Department of Energy's Office of Science,The National Science Foundation,The High Energy Physics Advisory Panel (HEPAP),A,"The Particle Physics Project Prioritization Panel (P5) is a scientific advisory panel tasked with recommending plans for U.S. investment in particle physics research over the next ten years, on the basis of various funding scenarios. The P5 is a temporary subcommittee of the High Energy Physics Advisory Panel (HEPAP), which serves the Department of Energy's Office of Science and the National Science Foundation. In 2014 the panel was chaired by Steven Ritz of the University of California, Santa Cruz. In 2023, the panel will be chaired by Hitoshi Murayama of the University of California, Berkeley.

2014 report
In 2013, HEPAP was asked to convene a panel (the P5) to evaluate research priorities in the context of anticipated developments in the field globally in the next 20 years. Recommendations were to be made on the basis of three funding scenarios for high-energy physics:
A constant funding level for the next three years followed by an annual 2% increase, relative to the FY2013 budget
A constant funding level for the next three years followed by an annual 3% increase, relative to the proposed FY2014 budget
An unconstrained budget

Science drivers
In May 2014, the first P5 report since 2008 was released. The 2014 report identified five ""science drivers""—goals intended to inform funding priorities—drawn from a year-long discussion within the particle physics community."
What were the funding scenarios considered by the P5 panel?,A constant funding level followed by an annual 2% increase,A constant funding level followed by an annual 3% increase,An unconstrained budget,Both A and B,All of the above,E,"The Particle Physics Project Prioritization Panel (P5) is a scientific advisory panel tasked with recommending plans for U.S. investment in particle physics research over the next ten years, on the basis of various funding scenarios. The P5 is a temporary subcommittee of the High Energy Physics Advisory Panel (HEPAP), which serves the Department of Energy's Office of Science and the National Science Foundation. In 2014 the panel was chaired by Steven Ritz of the University of California, Santa Cruz. In 2023, the panel will be chaired by Hitoshi Murayama of the University of California, Berkeley.

2014 report
In 2013, HEPAP was asked to convene a panel (the P5) to evaluate research priorities in the context of anticipated developments in the field globally in the next 20 years. Recommendations were to be made on the basis of three funding scenarios for high-energy physics:
A constant funding level for the next three years followed by an annual 2% increase, relative to the FY2013 budget
A constant funding level for the next three years followed by an annual 3% increase, relative to the proposed FY2014 budget
An unconstrained budget

Science drivers
In May 2014, the first P5 report since 2008 was released. The 2014 report identified five ""science drivers""—goals intended to inform funding priorities—drawn from a year-long discussion within the particle physics community."
"What are the ""science drivers"" identified in the 2014 P5 report?",Goals intended to inform funding priorities,Recommendations for research priorities in the next 20 years,Anticipated developments in the field globally,A year-long discussion within the particle physics community,Funding scenarios for high-energy physics,A,"The Particle Physics Project Prioritization Panel (P5) is a scientific advisory panel tasked with recommending plans for U.S. investment in particle physics research over the next ten years, on the basis of various funding scenarios. The P5 is a temporary subcommittee of the High Energy Physics Advisory Panel (HEPAP), which serves the Department of Energy's Office of Science and the National Science Foundation. In 2014 the panel was chaired by Steven Ritz of the University of California, Santa Cruz. In 2023, the panel will be chaired by Hitoshi Murayama of the University of California, Berkeley.

2014 report
In 2013, HEPAP was asked to convene a panel (the P5) to evaluate research priorities in the context of anticipated developments in the field globally in the next 20 years. Recommendations were to be made on the basis of three funding scenarios for high-energy physics:
A constant funding level for the next three years followed by an annual 2% increase, relative to the FY2013 budget
A constant funding level for the next three years followed by an annual 3% increase, relative to the proposed FY2014 budget
An unconstrained budget

Science drivers
In May 2014, the first P5 report since 2008 was released. The 2014 report identified five ""science drivers""—goals intended to inform funding priorities—drawn from a year-long discussion within the particle physics community."
When will Hitoshi Murayama chair the P5 panel?,In 2014,In 2013,In 2023,In 2008,In 2018,C,"The Particle Physics Project Prioritization Panel (P5) is a scientific advisory panel tasked with recommending plans for U.S. investment in particle physics research over the next ten years, on the basis of various funding scenarios. The P5 is a temporary subcommittee of the High Energy Physics Advisory Panel (HEPAP), which serves the Department of Energy's Office of Science and the National Science Foundation. In 2014 the panel was chaired by Steven Ritz of the University of California, Santa Cruz. In 2023, the panel will be chaired by Hitoshi Murayama of the University of California, Berkeley.

2014 report
In 2013, HEPAP was asked to convene a panel (the P5) to evaluate research priorities in the context of anticipated developments in the field globally in the next 20 years. Recommendations were to be made on the basis of three funding scenarios for high-energy physics:
A constant funding level for the next three years followed by an annual 2% increase, relative to the FY2013 budget
A constant funding level for the next three years followed by an annual 3% increase, relative to the proposed FY2014 budget
An unconstrained budget

Science drivers
In May 2014, the first P5 report since 2008 was released. The 2014 report identified five ""science drivers""—goals intended to inform funding priorities—drawn from a year-long discussion within the particle physics community."
What is the definition of military engineering?,"The art, science, and practice of designing and building military works and maintaining lines of military transport and military communications","The art, science, and practice of designing and building civilian infrastructure","The art, science, and practice of designing and building bridges and roads","The art, science, and practice of designing and building military vehicles","The art, science, and practice of designing and building computer software",A,"Military engineering is loosely defined as the art, science, and practice of designing and building military works and maintaining lines of military transport and military communications. Military engineers are also responsible for logistics behind military tactics. Modern military engineering differs from civil engineering. In the 20th and 21st centuries, military engineering also includes CBRN defense and other engineering disciplines such as mechanical and electrical engineering techniques.According to NATO, ""military engineering is that engineer activity undertaken, regardless of component or service, to shape the physical operating environment. Military engineering incorporates support to maneuver and to the force as a whole, including military engineering functions such as engineer support to force protection, counter-improvised explosive devices, environmental protection, engineer intelligence and military search. Military engineering does not encompass the activities undertaken by those 'engineers' who maintain, repair and operate vehicles, vessels, aircraft, weapon systems and equipment.""Military engineering is an academic subject taught in military academies or schools of military engineering. The construction and demolition tasks related to military engineering are usually performed by military engineers including soldiers trained as sappers or pioneers."
What are military engineers responsible for?,Designing and building civilian infrastructure,Designing and building military vehicles,Maintaining lines of military transport and military communications,"Operating vehicles, vessels, aircraft, weapon systems, and equipment",Developing computer software,C,"Military engineering is loosely defined as the art, science, and practice of designing and building military works and maintaining lines of military transport and military communications. Military engineers are also responsible for logistics behind military tactics. Modern military engineering differs from civil engineering. In the 20th and 21st centuries, military engineering also includes CBRN defense and other engineering disciplines such as mechanical and electrical engineering techniques.According to NATO, ""military engineering is that engineer activity undertaken, regardless of component or service, to shape the physical operating environment. Military engineering incorporates support to maneuver and to the force as a whole, including military engineering functions such as engineer support to force protection, counter-improvised explosive devices, environmental protection, engineer intelligence and military search. Military engineering does not encompass the activities undertaken by those 'engineers' who maintain, repair and operate vehicles, vessels, aircraft, weapon systems and equipment.""Military engineering is an academic subject taught in military academies or schools of military engineering. The construction and demolition tasks related to military engineering are usually performed by military engineers including soldiers trained as sappers or pioneers."
How does modern military engineering differ from civil engineering?,Modern military engineering focuses on designing and building civilian infrastructure,Modern military engineering does not include CBRN defense,Modern military engineering does not include mechanical and electrical engineering techniques,Modern military engineering focuses on designing and building military works and maintaining lines of military transport and military communications,Modern military engineering focuses on developing computer software,D,"Military engineering is loosely defined as the art, science, and practice of designing and building military works and maintaining lines of military transport and military communications. Military engineers are also responsible for logistics behind military tactics. Modern military engineering differs from civil engineering. In the 20th and 21st centuries, military engineering also includes CBRN defense and other engineering disciplines such as mechanical and electrical engineering techniques.According to NATO, ""military engineering is that engineer activity undertaken, regardless of component or service, to shape the physical operating environment. Military engineering incorporates support to maneuver and to the force as a whole, including military engineering functions such as engineer support to force protection, counter-improvised explosive devices, environmental protection, engineer intelligence and military search. Military engineering does not encompass the activities undertaken by those 'engineers' who maintain, repair and operate vehicles, vessels, aircraft, weapon systems and equipment.""Military engineering is an academic subject taught in military academies or schools of military engineering. The construction and demolition tasks related to military engineering are usually performed by military engineers including soldiers trained as sappers or pioneers."
What disciplines are included in military engineering in the 20th and 21st centuries?,Civil engineering and mechanical engineering,"Chemical, biological, radiological, and nuclear defense",Computer science and software engineering,Environmental engineering and intelligence analysis,Aerospace engineering and marine engineering,B,"Military engineering is loosely defined as the art, science, and practice of designing and building military works and maintaining lines of military transport and military communications. Military engineers are also responsible for logistics behind military tactics. Modern military engineering differs from civil engineering. In the 20th and 21st centuries, military engineering also includes CBRN defense and other engineering disciplines such as mechanical and electrical engineering techniques.According to NATO, ""military engineering is that engineer activity undertaken, regardless of component or service, to shape the physical operating environment. Military engineering incorporates support to maneuver and to the force as a whole, including military engineering functions such as engineer support to force protection, counter-improvised explosive devices, environmental protection, engineer intelligence and military search. Military engineering does not encompass the activities undertaken by those 'engineers' who maintain, repair and operate vehicles, vessels, aircraft, weapon systems and equipment.""Military engineering is an academic subject taught in military academies or schools of military engineering. The construction and demolition tasks related to military engineering are usually performed by military engineers including soldiers trained as sappers or pioneers."
Who usually performs construction and demolition tasks related to military engineering?,Civilian contractors,Soldiers trained as sappers or pioneers,Military engineers specialized in mechanical and electrical engineering,Aircraft pilots and naval officers,Computer programmers and software engineers,B,"Military engineering is loosely defined as the art, science, and practice of designing and building military works and maintaining lines of military transport and military communications. Military engineers are also responsible for logistics behind military tactics. Modern military engineering differs from civil engineering. In the 20th and 21st centuries, military engineering also includes CBRN defense and other engineering disciplines such as mechanical and electrical engineering techniques.According to NATO, ""military engineering is that engineer activity undertaken, regardless of component or service, to shape the physical operating environment. Military engineering incorporates support to maneuver and to the force as a whole, including military engineering functions such as engineer support to force protection, counter-improvised explosive devices, environmental protection, engineer intelligence and military search. Military engineering does not encompass the activities undertaken by those 'engineers' who maintain, repair and operate vehicles, vessels, aircraft, weapon systems and equipment.""Military engineering is an academic subject taught in military academies or schools of military engineering. The construction and demolition tasks related to military engineering are usually performed by military engineers including soldiers trained as sappers or pioneers."
What is image analysis?,Extracting meaningful information from images,Reading bar coded tags,Identifying a person from their face,All of the above,None of the above,D,"Image analysis or imagery analysis is the extraction of meaningful information from images; mainly from digital images by means of digital image processing techniques. Image analysis tasks can be as simple as reading bar coded tags or as sophisticated as identifying a person from their face.
Computers are indispensable for the analysis of large amounts of data, for tasks that require complex computation, or for the extraction of quantitative information.  On the other hand, the human visual cortex is an excellent image analysis apparatus, especially for extracting higher-level information, and for many applications — including medicine, security, and remote sensing — human analysts still cannot be replaced by computers.  For this reason, many important image analysis tools such as edge detectors and neural networks are inspired by human visual perception models.

Digital
Digital Image Analysis or Computer Image Analysis is when a computer or electrical device automatically studies an image to obtain useful information from it. Note that the device is often a computer but may also be an electrical circuit, a digital camera or a mobile phone. 
It involves the fields of computer or machine vision, and medical imaging, and makes heavy use of pattern recognition, digital geometry, and signal processing.  This field of computer science developed in the 1950s at academic institutions such as the MIT A.I."
What role do computers play in image analysis?,Computers are indispensable and can replace human analysts,Computers are not necessary for image analysis,Computers are used for tasks that require complex computation,Computers are only used for reading bar coded tags,None of the above,C,"Image analysis or imagery analysis is the extraction of meaningful information from images; mainly from digital images by means of digital image processing techniques. Image analysis tasks can be as simple as reading bar coded tags or as sophisticated as identifying a person from their face.
Computers are indispensable for the analysis of large amounts of data, for tasks that require complex computation, or for the extraction of quantitative information.  On the other hand, the human visual cortex is an excellent image analysis apparatus, especially for extracting higher-level information, and for many applications — including medicine, security, and remote sensing — human analysts still cannot be replaced by computers.  For this reason, many important image analysis tools such as edge detectors and neural networks are inspired by human visual perception models.

Digital
Digital Image Analysis or Computer Image Analysis is when a computer or electrical device automatically studies an image to obtain useful information from it. Note that the device is often a computer but may also be an electrical circuit, a digital camera or a mobile phone. 
It involves the fields of computer or machine vision, and medical imaging, and makes heavy use of pattern recognition, digital geometry, and signal processing.  This field of computer science developed in the 1950s at academic institutions such as the MIT A.I."
Which fields are involved in digital image analysis?,Computer vision and medical imaging,Pattern recognition and signal processing,Digital geometry,All of the above,None of the above,D,"Image analysis or imagery analysis is the extraction of meaningful information from images; mainly from digital images by means of digital image processing techniques. Image analysis tasks can be as simple as reading bar coded tags or as sophisticated as identifying a person from their face.
Computers are indispensable for the analysis of large amounts of data, for tasks that require complex computation, or for the extraction of quantitative information.  On the other hand, the human visual cortex is an excellent image analysis apparatus, especially for extracting higher-level information, and for many applications — including medicine, security, and remote sensing — human analysts still cannot be replaced by computers.  For this reason, many important image analysis tools such as edge detectors and neural networks are inspired by human visual perception models.

Digital
Digital Image Analysis or Computer Image Analysis is when a computer or electrical device automatically studies an image to obtain useful information from it. Note that the device is often a computer but may also be an electrical circuit, a digital camera or a mobile phone. 
It involves the fields of computer or machine vision, and medical imaging, and makes heavy use of pattern recognition, digital geometry, and signal processing.  This field of computer science developed in the 1950s at academic institutions such as the MIT A.I."
When did computer image analysis develop?,1950s,1980s,2000s,1970s,None of the above,A,"Image analysis or imagery analysis is the extraction of meaningful information from images; mainly from digital images by means of digital image processing techniques. Image analysis tasks can be as simple as reading bar coded tags or as sophisticated as identifying a person from their face.
Computers are indispensable for the analysis of large amounts of data, for tasks that require complex computation, or for the extraction of quantitative information.  On the other hand, the human visual cortex is an excellent image analysis apparatus, especially for extracting higher-level information, and for many applications — including medicine, security, and remote sensing — human analysts still cannot be replaced by computers.  For this reason, many important image analysis tools such as edge detectors and neural networks are inspired by human visual perception models.

Digital
Digital Image Analysis or Computer Image Analysis is when a computer or electrical device automatically studies an image to obtain useful information from it. Note that the device is often a computer but may also be an electrical circuit, a digital camera or a mobile phone. 
It involves the fields of computer or machine vision, and medical imaging, and makes heavy use of pattern recognition, digital geometry, and signal processing.  This field of computer science developed in the 1950s at academic institutions such as the MIT A.I."
What devices can be used for digital image analysis?,Computers only,Electrical circuits only,Digital cameras only,Mobile phones only,Any computer or electrical device,E,"Image analysis or imagery analysis is the extraction of meaningful information from images; mainly from digital images by means of digital image processing techniques. Image analysis tasks can be as simple as reading bar coded tags or as sophisticated as identifying a person from their face.
Computers are indispensable for the analysis of large amounts of data, for tasks that require complex computation, or for the extraction of quantitative information.  On the other hand, the human visual cortex is an excellent image analysis apparatus, especially for extracting higher-level information, and for many applications — including medicine, security, and remote sensing — human analysts still cannot be replaced by computers.  For this reason, many important image analysis tools such as edge detectors and neural networks are inspired by human visual perception models.

Digital
Digital Image Analysis or Computer Image Analysis is when a computer or electrical device automatically studies an image to obtain useful information from it. Note that the device is often a computer but may also be an electrical circuit, a digital camera or a mobile phone. 
It involves the fields of computer or machine vision, and medical imaging, and makes heavy use of pattern recognition, digital geometry, and signal processing.  This field of computer science developed in the 1950s at academic institutions such as the MIT A.I."
What is systems engineering?,An interdisciplinary field of engineering that focuses on designing complex systems,A manufacturing process that achieves high-quality outputs with minimum cost and time,A process that integrates various disciplines for successful system design,A process that only deals with technical disciplines like industrial engineering,A process that focuses on components working synergistically,A,"Systems engineering is an interdisciplinary field of engineering and engineering management that focuses on how to design, integrate, and manage complex systems over their life cycles. At its core, systems engineering utilizes systems thinking principles to organize this body of knowledge. The individual outcome of such efforts, an engineered system, can be defined as a combination of components that work in synergy to collectively perform a useful function.
Issues such as requirements engineering, reliability, logistics, coordination of different teams, testing and evaluation, maintainability and many other disciplines necessary for successful system design, development, implementation, and ultimate decommission become more difficult when dealing with large or complex projects. Systems engineering deals with work-processes, optimization methods, and risk management tools in such projects. It overlaps technical and human-centered disciplines such as industrial engineering, production systems engineering, process systems engineering, mechanical engineering, manufacturing engineering, production engineering, control engineering, software engineering, electrical engineering, cybernetics, aerospace engineering, organizational studies, civil engineering and project management. Systems engineering ensures that all likely aspects of a project or system are considered and integrated into a whole.
The systems engineering process is a discovery process that is quite unlike a manufacturing process. A manufacturing process is focused on repetitive activities that achieve high-quality outputs with minimum cost and time."
What are some of the disciplines that systems engineering overlaps with?,Industrial engineering and production systems engineering,Mechanical engineering and manufacturing engineering,Control engineering and software engineering,Electrical engineering and cybernetics,All of the above,E,"Systems engineering is an interdisciplinary field of engineering and engineering management that focuses on how to design, integrate, and manage complex systems over their life cycles. At its core, systems engineering utilizes systems thinking principles to organize this body of knowledge. The individual outcome of such efforts, an engineered system, can be defined as a combination of components that work in synergy to collectively perform a useful function.
Issues such as requirements engineering, reliability, logistics, coordination of different teams, testing and evaluation, maintainability and many other disciplines necessary for successful system design, development, implementation, and ultimate decommission become more difficult when dealing with large or complex projects. Systems engineering deals with work-processes, optimization methods, and risk management tools in such projects. It overlaps technical and human-centered disciplines such as industrial engineering, production systems engineering, process systems engineering, mechanical engineering, manufacturing engineering, production engineering, control engineering, software engineering, electrical engineering, cybernetics, aerospace engineering, organizational studies, civil engineering and project management. Systems engineering ensures that all likely aspects of a project or system are considered and integrated into a whole.
The systems engineering process is a discovery process that is quite unlike a manufacturing process. A manufacturing process is focused on repetitive activities that achieve high-quality outputs with minimum cost and time."
What are some of the challenges faced in systems engineering?,Requirements engineering and reliability,Coordination of different teams and testing and evaluation,Maintainability and risk management,All of the above,None of the above,D,"Systems engineering is an interdisciplinary field of engineering and engineering management that focuses on how to design, integrate, and manage complex systems over their life cycles. At its core, systems engineering utilizes systems thinking principles to organize this body of knowledge. The individual outcome of such efforts, an engineered system, can be defined as a combination of components that work in synergy to collectively perform a useful function.
Issues such as requirements engineering, reliability, logistics, coordination of different teams, testing and evaluation, maintainability and many other disciplines necessary for successful system design, development, implementation, and ultimate decommission become more difficult when dealing with large or complex projects. Systems engineering deals with work-processes, optimization methods, and risk management tools in such projects. It overlaps technical and human-centered disciplines such as industrial engineering, production systems engineering, process systems engineering, mechanical engineering, manufacturing engineering, production engineering, control engineering, software engineering, electrical engineering, cybernetics, aerospace engineering, organizational studies, civil engineering and project management. Systems engineering ensures that all likely aspects of a project or system are considered and integrated into a whole.
The systems engineering process is a discovery process that is quite unlike a manufacturing process. A manufacturing process is focused on repetitive activities that achieve high-quality outputs with minimum cost and time."
How is the systems engineering process different from a manufacturing process?,Systems engineering focuses on repetitive activities,Manufacturing process aims for high-quality outputs with minimum cost and time,Systems engineering integrates various disciplines,Manufacturing process is a discovery process,None of the above,B,"Systems engineering is an interdisciplinary field of engineering and engineering management that focuses on how to design, integrate, and manage complex systems over their life cycles. At its core, systems engineering utilizes systems thinking principles to organize this body of knowledge. The individual outcome of such efforts, an engineered system, can be defined as a combination of components that work in synergy to collectively perform a useful function.
Issues such as requirements engineering, reliability, logistics, coordination of different teams, testing and evaluation, maintainability and many other disciplines necessary for successful system design, development, implementation, and ultimate decommission become more difficult when dealing with large or complex projects. Systems engineering deals with work-processes, optimization methods, and risk management tools in such projects. It overlaps technical and human-centered disciplines such as industrial engineering, production systems engineering, process systems engineering, mechanical engineering, manufacturing engineering, production engineering, control engineering, software engineering, electrical engineering, cybernetics, aerospace engineering, organizational studies, civil engineering and project management. Systems engineering ensures that all likely aspects of a project or system are considered and integrated into a whole.
The systems engineering process is a discovery process that is quite unlike a manufacturing process. A manufacturing process is focused on repetitive activities that achieve high-quality outputs with minimum cost and time."
What does an engineered system consist of?,Components that work independently,Components that work synergistically,Components that perform a single function,Components that are optimized for minimum cost,None of the above,B,"Systems engineering is an interdisciplinary field of engineering and engineering management that focuses on how to design, integrate, and manage complex systems over their life cycles. At its core, systems engineering utilizes systems thinking principles to organize this body of knowledge. The individual outcome of such efforts, an engineered system, can be defined as a combination of components that work in synergy to collectively perform a useful function.
Issues such as requirements engineering, reliability, logistics, coordination of different teams, testing and evaluation, maintainability and many other disciplines necessary for successful system design, development, implementation, and ultimate decommission become more difficult when dealing with large or complex projects. Systems engineering deals with work-processes, optimization methods, and risk management tools in such projects. It overlaps technical and human-centered disciplines such as industrial engineering, production systems engineering, process systems engineering, mechanical engineering, manufacturing engineering, production engineering, control engineering, software engineering, electrical engineering, cybernetics, aerospace engineering, organizational studies, civil engineering and project management. Systems engineering ensures that all likely aspects of a project or system are considered and integrated into a whole.
The systems engineering process is a discovery process that is quite unlike a manufacturing process. A manufacturing process is focused on repetitive activities that achieve high-quality outputs with minimum cost and time."
What is the purpose of Marston Mat (pierced steel planking)?,To build temporary runways and landing strips,To manufacture U-shaped channels,To create steel strips with lightening holes,To develop steel clips for interlocking,To connect adjacent mats with hooks,A,"Marston Mat, more properly called pierced (or perforated) steel planking (PSP), is standardized, perforated steel matting material developed by the United States at the Waterways Experiment Station shortly before World War II, primarily for the rapid construction of temporary runways and landing strips (also misspelled as Marsden matting). The nickname came from Marston, North Carolina, adjacent to Camp Mackall airfield where the material was first used.

Description
Pierced (pressed, steel planking, named after the manufacturing process) steel planking consisted of steel strips with punched lightening holes in it. These holes were in rows, and a formation of U-shaped channels between the holes. Hooks were formed along one long edge and slots along the other long edge so that adjacent mats could be connected. The short edges were cut straight with no holes or hooks. To achieve lengthwise interlocking, the mats were laid in a staggered pattern.
The hooks were usually held in the slots by a steel clip that filled the part of the slot that is empty when the adjacent sheets are properly engaged. The holes were bent up at their edges so that the beveled edge stiffened the area around the hole."
Where was Marston Mat first used?,Camp Mackall airfield,Waterways Experiment Station,"Marston, North Carolina",World War II,Pierced steel planking,A,"Marston Mat, more properly called pierced (or perforated) steel planking (PSP), is standardized, perforated steel matting material developed by the United States at the Waterways Experiment Station shortly before World War II, primarily for the rapid construction of temporary runways and landing strips (also misspelled as Marsden matting). The nickname came from Marston, North Carolina, adjacent to Camp Mackall airfield where the material was first used.

Description
Pierced (pressed, steel planking, named after the manufacturing process) steel planking consisted of steel strips with punched lightening holes in it. These holes were in rows, and a formation of U-shaped channels between the holes. Hooks were formed along one long edge and slots along the other long edge so that adjacent mats could be connected. The short edges were cut straight with no holes or hooks. To achieve lengthwise interlocking, the mats were laid in a staggered pattern.
The hooks were usually held in the slots by a steel clip that filled the part of the slot that is empty when the adjacent sheets are properly engaged. The holes were bent up at their edges so that the beveled edge stiffened the area around the hole."
What is the manufacturing process for pierced steel planking?,Punching holes in steel strips,Forming hooks along one long edge,Cutting short edges straight,Bending holes up at their edges,Laying mats in a staggered pattern,A,"Marston Mat, more properly called pierced (or perforated) steel planking (PSP), is standardized, perforated steel matting material developed by the United States at the Waterways Experiment Station shortly before World War II, primarily for the rapid construction of temporary runways and landing strips (also misspelled as Marsden matting). The nickname came from Marston, North Carolina, adjacent to Camp Mackall airfield where the material was first used.

Description
Pierced (pressed, steel planking, named after the manufacturing process) steel planking consisted of steel strips with punched lightening holes in it. These holes were in rows, and a formation of U-shaped channels between the holes. Hooks were formed along one long edge and slots along the other long edge so that adjacent mats could be connected. The short edges were cut straight with no holes or hooks. To achieve lengthwise interlocking, the mats were laid in a staggered pattern.
The hooks were usually held in the slots by a steel clip that filled the part of the slot that is empty when the adjacent sheets are properly engaged. The holes were bent up at their edges so that the beveled edge stiffened the area around the hole."
How are adjacent mats connected using Marston Mat?,Using steel clips in the slots,Through punched lightening holes,By forming U-shaped channels,With hooks and slots,By laying the mats in a staggered pattern,D,"Marston Mat, more properly called pierced (or perforated) steel planking (PSP), is standardized, perforated steel matting material developed by the United States at the Waterways Experiment Station shortly before World War II, primarily for the rapid construction of temporary runways and landing strips (also misspelled as Marsden matting). The nickname came from Marston, North Carolina, adjacent to Camp Mackall airfield where the material was first used.

Description
Pierced (pressed, steel planking, named after the manufacturing process) steel planking consisted of steel strips with punched lightening holes in it. These holes were in rows, and a formation of U-shaped channels between the holes. Hooks were formed along one long edge and slots along the other long edge so that adjacent mats could be connected. The short edges were cut straight with no holes or hooks. To achieve lengthwise interlocking, the mats were laid in a staggered pattern.
The hooks were usually held in the slots by a steel clip that filled the part of the slot that is empty when the adjacent sheets are properly engaged. The holes were bent up at their edges so that the beveled edge stiffened the area around the hole."
What is the function of the beveled edge around the holes in Marston Mat?,To stiffen the area around the hole,To connect adjacent mats,To hold the hooks in the slots,To achieve lengthwise interlocking,To fill the empty part of the slot,A,"Marston Mat, more properly called pierced (or perforated) steel planking (PSP), is standardized, perforated steel matting material developed by the United States at the Waterways Experiment Station shortly before World War II, primarily for the rapid construction of temporary runways and landing strips (also misspelled as Marsden matting). The nickname came from Marston, North Carolina, adjacent to Camp Mackall airfield where the material was first used.

Description
Pierced (pressed, steel planking, named after the manufacturing process) steel planking consisted of steel strips with punched lightening holes in it. These holes were in rows, and a formation of U-shaped channels between the holes. Hooks were formed along one long edge and slots along the other long edge so that adjacent mats could be connected. The short edges were cut straight with no holes or hooks. To achieve lengthwise interlocking, the mats were laid in a staggered pattern.
The hooks were usually held in the slots by a steel clip that filled the part of the slot that is empty when the adjacent sheets are properly engaged. The holes were bent up at their edges so that the beveled edge stiffened the area around the hole."
What is Macaulay's method used for?,Determining the deflection of Euler-Bernoulli beams,Calculating the stress in structural components,Analyzing the thermal properties of materials,Solving differential equations,Optimizing the design of mechanical devices,A,"This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.

M
Macaulay's method(The double integration method) is a technique used in structural analysis to determine the deflection of Euler-Bernoulli beams. Use of Macaulay's technique is very convenient for cases of discontinuous and/or discrete loading. Typically partial uniformly distributed loads (u.d.l.) and uniformly varying loads (u.v.l.) over the span and a number of concentrated loads are conveniently handled using this technique.
Mach numberThe ratio of the speed of an object to the speed of sound. 
MachineA machine (or mechanical device) is a mechanical structure that uses power to apply forces and control movement to perform an intended action. Machines can be driven by animals and people, by natural forces such as wind and water, and by chemical, thermal, or electrical power, and include a system of mechanisms that shape the actuator input to achieve a specific application of output forces and movement. They can also include computers and sensors that monitor performance and plan movement, often called mechanical systems.
Machine codeIn computer programming, machine code, consisting of machine language instructions, is a low-level programming language used to directly control a computer's central processing unit (CPU)."
What is the Mach number?,The speed of sound,The speed of light,The ratio of an object's speed to the speed of sound,The ratio of an object's speed to the speed of light,The speed at which an object travels in a vacuum,C,"This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.

M
Macaulay's method(The double integration method) is a technique used in structural analysis to determine the deflection of Euler-Bernoulli beams. Use of Macaulay's technique is very convenient for cases of discontinuous and/or discrete loading. Typically partial uniformly distributed loads (u.d.l.) and uniformly varying loads (u.v.l.) over the span and a number of concentrated loads are conveniently handled using this technique.
Mach numberThe ratio of the speed of an object to the speed of sound. 
MachineA machine (or mechanical device) is a mechanical structure that uses power to apply forces and control movement to perform an intended action. Machines can be driven by animals and people, by natural forces such as wind and water, and by chemical, thermal, or electrical power, and include a system of mechanisms that shape the actuator input to achieve a specific application of output forces and movement. They can also include computers and sensors that monitor performance and plan movement, often called mechanical systems.
Machine codeIn computer programming, machine code, consisting of machine language instructions, is a low-level programming language used to directly control a computer's central processing unit (CPU)."
What is a machine?,A mechanical structure that uses power to apply forces and control movement,A computer program that performs specific tasks,A device that converts electrical energy into mechanical energy,A system of mechanisms that shape actuator input,A device that harnesses natural forces such as wind and water,A,"This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.

M
Macaulay's method(The double integration method) is a technique used in structural analysis to determine the deflection of Euler-Bernoulli beams. Use of Macaulay's technique is very convenient for cases of discontinuous and/or discrete loading. Typically partial uniformly distributed loads (u.d.l.) and uniformly varying loads (u.v.l.) over the span and a number of concentrated loads are conveniently handled using this technique.
Mach numberThe ratio of the speed of an object to the speed of sound. 
MachineA machine (or mechanical device) is a mechanical structure that uses power to apply forces and control movement to perform an intended action. Machines can be driven by animals and people, by natural forces such as wind and water, and by chemical, thermal, or electrical power, and include a system of mechanisms that shape the actuator input to achieve a specific application of output forces and movement. They can also include computers and sensors that monitor performance and plan movement, often called mechanical systems.
Machine codeIn computer programming, machine code, consisting of machine language instructions, is a low-level programming language used to directly control a computer's central processing unit (CPU)."
What is machine code?,A high-level programming language used to develop complex software,A language used to control robots,A low-level programming language used to control a computer's CPU,A set of instructions used to program microprocessors,A code used for debugging software applications,C,"This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.

M
Macaulay's method(The double integration method) is a technique used in structural analysis to determine the deflection of Euler-Bernoulli beams. Use of Macaulay's technique is very convenient for cases of discontinuous and/or discrete loading. Typically partial uniformly distributed loads (u.d.l.) and uniformly varying loads (u.v.l.) over the span and a number of concentrated loads are conveniently handled using this technique.
Mach numberThe ratio of the speed of an object to the speed of sound. 
MachineA machine (or mechanical device) is a mechanical structure that uses power to apply forces and control movement to perform an intended action. Machines can be driven by animals and people, by natural forces such as wind and water, and by chemical, thermal, or electrical power, and include a system of mechanisms that shape the actuator input to achieve a specific application of output forces and movement. They can also include computers and sensors that monitor performance and plan movement, often called mechanical systems.
Machine codeIn computer programming, machine code, consisting of machine language instructions, is a low-level programming language used to directly control a computer's central processing unit (CPU)."
What types of loads can be conveniently handled using Macaulay's technique?,Point loads only,Uniformly distributed loads only,Discontinuous and/or discrete loads,Variable loads only,All of the above,C,"This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.

M
Macaulay's method(The double integration method) is a technique used in structural analysis to determine the deflection of Euler-Bernoulli beams. Use of Macaulay's technique is very convenient for cases of discontinuous and/or discrete loading. Typically partial uniformly distributed loads (u.d.l.) and uniformly varying loads (u.v.l.) over the span and a number of concentrated loads are conveniently handled using this technique.
Mach numberThe ratio of the speed of an object to the speed of sound. 
MachineA machine (or mechanical device) is a mechanical structure that uses power to apply forces and control movement to perform an intended action. Machines can be driven by animals and people, by natural forces such as wind and water, and by chemical, thermal, or electrical power, and include a system of mechanisms that shape the actuator input to achieve a specific application of output forces and movement. They can also include computers and sensors that monitor performance and plan movement, often called mechanical systems.
Machine codeIn computer programming, machine code, consisting of machine language instructions, is a low-level programming language used to directly control a computer's central processing unit (CPU)."
What is the Borugak Jagyeongnu?,A musical instrument,A scientific instrument,A weapon,A painting,A cooking utensil,B,"The Borugak Jagyeongnu (""Water Clock of Borugak Pavilion""), classified as a scientific instrument, is the 229th National Treasure of South Korea and was designated by the South Korean government on March 3, 1985.  The water clock is currently held and managed by the National Palace Museum of Korea in Seoul.  It dates to the time of King SeoJong of the Joseon Dynasty.
Water clocks have a long history of use in Korea with the first mention of one in the records of the Samguk Sagi during the Three Kingdoms era.  In 1434, during the reign of King Sejong the Great a water clock was made by Jang Yeong-sil which would mark the hour automatically with the sounds of a bell, gong, and drum.  That clock, Jagyeokru (""self-striking water clock""), was used to keep the standard of time in the Joseon Dynasty. The clock was not preserved well and so in 1536, King Jungjong ordered the clock remade and improved which was done by Yu Jeon.  Today, only three water bowls and two cylindrical water containers survive from the 1434 version of the clock, and no records mentioning its existence."
When was the Borugak Jagyeongnu designated as a National Treasure?,"March 3, 1985",1434,1536,During the Three Kingdoms era,During the reign of King Sejong the Great,A,"The Borugak Jagyeongnu (""Water Clock of Borugak Pavilion""), classified as a scientific instrument, is the 229th National Treasure of South Korea and was designated by the South Korean government on March 3, 1985.  The water clock is currently held and managed by the National Palace Museum of Korea in Seoul.  It dates to the time of King SeoJong of the Joseon Dynasty.
Water clocks have a long history of use in Korea with the first mention of one in the records of the Samguk Sagi during the Three Kingdoms era.  In 1434, during the reign of King Sejong the Great a water clock was made by Jang Yeong-sil which would mark the hour automatically with the sounds of a bell, gong, and drum.  That clock, Jagyeokru (""self-striking water clock""), was used to keep the standard of time in the Joseon Dynasty. The clock was not preserved well and so in 1536, King Jungjong ordered the clock remade and improved which was done by Yu Jeon.  Today, only three water bowls and two cylindrical water containers survive from the 1434 version of the clock, and no records mentioning its existence."
Where is the Borugak Jagyeongnu currently held and managed?,National Palace Museum of Korea in Seoul,National Museum of Modern and Contemporary Art in Seoul,National Museum of Korea in Seoul,National Folk Museum of Korea in Seoul,National Museum of Korean Contemporary History in Seoul,A,"The Borugak Jagyeongnu (""Water Clock of Borugak Pavilion""), classified as a scientific instrument, is the 229th National Treasure of South Korea and was designated by the South Korean government on March 3, 1985.  The water clock is currently held and managed by the National Palace Museum of Korea in Seoul.  It dates to the time of King SeoJong of the Joseon Dynasty.
Water clocks have a long history of use in Korea with the first mention of one in the records of the Samguk Sagi during the Three Kingdoms era.  In 1434, during the reign of King Sejong the Great a water clock was made by Jang Yeong-sil which would mark the hour automatically with the sounds of a bell, gong, and drum.  That clock, Jagyeokru (""self-striking water clock""), was used to keep the standard of time in the Joseon Dynasty. The clock was not preserved well and so in 1536, King Jungjong ordered the clock remade and improved which was done by Yu Jeon.  Today, only three water bowls and two cylindrical water containers survive from the 1434 version of the clock, and no records mentioning its existence."
"Who made the water clock that marked the hour automatically with the sounds of a bell, gong, and drum?",King Sejong the Great,Jang Yeong-sil,King Jungjong,Yu Jeon,King SeoJong of the Joseon Dynasty,B,"The Borugak Jagyeongnu (""Water Clock of Borugak Pavilion""), classified as a scientific instrument, is the 229th National Treasure of South Korea and was designated by the South Korean government on March 3, 1985.  The water clock is currently held and managed by the National Palace Museum of Korea in Seoul.  It dates to the time of King SeoJong of the Joseon Dynasty.
Water clocks have a long history of use in Korea with the first mention of one in the records of the Samguk Sagi during the Three Kingdoms era.  In 1434, during the reign of King Sejong the Great a water clock was made by Jang Yeong-sil which would mark the hour automatically with the sounds of a bell, gong, and drum.  That clock, Jagyeokru (""self-striking water clock""), was used to keep the standard of time in the Joseon Dynasty. The clock was not preserved well and so in 1536, King Jungjong ordered the clock remade and improved which was done by Yu Jeon.  Today, only three water bowls and two cylindrical water containers survive from the 1434 version of the clock, and no records mentioning its existence."
What is the significance of the clock remade and improved by Yu Jeon in 1536?,It became a painting,It was used to keep the standard of time in the Joseon Dynasty,It was used as a weapon,It became a cooking utensil,It was used as a musical instrument,B,"The Borugak Jagyeongnu (""Water Clock of Borugak Pavilion""), classified as a scientific instrument, is the 229th National Treasure of South Korea and was designated by the South Korean government on March 3, 1985.  The water clock is currently held and managed by the National Palace Museum of Korea in Seoul.  It dates to the time of King SeoJong of the Joseon Dynasty.
Water clocks have a long history of use in Korea with the first mention of one in the records of the Samguk Sagi during the Three Kingdoms era.  In 1434, during the reign of King Sejong the Great a water clock was made by Jang Yeong-sil which would mark the hour automatically with the sounds of a bell, gong, and drum.  That clock, Jagyeokru (""self-striking water clock""), was used to keep the standard of time in the Joseon Dynasty. The clock was not preserved well and so in 1536, King Jungjong ordered the clock remade and improved which was done by Yu Jeon.  Today, only three water bowls and two cylindrical water containers survive from the 1434 version of the clock, and no records mentioning its existence."
What is a mix-minus or clean feed used for in audio engineering?,To prevent echoes and feedback in broadcast or sound reinforcement systems,To create a designated input for a mixing console or matrix mixer,To transmit audio over IP connections with a slight delay,To connect a telephone hybrid to a console at a radio station,To remove the remote subject's voice from their earpiece in ENG systems,A,"In audio engineering, a mix-minus or clean feed is a particular setup of a mixing console or matrix mixer, such that an output of the mixer contains everything except a designated input.  Mix-minus is often used to prevent echoes or feedback in broadcast or sound reinforcement systems.

Examples
A common situation in which a mix-minus is used is when a telephone hybrid is connected to a console, usually at a radio station.  The person on the telephone hears all relevant feeds, usually an identical mix to that of the master bus, including the DJ's mic feed, except that the caller does not hear their own voice.Mix-minus is also often used together with IFB systems in electronic news gathering (ENG) for television news reporters and interview subjects speaking to a host from a remote outside broadcast (OB) location.  Because of the delay that is introduced in most means of transmission (including satellite feeds and audio over IP connections), the remote subject's voice has to be removed from their earpiece.  Otherwise, the subject would hear themselves with a slight (but very distracting) delay.Another common example is in the field of sound reinforcement, when people need to hear a full mix except their own microphone.  Legislative bodies of government may use a large mix-minus system, for instance houses of parliament or congressional groups that have a small loudspeaker and a microphone at each seat. From the desktop loudspeaker, each person hears every microphone except their own."
How is mix-minus used in electronic news gathering (ENG)?,To prevent echoes and feedback in broadcast or sound reinforcement systems,To create a designated input for a mixing console or matrix mixer,To transmit audio over IP connections with a slight delay,To remove the remote subject's voice from their earpiece,To connect a telephone hybrid to a console at a radio station,D,"In audio engineering, a mix-minus or clean feed is a particular setup of a mixing console or matrix mixer, such that an output of the mixer contains everything except a designated input.  Mix-minus is often used to prevent echoes or feedback in broadcast or sound reinforcement systems.

Examples
A common situation in which a mix-minus is used is when a telephone hybrid is connected to a console, usually at a radio station.  The person on the telephone hears all relevant feeds, usually an identical mix to that of the master bus, including the DJ's mic feed, except that the caller does not hear their own voice.Mix-minus is also often used together with IFB systems in electronic news gathering (ENG) for television news reporters and interview subjects speaking to a host from a remote outside broadcast (OB) location.  Because of the delay that is introduced in most means of transmission (including satellite feeds and audio over IP connections), the remote subject's voice has to be removed from their earpiece.  Otherwise, the subject would hear themselves with a slight (but very distracting) delay.Another common example is in the field of sound reinforcement, when people need to hear a full mix except their own microphone.  Legislative bodies of government may use a large mix-minus system, for instance houses of parliament or congressional groups that have a small loudspeaker and a microphone at each seat. From the desktop loudspeaker, each person hears every microphone except their own."
Why is it necessary to remove the remote subject's voice from their earpiece in ENG systems?,To prevent echoes and feedback in broadcast or sound reinforcement systems,To create a designated input for a mixing console or matrix mixer,To transmit audio over IP connections with a slight delay,To remove distractions caused by a delay in transmission,To prevent the remote subject from hearing themselves,E,"In audio engineering, a mix-minus or clean feed is a particular setup of a mixing console or matrix mixer, such that an output of the mixer contains everything except a designated input.  Mix-minus is often used to prevent echoes or feedback in broadcast or sound reinforcement systems.

Examples
A common situation in which a mix-minus is used is when a telephone hybrid is connected to a console, usually at a radio station.  The person on the telephone hears all relevant feeds, usually an identical mix to that of the master bus, including the DJ's mic feed, except that the caller does not hear their own voice.Mix-minus is also often used together with IFB systems in electronic news gathering (ENG) for television news reporters and interview subjects speaking to a host from a remote outside broadcast (OB) location.  Because of the delay that is introduced in most means of transmission (including satellite feeds and audio over IP connections), the remote subject's voice has to be removed from their earpiece.  Otherwise, the subject would hear themselves with a slight (but very distracting) delay.Another common example is in the field of sound reinforcement, when people need to hear a full mix except their own microphone.  Legislative bodies of government may use a large mix-minus system, for instance houses of parliament or congressional groups that have a small loudspeaker and a microphone at each seat. From the desktop loudspeaker, each person hears every microphone except their own."
In what field is mix-minus commonly used for sound reinforcement?,Legislative bodies of government,Television news reporting,Radio broadcasting,Audio engineering,Satellite communication,D,"In audio engineering, a mix-minus or clean feed is a particular setup of a mixing console or matrix mixer, such that an output of the mixer contains everything except a designated input.  Mix-minus is often used to prevent echoes or feedback in broadcast or sound reinforcement systems.

Examples
A common situation in which a mix-minus is used is when a telephone hybrid is connected to a console, usually at a radio station.  The person on the telephone hears all relevant feeds, usually an identical mix to that of the master bus, including the DJ's mic feed, except that the caller does not hear their own voice.Mix-minus is also often used together with IFB systems in electronic news gathering (ENG) for television news reporters and interview subjects speaking to a host from a remote outside broadcast (OB) location.  Because of the delay that is introduced in most means of transmission (including satellite feeds and audio over IP connections), the remote subject's voice has to be removed from their earpiece.  Otherwise, the subject would hear themselves with a slight (but very distracting) delay.Another common example is in the field of sound reinforcement, when people need to hear a full mix except their own microphone.  Legislative bodies of government may use a large mix-minus system, for instance houses of parliament or congressional groups that have a small loudspeaker and a microphone at each seat. From the desktop loudspeaker, each person hears every microphone except their own."
What is the purpose of a large mix-minus system used by legislative bodies of government?,To prevent echoes and feedback in broadcast or sound reinforcement systems,To create a designated input for a mixing console or matrix mixer,To transmit audio over IP connections with a slight delay,To allow each person to hear every microphone except their own,To connect a telephone hybrid to a console at a radio station,D,"In audio engineering, a mix-minus or clean feed is a particular setup of a mixing console or matrix mixer, such that an output of the mixer contains everything except a designated input.  Mix-minus is often used to prevent echoes or feedback in broadcast or sound reinforcement systems.

Examples
A common situation in which a mix-minus is used is when a telephone hybrid is connected to a console, usually at a radio station.  The person on the telephone hears all relevant feeds, usually an identical mix to that of the master bus, including the DJ's mic feed, except that the caller does not hear their own voice.Mix-minus is also often used together with IFB systems in electronic news gathering (ENG) for television news reporters and interview subjects speaking to a host from a remote outside broadcast (OB) location.  Because of the delay that is introduced in most means of transmission (including satellite feeds and audio over IP connections), the remote subject's voice has to be removed from their earpiece.  Otherwise, the subject would hear themselves with a slight (but very distracting) delay.Another common example is in the field of sound reinforcement, when people need to hear a full mix except their own microphone.  Legislative bodies of government may use a large mix-minus system, for instance houses of parliament or congressional groups that have a small loudspeaker and a microphone at each seat. From the desktop loudspeaker, each person hears every microphone except their own."
What are superslow processes?,Processes that are difficult to capture due to their smallness compared to measurement error.,Processes that change values slowly over time.,Processes that lie beyond the scope of investigation.,Processes that have multiple gaps in different fields.,"Processes that are challenging to study in biology, astronomy, physics, mechanics, economics, linguistics, ecology, gerontology, etc.",A,"Superslow processes are processes in which values change so little that their capture is very difficult because of their smallness in comparison with the measurement error.

Applications
Most of the time, the superslow processes lie beyond the scope of investigation due to the reason of their superslowness.  Multiple gaps can be easily detected in biology, astronomy, physics, mechanics, economics, linguistics, ecology, gerontology, etc.
Biology: Traditional scientific research in this area was focused on the describing some brain reactions.Mathematics: In mathematics, when the fluid flows through thin and long tubes it forms stagnation zones where the flow becomes almost immobile.  If the ratio of tube length to its diameter is large, then the potential function and stream function are almost invariable on very extended areas.  The situation seems uninteresting, but if we remember that these minor changes occur in the extra-long intervals, we see here a series of first-class tasks that require the development of special mathematical methods.Mathematics: Apriori information regarding the stagnation zones  contributes to optimization of the computational process by replacing the unknown functions with the corresponding constants in such zones.  Sometimes this makes it possible to significantly reduce the amount of computation, for example in approximate calculation of conformal mappings of strongly elongated rectangles.Economic Geography: The obtained results are particularly useful for applications in economic geography.  In a case where the function describes the intensity of commodity trade, a theorem about its stagnation zones gives us (under appropriate restrictions on the selected  model) geometric dimensions estimates of the stagnation zone of the world-economy (for more information about a stagnation zone of the world-economy, see Fernand Braudel, Les Jeux de L'echange).For example, if the subarc of a domain boundary has zero transparency, and the flow of the gradient vector field of the function through the rest of the boundary is small enough, then the domain for such function is its stagnation zone.Stagnation zones theorems are closely related to pre-Liouville's theorems about evaluation of solutions fluctuation, which direct consequences are the different versions of the classic Liouville theorem about conversion of the entire doubly periodic function into the identical constant.Identification of what parameters impact the sizes of stagnation zones opens up opportunities for practical recommendations on targeted changes in configuration (reduction or increase) of such zones.


== References ==."
In which areas can superslow processes be detected?,"Biology, astronomy, physics, mechanics, economics, linguistics, ecology, gerontology.",Traditional scientific research focused on describing brain reactions.,Fluid flows through thin and long tubes forming stagnation zones.,Optimization of computational processes in mathematics.,Applications in economic geography.,A,"Superslow processes are processes in which values change so little that their capture is very difficult because of their smallness in comparison with the measurement error.

Applications
Most of the time, the superslow processes lie beyond the scope of investigation due to the reason of their superslowness.  Multiple gaps can be easily detected in biology, astronomy, physics, mechanics, economics, linguistics, ecology, gerontology, etc.
Biology: Traditional scientific research in this area was focused on the describing some brain reactions.Mathematics: In mathematics, when the fluid flows through thin and long tubes it forms stagnation zones where the flow becomes almost immobile.  If the ratio of tube length to its diameter is large, then the potential function and stream function are almost invariable on very extended areas.  The situation seems uninteresting, but if we remember that these minor changes occur in the extra-long intervals, we see here a series of first-class tasks that require the development of special mathematical methods.Mathematics: Apriori information regarding the stagnation zones  contributes to optimization of the computational process by replacing the unknown functions with the corresponding constants in such zones.  Sometimes this makes it possible to significantly reduce the amount of computation, for example in approximate calculation of conformal mappings of strongly elongated rectangles.Economic Geography: The obtained results are particularly useful for applications in economic geography.  In a case where the function describes the intensity of commodity trade, a theorem about its stagnation zones gives us (under appropriate restrictions on the selected  model) geometric dimensions estimates of the stagnation zone of the world-economy (for more information about a stagnation zone of the world-economy, see Fernand Braudel, Les Jeux de L'echange).For example, if the subarc of a domain boundary has zero transparency, and the flow of the gradient vector field of the function through the rest of the boundary is small enough, then the domain for such function is its stagnation zone.Stagnation zones theorems are closely related to pre-Liouville's theorems about evaluation of solutions fluctuation, which direct consequences are the different versions of the classic Liouville theorem about conversion of the entire doubly periodic function into the identical constant.Identification of what parameters impact the sizes of stagnation zones opens up opportunities for practical recommendations on targeted changes in configuration (reduction or increase) of such zones.


== References ==."
How can stagnation zones be used in mathematics?,By replacing unknown functions with corresponding constants in stagnation zones.,By reducing the amount of computation in approximate calculation of conformal mappings.,By developing special mathematical methods for first-class tasks.,By providing geometric dimensions estimates of the world-economy stagnation zone.,By evaluating solutions fluctuation in pre-Liouville's theorems.,A,"Superslow processes are processes in which values change so little that their capture is very difficult because of their smallness in comparison with the measurement error.

Applications
Most of the time, the superslow processes lie beyond the scope of investigation due to the reason of their superslowness.  Multiple gaps can be easily detected in biology, astronomy, physics, mechanics, economics, linguistics, ecology, gerontology, etc.
Biology: Traditional scientific research in this area was focused on the describing some brain reactions.Mathematics: In mathematics, when the fluid flows through thin and long tubes it forms stagnation zones where the flow becomes almost immobile.  If the ratio of tube length to its diameter is large, then the potential function and stream function are almost invariable on very extended areas.  The situation seems uninteresting, but if we remember that these minor changes occur in the extra-long intervals, we see here a series of first-class tasks that require the development of special mathematical methods.Mathematics: Apriori information regarding the stagnation zones  contributes to optimization of the computational process by replacing the unknown functions with the corresponding constants in such zones.  Sometimes this makes it possible to significantly reduce the amount of computation, for example in approximate calculation of conformal mappings of strongly elongated rectangles.Economic Geography: The obtained results are particularly useful for applications in economic geography.  In a case where the function describes the intensity of commodity trade, a theorem about its stagnation zones gives us (under appropriate restrictions on the selected  model) geometric dimensions estimates of the stagnation zone of the world-economy (for more information about a stagnation zone of the world-economy, see Fernand Braudel, Les Jeux de L'echange).For example, if the subarc of a domain boundary has zero transparency, and the flow of the gradient vector field of the function through the rest of the boundary is small enough, then the domain for such function is its stagnation zone.Stagnation zones theorems are closely related to pre-Liouville's theorems about evaluation of solutions fluctuation, which direct consequences are the different versions of the classic Liouville theorem about conversion of the entire doubly periodic function into the identical constant.Identification of what parameters impact the sizes of stagnation zones opens up opportunities for practical recommendations on targeted changes in configuration (reduction or increase) of such zones.


== References ==."
What is the relationship between stagnation zones theorems and Liouville's theorem?,Stagnation zones theorems are direct consequences of Liouville's theorem.,Stagnation zones theorems can be used to prove Liouville's theorem.,Stagnation zones theorems are closely related to pre-Liouville's theorems.,Liouville's theorem is a version of the classic Liouville theorem about conversion of doubly periodic function into the identical constant.,Stagnation zones theorems evaluate solutions fluctuation in different versions of Liouville's theorem.,C,"Superslow processes are processes in which values change so little that their capture is very difficult because of their smallness in comparison with the measurement error.

Applications
Most of the time, the superslow processes lie beyond the scope of investigation due to the reason of their superslowness.  Multiple gaps can be easily detected in biology, astronomy, physics, mechanics, economics, linguistics, ecology, gerontology, etc.
Biology: Traditional scientific research in this area was focused on the describing some brain reactions.Mathematics: In mathematics, when the fluid flows through thin and long tubes it forms stagnation zones where the flow becomes almost immobile.  If the ratio of tube length to its diameter is large, then the potential function and stream function are almost invariable on very extended areas.  The situation seems uninteresting, but if we remember that these minor changes occur in the extra-long intervals, we see here a series of first-class tasks that require the development of special mathematical methods.Mathematics: Apriori information regarding the stagnation zones  contributes to optimization of the computational process by replacing the unknown functions with the corresponding constants in such zones.  Sometimes this makes it possible to significantly reduce the amount of computation, for example in approximate calculation of conformal mappings of strongly elongated rectangles.Economic Geography: The obtained results are particularly useful for applications in economic geography.  In a case where the function describes the intensity of commodity trade, a theorem about its stagnation zones gives us (under appropriate restrictions on the selected  model) geometric dimensions estimates of the stagnation zone of the world-economy (for more information about a stagnation zone of the world-economy, see Fernand Braudel, Les Jeux de L'echange).For example, if the subarc of a domain boundary has zero transparency, and the flow of the gradient vector field of the function through the rest of the boundary is small enough, then the domain for such function is its stagnation zone.Stagnation zones theorems are closely related to pre-Liouville's theorems about evaluation of solutions fluctuation, which direct consequences are the different versions of the classic Liouville theorem about conversion of the entire doubly periodic function into the identical constant.Identification of what parameters impact the sizes of stagnation zones opens up opportunities for practical recommendations on targeted changes in configuration (reduction or increase) of such zones.


== References ==."
What do the sizes of stagnation zones help identify?,The impact of parameters on the sizes of stagnation zones.,The targeted changes in configuration of stagnation zones.,The reduction or increase of stagnation zone sizes.,The practical recommendations for changes in stagnation zone configuration.,The domain boundaries with zero transparency.,A,"Superslow processes are processes in which values change so little that their capture is very difficult because of their smallness in comparison with the measurement error.

Applications
Most of the time, the superslow processes lie beyond the scope of investigation due to the reason of their superslowness.  Multiple gaps can be easily detected in biology, astronomy, physics, mechanics, economics, linguistics, ecology, gerontology, etc.
Biology: Traditional scientific research in this area was focused on the describing some brain reactions.Mathematics: In mathematics, when the fluid flows through thin and long tubes it forms stagnation zones where the flow becomes almost immobile.  If the ratio of tube length to its diameter is large, then the potential function and stream function are almost invariable on very extended areas.  The situation seems uninteresting, but if we remember that these minor changes occur in the extra-long intervals, we see here a series of first-class tasks that require the development of special mathematical methods.Mathematics: Apriori information regarding the stagnation zones  contributes to optimization of the computational process by replacing the unknown functions with the corresponding constants in such zones.  Sometimes this makes it possible to significantly reduce the amount of computation, for example in approximate calculation of conformal mappings of strongly elongated rectangles.Economic Geography: The obtained results are particularly useful for applications in economic geography.  In a case where the function describes the intensity of commodity trade, a theorem about its stagnation zones gives us (under appropriate restrictions on the selected  model) geometric dimensions estimates of the stagnation zone of the world-economy (for more information about a stagnation zone of the world-economy, see Fernand Braudel, Les Jeux de L'echange).For example, if the subarc of a domain boundary has zero transparency, and the flow of the gradient vector field of the function through the rest of the boundary is small enough, then the domain for such function is its stagnation zone.Stagnation zones theorems are closely related to pre-Liouville's theorems about evaluation of solutions fluctuation, which direct consequences are the different versions of the classic Liouville theorem about conversion of the entire doubly periodic function into the identical constant.Identification of what parameters impact the sizes of stagnation zones opens up opportunities for practical recommendations on targeted changes in configuration (reduction or increase) of such zones.


== References ==."
What is double switching?,The practice of using a multipole switch to close or open both the positive and negative sides of a DC electrical circuit,The practice of using a multipole switch to close or open both the hot and neutral sides of an AC circuit,The practice of using a multipole switch to close or open either the positive or negative side of a DC electrical circuit,The practice of using a multipole switch to close or open either the hot or neutral side of an AC circuit,The practice of using a single pole switch to close or open both the positive and negative sides of a DC electrical circuit,A,"Double switching, double cutting, or double breaking is the practice of using a multipole switch to close or open both the positive and negative sides of a DC electrical circuit, or both the hot and neutral sides of an AC circuit. This technique is used to prevent shock hazard in electric devices connected with unpolarised AC power plugs and sockets. Double switching is a crucial safety engineering practice in railway signalling, wherein it is used to ensure that a single false feed of current to a relay is unlikely to cause a wrong-side failure. It is an example of using redundancy to increase safety and reduce the likelihood of failure, analogous to double insulation. Double switching increases the cost and complexity of systems in which it is employed, for example by extra relay contacts and extra relays, so the technique is applied selectively where it can provide a cost-effective safety improvement.

Examples
Landslip and Washaway Detectors
A landslip or washaway detector is buried in the earth embankment, and opens a circuit should a landslide occur.  It is not possible to guarantee that the wet earth of the embankment will not complete the circuit which is supposed to break.  If the circuit is double cut with positive and negative wires, any wet conductive earth is likely to blow a fuse on the one hand, and short the detecting relay on the other hand, either of which is almost certain to apply the correct warning signal.

Accidents
Clapham
The Clapham Junction rail crash of 1988 was caused in part by the lack of double switching (known as ""double cutting"" in the British railway industry)."
Why is double switching used in railway signalling?,To increase the cost and complexity of systems,To reduce the likelihood of failure,To provide a cost-effective safety improvement,To prevent shock hazard in electric devices,To guarantee that the wet earth of the embankment will not complete the circuit,B,"Double switching, double cutting, or double breaking is the practice of using a multipole switch to close or open both the positive and negative sides of a DC electrical circuit, or both the hot and neutral sides of an AC circuit. This technique is used to prevent shock hazard in electric devices connected with unpolarised AC power plugs and sockets. Double switching is a crucial safety engineering practice in railway signalling, wherein it is used to ensure that a single false feed of current to a relay is unlikely to cause a wrong-side failure. It is an example of using redundancy to increase safety and reduce the likelihood of failure, analogous to double insulation. Double switching increases the cost and complexity of systems in which it is employed, for example by extra relay contacts and extra relays, so the technique is applied selectively where it can provide a cost-effective safety improvement.

Examples
Landslip and Washaway Detectors
A landslip or washaway detector is buried in the earth embankment, and opens a circuit should a landslide occur.  It is not possible to guarantee that the wet earth of the embankment will not complete the circuit which is supposed to break.  If the circuit is double cut with positive and negative wires, any wet conductive earth is likely to blow a fuse on the one hand, and short the detecting relay on the other hand, either of which is almost certain to apply the correct warning signal.

Accidents
Clapham
The Clapham Junction rail crash of 1988 was caused in part by the lack of double switching (known as ""double cutting"" in the British railway industry)."
What is an example of using redundancy to increase safety and reduce the likelihood of failure?,Double switching,Double insulation,Multipole switch,Relay contacts,Detecting relay,A,"Double switching, double cutting, or double breaking is the practice of using a multipole switch to close or open both the positive and negative sides of a DC electrical circuit, or both the hot and neutral sides of an AC circuit. This technique is used to prevent shock hazard in electric devices connected with unpolarised AC power plugs and sockets. Double switching is a crucial safety engineering practice in railway signalling, wherein it is used to ensure that a single false feed of current to a relay is unlikely to cause a wrong-side failure. It is an example of using redundancy to increase safety and reduce the likelihood of failure, analogous to double insulation. Double switching increases the cost and complexity of systems in which it is employed, for example by extra relay contacts and extra relays, so the technique is applied selectively where it can provide a cost-effective safety improvement.

Examples
Landslip and Washaway Detectors
A landslip or washaway detector is buried in the earth embankment, and opens a circuit should a landslide occur.  It is not possible to guarantee that the wet earth of the embankment will not complete the circuit which is supposed to break.  If the circuit is double cut with positive and negative wires, any wet conductive earth is likely to blow a fuse on the one hand, and short the detecting relay on the other hand, either of which is almost certain to apply the correct warning signal.

Accidents
Clapham
The Clapham Junction rail crash of 1988 was caused in part by the lack of double switching (known as ""double cutting"" in the British railway industry)."
What is the purpose of a landslip or washaway detector?,To open a circuit if a landslide occurs,To close a circuit if a landslide occurs,To prevent shock hazard in electric devices,To increase the cost and complexity of systems,To provide a cost-effective safety improvement,A,"Double switching, double cutting, or double breaking is the practice of using a multipole switch to close or open both the positive and negative sides of a DC electrical circuit, or both the hot and neutral sides of an AC circuit. This technique is used to prevent shock hazard in electric devices connected with unpolarised AC power plugs and sockets. Double switching is a crucial safety engineering practice in railway signalling, wherein it is used to ensure that a single false feed of current to a relay is unlikely to cause a wrong-side failure. It is an example of using redundancy to increase safety and reduce the likelihood of failure, analogous to double insulation. Double switching increases the cost and complexity of systems in which it is employed, for example by extra relay contacts and extra relays, so the technique is applied selectively where it can provide a cost-effective safety improvement.

Examples
Landslip and Washaway Detectors
A landslip or washaway detector is buried in the earth embankment, and opens a circuit should a landslide occur.  It is not possible to guarantee that the wet earth of the embankment will not complete the circuit which is supposed to break.  If the circuit is double cut with positive and negative wires, any wet conductive earth is likely to blow a fuse on the one hand, and short the detecting relay on the other hand, either of which is almost certain to apply the correct warning signal.

Accidents
Clapham
The Clapham Junction rail crash of 1988 was caused in part by the lack of double switching (known as ""double cutting"" in the British railway industry)."
What was a contributing factor to the Clapham Junction rail crash of 1988?,Double switching,Double cutting,Lack of double switching,Lack of double cutting,Double insulation,C,"Double switching, double cutting, or double breaking is the practice of using a multipole switch to close or open both the positive and negative sides of a DC electrical circuit, or both the hot and neutral sides of an AC circuit. This technique is used to prevent shock hazard in electric devices connected with unpolarised AC power plugs and sockets. Double switching is a crucial safety engineering practice in railway signalling, wherein it is used to ensure that a single false feed of current to a relay is unlikely to cause a wrong-side failure. It is an example of using redundancy to increase safety and reduce the likelihood of failure, analogous to double insulation. Double switching increases the cost and complexity of systems in which it is employed, for example by extra relay contacts and extra relays, so the technique is applied selectively where it can provide a cost-effective safety improvement.

Examples
Landslip and Washaway Detectors
A landslip or washaway detector is buried in the earth embankment, and opens a circuit should a landslide occur.  It is not possible to guarantee that the wet earth of the embankment will not complete the circuit which is supposed to break.  If the circuit is double cut with positive and negative wires, any wet conductive earth is likely to blow a fuse on the one hand, and short the detecting relay on the other hand, either of which is almost certain to apply the correct warning signal.

Accidents
Clapham
The Clapham Junction rail crash of 1988 was caused in part by the lack of double switching (known as ""double cutting"" in the British railway industry)."
What is sustainable engineering?,The process of designing systems that use energy and resources sustainably,The process of designing systems that compromise the natural environment,The process of designing systems that meet the needs of future generations,The process of designing systems that focus on water supply,The process of designing systems that focus on energy development,A,"Sustainable engineering is the process of designing or operating systems such that they use energy and resources sustainably, in other words, at a rate that does not compromise the natural environment, or the ability of future generations to meet their own needs.

Common engineering focuses
Sustainable Engineering focuses on the following -

Water supply
Food production
Housing and shelter
Sanitation and waste management
Energy development
Transportation
Industrial processing
Development of natural resources
Cleaning up polluted waste sites
Planning projects to reduce environmental and social impacts
Restoring natural environments such as forests, lakes, streams, and wetlands
Providing medical care to those in need
Minimizing and responsibly disposing of waste to benefit all
Improving industrial processes to eliminate waste and reduce consumption
Recommending the appropriate and innovative use of technology

Aspects of engineering disciplines
Every engineering discipline is engaged in sustainable design, employing numerous initiatives, especially life cycle analysis (LCA), pollution prevention, Design for the Environment (DfE), Design for Disassembly (DfD), and Design for Recycling (DfR).  These are replacing or at least changing pollution control paradigms. For example, concept of a ""cap and trade"" has been tested and works well for some pollutants. This is a system where companies are allowed to place a ""bubble"" over a whole manufacturing complex or trade pollution credits with other companies in their industry instead of a ""stack-by-stack"" and ""pipe-by-pipe"" approach, i.e. the so-called ""command and control"" approach. Such policy and regulatory innovations call for some improved technology based approaches as well as better quality-based approaches, such as leveling out the pollutant loadings and using less expensive technologies to remove the first large bulk of pollutants, followed by higher operation and maintenance (O&M) technologies for the more difficult to treat stacks and pipes. But, the net effect can be a greater reduction of pollutant emissions and effluents than treating each stack or pipe as an independent entity."
Which of the following is NOT a common focus of sustainable engineering?,Food production,Housing and shelter,Transportation,Industrial processing,Providing medical care to those in need,E,"Sustainable engineering is the process of designing or operating systems such that they use energy and resources sustainably, in other words, at a rate that does not compromise the natural environment, or the ability of future generations to meet their own needs.

Common engineering focuses
Sustainable Engineering focuses on the following -

Water supply
Food production
Housing and shelter
Sanitation and waste management
Energy development
Transportation
Industrial processing
Development of natural resources
Cleaning up polluted waste sites
Planning projects to reduce environmental and social impacts
Restoring natural environments such as forests, lakes, streams, and wetlands
Providing medical care to those in need
Minimizing and responsibly disposing of waste to benefit all
Improving industrial processes to eliminate waste and reduce consumption
Recommending the appropriate and innovative use of technology

Aspects of engineering disciplines
Every engineering discipline is engaged in sustainable design, employing numerous initiatives, especially life cycle analysis (LCA), pollution prevention, Design for the Environment (DfE), Design for Disassembly (DfD), and Design for Recycling (DfR).  These are replacing or at least changing pollution control paradigms. For example, concept of a ""cap and trade"" has been tested and works well for some pollutants. This is a system where companies are allowed to place a ""bubble"" over a whole manufacturing complex or trade pollution credits with other companies in their industry instead of a ""stack-by-stack"" and ""pipe-by-pipe"" approach, i.e. the so-called ""command and control"" approach. Such policy and regulatory innovations call for some improved technology based approaches as well as better quality-based approaches, such as leveling out the pollutant loadings and using less expensive technologies to remove the first large bulk of pollutants, followed by higher operation and maintenance (O&M) technologies for the more difficult to treat stacks and pipes. But, the net effect can be a greater reduction of pollutant emissions and effluents than treating each stack or pipe as an independent entity."
What are some aspects of sustainable design in engineering?,Life cycle analysis and pollution prevention,Design for the Environment and Design for Disassembly,Design for Recycling and Design for the Environment,Life cycle analysis and Design for Recycling,Pollution prevention and Design for Disassembly,A,"Sustainable engineering is the process of designing or operating systems such that they use energy and resources sustainably, in other words, at a rate that does not compromise the natural environment, or the ability of future generations to meet their own needs.

Common engineering focuses
Sustainable Engineering focuses on the following -

Water supply
Food production
Housing and shelter
Sanitation and waste management
Energy development
Transportation
Industrial processing
Development of natural resources
Cleaning up polluted waste sites
Planning projects to reduce environmental and social impacts
Restoring natural environments such as forests, lakes, streams, and wetlands
Providing medical care to those in need
Minimizing and responsibly disposing of waste to benefit all
Improving industrial processes to eliminate waste and reduce consumption
Recommending the appropriate and innovative use of technology

Aspects of engineering disciplines
Every engineering discipline is engaged in sustainable design, employing numerous initiatives, especially life cycle analysis (LCA), pollution prevention, Design for the Environment (DfE), Design for Disassembly (DfD), and Design for Recycling (DfR).  These are replacing or at least changing pollution control paradigms. For example, concept of a ""cap and trade"" has been tested and works well for some pollutants. This is a system where companies are allowed to place a ""bubble"" over a whole manufacturing complex or trade pollution credits with other companies in their industry instead of a ""stack-by-stack"" and ""pipe-by-pipe"" approach, i.e. the so-called ""command and control"" approach. Such policy and regulatory innovations call for some improved technology based approaches as well as better quality-based approaches, such as leveling out the pollutant loadings and using less expensive technologies to remove the first large bulk of pollutants, followed by higher operation and maintenance (O&M) technologies for the more difficult to treat stacks and pipes. But, the net effect can be a greater reduction of pollutant emissions and effluents than treating each stack or pipe as an independent entity."
"What is the concept of ""cap and trade""?",The process of placing a bubble over a manufacturing complex,The process of trading pollution credits with other companies,The process of implementing a command and control approach,The process of reducing pollutant emissions and effluents,The process of using less expensive technologies for pollutant removal,B,"Sustainable engineering is the process of designing or operating systems such that they use energy and resources sustainably, in other words, at a rate that does not compromise the natural environment, or the ability of future generations to meet their own needs.

Common engineering focuses
Sustainable Engineering focuses on the following -

Water supply
Food production
Housing and shelter
Sanitation and waste management
Energy development
Transportation
Industrial processing
Development of natural resources
Cleaning up polluted waste sites
Planning projects to reduce environmental and social impacts
Restoring natural environments such as forests, lakes, streams, and wetlands
Providing medical care to those in need
Minimizing and responsibly disposing of waste to benefit all
Improving industrial processes to eliminate waste and reduce consumption
Recommending the appropriate and innovative use of technology

Aspects of engineering disciplines
Every engineering discipline is engaged in sustainable design, employing numerous initiatives, especially life cycle analysis (LCA), pollution prevention, Design for the Environment (DfE), Design for Disassembly (DfD), and Design for Recycling (DfR).  These are replacing or at least changing pollution control paradigms. For example, concept of a ""cap and trade"" has been tested and works well for some pollutants. This is a system where companies are allowed to place a ""bubble"" over a whole manufacturing complex or trade pollution credits with other companies in their industry instead of a ""stack-by-stack"" and ""pipe-by-pipe"" approach, i.e. the so-called ""command and control"" approach. Such policy and regulatory innovations call for some improved technology based approaches as well as better quality-based approaches, such as leveling out the pollutant loadings and using less expensive technologies to remove the first large bulk of pollutants, followed by higher operation and maintenance (O&M) technologies for the more difficult to treat stacks and pipes. But, the net effect can be a greater reduction of pollutant emissions and effluents than treating each stack or pipe as an independent entity."
What is the net effect of policy and regulatory innovations in sustainable engineering?,A greater reduction of pollutant emissions and effluents,Leveling out the pollutant loadings in stacks and pipes,Using better quality-based approaches for pollutant removal,Treating each stack or pipe as an independent entity,Implementing a command and control approach,A,"Sustainable engineering is the process of designing or operating systems such that they use energy and resources sustainably, in other words, at a rate that does not compromise the natural environment, or the ability of future generations to meet their own needs.

Common engineering focuses
Sustainable Engineering focuses on the following -

Water supply
Food production
Housing and shelter
Sanitation and waste management
Energy development
Transportation
Industrial processing
Development of natural resources
Cleaning up polluted waste sites
Planning projects to reduce environmental and social impacts
Restoring natural environments such as forests, lakes, streams, and wetlands
Providing medical care to those in need
Minimizing and responsibly disposing of waste to benefit all
Improving industrial processes to eliminate waste and reduce consumption
Recommending the appropriate and innovative use of technology

Aspects of engineering disciplines
Every engineering discipline is engaged in sustainable design, employing numerous initiatives, especially life cycle analysis (LCA), pollution prevention, Design for the Environment (DfE), Design for Disassembly (DfD), and Design for Recycling (DfR).  These are replacing or at least changing pollution control paradigms. For example, concept of a ""cap and trade"" has been tested and works well for some pollutants. This is a system where companies are allowed to place a ""bubble"" over a whole manufacturing complex or trade pollution credits with other companies in their industry instead of a ""stack-by-stack"" and ""pipe-by-pipe"" approach, i.e. the so-called ""command and control"" approach. Such policy and regulatory innovations call for some improved technology based approaches as well as better quality-based approaches, such as leveling out the pollutant loadings and using less expensive technologies to remove the first large bulk of pollutants, followed by higher operation and maintenance (O&M) technologies for the more difficult to treat stacks and pipes. But, the net effect can be a greater reduction of pollutant emissions and effluents than treating each stack or pipe as an independent entity."
What are some examples of fields in mathematics?,"The field of rational numbers, the field of real numbers and the field of complex numbers.","The field of polynomials, the field of integers, and the field of fractions.","The field of irrational numbers, the field of prime numbers, and the field of even numbers.","The field of exponential functions, the field of logarithmic functions, and the field of trigonometric functions.","The field of vectors, the field of matrices, and the field of determinants.",A,"In mathematics, a field is a set on which addition, subtraction, multiplication, and division are defined and behave as the corresponding operations on rational and real numbers do. A field is thus a fundamental algebraic structure which is widely used in algebra, number theory, and many other areas of mathematics.
The best known fields are the field of rational numbers, the field of real numbers and the field of complex numbers. Many other fields, such as fields of rational functions, algebraic function fields, algebraic number fields, and p-adic fields are commonly used and studied in mathematics, particularly in number theory and algebraic geometry. Most cryptographic protocols rely on finite fields, i.e., fields with finitely many elements.
The relation of two fields is expressed by the notion of a field extension. Galois theory, initiated by Évariste Galois in the 1830s, is devoted to understanding the symmetries of field extensions. Among other results, this theory shows that angle trisection and squaring the circle cannot be done with a compass and straightedge. Moreover, it shows that quintic equations are, in general, algebraically unsolvable.
Fields serve as foundational notions in several mathematical domains."
What are some other fields commonly used and studied in mathematics?,"Fields of rational exponents, fields of logarithmic functions, and fields of exponential functions.","Fields of quadratic equations, fields of linear equations, and fields of cubic equations.","Fields of rational functions, algebraic function fields, and algebraic number fields.","Fields of prime numbers, fields of composite numbers, and fields of rational numbers.","Fields of sine functions, fields of cosine functions, and fields of tangent functions.",C,"In mathematics, a field is a set on which addition, subtraction, multiplication, and division are defined and behave as the corresponding operations on rational and real numbers do. A field is thus a fundamental algebraic structure which is widely used in algebra, number theory, and many other areas of mathematics.
The best known fields are the field of rational numbers, the field of real numbers and the field of complex numbers. Many other fields, such as fields of rational functions, algebraic function fields, algebraic number fields, and p-adic fields are commonly used and studied in mathematics, particularly in number theory and algebraic geometry. Most cryptographic protocols rely on finite fields, i.e., fields with finitely many elements.
The relation of two fields is expressed by the notion of a field extension. Galois theory, initiated by Évariste Galois in the 1830s, is devoted to understanding the symmetries of field extensions. Among other results, this theory shows that angle trisection and squaring the circle cannot be done with a compass and straightedge. Moreover, it shows that quintic equations are, in general, algebraically unsolvable.
Fields serve as foundational notions in several mathematical domains."
What is the relation between two fields expressed by?,The notion of a field multiplication.,The notion of a field addition.,The notion of a field division.,The notion of a field subtraction.,The notion of a field extension.,E,"In mathematics, a field is a set on which addition, subtraction, multiplication, and division are defined and behave as the corresponding operations on rational and real numbers do. A field is thus a fundamental algebraic structure which is widely used in algebra, number theory, and many other areas of mathematics.
The best known fields are the field of rational numbers, the field of real numbers and the field of complex numbers. Many other fields, such as fields of rational functions, algebraic function fields, algebraic number fields, and p-adic fields are commonly used and studied in mathematics, particularly in number theory and algebraic geometry. Most cryptographic protocols rely on finite fields, i.e., fields with finitely many elements.
The relation of two fields is expressed by the notion of a field extension. Galois theory, initiated by Évariste Galois in the 1830s, is devoted to understanding the symmetries of field extensions. Among other results, this theory shows that angle trisection and squaring the circle cannot be done with a compass and straightedge. Moreover, it shows that quintic equations are, in general, algebraically unsolvable.
Fields serve as foundational notions in several mathematical domains."
What does Galois theory study?,The symmetries of geometric shapes.,The symmetries of polynomial equations.,The symmetries of complex numbers.,The symmetries of vector spaces.,The symmetries of field extensions.,E,"In mathematics, a field is a set on which addition, subtraction, multiplication, and division are defined and behave as the corresponding operations on rational and real numbers do. A field is thus a fundamental algebraic structure which is widely used in algebra, number theory, and many other areas of mathematics.
The best known fields are the field of rational numbers, the field of real numbers and the field of complex numbers. Many other fields, such as fields of rational functions, algebraic function fields, algebraic number fields, and p-adic fields are commonly used and studied in mathematics, particularly in number theory and algebraic geometry. Most cryptographic protocols rely on finite fields, i.e., fields with finitely many elements.
The relation of two fields is expressed by the notion of a field extension. Galois theory, initiated by Évariste Galois in the 1830s, is devoted to understanding the symmetries of field extensions. Among other results, this theory shows that angle trisection and squaring the circle cannot be done with a compass and straightedge. Moreover, it shows that quintic equations are, in general, algebraically unsolvable.
Fields serve as foundational notions in several mathematical domains."
What does Galois theory show about angle trisection and squaring the circle?,They can be done with a compass and straightedge.,They cannot be done with a compass and straightedge.,They can only be done with a compass and straightedge.,They can be done with a ruler and protractor.,They cannot be done with a ruler and protractor.,B,"In mathematics, a field is a set on which addition, subtraction, multiplication, and division are defined and behave as the corresponding operations on rational and real numbers do. A field is thus a fundamental algebraic structure which is widely used in algebra, number theory, and many other areas of mathematics.
The best known fields are the field of rational numbers, the field of real numbers and the field of complex numbers. Many other fields, such as fields of rational functions, algebraic function fields, algebraic number fields, and p-adic fields are commonly used and studied in mathematics, particularly in number theory and algebraic geometry. Most cryptographic protocols rely on finite fields, i.e., fields with finitely many elements.
The relation of two fields is expressed by the notion of a field extension. Galois theory, initiated by Évariste Galois in the 1830s, is devoted to understanding the symmetries of field extensions. Among other results, this theory shows that angle trisection and squaring the circle cannot be done with a compass and straightedge. Moreover, it shows that quintic equations are, in general, algebraically unsolvable.
Fields serve as foundational notions in several mathematical domains."
When was the Faculty of Natural Sciences formed?,1848,2001,1870s,1907,1890,B,"The Faculty of Natural Sciences is one of the three main faculties of Imperial College London in London, England. It was formed in 2001 from the former Royal College of Science, a constituent college of Imperial College which dated back to 1848, and the faculty largely consists of the original departments of the College. Undergraduate teaching occurs for all departments at the South Kensington campus, with research being split between South Kensington and the new innovation campus at White City.Students who study at the departments of the Faculty are represented by the Royal College of Science Union, a constituent union of the college union which caters specifically to students on natural science courses. Graduates who obtain an undergraduate degree, either BSc or MSci, from the faculty are awarded the Associateship of the Royal College of Science (ARCS) as an additional degree.

History
The origins of the faculty lie in the Royal College of Chemistry, which, after being founded in 1845, moved to a new site in South Kensington in the early 1870s. Incorporated into the Normal School of Science, the college was later renamed the Royal College of Science in 1890, and in 1907 became a constituent college of the newly formed Imperial College of Science and Technology.In 2001, Imperial was restructured to form four new faculties, including the faculties of Physical Sciences and Life Sciences, which took over the role of the Royal College of Science. These faculties were later re-merged over the course of 2005-2006 to form the Faculty of Natural Sciences, which comprises the same departments as the original Royal College of Science.

Departments
The faculty includes five academic departments:

Chemistry
Mathematics
Physics, The Blackett Laboratory
Life Sciences
Centre for Environmental Policy

References
External links
Imperial College Faculty of Natural Sciences website."
Which campus is undergraduate teaching for the Faculty of Natural Sciences located at?,White City,South Kensington,Imperial College,Royal College of Science,Normal School of Science,B,"The Faculty of Natural Sciences is one of the three main faculties of Imperial College London in London, England. It was formed in 2001 from the former Royal College of Science, a constituent college of Imperial College which dated back to 1848, and the faculty largely consists of the original departments of the College. Undergraduate teaching occurs for all departments at the South Kensington campus, with research being split between South Kensington and the new innovation campus at White City.Students who study at the departments of the Faculty are represented by the Royal College of Science Union, a constituent union of the college union which caters specifically to students on natural science courses. Graduates who obtain an undergraduate degree, either BSc or MSci, from the faculty are awarded the Associateship of the Royal College of Science (ARCS) as an additional degree.

History
The origins of the faculty lie in the Royal College of Chemistry, which, after being founded in 1845, moved to a new site in South Kensington in the early 1870s. Incorporated into the Normal School of Science, the college was later renamed the Royal College of Science in 1890, and in 1907 became a constituent college of the newly formed Imperial College of Science and Technology.In 2001, Imperial was restructured to form four new faculties, including the faculties of Physical Sciences and Life Sciences, which took over the role of the Royal College of Science. These faculties were later re-merged over the course of 2005-2006 to form the Faculty of Natural Sciences, which comprises the same departments as the original Royal College of Science.

Departments
The faculty includes five academic departments:

Chemistry
Mathematics
Physics, The Blackett Laboratory
Life Sciences
Centre for Environmental Policy

References
External links
Imperial College Faculty of Natural Sciences website."
What is the additional degree awarded to graduates from the faculty?,BSc,MSci,ARCS,Royal College of Science Union,Imperial College of Science and Technology,C,"The Faculty of Natural Sciences is one of the three main faculties of Imperial College London in London, England. It was formed in 2001 from the former Royal College of Science, a constituent college of Imperial College which dated back to 1848, and the faculty largely consists of the original departments of the College. Undergraduate teaching occurs for all departments at the South Kensington campus, with research being split between South Kensington and the new innovation campus at White City.Students who study at the departments of the Faculty are represented by the Royal College of Science Union, a constituent union of the college union which caters specifically to students on natural science courses. Graduates who obtain an undergraduate degree, either BSc or MSci, from the faculty are awarded the Associateship of the Royal College of Science (ARCS) as an additional degree.

History
The origins of the faculty lie in the Royal College of Chemistry, which, after being founded in 1845, moved to a new site in South Kensington in the early 1870s. Incorporated into the Normal School of Science, the college was later renamed the Royal College of Science in 1890, and in 1907 became a constituent college of the newly formed Imperial College of Science and Technology.In 2001, Imperial was restructured to form four new faculties, including the faculties of Physical Sciences and Life Sciences, which took over the role of the Royal College of Science. These faculties were later re-merged over the course of 2005-2006 to form the Faculty of Natural Sciences, which comprises the same departments as the original Royal College of Science.

Departments
The faculty includes five academic departments:

Chemistry
Mathematics
Physics, The Blackett Laboratory
Life Sciences
Centre for Environmental Policy

References
External links
Imperial College Faculty of Natural Sciences website."
Which department is part of the Faculty of Natural Sciences?,Centre for Environmental Policy,Royal College of Chemistry,The Blackett Laboratory,Life Sciences,Royal College of Science Union,D,"The Faculty of Natural Sciences is one of the three main faculties of Imperial College London in London, England. It was formed in 2001 from the former Royal College of Science, a constituent college of Imperial College which dated back to 1848, and the faculty largely consists of the original departments of the College. Undergraduate teaching occurs for all departments at the South Kensington campus, with research being split between South Kensington and the new innovation campus at White City.Students who study at the departments of the Faculty are represented by the Royal College of Science Union, a constituent union of the college union which caters specifically to students on natural science courses. Graduates who obtain an undergraduate degree, either BSc or MSci, from the faculty are awarded the Associateship of the Royal College of Science (ARCS) as an additional degree.

History
The origins of the faculty lie in the Royal College of Chemistry, which, after being founded in 1845, moved to a new site in South Kensington in the early 1870s. Incorporated into the Normal School of Science, the college was later renamed the Royal College of Science in 1890, and in 1907 became a constituent college of the newly formed Imperial College of Science and Technology.In 2001, Imperial was restructured to form four new faculties, including the faculties of Physical Sciences and Life Sciences, which took over the role of the Royal College of Science. These faculties were later re-merged over the course of 2005-2006 to form the Faculty of Natural Sciences, which comprises the same departments as the original Royal College of Science.

Departments
The faculty includes five academic departments:

Chemistry
Mathematics
Physics, The Blackett Laboratory
Life Sciences
Centre for Environmental Policy

References
External links
Imperial College Faculty of Natural Sciences website."
What were the faculties that originally took over the role of the Royal College of Science?,Physical Sciences and Life Sciences,Chemistry and Mathematics,Centre for Environmental Policy and Life Sciences,Physics and The Blackett Laboratory,Royal College of Science Union,A,"The Faculty of Natural Sciences is one of the three main faculties of Imperial College London in London, England. It was formed in 2001 from the former Royal College of Science, a constituent college of Imperial College which dated back to 1848, and the faculty largely consists of the original departments of the College. Undergraduate teaching occurs for all departments at the South Kensington campus, with research being split between South Kensington and the new innovation campus at White City.Students who study at the departments of the Faculty are represented by the Royal College of Science Union, a constituent union of the college union which caters specifically to students on natural science courses. Graduates who obtain an undergraduate degree, either BSc or MSci, from the faculty are awarded the Associateship of the Royal College of Science (ARCS) as an additional degree.

History
The origins of the faculty lie in the Royal College of Chemistry, which, after being founded in 1845, moved to a new site in South Kensington in the early 1870s. Incorporated into the Normal School of Science, the college was later renamed the Royal College of Science in 1890, and in 1907 became a constituent college of the newly formed Imperial College of Science and Technology.In 2001, Imperial was restructured to form four new faculties, including the faculties of Physical Sciences and Life Sciences, which took over the role of the Royal College of Science. These faculties were later re-merged over the course of 2005-2006 to form the Faculty of Natural Sciences, which comprises the same departments as the original Royal College of Science.

Departments
The faculty includes five academic departments:

Chemistry
Mathematics
Physics, The Blackett Laboratory
Life Sciences
Centre for Environmental Policy

References
External links
Imperial College Faculty of Natural Sciences website."
What is acoustical engineering?,The branch of engineering dealing with sound and vibration,The branch of engineering dealing with electricity and magnetism,The branch of engineering dealing with chemical reactions,The branch of engineering dealing with computer programming,The branch of engineering dealing with civil structures,A,"Acoustical engineering (also known as acoustic engineering) is the branch of engineering dealing with sound and vibration. It includes the application of acoustics, the science of sound and vibration, in technology. Acoustical engineers are typically concerned with the design, analysis and control of sound.
One goal of acoustical engineering can be the reduction of unwanted noise, which is referred to as noise control. Unwanted noise can have significant impacts on animal and human health and well-being, reduce attainment by students in schools, and cause hearing loss. Noise control principles are implemented into technology and design in a variety of ways, including control by redesigning sound sources, the design of noise barriers, sound absorbers, suppressors, and buffer zones, and the use of hearing protection (earmuffs or earplugs).

Besides noise control, acoustical engineering also covers positive uses of sound, such as the use of ultrasound in medicine, programming digital synthesizers, designing concert halls to enhance the sound of orchestras and specifying railway station sound systems so that announcements are intelligible.

Acoustic engineer (professional)
Acoustic engineers usually possess a bachelor's degree or higher qualification in acoustics, physics or another engineering discipline. Practicing as an acoustic engineer usually requires a bachelor's degree with significant scientific and mathematical content. Acoustic engineers might work in acoustic consultancy, specializing in particular fields, such as architectural acoustics, environmental noise or vibration control."
What is one goal of acoustical engineering?,To reduce the impact of noise on animal and human health,To increase noise levels in schools,To cause hearing loss in individuals,To design sound sources with high noise levels,To eliminate the need for hearing protection,A,"Acoustical engineering (also known as acoustic engineering) is the branch of engineering dealing with sound and vibration. It includes the application of acoustics, the science of sound and vibration, in technology. Acoustical engineers are typically concerned with the design, analysis and control of sound.
One goal of acoustical engineering can be the reduction of unwanted noise, which is referred to as noise control. Unwanted noise can have significant impacts on animal and human health and well-being, reduce attainment by students in schools, and cause hearing loss. Noise control principles are implemented into technology and design in a variety of ways, including control by redesigning sound sources, the design of noise barriers, sound absorbers, suppressors, and buffer zones, and the use of hearing protection (earmuffs or earplugs).

Besides noise control, acoustical engineering also covers positive uses of sound, such as the use of ultrasound in medicine, programming digital synthesizers, designing concert halls to enhance the sound of orchestras and specifying railway station sound systems so that announcements are intelligible.

Acoustic engineer (professional)
Acoustic engineers usually possess a bachelor's degree or higher qualification in acoustics, physics or another engineering discipline. Practicing as an acoustic engineer usually requires a bachelor's degree with significant scientific and mathematical content. Acoustic engineers might work in acoustic consultancy, specializing in particular fields, such as architectural acoustics, environmental noise or vibration control."
How are noise control principles implemented in technology and design?,By redesigning sound sources,By eliminating the need for sound barriers,By increasing the use of hearing protection,By minimizing the use of sound absorbers,By reducing the effectiveness of buffer zones,A,"Acoustical engineering (also known as acoustic engineering) is the branch of engineering dealing with sound and vibration. It includes the application of acoustics, the science of sound and vibration, in technology. Acoustical engineers are typically concerned with the design, analysis and control of sound.
One goal of acoustical engineering can be the reduction of unwanted noise, which is referred to as noise control. Unwanted noise can have significant impacts on animal and human health and well-being, reduce attainment by students in schools, and cause hearing loss. Noise control principles are implemented into technology and design in a variety of ways, including control by redesigning sound sources, the design of noise barriers, sound absorbers, suppressors, and buffer zones, and the use of hearing protection (earmuffs or earplugs).

Besides noise control, acoustical engineering also covers positive uses of sound, such as the use of ultrasound in medicine, programming digital synthesizers, designing concert halls to enhance the sound of orchestras and specifying railway station sound systems so that announcements are intelligible.

Acoustic engineer (professional)
Acoustic engineers usually possess a bachelor's degree or higher qualification in acoustics, physics or another engineering discipline. Practicing as an acoustic engineer usually requires a bachelor's degree with significant scientific and mathematical content. Acoustic engineers might work in acoustic consultancy, specializing in particular fields, such as architectural acoustics, environmental noise or vibration control."
What are some positive uses of sound covered by acoustical engineering?,The use of sound in cooking,The use of sound in transportation,The use of sound in architecture,The use of sound in communication,The use of sound in gardening,D,"Acoustical engineering (also known as acoustic engineering) is the branch of engineering dealing with sound and vibration. It includes the application of acoustics, the science of sound and vibration, in technology. Acoustical engineers are typically concerned with the design, analysis and control of sound.
One goal of acoustical engineering can be the reduction of unwanted noise, which is referred to as noise control. Unwanted noise can have significant impacts on animal and human health and well-being, reduce attainment by students in schools, and cause hearing loss. Noise control principles are implemented into technology and design in a variety of ways, including control by redesigning sound sources, the design of noise barriers, sound absorbers, suppressors, and buffer zones, and the use of hearing protection (earmuffs or earplugs).

Besides noise control, acoustical engineering also covers positive uses of sound, such as the use of ultrasound in medicine, programming digital synthesizers, designing concert halls to enhance the sound of orchestras and specifying railway station sound systems so that announcements are intelligible.

Acoustic engineer (professional)
Acoustic engineers usually possess a bachelor's degree or higher qualification in acoustics, physics or another engineering discipline. Practicing as an acoustic engineer usually requires a bachelor's degree with significant scientific and mathematical content. Acoustic engineers might work in acoustic consultancy, specializing in particular fields, such as architectural acoustics, environmental noise or vibration control."
What qualifications are typically required for practicing as an acoustic engineer?,"Bachelor's degree in acoustics, physics or another engineering discipline",Bachelor's degree in computer programming,Bachelor's degree in chemistry,Bachelor's degree in civil engineering,Bachelor's degree in electrical engineering,A,"Acoustical engineering (also known as acoustic engineering) is the branch of engineering dealing with sound and vibration. It includes the application of acoustics, the science of sound and vibration, in technology. Acoustical engineers are typically concerned with the design, analysis and control of sound.
One goal of acoustical engineering can be the reduction of unwanted noise, which is referred to as noise control. Unwanted noise can have significant impacts on animal and human health and well-being, reduce attainment by students in schools, and cause hearing loss. Noise control principles are implemented into technology and design in a variety of ways, including control by redesigning sound sources, the design of noise barriers, sound absorbers, suppressors, and buffer zones, and the use of hearing protection (earmuffs or earplugs).

Besides noise control, acoustical engineering also covers positive uses of sound, such as the use of ultrasound in medicine, programming digital synthesizers, designing concert halls to enhance the sound of orchestras and specifying railway station sound systems so that announcements are intelligible.

Acoustic engineer (professional)
Acoustic engineers usually possess a bachelor's degree or higher qualification in acoustics, physics or another engineering discipline. Practicing as an acoustic engineer usually requires a bachelor's degree with significant scientific and mathematical content. Acoustic engineers might work in acoustic consultancy, specializing in particular fields, such as architectural acoustics, environmental noise or vibration control."
What is a Schlenk flask?,A reaction vessel used in air-sensitive chemistry,A type of glassware used in cooking,A device used to measure pressure,A type of glassware used in distillation,A device used to measure temperature,A,"A Schlenk flask, or Schlenk tube, is a reaction vessel typically used in air-sensitive chemistry, invented by Wilhelm Schlenk. It has a side arm fitted with a PTFE or ground glass stopcock, which allows the vessel to be evacuated or filled with gases (usually inert gases like nitrogen or argon). These flasks are often connected to Schlenk lines, which allow both operations to be done easily.
Schlenk flasks and Schlenk tubes, like most laboratory glassware, are made from borosilicate glass such as Pyrex.
Schlenk flasks are round-bottomed, while Schlenk tubes are elongated. They may be purchased off-the-shelf from laboratory suppliers or made from round-bottom flasks or glass tubing by a skilled glassblower.

Evacuating a Schlenk flask
Typically, before solvent or reagents are introduced into a Schlenk flask, the flask is dried and the atmosphere of the flask is exchanged with an inert gas.  A common method of exchanging the atmosphere of the flask is to flush the flask out with an inert gas. The gas can be introduced through the sidearm of the flask, or via a wide bore needle (attached to a gas line).  The contents of the flask exit the flask through the neck portion of the flask."
Who invented the Schlenk flask?,Wilhelm Schlenk,Friedrich Schlenk,Otto Schlenk,Hermann Schlenk,Ernst Schlenk,A,"A Schlenk flask, or Schlenk tube, is a reaction vessel typically used in air-sensitive chemistry, invented by Wilhelm Schlenk. It has a side arm fitted with a PTFE or ground glass stopcock, which allows the vessel to be evacuated or filled with gases (usually inert gases like nitrogen or argon). These flasks are often connected to Schlenk lines, which allow both operations to be done easily.
Schlenk flasks and Schlenk tubes, like most laboratory glassware, are made from borosilicate glass such as Pyrex.
Schlenk flasks are round-bottomed, while Schlenk tubes are elongated. They may be purchased off-the-shelf from laboratory suppliers or made from round-bottom flasks or glass tubing by a skilled glassblower.

Evacuating a Schlenk flask
Typically, before solvent or reagents are introduced into a Schlenk flask, the flask is dried and the atmosphere of the flask is exchanged with an inert gas.  A common method of exchanging the atmosphere of the flask is to flush the flask out with an inert gas. The gas can be introduced through the sidearm of the flask, or via a wide bore needle (attached to a gas line).  The contents of the flask exit the flask through the neck portion of the flask."
What is the purpose of the PTFE or ground glass stopcock?,To allow the vessel to be evacuated or filled with gases,To measure the volume of the flask,To heat the contents of the flask,To stir the contents of the flask,To measure the mass of the flask,A,"A Schlenk flask, or Schlenk tube, is a reaction vessel typically used in air-sensitive chemistry, invented by Wilhelm Schlenk. It has a side arm fitted with a PTFE or ground glass stopcock, which allows the vessel to be evacuated or filled with gases (usually inert gases like nitrogen or argon). These flasks are often connected to Schlenk lines, which allow both operations to be done easily.
Schlenk flasks and Schlenk tubes, like most laboratory glassware, are made from borosilicate glass such as Pyrex.
Schlenk flasks are round-bottomed, while Schlenk tubes are elongated. They may be purchased off-the-shelf from laboratory suppliers or made from round-bottom flasks or glass tubing by a skilled glassblower.

Evacuating a Schlenk flask
Typically, before solvent or reagents are introduced into a Schlenk flask, the flask is dried and the atmosphere of the flask is exchanged with an inert gas.  A common method of exchanging the atmosphere of the flask is to flush the flask out with an inert gas. The gas can be introduced through the sidearm of the flask, or via a wide bore needle (attached to a gas line).  The contents of the flask exit the flask through the neck portion of the flask."
What type of glass are Schlenk flasks and Schlenk tubes typically made from?,Borosilicate glass,Soda-lime glass,Quartz glass,Lead glass,Aluminum glass,A,"A Schlenk flask, or Schlenk tube, is a reaction vessel typically used in air-sensitive chemistry, invented by Wilhelm Schlenk. It has a side arm fitted with a PTFE or ground glass stopcock, which allows the vessel to be evacuated or filled with gases (usually inert gases like nitrogen or argon). These flasks are often connected to Schlenk lines, which allow both operations to be done easily.
Schlenk flasks and Schlenk tubes, like most laboratory glassware, are made from borosilicate glass such as Pyrex.
Schlenk flasks are round-bottomed, while Schlenk tubes are elongated. They may be purchased off-the-shelf from laboratory suppliers or made from round-bottom flasks or glass tubing by a skilled glassblower.

Evacuating a Schlenk flask
Typically, before solvent or reagents are introduced into a Schlenk flask, the flask is dried and the atmosphere of the flask is exchanged with an inert gas.  A common method of exchanging the atmosphere of the flask is to flush the flask out with an inert gas. The gas can be introduced through the sidearm of the flask, or via a wide bore needle (attached to a gas line).  The contents of the flask exit the flask through the neck portion of the flask."
How is the atmosphere inside a Schlenk flask exchanged with an inert gas?,By flushing the flask out with an inert gas,By shaking the flask vigorously,By heating the flask to a high temperature,By adding liquid nitrogen to the flask,By attaching a gas line to the sidearm of the flask,A,"A Schlenk flask, or Schlenk tube, is a reaction vessel typically used in air-sensitive chemistry, invented by Wilhelm Schlenk. It has a side arm fitted with a PTFE or ground glass stopcock, which allows the vessel to be evacuated or filled with gases (usually inert gases like nitrogen or argon). These flasks are often connected to Schlenk lines, which allow both operations to be done easily.
Schlenk flasks and Schlenk tubes, like most laboratory glassware, are made from borosilicate glass such as Pyrex.
Schlenk flasks are round-bottomed, while Schlenk tubes are elongated. They may be purchased off-the-shelf from laboratory suppliers or made from round-bottom flasks or glass tubing by a skilled glassblower.

Evacuating a Schlenk flask
Typically, before solvent or reagents are introduced into a Schlenk flask, the flask is dried and the atmosphere of the flask is exchanged with an inert gas.  A common method of exchanging the atmosphere of the flask is to flush the flask out with an inert gas. The gas can be introduced through the sidearm of the flask, or via a wide bore needle (attached to a gas line).  The contents of the flask exit the flask through the neck portion of the flask."
When was STAAD.Pro originally developed?,1995,1997,2005,2010,2013,B,"STAAD or (STAAD.Pro) is a structural analysis and design software application originally developed by Research Engineers International in 1997. In late 2005, Research Engineers International was bought by Bentley Systems. STAAD stands for STructural Analysis And Design.STAAD.Pro is one of the most widely used structural analysis and design software products worldwide. It can apply more than 90 international steel, concrete, timber and aluminium design codes.
It can make use of various forms of analysis from the traditional static analysis to more recent analysis methods like p-delta analysis, geometric non-linear analysis, Pushover analysis (Static-Non Linear Analysis) or a buckling analysis. It can also make use of various forms of dynamic analysis methods from time history analysis to response spectrum analysis. The response spectrum analysis feature is supported for both user defined spectra as well as a number of international code specified spectra.
Additionally, STAAD.Pro is interoperable with applications such as RAM Connection, AutoPIPE, SACS and many more engineering design and analysis applications to further improve collaboration between the different disciplines involved in a project. STAAD can be used for analysis and design of all types of structural projects from plants, buildings, and bridges to towers, tunnels, metro stations, water/wastewater treatment plants and more.

Important Features
Analytical Modeling
Analytical model can be created using the ribbon-based user interface, by editing the command file or by importing several other files types like dxf, cis/2 etc."
What does STAAD stand for?,Structural Analysis And Design,Software Technology for Analysis And Design,Systems for Testing Analysis And Design,Software Techniques for Analyzing And Designing,Structural Testing And Design,A,"STAAD or (STAAD.Pro) is a structural analysis and design software application originally developed by Research Engineers International in 1997. In late 2005, Research Engineers International was bought by Bentley Systems. STAAD stands for STructural Analysis And Design.STAAD.Pro is one of the most widely used structural analysis and design software products worldwide. It can apply more than 90 international steel, concrete, timber and aluminium design codes.
It can make use of various forms of analysis from the traditional static analysis to more recent analysis methods like p-delta analysis, geometric non-linear analysis, Pushover analysis (Static-Non Linear Analysis) or a buckling analysis. It can also make use of various forms of dynamic analysis methods from time history analysis to response spectrum analysis. The response spectrum analysis feature is supported for both user defined spectra as well as a number of international code specified spectra.
Additionally, STAAD.Pro is interoperable with applications such as RAM Connection, AutoPIPE, SACS and many more engineering design and analysis applications to further improve collaboration between the different disciplines involved in a project. STAAD can be used for analysis and design of all types of structural projects from plants, buildings, and bridges to towers, tunnels, metro stations, water/wastewater treatment plants and more.

Important Features
Analytical Modeling
Analytical model can be created using the ribbon-based user interface, by editing the command file or by importing several other files types like dxf, cis/2 etc."
Which of the following analysis methods can STAAD.Pro utilize?,Static analysis only,Dynamic analysis only,Geometric non-linear analysis only,All of the above,None of the above,D,"STAAD or (STAAD.Pro) is a structural analysis and design software application originally developed by Research Engineers International in 1997. In late 2005, Research Engineers International was bought by Bentley Systems. STAAD stands for STructural Analysis And Design.STAAD.Pro is one of the most widely used structural analysis and design software products worldwide. It can apply more than 90 international steel, concrete, timber and aluminium design codes.
It can make use of various forms of analysis from the traditional static analysis to more recent analysis methods like p-delta analysis, geometric non-linear analysis, Pushover analysis (Static-Non Linear Analysis) or a buckling analysis. It can also make use of various forms of dynamic analysis methods from time history analysis to response spectrum analysis. The response spectrum analysis feature is supported for both user defined spectra as well as a number of international code specified spectra.
Additionally, STAAD.Pro is interoperable with applications such as RAM Connection, AutoPIPE, SACS and many more engineering design and analysis applications to further improve collaboration between the different disciplines involved in a project. STAAD can be used for analysis and design of all types of structural projects from plants, buildings, and bridges to towers, tunnels, metro stations, water/wastewater treatment plants and more.

Important Features
Analytical Modeling
Analytical model can be created using the ribbon-based user interface, by editing the command file or by importing several other files types like dxf, cis/2 etc."
What types of structural projects can STAAD be used for?,Only buildings and bridges,Only towers and tunnels,Only water/wastewater treatment plants,All of the above,None of the above,D,"STAAD or (STAAD.Pro) is a structural analysis and design software application originally developed by Research Engineers International in 1997. In late 2005, Research Engineers International was bought by Bentley Systems. STAAD stands for STructural Analysis And Design.STAAD.Pro is one of the most widely used structural analysis and design software products worldwide. It can apply more than 90 international steel, concrete, timber and aluminium design codes.
It can make use of various forms of analysis from the traditional static analysis to more recent analysis methods like p-delta analysis, geometric non-linear analysis, Pushover analysis (Static-Non Linear Analysis) or a buckling analysis. It can also make use of various forms of dynamic analysis methods from time history analysis to response spectrum analysis. The response spectrum analysis feature is supported for both user defined spectra as well as a number of international code specified spectra.
Additionally, STAAD.Pro is interoperable with applications such as RAM Connection, AutoPIPE, SACS and many more engineering design and analysis applications to further improve collaboration between the different disciplines involved in a project. STAAD can be used for analysis and design of all types of structural projects from plants, buildings, and bridges to towers, tunnels, metro stations, water/wastewater treatment plants and more.

Important Features
Analytical Modeling
Analytical model can be created using the ribbon-based user interface, by editing the command file or by importing several other files types like dxf, cis/2 etc."
Which of the following applications is STAAD.Pro interoperable with?,RAM Connection,AutoPIPE,SACS,All of the above,None of the above,D,"STAAD or (STAAD.Pro) is a structural analysis and design software application originally developed by Research Engineers International in 1997. In late 2005, Research Engineers International was bought by Bentley Systems. STAAD stands for STructural Analysis And Design.STAAD.Pro is one of the most widely used structural analysis and design software products worldwide. It can apply more than 90 international steel, concrete, timber and aluminium design codes.
It can make use of various forms of analysis from the traditional static analysis to more recent analysis methods like p-delta analysis, geometric non-linear analysis, Pushover analysis (Static-Non Linear Analysis) or a buckling analysis. It can also make use of various forms of dynamic analysis methods from time history analysis to response spectrum analysis. The response spectrum analysis feature is supported for both user defined spectra as well as a number of international code specified spectra.
Additionally, STAAD.Pro is interoperable with applications such as RAM Connection, AutoPIPE, SACS and many more engineering design and analysis applications to further improve collaboration between the different disciplines involved in a project. STAAD can be used for analysis and design of all types of structural projects from plants, buildings, and bridges to towers, tunnels, metro stations, water/wastewater treatment plants and more.

Important Features
Analytical Modeling
Analytical model can be created using the ribbon-based user interface, by editing the command file or by importing several other files types like dxf, cis/2 etc."
What is export control?,"Legislation that regulates the import of goods, software, and technology","Legislation that regulates the export of goods, software, and technology","Legislation that regulates the transportation of goods, software, and technology","Legislation that regulates the production of goods, software, and technology","Legislation that regulates the distribution of goods, software, and technology",B,"Export control is legislation that regulates the export of goods, software and technology. Some items could potentially be useful for purposes that are contrary to the interest of the exporting country. These items are considered to be controlled. The export of controlled item is  regulated to restrict the harmful use of those items. Many governments implement export controls. Typically, legislation lists and classifies the controlled items, classifies the destinations, and requires exporters to apply for a licence to a local government department.
A wide range of goods have been subject to export control in different jurisdictions, including arms, goods with a military potential, cryptography, currency, and precious stones or metals. Some countries prohibit the export of uranium, endangered animals, cultural artefacts, and goods in short supply in the country, such as medicines.

History
The United States has had export controls since the American Revolution, although the modern export control regimes can be traced back to the Trading with the Enemy Act of 1917."
Why are some items considered to be controlled?,Because they are harmful to the exporting country,Because they are useful for purposes contrary to the interest of the exporting country,Because they have a high commercial value,Because they are in short supply in the exporting country,Because they are prohibited by international treaties,B,"Export control is legislation that regulates the export of goods, software and technology. Some items could potentially be useful for purposes that are contrary to the interest of the exporting country. These items are considered to be controlled. The export of controlled item is  regulated to restrict the harmful use of those items. Many governments implement export controls. Typically, legislation lists and classifies the controlled items, classifies the destinations, and requires exporters to apply for a licence to a local government department.
A wide range of goods have been subject to export control in different jurisdictions, including arms, goods with a military potential, cryptography, currency, and precious stones or metals. Some countries prohibit the export of uranium, endangered animals, cultural artefacts, and goods in short supply in the country, such as medicines.

History
The United States has had export controls since the American Revolution, although the modern export control regimes can be traced back to the Trading with the Enemy Act of 1917."
What do export controls typically require exporters to do?,Apply for a license to a local government department,Pay a fee to the exporting country,Provide documentation of importers in the destination country,Apply for a license to an international governing body,Register their export activities with the local government department,A,"Export control is legislation that regulates the export of goods, software and technology. Some items could potentially be useful for purposes that are contrary to the interest of the exporting country. These items are considered to be controlled. The export of controlled item is  regulated to restrict the harmful use of those items. Many governments implement export controls. Typically, legislation lists and classifies the controlled items, classifies the destinations, and requires exporters to apply for a licence to a local government department.
A wide range of goods have been subject to export control in different jurisdictions, including arms, goods with a military potential, cryptography, currency, and precious stones or metals. Some countries prohibit the export of uranium, endangered animals, cultural artefacts, and goods in short supply in the country, such as medicines.

History
The United States has had export controls since the American Revolution, although the modern export control regimes can be traced back to the Trading with the Enemy Act of 1917."
Which of the following goods have been subject to export control?,Food and agricultural products,Electronics and consumer goods,Arms and goods with a military potential,Art and cultural artifacts,Pharmaceuticals and medical equipment,C,"Export control is legislation that regulates the export of goods, software and technology. Some items could potentially be useful for purposes that are contrary to the interest of the exporting country. These items are considered to be controlled. The export of controlled item is  regulated to restrict the harmful use of those items. Many governments implement export controls. Typically, legislation lists and classifies the controlled items, classifies the destinations, and requires exporters to apply for a licence to a local government department.
A wide range of goods have been subject to export control in different jurisdictions, including arms, goods with a military potential, cryptography, currency, and precious stones or metals. Some countries prohibit the export of uranium, endangered animals, cultural artefacts, and goods in short supply in the country, such as medicines.

History
The United States has had export controls since the American Revolution, although the modern export control regimes can be traced back to the Trading with the Enemy Act of 1917."
When did the United States first implement export controls?,During the American Revolution,After World War I,During the Cold War,After the September 11 attacks,During the Vietnam War,A,"Export control is legislation that regulates the export of goods, software and technology. Some items could potentially be useful for purposes that are contrary to the interest of the exporting country. These items are considered to be controlled. The export of controlled item is  regulated to restrict the harmful use of those items. Many governments implement export controls. Typically, legislation lists and classifies the controlled items, classifies the destinations, and requires exporters to apply for a licence to a local government department.
A wide range of goods have been subject to export control in different jurisdictions, including arms, goods with a military potential, cryptography, currency, and precious stones or metals. Some countries prohibit the export of uranium, endangered animals, cultural artefacts, and goods in short supply in the country, such as medicines.

History
The United States has had export controls since the American Revolution, although the modern export control regimes can be traced back to the Trading with the Enemy Act of 1917."
What is field arithmetic?,The study of arithmetic properties of a field,The study of geometric properties of a field,The study of group theory in a field,The study of algebraic number theory in a field,The study of finite groups in a field,A,"In mathematics, field arithmetic is a subject that studies the interrelations between arithmetic properties of a field and its absolute Galois group.
It is an interdisciplinary subject as it uses tools from algebraic number theory, arithmetic geometry, algebraic geometry, model theory, the theory of finite groups and of profinite groups.

Fields with finite absolute Galois groups
Let K be a field and let G = Gal(K) be its absolute Galois group. If K is algebraically closed, then G = 1. If K = R is the real numbers, then

  
    
      
        G
        =
        Gal
        ⁡
        (
        
          C
        
        
          /
        
        
          R
        
        )
        =
        
          Z
        
        
          /
        
        2
        
          Z
        
        .
      
    
    {\displaystyle G=\operatorname {Gal} (\mathbf {C} /\mathbf {R} )=\mathbf {Z} /2\mathbf {Z} .}
  Here C is the field of complex numbers and Z is the ring of integer numbers. 
A theorem of Artin and Schreier asserts that (essentially) these are all the possibilities for finite absolute Galois groups.
Artin–Schreier theorem. Let K be a field whose absolute Galois group G is finite. Then either K is separably closed and G is trivial or K is real closed and G = Z/2Z.

Fields that are defined by their absolute Galois groups
Some profinite groups occur as the absolute Galois group of non-isomorphic fields.  A first example for this is

  
    
      
        
          
            
              
                Z
              
              ^
            
          
        
        =
        
          lim
          
            ⟵
          
        
        
          Z
        
        
          /
        
        n
        
          Z
        
        .
      
    
    {\displaystyle {\hat {\mathbf {Z} }}=\lim _{\longleftarrow }\mathbf {Z} /n\mathbf {Z} .}
  This group is isomorphic to the absolute Galois group of an arbitrary finite field."
Which tools are used in field arithmetic?,Algebraic number theory,Arithmetic geometry,Algebraic geometry,Model theory,The theory of finite groups,C,"In mathematics, field arithmetic is a subject that studies the interrelations between arithmetic properties of a field and its absolute Galois group.
It is an interdisciplinary subject as it uses tools from algebraic number theory, arithmetic geometry, algebraic geometry, model theory, the theory of finite groups and of profinite groups.

Fields with finite absolute Galois groups
Let K be a field and let G = Gal(K) be its absolute Galois group. If K is algebraically closed, then G = 1. If K = R is the real numbers, then

  
    
      
        G
        =
        Gal
        ⁡
        (
        
          C
        
        
          /
        
        
          R
        
        )
        =
        
          Z
        
        
          /
        
        2
        
          Z
        
        .
      
    
    {\displaystyle G=\operatorname {Gal} (\mathbf {C} /\mathbf {R} )=\mathbf {Z} /2\mathbf {Z} .}
  Here C is the field of complex numbers and Z is the ring of integer numbers. 
A theorem of Artin and Schreier asserts that (essentially) these are all the possibilities for finite absolute Galois groups.
Artin–Schreier theorem. Let K be a field whose absolute Galois group G is finite. Then either K is separably closed and G is trivial or K is real closed and G = Z/2Z.

Fields that are defined by their absolute Galois groups
Some profinite groups occur as the absolute Galois group of non-isomorphic fields.  A first example for this is

  
    
      
        
          
            
              
                Z
              
              ^
            
          
        
        =
        
          lim
          
            ⟵
          
        
        
          Z
        
        
          /
        
        n
        
          Z
        
        .
      
    
    {\displaystyle {\hat {\mathbf {Z} }}=\lim _{\longleftarrow }\mathbf {Z} /n\mathbf {Z} .}
  This group is isomorphic to the absolute Galois group of an arbitrary finite field."
What is the absolute Galois group of an algebraically closed field?,1,G,Z,Z/2Z,C,A,"In mathematics, field arithmetic is a subject that studies the interrelations between arithmetic properties of a field and its absolute Galois group.
It is an interdisciplinary subject as it uses tools from algebraic number theory, arithmetic geometry, algebraic geometry, model theory, the theory of finite groups and of profinite groups.

Fields with finite absolute Galois groups
Let K be a field and let G = Gal(K) be its absolute Galois group. If K is algebraically closed, then G = 1. If K = R is the real numbers, then

  
    
      
        G
        =
        Gal
        ⁡
        (
        
          C
        
        
          /
        
        
          R
        
        )
        =
        
          Z
        
        
          /
        
        2
        
          Z
        
        .
      
    
    {\displaystyle G=\operatorname {Gal} (\mathbf {C} /\mathbf {R} )=\mathbf {Z} /2\mathbf {Z} .}
  Here C is the field of complex numbers and Z is the ring of integer numbers. 
A theorem of Artin and Schreier asserts that (essentially) these are all the possibilities for finite absolute Galois groups.
Artin–Schreier theorem. Let K be a field whose absolute Galois group G is finite. Then either K is separably closed and G is trivial or K is real closed and G = Z/2Z.

Fields that are defined by their absolute Galois groups
Some profinite groups occur as the absolute Galois group of non-isomorphic fields.  A first example for this is

  
    
      
        
          
            
              
                Z
              
              ^
            
          
        
        =
        
          lim
          
            ⟵
          
        
        
          Z
        
        
          /
        
        n
        
          Z
        
        .
      
    
    {\displaystyle {\hat {\mathbf {Z} }}=\lim _{\longleftarrow }\mathbf {Z} /n\mathbf {Z} .}
  This group is isomorphic to the absolute Galois group of an arbitrary finite field."
What is the absolute Galois group of the field of real numbers?,1,G,Z,Z/2Z,C,D,"In mathematics, field arithmetic is a subject that studies the interrelations between arithmetic properties of a field and its absolute Galois group.
It is an interdisciplinary subject as it uses tools from algebraic number theory, arithmetic geometry, algebraic geometry, model theory, the theory of finite groups and of profinite groups.

Fields with finite absolute Galois groups
Let K be a field and let G = Gal(K) be its absolute Galois group. If K is algebraically closed, then G = 1. If K = R is the real numbers, then

  
    
      
        G
        =
        Gal
        ⁡
        (
        
          C
        
        
          /
        
        
          R
        
        )
        =
        
          Z
        
        
          /
        
        2
        
          Z
        
        .
      
    
    {\displaystyle G=\operatorname {Gal} (\mathbf {C} /\mathbf {R} )=\mathbf {Z} /2\mathbf {Z} .}
  Here C is the field of complex numbers and Z is the ring of integer numbers. 
A theorem of Artin and Schreier asserts that (essentially) these are all the possibilities for finite absolute Galois groups.
Artin–Schreier theorem. Let K be a field whose absolute Galois group G is finite. Then either K is separably closed and G is trivial or K is real closed and G = Z/2Z.

Fields that are defined by their absolute Galois groups
Some profinite groups occur as the absolute Galois group of non-isomorphic fields.  A first example for this is

  
    
      
        
          
            
              
                Z
              
              ^
            
          
        
        =
        
          lim
          
            ⟵
          
        
        
          Z
        
        
          /
        
        n
        
          Z
        
        .
      
    
    {\displaystyle {\hat {\mathbf {Z} }}=\lim _{\longleftarrow }\mathbf {Z} /n\mathbf {Z} .}
  This group is isomorphic to the absolute Galois group of an arbitrary finite field."
What does the Artin-Schreier theorem state?,K is separably closed and G is trivial,K is real closed and G = Z/2Z,K is algebraically closed and G = 1,G is finite,G is trivial,B,"In mathematics, field arithmetic is a subject that studies the interrelations between arithmetic properties of a field and its absolute Galois group.
It is an interdisciplinary subject as it uses tools from algebraic number theory, arithmetic geometry, algebraic geometry, model theory, the theory of finite groups and of profinite groups.

Fields with finite absolute Galois groups
Let K be a field and let G = Gal(K) be its absolute Galois group. If K is algebraically closed, then G = 1. If K = R is the real numbers, then

  
    
      
        G
        =
        Gal
        ⁡
        (
        
          C
        
        
          /
        
        
          R
        
        )
        =
        
          Z
        
        
          /
        
        2
        
          Z
        
        .
      
    
    {\displaystyle G=\operatorname {Gal} (\mathbf {C} /\mathbf {R} )=\mathbf {Z} /2\mathbf {Z} .}
  Here C is the field of complex numbers and Z is the ring of integer numbers. 
A theorem of Artin and Schreier asserts that (essentially) these are all the possibilities for finite absolute Galois groups.
Artin–Schreier theorem. Let K be a field whose absolute Galois group G is finite. Then either K is separably closed and G is trivial or K is real closed and G = Z/2Z.

Fields that are defined by their absolute Galois groups
Some profinite groups occur as the absolute Galois group of non-isomorphic fields.  A first example for this is

  
    
      
        
          
            
              
                Z
              
              ^
            
          
        
        =
        
          lim
          
            ⟵
          
        
        
          Z
        
        
          /
        
        n
        
          Z
        
        .
      
    
    {\displaystyle {\hat {\mathbf {Z} }}=\lim _{\longleftarrow }\mathbf {Z} /n\mathbf {Z} .}
  This group is isomorphic to the absolute Galois group of an arbitrary finite field."
What is static program analysis?,The analysis of computer programs performed without executing them,The analysis of computer programs performed during their execution,The analysis of computer programs performed by humans,The analysis of computer programs performed on object code,The analysis of computer programs performed by automated tools,A,"In computer science, static program analysis (or static analysis) is the analysis of computer programs performed without executing them, in contrast with dynamic program analysis, which is performed on programs during their execution.The term is usually applied to analysis performed by an automated tool, with human analysis typically being called ""program understanding"", program comprehension, or code review. In the last of these, software inspection and software walkthroughs are also used. In most cases the analysis is performed on some version of a program's source code, and, in other cases, on some form of its object code.

Rationale
The sophistication of the analysis performed by tools varies from those that only consider the behaviour of individual statements and declarations, to those that include the complete source code of a program in their analysis. The uses of the information obtained from the analysis vary from highlighting possible coding errors (e.g., the lint tool) to formal methods that mathematically prove properties about a given program (e.g., its behaviour matches that of its specification).
Software metrics and reverse engineering can be described as forms of static analysis. Deriving software metrics and static analysis are increasingly deployed together, especially in creation of embedded systems, by defining so-called software quality objectives.A growing commercial use of static analysis is in the verification of properties of software used in safety-critical computer systems and
locating potentially vulnerable code. For example, the following industries have identified the use of static code analysis as a means of improving the quality of increasingly sophisticated and complex software:

Medical software: The US Food and Drug Administration (FDA) has identified the use of static analysis for medical devices.
Nuclear software: In the UK the Office for Nuclear Regulation (ONR) recommends the use of static analysis on reactor protection systems.
Aviation software (in combination with dynamic analysis).
Automotive & Machines (functional safety features form an integral part of each automotive product development phase, ISO 26262, section 8).A study in 2012 by VDC Research reported that 28.7% of the embedded software engineers surveyed currently use static analysis tools and 39.7% expect to use them within 2 years.
A study from 2010 found that 60% of the interviewed developers in European research projects made at least use of their basic IDE built-in static analyzers. However, only about 10% employed an additional other (and perhaps more advanced) analysis tool.In the application security industry the name static application security testing (SAST) is also used."
What is the difference between static program analysis and dynamic program analysis?,"Static program analysis is performed without executing the programs, while dynamic program analysis is performed during their execution","Static program analysis is performed during the execution of programs, while dynamic program analysis is performed without executing them","Static program analysis is performed on object code, while dynamic program analysis is performed on source code","Static program analysis is performed by humans, while dynamic program analysis is performed by automated tools","Static program analysis is performed by automated tools, while dynamic program analysis is performed by humans",A,"In computer science, static program analysis (or static analysis) is the analysis of computer programs performed without executing them, in contrast with dynamic program analysis, which is performed on programs during their execution.The term is usually applied to analysis performed by an automated tool, with human analysis typically being called ""program understanding"", program comprehension, or code review. In the last of these, software inspection and software walkthroughs are also used. In most cases the analysis is performed on some version of a program's source code, and, in other cases, on some form of its object code.

Rationale
The sophistication of the analysis performed by tools varies from those that only consider the behaviour of individual statements and declarations, to those that include the complete source code of a program in their analysis. The uses of the information obtained from the analysis vary from highlighting possible coding errors (e.g., the lint tool) to formal methods that mathematically prove properties about a given program (e.g., its behaviour matches that of its specification).
Software metrics and reverse engineering can be described as forms of static analysis. Deriving software metrics and static analysis are increasingly deployed together, especially in creation of embedded systems, by defining so-called software quality objectives.A growing commercial use of static analysis is in the verification of properties of software used in safety-critical computer systems and
locating potentially vulnerable code. For example, the following industries have identified the use of static code analysis as a means of improving the quality of increasingly sophisticated and complex software:

Medical software: The US Food and Drug Administration (FDA) has identified the use of static analysis for medical devices.
Nuclear software: In the UK the Office for Nuclear Regulation (ONR) recommends the use of static analysis on reactor protection systems.
Aviation software (in combination with dynamic analysis).
Automotive & Machines (functional safety features form an integral part of each automotive product development phase, ISO 26262, section 8).A study in 2012 by VDC Research reported that 28.7% of the embedded software engineers surveyed currently use static analysis tools and 39.7% expect to use them within 2 years.
A study from 2010 found that 60% of the interviewed developers in European research projects made at least use of their basic IDE built-in static analyzers. However, only about 10% employed an additional other (and perhaps more advanced) analysis tool.In the application security industry the name static application security testing (SAST) is also used."
What are some uses of static program analysis?,Highlighting possible coding errors,Mathematically proving properties about a given program,Deriving software metrics,Verifying properties of software used in safety-critical computer systems,All of the above,E,"In computer science, static program analysis (or static analysis) is the analysis of computer programs performed without executing them, in contrast with dynamic program analysis, which is performed on programs during their execution.The term is usually applied to analysis performed by an automated tool, with human analysis typically being called ""program understanding"", program comprehension, or code review. In the last of these, software inspection and software walkthroughs are also used. In most cases the analysis is performed on some version of a program's source code, and, in other cases, on some form of its object code.

Rationale
The sophistication of the analysis performed by tools varies from those that only consider the behaviour of individual statements and declarations, to those that include the complete source code of a program in their analysis. The uses of the information obtained from the analysis vary from highlighting possible coding errors (e.g., the lint tool) to formal methods that mathematically prove properties about a given program (e.g., its behaviour matches that of its specification).
Software metrics and reverse engineering can be described as forms of static analysis. Deriving software metrics and static analysis are increasingly deployed together, especially in creation of embedded systems, by defining so-called software quality objectives.A growing commercial use of static analysis is in the verification of properties of software used in safety-critical computer systems and
locating potentially vulnerable code. For example, the following industries have identified the use of static code analysis as a means of improving the quality of increasingly sophisticated and complex software:

Medical software: The US Food and Drug Administration (FDA) has identified the use of static analysis for medical devices.
Nuclear software: In the UK the Office for Nuclear Regulation (ONR) recommends the use of static analysis on reactor protection systems.
Aviation software (in combination with dynamic analysis).
Automotive & Machines (functional safety features form an integral part of each automotive product development phase, ISO 26262, section 8).A study in 2012 by VDC Research reported that 28.7% of the embedded software engineers surveyed currently use static analysis tools and 39.7% expect to use them within 2 years.
A study from 2010 found that 60% of the interviewed developers in European research projects made at least use of their basic IDE built-in static analyzers. However, only about 10% employed an additional other (and perhaps more advanced) analysis tool.In the application security industry the name static application security testing (SAST) is also used."
Which industry has identified the use of static code analysis for improving the quality of software?,Medical software,Nuclear software,Aviation software,Automotive & Machines,All of the above,E,"In computer science, static program analysis (or static analysis) is the analysis of computer programs performed without executing them, in contrast with dynamic program analysis, which is performed on programs during their execution.The term is usually applied to analysis performed by an automated tool, with human analysis typically being called ""program understanding"", program comprehension, or code review. In the last of these, software inspection and software walkthroughs are also used. In most cases the analysis is performed on some version of a program's source code, and, in other cases, on some form of its object code.

Rationale
The sophistication of the analysis performed by tools varies from those that only consider the behaviour of individual statements and declarations, to those that include the complete source code of a program in their analysis. The uses of the information obtained from the analysis vary from highlighting possible coding errors (e.g., the lint tool) to formal methods that mathematically prove properties about a given program (e.g., its behaviour matches that of its specification).
Software metrics and reverse engineering can be described as forms of static analysis. Deriving software metrics and static analysis are increasingly deployed together, especially in creation of embedded systems, by defining so-called software quality objectives.A growing commercial use of static analysis is in the verification of properties of software used in safety-critical computer systems and
locating potentially vulnerable code. For example, the following industries have identified the use of static code analysis as a means of improving the quality of increasingly sophisticated and complex software:

Medical software: The US Food and Drug Administration (FDA) has identified the use of static analysis for medical devices.
Nuclear software: In the UK the Office for Nuclear Regulation (ONR) recommends the use of static analysis on reactor protection systems.
Aviation software (in combination with dynamic analysis).
Automotive & Machines (functional safety features form an integral part of each automotive product development phase, ISO 26262, section 8).A study in 2012 by VDC Research reported that 28.7% of the embedded software engineers surveyed currently use static analysis tools and 39.7% expect to use them within 2 years.
A study from 2010 found that 60% of the interviewed developers in European research projects made at least use of their basic IDE built-in static analyzers. However, only about 10% employed an additional other (and perhaps more advanced) analysis tool.In the application security industry the name static application security testing (SAST) is also used."
What percentage of embedded software engineers currently use static analysis tools?,28.7%,39.7%,60%,10%,None of the above,A,"In computer science, static program analysis (or static analysis) is the analysis of computer programs performed without executing them, in contrast with dynamic program analysis, which is performed on programs during their execution.The term is usually applied to analysis performed by an automated tool, with human analysis typically being called ""program understanding"", program comprehension, or code review. In the last of these, software inspection and software walkthroughs are also used. In most cases the analysis is performed on some version of a program's source code, and, in other cases, on some form of its object code.

Rationale
The sophistication of the analysis performed by tools varies from those that only consider the behaviour of individual statements and declarations, to those that include the complete source code of a program in their analysis. The uses of the information obtained from the analysis vary from highlighting possible coding errors (e.g., the lint tool) to formal methods that mathematically prove properties about a given program (e.g., its behaviour matches that of its specification).
Software metrics and reverse engineering can be described as forms of static analysis. Deriving software metrics and static analysis are increasingly deployed together, especially in creation of embedded systems, by defining so-called software quality objectives.A growing commercial use of static analysis is in the verification of properties of software used in safety-critical computer systems and
locating potentially vulnerable code. For example, the following industries have identified the use of static code analysis as a means of improving the quality of increasingly sophisticated and complex software:

Medical software: The US Food and Drug Administration (FDA) has identified the use of static analysis for medical devices.
Nuclear software: In the UK the Office for Nuclear Regulation (ONR) recommends the use of static analysis on reactor protection systems.
Aviation software (in combination with dynamic analysis).
Automotive & Machines (functional safety features form an integral part of each automotive product development phase, ISO 26262, section 8).A study in 2012 by VDC Research reported that 28.7% of the embedded software engineers surveyed currently use static analysis tools and 39.7% expect to use them within 2 years.
A study from 2010 found that 60% of the interviewed developers in European research projects made at least use of their basic IDE built-in static analyzers. However, only about 10% employed an additional other (and perhaps more advanced) analysis tool.In the application security industry the name static application security testing (SAST) is also used."
What is HACCP?,A system to inspect finished products for hazards,A preventive approach to food safety,A method to reduce the risks of food poisoning,A process to monitor food production and preparation,A program for food packaging and distribution,B,"Hazard analysis and critical control points, or HACCP (), is a systematic preventive approach to food safety from biological, chemical, and physical hazards in production processes that can cause the finished product to be unsafe and designs measures to reduce these risks to a safe level. In this manner, HACCP attempts to avoid hazards rather than attempting to inspect finished products for the effects of those hazards. The HACCP system can be used at all stages of a food chain, from food production and preparation processes including packaging, distribution, etc. The Food and Drug Administration (FDA) and the United States Department of Agriculture (USDA) require mandatory HACCP programs for juice and meat as an effective approach to food safety and protecting public health. Meat HACCP systems are regulated by the USDA, while seafood and juice are regulated by the FDA. All other food companies in the United States that are required to register with the FDA under the Public Health Security and Bioterrorism Preparedness and Response Act of 2002, as well as firms outside the US that export food to the US, are transitioning to mandatory hazard analysis and risk-based preventive controls (HARPC) plans.It is believed to stem from a production process monitoring used during World War II because traditional ""end of the pipe"" testing on artillery shells' firing mechanisms could not be performed, and a large percentage of the artillery shells made at the time were either duds or misfiring. HACCP itself was conceived in the 1960s when the US National Aeronautics and Space Administration (NASA) asked Pillsbury to design and manufacture the first foods for space flights."
Which government organizations require mandatory HACCP programs?,FDA and USDA,USDA and HARPC,FDA and HARPC,NASA and USDA,FDA and NASA,A,"Hazard analysis and critical control points, or HACCP (), is a systematic preventive approach to food safety from biological, chemical, and physical hazards in production processes that can cause the finished product to be unsafe and designs measures to reduce these risks to a safe level. In this manner, HACCP attempts to avoid hazards rather than attempting to inspect finished products for the effects of those hazards. The HACCP system can be used at all stages of a food chain, from food production and preparation processes including packaging, distribution, etc. The Food and Drug Administration (FDA) and the United States Department of Agriculture (USDA) require mandatory HACCP programs for juice and meat as an effective approach to food safety and protecting public health. Meat HACCP systems are regulated by the USDA, while seafood and juice are regulated by the FDA. All other food companies in the United States that are required to register with the FDA under the Public Health Security and Bioterrorism Preparedness and Response Act of 2002, as well as firms outside the US that export food to the US, are transitioning to mandatory hazard analysis and risk-based preventive controls (HARPC) plans.It is believed to stem from a production process monitoring used during World War II because traditional ""end of the pipe"" testing on artillery shells' firing mechanisms could not be performed, and a large percentage of the artillery shells made at the time were either duds or misfiring. HACCP itself was conceived in the 1960s when the US National Aeronautics and Space Administration (NASA) asked Pillsbury to design and manufacture the first foods for space flights."
Why was HACCP developed?,To improve the quality of artillery shells in World War II,To design foods for space flights,To prevent food poisoning,To inspect finished products for hazards,To regulate food packaging and distribution,B,"Hazard analysis and critical control points, or HACCP (), is a systematic preventive approach to food safety from biological, chemical, and physical hazards in production processes that can cause the finished product to be unsafe and designs measures to reduce these risks to a safe level. In this manner, HACCP attempts to avoid hazards rather than attempting to inspect finished products for the effects of those hazards. The HACCP system can be used at all stages of a food chain, from food production and preparation processes including packaging, distribution, etc. The Food and Drug Administration (FDA) and the United States Department of Agriculture (USDA) require mandatory HACCP programs for juice and meat as an effective approach to food safety and protecting public health. Meat HACCP systems are regulated by the USDA, while seafood and juice are regulated by the FDA. All other food companies in the United States that are required to register with the FDA under the Public Health Security and Bioterrorism Preparedness and Response Act of 2002, as well as firms outside the US that export food to the US, are transitioning to mandatory hazard analysis and risk-based preventive controls (HARPC) plans.It is believed to stem from a production process monitoring used during World War II because traditional ""end of the pipe"" testing on artillery shells' firing mechanisms could not be performed, and a large percentage of the artillery shells made at the time were either duds or misfiring. HACCP itself was conceived in the 1960s when the US National Aeronautics and Space Administration (NASA) asked Pillsbury to design and manufacture the first foods for space flights."
Who regulates HACCP systems for meat?,FDA,USDA,NASA,HARPC,Food companies,B,"Hazard analysis and critical control points, or HACCP (), is a systematic preventive approach to food safety from biological, chemical, and physical hazards in production processes that can cause the finished product to be unsafe and designs measures to reduce these risks to a safe level. In this manner, HACCP attempts to avoid hazards rather than attempting to inspect finished products for the effects of those hazards. The HACCP system can be used at all stages of a food chain, from food production and preparation processes including packaging, distribution, etc. The Food and Drug Administration (FDA) and the United States Department of Agriculture (USDA) require mandatory HACCP programs for juice and meat as an effective approach to food safety and protecting public health. Meat HACCP systems are regulated by the USDA, while seafood and juice are regulated by the FDA. All other food companies in the United States that are required to register with the FDA under the Public Health Security and Bioterrorism Preparedness and Response Act of 2002, as well as firms outside the US that export food to the US, are transitioning to mandatory hazard analysis and risk-based preventive controls (HARPC) plans.It is believed to stem from a production process monitoring used during World War II because traditional ""end of the pipe"" testing on artillery shells' firing mechanisms could not be performed, and a large percentage of the artillery shells made at the time were either duds or misfiring. HACCP itself was conceived in the 1960s when the US National Aeronautics and Space Administration (NASA) asked Pillsbury to design and manufacture the first foods for space flights."
What are food companies transitioning to?,HACCP systems,FDA regulations,Meat production,HARPC plans,USDA regulations,D,"Hazard analysis and critical control points, or HACCP (), is a systematic preventive approach to food safety from biological, chemical, and physical hazards in production processes that can cause the finished product to be unsafe and designs measures to reduce these risks to a safe level. In this manner, HACCP attempts to avoid hazards rather than attempting to inspect finished products for the effects of those hazards. The HACCP system can be used at all stages of a food chain, from food production and preparation processes including packaging, distribution, etc. The Food and Drug Administration (FDA) and the United States Department of Agriculture (USDA) require mandatory HACCP programs for juice and meat as an effective approach to food safety and protecting public health. Meat HACCP systems are regulated by the USDA, while seafood and juice are regulated by the FDA. All other food companies in the United States that are required to register with the FDA under the Public Health Security and Bioterrorism Preparedness and Response Act of 2002, as well as firms outside the US that export food to the US, are transitioning to mandatory hazard analysis and risk-based preventive controls (HARPC) plans.It is believed to stem from a production process monitoring used during World War II because traditional ""end of the pipe"" testing on artillery shells' firing mechanisms could not be performed, and a large percentage of the artillery shells made at the time were either duds or misfiring. HACCP itself was conceived in the 1960s when the US National Aeronautics and Space Administration (NASA) asked Pillsbury to design and manufacture the first foods for space flights."
What is the main objective of operations research?,To improve decision-making in industries,To develop analytical methods,To determine extreme values of real-world objectives,To overlap with other disciplines,To emphasize practical applications,C,"Operations research (British English: operational research) (U.S. Air Force Specialty Code: Operations Analysis), often shortened to the initialism OR, is a discipline that deals with the development and application of analytical methods to improve decision-making. The term management science is occasionally used as a synonym.Employing techniques from other mathematical sciences, such as modeling, statistics, and optimization, operations research arrives at optimal or near-optimal solutions to decision-making problems. Because of its emphasis on practical applications, operations research has overlapped with many other disciplines, notably industrial engineering. Operations research is often concerned with determining the extreme values of some real-world objective: the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost). Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.

Overview
Operational research (OR) encompasses the development and the use of a wide range of problem-solving techniques and methods applied in the pursuit of improved decision-making and efficiency, such as simulation, mathematical optimization, queueing theory and other stochastic-process models, Markov decision processes, econometric methods, data envelopment analysis, ordinal priority approach, neural networks, expert systems, decision analysis, and the analytic hierarchy process. Nearly all of these techniques involve the construction of mathematical models that attempt to describe the system."
Which technique is NOT used in operations research?,Simulation,Mathematical optimization,Queueing theory,Statistical analysis,Neural networks,D,"Operations research (British English: operational research) (U.S. Air Force Specialty Code: Operations Analysis), often shortened to the initialism OR, is a discipline that deals with the development and application of analytical methods to improve decision-making. The term management science is occasionally used as a synonym.Employing techniques from other mathematical sciences, such as modeling, statistics, and optimization, operations research arrives at optimal or near-optimal solutions to decision-making problems. Because of its emphasis on practical applications, operations research has overlapped with many other disciplines, notably industrial engineering. Operations research is often concerned with determining the extreme values of some real-world objective: the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost). Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.

Overview
Operational research (OR) encompasses the development and the use of a wide range of problem-solving techniques and methods applied in the pursuit of improved decision-making and efficiency, such as simulation, mathematical optimization, queueing theory and other stochastic-process models, Markov decision processes, econometric methods, data envelopment analysis, ordinal priority approach, neural networks, expert systems, decision analysis, and the analytic hierarchy process. Nearly all of these techniques involve the construction of mathematical models that attempt to describe the system."
What is another term that is occasionally used as a synonym for operations research?,Industrial engineering,Management science,Econometric methods,Markov decision processes,Data envelopment analysis,B,"Operations research (British English: operational research) (U.S. Air Force Specialty Code: Operations Analysis), often shortened to the initialism OR, is a discipline that deals with the development and application of analytical methods to improve decision-making. The term management science is occasionally used as a synonym.Employing techniques from other mathematical sciences, such as modeling, statistics, and optimization, operations research arrives at optimal or near-optimal solutions to decision-making problems. Because of its emphasis on practical applications, operations research has overlapped with many other disciplines, notably industrial engineering. Operations research is often concerned with determining the extreme values of some real-world objective: the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost). Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.

Overview
Operational research (OR) encompasses the development and the use of a wide range of problem-solving techniques and methods applied in the pursuit of improved decision-making and efficiency, such as simulation, mathematical optimization, queueing theory and other stochastic-process models, Markov decision processes, econometric methods, data envelopment analysis, ordinal priority approach, neural networks, expert systems, decision analysis, and the analytic hierarchy process. Nearly all of these techniques involve the construction of mathematical models that attempt to describe the system."
Where did operations research originate from?,Industrial engineering,World War II military efforts,Management science,Data envelopment analysis,Econometric methods,B,"Operations research (British English: operational research) (U.S. Air Force Specialty Code: Operations Analysis), often shortened to the initialism OR, is a discipline that deals with the development and application of analytical methods to improve decision-making. The term management science is occasionally used as a synonym.Employing techniques from other mathematical sciences, such as modeling, statistics, and optimization, operations research arrives at optimal or near-optimal solutions to decision-making problems. Because of its emphasis on practical applications, operations research has overlapped with many other disciplines, notably industrial engineering. Operations research is often concerned with determining the extreme values of some real-world objective: the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost). Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.

Overview
Operational research (OR) encompasses the development and the use of a wide range of problem-solving techniques and methods applied in the pursuit of improved decision-making and efficiency, such as simulation, mathematical optimization, queueing theory and other stochastic-process models, Markov decision processes, econometric methods, data envelopment analysis, ordinal priority approach, neural networks, expert systems, decision analysis, and the analytic hierarchy process. Nearly all of these techniques involve the construction of mathematical models that attempt to describe the system."
Which discipline has operations research overlapped with?,Simulation,Mathematical optimization,Queueing theory,Industrial engineering,Neural networks,D,"Operations research (British English: operational research) (U.S. Air Force Specialty Code: Operations Analysis), often shortened to the initialism OR, is a discipline that deals with the development and application of analytical methods to improve decision-making. The term management science is occasionally used as a synonym.Employing techniques from other mathematical sciences, such as modeling, statistics, and optimization, operations research arrives at optimal or near-optimal solutions to decision-making problems. Because of its emphasis on practical applications, operations research has overlapped with many other disciplines, notably industrial engineering. Operations research is often concerned with determining the extreme values of some real-world objective: the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost). Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.

Overview
Operational research (OR) encompasses the development and the use of a wide range of problem-solving techniques and methods applied in the pursuit of improved decision-making and efficiency, such as simulation, mathematical optimization, queueing theory and other stochastic-process models, Markov decision processes, econometric methods, data envelopment analysis, ordinal priority approach, neural networks, expert systems, decision analysis, and the analytic hierarchy process. Nearly all of these techniques involve the construction of mathematical models that attempt to describe the system."
What is the definition of technology?,The study and design of new technology,The use of tools and machinery by humans,The ability to control and adapt to natural environments,The awareness of facts and being competent,The systematic endeavor for gaining knowledge,B,"The following outline is provided as an overview of and topical guide to technology: collection of tools, including machinery, modifications, arrangements and procedures used by humans. Engineering is the discipline that seeks to study and design new technology. Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments.

Components of technology
Knowledge – Awareness of facts or being competent
Engineering – Applied science and research
Process – Series of activities
Science – Systematic endeavor for gaining knowledge
Skill – Ability to carry out a task
Tool – Object can be used to achieve a goal
Weapon – Implement or device used to inflict damage, harm, or kill
Utensil – Tool used for food preparation
Equipment – Items required to exercise a certain activity
Invention – Novel device, material or technical process
Machinery – Powered mechanical device
Structure – Arrangement of interrelated elements in an object/system, or the object/system itself
Building – Structure, typically with a roof and walls, standing more or less permanently in one place
Road – Land route for travel by vehicles
Bridge – Structure built to span physical obstacles
Canal – Artificial channel for water
Dam – Barrier that stops or restricts the flow of surface or underground streams
Man-made systems – Interrelated entities that form a whole
Infrastructure – Facilities and systems serving society
Public utility – Organization which maintains the infrastructure for a public service

Branches of technology
Aerospace – flight or transport above the surface of the Earth.
Space exploration – the physical investigation of the space more than 100 km above the Earth by either crewed or uncrewed spacecraft.
General aviation
Aeronautics
Astronautics
Aerospace engineering
Applied physics – physics which is intended for a particular technological or practical use. It is usually considered as a bridge or a connection between ""pure"" physics and engineering.
Agriculture – cultivation of plants, animals, and other living organisms.
Fishing – activity of trying to catch fish. Fish are normally caught in the wild. Techniques for catching fish include hand gathering, spearing, netting, angling and trapping.
Fisheries – a fishery is an entity engaged in raising or harvesting fish which is determined by some authority to be a fishery. According to the FAO, a fishery is typically defined in terms of the ""people involved, species or type of fish, area of water or seabed, method of fishing, class of boats, purpose of the activities or a combination of the foregoing features"".
Fishing industry –  industry or activity concerned with taking, culturing, processing, preserving, storing, transporting, marketing or selling fish or fish products."
What is the purpose of engineering?,To gain knowledge about technology,To create new technology,To adapt to natural environments,To carry out tasks,To study and design new technology,E,"The following outline is provided as an overview of and topical guide to technology: collection of tools, including machinery, modifications, arrangements and procedures used by humans. Engineering is the discipline that seeks to study and design new technology. Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments.

Components of technology
Knowledge – Awareness of facts or being competent
Engineering – Applied science and research
Process – Series of activities
Science – Systematic endeavor for gaining knowledge
Skill – Ability to carry out a task
Tool – Object can be used to achieve a goal
Weapon – Implement or device used to inflict damage, harm, or kill
Utensil – Tool used for food preparation
Equipment – Items required to exercise a certain activity
Invention – Novel device, material or technical process
Machinery – Powered mechanical device
Structure – Arrangement of interrelated elements in an object/system, or the object/system itself
Building – Structure, typically with a roof and walls, standing more or less permanently in one place
Road – Land route for travel by vehicles
Bridge – Structure built to span physical obstacles
Canal – Artificial channel for water
Dam – Barrier that stops or restricts the flow of surface or underground streams
Man-made systems – Interrelated entities that form a whole
Infrastructure – Facilities and systems serving society
Public utility – Organization which maintains the infrastructure for a public service

Branches of technology
Aerospace – flight or transport above the surface of the Earth.
Space exploration – the physical investigation of the space more than 100 km above the Earth by either crewed or uncrewed spacecraft.
General aviation
Aeronautics
Astronautics
Aerospace engineering
Applied physics – physics which is intended for a particular technological or practical use. It is usually considered as a bridge or a connection between ""pure"" physics and engineering.
Agriculture – cultivation of plants, animals, and other living organisms.
Fishing – activity of trying to catch fish. Fish are normally caught in the wild. Techniques for catching fish include hand gathering, spearing, netting, angling and trapping.
Fisheries – a fishery is an entity engaged in raising or harvesting fish which is determined by some authority to be a fishery. According to the FAO, a fishery is typically defined in terms of the ""people involved, species or type of fish, area of water or seabed, method of fishing, class of boats, purpose of the activities or a combination of the foregoing features"".
Fishing industry –  industry or activity concerned with taking, culturing, processing, preserving, storing, transporting, marketing or selling fish or fish products."
What is a utensil?,A tool used for food preparation,"A device used to inflict damage, harm, or kill","A novel device, material, or technical process",A powered mechanical device,An arrangement of interrelated elements in an object/system,A,"The following outline is provided as an overview of and topical guide to technology: collection of tools, including machinery, modifications, arrangements and procedures used by humans. Engineering is the discipline that seeks to study and design new technology. Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments.

Components of technology
Knowledge – Awareness of facts or being competent
Engineering – Applied science and research
Process – Series of activities
Science – Systematic endeavor for gaining knowledge
Skill – Ability to carry out a task
Tool – Object can be used to achieve a goal
Weapon – Implement or device used to inflict damage, harm, or kill
Utensil – Tool used for food preparation
Equipment – Items required to exercise a certain activity
Invention – Novel device, material or technical process
Machinery – Powered mechanical device
Structure – Arrangement of interrelated elements in an object/system, or the object/system itself
Building – Structure, typically with a roof and walls, standing more or less permanently in one place
Road – Land route for travel by vehicles
Bridge – Structure built to span physical obstacles
Canal – Artificial channel for water
Dam – Barrier that stops or restricts the flow of surface or underground streams
Man-made systems – Interrelated entities that form a whole
Infrastructure – Facilities and systems serving society
Public utility – Organization which maintains the infrastructure for a public service

Branches of technology
Aerospace – flight or transport above the surface of the Earth.
Space exploration – the physical investigation of the space more than 100 km above the Earth by either crewed or uncrewed spacecraft.
General aviation
Aeronautics
Astronautics
Aerospace engineering
Applied physics – physics which is intended for a particular technological or practical use. It is usually considered as a bridge or a connection between ""pure"" physics and engineering.
Agriculture – cultivation of plants, animals, and other living organisms.
Fishing – activity of trying to catch fish. Fish are normally caught in the wild. Techniques for catching fish include hand gathering, spearing, netting, angling and trapping.
Fisheries – a fishery is an entity engaged in raising or harvesting fish which is determined by some authority to be a fishery. According to the FAO, a fishery is typically defined in terms of the ""people involved, species or type of fish, area of water or seabed, method of fishing, class of boats, purpose of the activities or a combination of the foregoing features"".
Fishing industry –  industry or activity concerned with taking, culturing, processing, preserving, storing, transporting, marketing or selling fish or fish products."
What is aerospace?,"The cultivation of plants, animals, and other living organisms",The flight or transport above the surface of the Earth,The physical investigation of space by spacecraft,The physics intended for a particular technological or practical use,The industry or activity concerned with fishing and fish products,B,"The following outline is provided as an overview of and topical guide to technology: collection of tools, including machinery, modifications, arrangements and procedures used by humans. Engineering is the discipline that seeks to study and design new technology. Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments.

Components of technology
Knowledge – Awareness of facts or being competent
Engineering – Applied science and research
Process – Series of activities
Science – Systematic endeavor for gaining knowledge
Skill – Ability to carry out a task
Tool – Object can be used to achieve a goal
Weapon – Implement or device used to inflict damage, harm, or kill
Utensil – Tool used for food preparation
Equipment – Items required to exercise a certain activity
Invention – Novel device, material or technical process
Machinery – Powered mechanical device
Structure – Arrangement of interrelated elements in an object/system, or the object/system itself
Building – Structure, typically with a roof and walls, standing more or less permanently in one place
Road – Land route for travel by vehicles
Bridge – Structure built to span physical obstacles
Canal – Artificial channel for water
Dam – Barrier that stops or restricts the flow of surface or underground streams
Man-made systems – Interrelated entities that form a whole
Infrastructure – Facilities and systems serving society
Public utility – Organization which maintains the infrastructure for a public service

Branches of technology
Aerospace – flight or transport above the surface of the Earth.
Space exploration – the physical investigation of the space more than 100 km above the Earth by either crewed or uncrewed spacecraft.
General aviation
Aeronautics
Astronautics
Aerospace engineering
Applied physics – physics which is intended for a particular technological or practical use. It is usually considered as a bridge or a connection between ""pure"" physics and engineering.
Agriculture – cultivation of plants, animals, and other living organisms.
Fishing – activity of trying to catch fish. Fish are normally caught in the wild. Techniques for catching fish include hand gathering, spearing, netting, angling and trapping.
Fisheries – a fishery is an entity engaged in raising or harvesting fish which is determined by some authority to be a fishery. According to the FAO, a fishery is typically defined in terms of the ""people involved, species or type of fish, area of water or seabed, method of fishing, class of boats, purpose of the activities or a combination of the foregoing features"".
Fishing industry –  industry or activity concerned with taking, culturing, processing, preserving, storing, transporting, marketing or selling fish or fish products."
What is the fishing industry?,The systematic endeavor for gaining knowledge,The industry or activity concerned with fishing and fish products,"The cultivation of plants, animals, and other living organisms",The physical investigation of space by spacecraft,The use of tools and machinery by humans,B,"The following outline is provided as an overview of and topical guide to technology: collection of tools, including machinery, modifications, arrangements and procedures used by humans. Engineering is the discipline that seeks to study and design new technology. Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments.

Components of technology
Knowledge – Awareness of facts or being competent
Engineering – Applied science and research
Process – Series of activities
Science – Systematic endeavor for gaining knowledge
Skill – Ability to carry out a task
Tool – Object can be used to achieve a goal
Weapon – Implement or device used to inflict damage, harm, or kill
Utensil – Tool used for food preparation
Equipment – Items required to exercise a certain activity
Invention – Novel device, material or technical process
Machinery – Powered mechanical device
Structure – Arrangement of interrelated elements in an object/system, or the object/system itself
Building – Structure, typically with a roof and walls, standing more or less permanently in one place
Road – Land route for travel by vehicles
Bridge – Structure built to span physical obstacles
Canal – Artificial channel for water
Dam – Barrier that stops or restricts the flow of surface or underground streams
Man-made systems – Interrelated entities that form a whole
Infrastructure – Facilities and systems serving society
Public utility – Organization which maintains the infrastructure for a public service

Branches of technology
Aerospace – flight or transport above the surface of the Earth.
Space exploration – the physical investigation of the space more than 100 km above the Earth by either crewed or uncrewed spacecraft.
General aviation
Aeronautics
Astronautics
Aerospace engineering
Applied physics – physics which is intended for a particular technological or practical use. It is usually considered as a bridge or a connection between ""pure"" physics and engineering.
Agriculture – cultivation of plants, animals, and other living organisms.
Fishing – activity of trying to catch fish. Fish are normally caught in the wild. Techniques for catching fish include hand gathering, spearing, netting, angling and trapping.
Fisheries – a fishery is an entity engaged in raising or harvesting fish which is determined by some authority to be a fishery. According to the FAO, a fishery is typically defined in terms of the ""people involved, species or type of fish, area of water or seabed, method of fishing, class of boats, purpose of the activities or a combination of the foregoing features"".
Fishing industry –  industry or activity concerned with taking, culturing, processing, preserving, storing, transporting, marketing or selling fish or fish products."
What is systems ecology?,A field of biology that studies the interactions between organisms and their environment,An interdisciplinary field of ecology that takes a holistic approach to the study of ecological systems,The study of the flow of energy and nutrients through ecosystems,The study of the interactions between humans and their environment,A branch of economics that focuses on the relationship between economic activity and the environment,B,"Systems ecology is an interdisciplinary field of ecology, a subset of Earth system science, that takes a holistic approach to the study of ecological systems, especially ecosystems.  Systems ecology can be seen as an application of general systems theory to ecology.  Central to the systems ecology approach is the idea that an ecosystem is a complex system exhibiting emergent properties.  Systems ecology focuses on interactions and transactions within and between biological and ecological systems, and is especially concerned with the way the functioning of ecosystems can be influenced by human interventions.  It uses and extends concepts from thermodynamics and develops other macroscopic descriptions of complex systems.

Overview
Systems ecology seeks a holistic view of the interactions and transactions within and between biological and ecological systems. Systems ecologists realise that the function of any ecosystem can be influenced by human economics in fundamental ways.  They have therefore taken an additional transdisciplinary step by including economics in the consideration of ecological-economic systems."
What is the central idea of systems ecology?,Ecosystems are simple systems with predictable behaviors,Human interventions have no impact on the functioning of ecosystems,An ecosystem is a complex system that exhibits emergent properties,Systems ecology is only concerned with biological systems,Systems ecology focuses on studying the flow of energy and nutrients through ecosystems,C,"Systems ecology is an interdisciplinary field of ecology, a subset of Earth system science, that takes a holistic approach to the study of ecological systems, especially ecosystems.  Systems ecology can be seen as an application of general systems theory to ecology.  Central to the systems ecology approach is the idea that an ecosystem is a complex system exhibiting emergent properties.  Systems ecology focuses on interactions and transactions within and between biological and ecological systems, and is especially concerned with the way the functioning of ecosystems can be influenced by human interventions.  It uses and extends concepts from thermodynamics and develops other macroscopic descriptions of complex systems.

Overview
Systems ecology seeks a holistic view of the interactions and transactions within and between biological and ecological systems. Systems ecologists realise that the function of any ecosystem can be influenced by human economics in fundamental ways.  They have therefore taken an additional transdisciplinary step by including economics in the consideration of ecological-economic systems."
What does systems ecology focus on?,The interactions between humans and their environment,The flow of energy and nutrients through ecosystems,Studying biological systems,The impact of economics on ecosystems,The interactions and transactions within and between biological and ecological systems,E,"Systems ecology is an interdisciplinary field of ecology, a subset of Earth system science, that takes a holistic approach to the study of ecological systems, especially ecosystems.  Systems ecology can be seen as an application of general systems theory to ecology.  Central to the systems ecology approach is the idea that an ecosystem is a complex system exhibiting emergent properties.  Systems ecology focuses on interactions and transactions within and between biological and ecological systems, and is especially concerned with the way the functioning of ecosystems can be influenced by human interventions.  It uses and extends concepts from thermodynamics and develops other macroscopic descriptions of complex systems.

Overview
Systems ecology seeks a holistic view of the interactions and transactions within and between biological and ecological systems. Systems ecologists realise that the function of any ecosystem can be influenced by human economics in fundamental ways.  They have therefore taken an additional transdisciplinary step by including economics in the consideration of ecological-economic systems."
What does systems ecology use and extend concepts from?,General systems theory,Thermodynamics,Ecology,Economics,Biology,A,"Systems ecology is an interdisciplinary field of ecology, a subset of Earth system science, that takes a holistic approach to the study of ecological systems, especially ecosystems.  Systems ecology can be seen as an application of general systems theory to ecology.  Central to the systems ecology approach is the idea that an ecosystem is a complex system exhibiting emergent properties.  Systems ecology focuses on interactions and transactions within and between biological and ecological systems, and is especially concerned with the way the functioning of ecosystems can be influenced by human interventions.  It uses and extends concepts from thermodynamics and develops other macroscopic descriptions of complex systems.

Overview
Systems ecology seeks a holistic view of the interactions and transactions within and between biological and ecological systems. Systems ecologists realise that the function of any ecosystem can be influenced by human economics in fundamental ways.  They have therefore taken an additional transdisciplinary step by including economics in the consideration of ecological-economic systems."
What additional step has systems ecology taken in considering ecological-economic systems?,Including psychology in the study of ecological-economic systems,Including sociology in the study of ecological-economic systems,Including politics in the study of ecological-economic systems,Including economics in the study of ecological-economic systems,Including philosophy in the study of ecological-economic systems,D,"Systems ecology is an interdisciplinary field of ecology, a subset of Earth system science, that takes a holistic approach to the study of ecological systems, especially ecosystems.  Systems ecology can be seen as an application of general systems theory to ecology.  Central to the systems ecology approach is the idea that an ecosystem is a complex system exhibiting emergent properties.  Systems ecology focuses on interactions and transactions within and between biological and ecological systems, and is especially concerned with the way the functioning of ecosystems can be influenced by human interventions.  It uses and extends concepts from thermodynamics and develops other macroscopic descriptions of complex systems.

Overview
Systems ecology seeks a holistic view of the interactions and transactions within and between biological and ecological systems. Systems ecologists realise that the function of any ecosystem can be influenced by human economics in fundamental ways.  They have therefore taken an additional transdisciplinary step by including economics in the consideration of ecological-economic systems."
What is the purpose of computer-aided production engineering (CAPE)?,To improve the productivity of manufacturing/industrial engineers,To develop new communication technologies,To design and implement future manufacturing systems and subsystems,To change the environment in which goods are produced,To enhance the efficiency of global manufacturing,A,"Computer-aided production engineering (CAPE) is a relatively new and significant branch of engineering. Global manufacturing has changed the environment in which goods are produced.  Meanwhile, the rapid development of electronics and communication technologies has required design and manufacturing to keep pace.[1]

Description of CAPE
CAPE is seen as a new     type of computer-aided engineering environment which will improve the productivity of manufacturing/industrial engineers. This environment would be used by engineers to design and implement future manufacturing systems and subsystems. Work is currently underway at the United States National Institute of Standards and Technology (NIST) on CAPE systems. The NIST project is aimed at advancing the development of software environments and tools for the design and engineering of manufacturing systems.[2]

CAPE and the Future of Manufacturing
The future of manufacturing will be determined by the efficiency with which it can incorporate new technologies. The current process in engineering manufacturing systems is often ad hoc, with computerized tools being used on a limited basis."
What is the current state of engineering manufacturing systems?,Highly efficient and standardized,Ad hoc with limited use of computerized tools,Fully automated and computerized,Inefficient and outdated,Lacking in communication technologies,B,"Computer-aided production engineering (CAPE) is a relatively new and significant branch of engineering. Global manufacturing has changed the environment in which goods are produced.  Meanwhile, the rapid development of electronics and communication technologies has required design and manufacturing to keep pace.[1]

Description of CAPE
CAPE is seen as a new     type of computer-aided engineering environment which will improve the productivity of manufacturing/industrial engineers. This environment would be used by engineers to design and implement future manufacturing systems and subsystems. Work is currently underway at the United States National Institute of Standards and Technology (NIST) on CAPE systems. The NIST project is aimed at advancing the development of software environments and tools for the design and engineering of manufacturing systems.[2]

CAPE and the Future of Manufacturing
The future of manufacturing will be determined by the efficiency with which it can incorporate new technologies. The current process in engineering manufacturing systems is often ad hoc, with computerized tools being used on a limited basis."
What is the focus of the NIST project on CAPE systems?,Advancing the development of software environments and tools,Incorporating new technologies into manufacturing,Improving the efficiency of global manufacturing,Designing and implementing future manufacturing systems,Developing communication technologies,A,"Computer-aided production engineering (CAPE) is a relatively new and significant branch of engineering. Global manufacturing has changed the environment in which goods are produced.  Meanwhile, the rapid development of electronics and communication technologies has required design and manufacturing to keep pace.[1]

Description of CAPE
CAPE is seen as a new     type of computer-aided engineering environment which will improve the productivity of manufacturing/industrial engineers. This environment would be used by engineers to design and implement future manufacturing systems and subsystems. Work is currently underway at the United States National Institute of Standards and Technology (NIST) on CAPE systems. The NIST project is aimed at advancing the development of software environments and tools for the design and engineering of manufacturing systems.[2]

CAPE and the Future of Manufacturing
The future of manufacturing will be determined by the efficiency with which it can incorporate new technologies. The current process in engineering manufacturing systems is often ad hoc, with computerized tools being used on a limited basis."
What is the role of electronics and communication technologies in manufacturing?,To improve the efficiency of manufacturing systems,To change the environment in which goods are produced,To design and implement future manufacturing systems,To enhance the productivity of manufacturing/industrial engineers,To keep pace with rapid technological developments,E,"Computer-aided production engineering (CAPE) is a relatively new and significant branch of engineering. Global manufacturing has changed the environment in which goods are produced.  Meanwhile, the rapid development of electronics and communication technologies has required design and manufacturing to keep pace.[1]

Description of CAPE
CAPE is seen as a new     type of computer-aided engineering environment which will improve the productivity of manufacturing/industrial engineers. This environment would be used by engineers to design and implement future manufacturing systems and subsystems. Work is currently underway at the United States National Institute of Standards and Technology (NIST) on CAPE systems. The NIST project is aimed at advancing the development of software environments and tools for the design and engineering of manufacturing systems.[2]

CAPE and the Future of Manufacturing
The future of manufacturing will be determined by the efficiency with which it can incorporate new technologies. The current process in engineering manufacturing systems is often ad hoc, with computerized tools being used on a limited basis."
What is the impact of global manufacturing on the production environment?,Limited the use of computerized tools,Changed the way goods are produced,Increased the productivity of manufacturing/industrial engineers,Improved the efficiency of manufacturing systems,Developed new communication technologies,B,"Computer-aided production engineering (CAPE) is a relatively new and significant branch of engineering. Global manufacturing has changed the environment in which goods are produced.  Meanwhile, the rapid development of electronics and communication technologies has required design and manufacturing to keep pace.[1]

Description of CAPE
CAPE is seen as a new     type of computer-aided engineering environment which will improve the productivity of manufacturing/industrial engineers. This environment would be used by engineers to design and implement future manufacturing systems and subsystems. Work is currently underway at the United States National Institute of Standards and Technology (NIST) on CAPE systems. The NIST project is aimed at advancing the development of software environments and tools for the design and engineering of manufacturing systems.[2]

CAPE and the Future of Manufacturing
The future of manufacturing will be determined by the efficiency with which it can incorporate new technologies. The current process in engineering manufacturing systems is often ad hoc, with computerized tools being used on a limited basis."
What is the key col of a peak?,The lowest point on a path connecting the peak to higher terrain,The highest point on a path connecting the peak to higher terrain,The highest point on a path connecting the peak to lower terrain,The lowest point on a path connecting the peak to lower terrain,The point where two peaks meet,A,"In topography, prominence (also referred to as autonomous height, relative height, and shoulder drop in US English, and drop or relative height in British English) measures the height of a mountain or hill's summit relative to the lowest contour line encircling it but containing no higher summit within it. It is a measure of the independence of a summit. A peak's key col (the highest col surrounding the peak) is a unique point on this contour line and the parent peak is some higher mountain, selected according to various criteria.

Definitions
The prominence of a peak may be defined as the least drop in height necessary in order to get from the summit to any higher terrain. This can be calculated for a given peak in the following way: for every path connecting the peak to higher terrain, find the lowest point on the path; the key col (or key saddle, or linking col, or link) is defined as the highest of these points, along all connecting paths; the prominence is the difference between the elevation of the peak and the elevation of its key col. Mount Everest's prominence is defined by convention as its height, making it consistent with prominence of the highest peaks on other landmasses. An alternative equivalent definition is that the prominence is the height of the peak's summit above the lowest contour line encircling it, but containing no higher summit within it; see Figure 1.

Illustration
The parent peak may be either close or far from the subject peak.  The summit of Mount Everest is the parent peak of Aconcagua in Argentina at a distance of 17,755 km (11,032 miles), as well as the parent of the South Summit of Mount Everest at a distance of 360 m (1200 feet)."
How is prominence defined for a peak?,The height of the peak above the lowest contour line encircling it,The height difference between the peak and its parent peak,The distance between the peak and its parent peak,The height difference between the peak and the key col,The height difference between the peak and the lowest point on a connecting path,D,"In topography, prominence (also referred to as autonomous height, relative height, and shoulder drop in US English, and drop or relative height in British English) measures the height of a mountain or hill's summit relative to the lowest contour line encircling it but containing no higher summit within it. It is a measure of the independence of a summit. A peak's key col (the highest col surrounding the peak) is a unique point on this contour line and the parent peak is some higher mountain, selected according to various criteria.

Definitions
The prominence of a peak may be defined as the least drop in height necessary in order to get from the summit to any higher terrain. This can be calculated for a given peak in the following way: for every path connecting the peak to higher terrain, find the lowest point on the path; the key col (or key saddle, or linking col, or link) is defined as the highest of these points, along all connecting paths; the prominence is the difference between the elevation of the peak and the elevation of its key col. Mount Everest's prominence is defined by convention as its height, making it consistent with prominence of the highest peaks on other landmasses. An alternative equivalent definition is that the prominence is the height of the peak's summit above the lowest contour line encircling it, but containing no higher summit within it; see Figure 1.

Illustration
The parent peak may be either close or far from the subject peak.  The summit of Mount Everest is the parent peak of Aconcagua in Argentina at a distance of 17,755 km (11,032 miles), as well as the parent of the South Summit of Mount Everest at a distance of 360 m (1200 feet)."
Which peak is the parent peak of Aconcagua in Argentina?,Mount Everest,Mount Kilimanjaro,Mount McKinley,Mount Fuji,Mount Blanc,A,"In topography, prominence (also referred to as autonomous height, relative height, and shoulder drop in US English, and drop or relative height in British English) measures the height of a mountain or hill's summit relative to the lowest contour line encircling it but containing no higher summit within it. It is a measure of the independence of a summit. A peak's key col (the highest col surrounding the peak) is a unique point on this contour line and the parent peak is some higher mountain, selected according to various criteria.

Definitions
The prominence of a peak may be defined as the least drop in height necessary in order to get from the summit to any higher terrain. This can be calculated for a given peak in the following way: for every path connecting the peak to higher terrain, find the lowest point on the path; the key col (or key saddle, or linking col, or link) is defined as the highest of these points, along all connecting paths; the prominence is the difference between the elevation of the peak and the elevation of its key col. Mount Everest's prominence is defined by convention as its height, making it consistent with prominence of the highest peaks on other landmasses. An alternative equivalent definition is that the prominence is the height of the peak's summit above the lowest contour line encircling it, but containing no higher summit within it; see Figure 1.

Illustration
The parent peak may be either close or far from the subject peak.  The summit of Mount Everest is the parent peak of Aconcagua in Argentina at a distance of 17,755 km (11,032 miles), as well as the parent of the South Summit of Mount Everest at a distance of 360 m (1200 feet)."
What is the prominence of Mount Everest?,The height of the peak above the lowest contour line encircling it,The height difference between the peak and its parent peak,The distance between the peak and its parent peak,The height difference between the peak and the key col,The height difference between the peak and the lowest point on a connecting path,A,"In topography, prominence (also referred to as autonomous height, relative height, and shoulder drop in US English, and drop or relative height in British English) measures the height of a mountain or hill's summit relative to the lowest contour line encircling it but containing no higher summit within it. It is a measure of the independence of a summit. A peak's key col (the highest col surrounding the peak) is a unique point on this contour line and the parent peak is some higher mountain, selected according to various criteria.

Definitions
The prominence of a peak may be defined as the least drop in height necessary in order to get from the summit to any higher terrain. This can be calculated for a given peak in the following way: for every path connecting the peak to higher terrain, find the lowest point on the path; the key col (or key saddle, or linking col, or link) is defined as the highest of these points, along all connecting paths; the prominence is the difference between the elevation of the peak and the elevation of its key col. Mount Everest's prominence is defined by convention as its height, making it consistent with prominence of the highest peaks on other landmasses. An alternative equivalent definition is that the prominence is the height of the peak's summit above the lowest contour line encircling it, but containing no higher summit within it; see Figure 1.

Illustration
The parent peak may be either close or far from the subject peak.  The summit of Mount Everest is the parent peak of Aconcagua in Argentina at a distance of 17,755 km (11,032 miles), as well as the parent of the South Summit of Mount Everest at a distance of 360 m (1200 feet)."
What is the alternative equivalent definition of prominence?,The height difference between the peak and its parent peak,The height difference between the peak and the key col,The height of the peak above the lowest contour line encircling it,The distance between the peak and its parent peak,The height difference between the peak and the lowest point on a connecting path,C,"In topography, prominence (also referred to as autonomous height, relative height, and shoulder drop in US English, and drop or relative height in British English) measures the height of a mountain or hill's summit relative to the lowest contour line encircling it but containing no higher summit within it. It is a measure of the independence of a summit. A peak's key col (the highest col surrounding the peak) is a unique point on this contour line and the parent peak is some higher mountain, selected according to various criteria.

Definitions
The prominence of a peak may be defined as the least drop in height necessary in order to get from the summit to any higher terrain. This can be calculated for a given peak in the following way: for every path connecting the peak to higher terrain, find the lowest point on the path; the key col (or key saddle, or linking col, or link) is defined as the highest of these points, along all connecting paths; the prominence is the difference between the elevation of the peak and the elevation of its key col. Mount Everest's prominence is defined by convention as its height, making it consistent with prominence of the highest peaks on other landmasses. An alternative equivalent definition is that the prominence is the height of the peak's summit above the lowest contour line encircling it, but containing no higher summit within it; see Figure 1.

Illustration
The parent peak may be either close or far from the subject peak.  The summit of Mount Everest is the parent peak of Aconcagua in Argentina at a distance of 17,755 km (11,032 miles), as well as the parent of the South Summit of Mount Everest at a distance of 360 m (1200 feet)."
What is a Ćuk converter?,A type of boost converter,A type of buck converter,A combination of boost converter and buck converter,A type of flyback converter,A type of forward converter,C,"The Ćuk converter (pronounced chook; sometimes incorrectly spelled Cuk, Čuk or Cúk) is a type of buck-boost converter with low ripple current. A Ćuk converter can be seen as a combination of boost converter and buck converter, having one switching device and a mutual capacitor, to couple the energy.
Similar to the buck-boost converter with inverting topology, the output voltage of non-isolated Ćuk converter is typically inverted, with lower or higher values with respect to the input voltage. Usually in DC converters, the inductor is used as a main energy-storage component. In ćuk converter, the main energy-storage component is the capacitor. It is named after Slobodan Ćuk of the California Institute of Technology, who first presented the design.

Non-isolated Ćuk converter
There are variations on the basic Ćuk converter. For example, the coils may share single magnetic core, which drops the output ripple, and adds efficiency. Because the power transfer flows continuously via the capacitor, this type of switcher has minimized EMI radiation."
What is the main energy-storage component in a Ćuk converter?,Inductor,Capacitor,Resistor,Transformer,Diode,B,"The Ćuk converter (pronounced chook; sometimes incorrectly spelled Cuk, Čuk or Cúk) is a type of buck-boost converter with low ripple current. A Ćuk converter can be seen as a combination of boost converter and buck converter, having one switching device and a mutual capacitor, to couple the energy.
Similar to the buck-boost converter with inverting topology, the output voltage of non-isolated Ćuk converter is typically inverted, with lower or higher values with respect to the input voltage. Usually in DC converters, the inductor is used as a main energy-storage component. In ćuk converter, the main energy-storage component is the capacitor. It is named after Slobodan Ćuk of the California Institute of Technology, who first presented the design.

Non-isolated Ćuk converter
There are variations on the basic Ćuk converter. For example, the coils may share single magnetic core, which drops the output ripple, and adds efficiency. Because the power transfer flows continuously via the capacitor, this type of switcher has minimized EMI radiation."
Who is the Ćuk converter named after?,Slobodan Ćuk,Nikola Tesla,Thomas Edison,Albert Einstein,Isaac Newton,A,"The Ćuk converter (pronounced chook; sometimes incorrectly spelled Cuk, Čuk or Cúk) is a type of buck-boost converter with low ripple current. A Ćuk converter can be seen as a combination of boost converter and buck converter, having one switching device and a mutual capacitor, to couple the energy.
Similar to the buck-boost converter with inverting topology, the output voltage of non-isolated Ćuk converter is typically inverted, with lower or higher values with respect to the input voltage. Usually in DC converters, the inductor is used as a main energy-storage component. In ćuk converter, the main energy-storage component is the capacitor. It is named after Slobodan Ćuk of the California Institute of Technology, who first presented the design.

Non-isolated Ćuk converter
There are variations on the basic Ćuk converter. For example, the coils may share single magnetic core, which drops the output ripple, and adds efficiency. Because the power transfer flows continuously via the capacitor, this type of switcher has minimized EMI radiation."
What is the advantage of a Ćuk converter with a single magnetic core for the coils?,Higher output ripple,Lower efficiency,Minimized EMI radiation,Decreased power transfer,Increased power transfer,C,"The Ćuk converter (pronounced chook; sometimes incorrectly spelled Cuk, Čuk or Cúk) is a type of buck-boost converter with low ripple current. A Ćuk converter can be seen as a combination of boost converter and buck converter, having one switching device and a mutual capacitor, to couple the energy.
Similar to the buck-boost converter with inverting topology, the output voltage of non-isolated Ćuk converter is typically inverted, with lower or higher values with respect to the input voltage. Usually in DC converters, the inductor is used as a main energy-storage component. In ćuk converter, the main energy-storage component is the capacitor. It is named after Slobodan Ćuk of the California Institute of Technology, who first presented the design.

Non-isolated Ćuk converter
There are variations on the basic Ćuk converter. For example, the coils may share single magnetic core, which drops the output ripple, and adds efficiency. Because the power transfer flows continuously via the capacitor, this type of switcher has minimized EMI radiation."
What is the output voltage of a non-isolated Ćuk converter typically like?,Always the same as the input voltage,Always higher than the input voltage,Always lower than the input voltage,Sometimes lower and sometimes higher than the input voltage,Cannot be determined,D,"The Ćuk converter (pronounced chook; sometimes incorrectly spelled Cuk, Čuk or Cúk) is a type of buck-boost converter with low ripple current. A Ćuk converter can be seen as a combination of boost converter and buck converter, having one switching device and a mutual capacitor, to couple the energy.
Similar to the buck-boost converter with inverting topology, the output voltage of non-isolated Ćuk converter is typically inverted, with lower or higher values with respect to the input voltage. Usually in DC converters, the inductor is used as a main energy-storage component. In ćuk converter, the main energy-storage component is the capacitor. It is named after Slobodan Ćuk of the California Institute of Technology, who first presented the design.

Non-isolated Ćuk converter
There are variations on the basic Ćuk converter. For example, the coils may share single magnetic core, which drops the output ripple, and adds efficiency. Because the power transfer flows continuously via the capacitor, this type of switcher has minimized EMI radiation."
What is the goal of a formal ontology?,To provide a biased view on reality,To model large-scale ontologies,To avoid erroneous ontological assumptions,To expand the content of the ontology,To accommodate different levels of granularity,C,"In philosophy, the term formal ontology is used to refer to an ontology defined by axioms in a formal language with the goal to provide an unbiased (domain- and application-independent) view on reality, which can help the modeler of domain- or application-specific ontologies (information science) to avoid possibly erroneous ontological assumptions encountered in modeling large-scale ontologies.
By maintaining an independent view on reality a formal (upper level) ontology gains the following properties:

indefinite expandability:
the ontology remains consistent with increasing content.
content and context independence:
any kind of 'concept' can find its place.
accommodate different levels of granularity.

Historical background
Theories on how to conceptualize reality date back as far as Plato and Aristotle. The term 'formal ontology' itself was coined by Edmund Husserl in the second edition of his Logical Investigations (1900–01), where it refers to an ontological counterpart of formal logic. Formal ontology for Husserl embraces an axiomatized mereology and a theory of dependence relations, for example between the qualities of an object and the object itself. 'Formal' signifies not the use of a formal-logical language, but rather: non-material, or in other words domain-independent (of universal application). Husserl's ideas on formal ontology were developed especially by his Polish student Roman Ingarden in his Controversy over the Existence of the World. The relations between the Husserlian tradition of formal ontology and the Polish tradition of mereology are set forth in Parts and Moments. Studies in Logic and Formal Ontology, edited by Barry Smith.

Existing formal ontologies (foundational ontologies)
BFO – Basic Formal Ontology
GFO – General Formal Ontology
BORO – Business Objects Reference Ontology
CIDOC Conceptual Reference Model
Cyc (Cyc is not just an upper ontology, it also contains many mid-level and specialized ontologies as well)
UMBEL – Upper Mapping and Binding Exchange Layer, a subset of OpenCyc
DOLCE – Descriptive Ontology for Linguistic and Cognitive Engineering
SUMO – Suggested Upper Merged Ontology
YAMATO - Yet Another More Advanced Top Ontology

Common terms in formal (upper-level) ontologies
The differences in terminology used between separate formal upper-level ontologies can be quite substantial, but most formal upper-level ontologies apply one foremost dichotomy: that between endurants and perdurants.

Endurant
Also known as continuants, or in some cases as ""substance"", endurants are those entities that can be observed-perceived as a complete concept, at no matter which given snapshot of time.
Were we to freeze time we would still be able to perceive/conceive the entire endurant.
Examples include material objects (such as an apple or a human), and abstract ""fiat"" objects (such as an organization, or the border of a country).

Perdurant
Also known as occurrents, accidents or happenings, perdurants are those entities for which only a part exists if we look at them at any given snapshot in time.
When we freeze time we can only see a part of the perdurant."
"Who coined the term ""formal ontology""?",Plato,Aristotle,Edmund Husserl,Roman Ingarden,Barry Smith,C,"In philosophy, the term formal ontology is used to refer to an ontology defined by axioms in a formal language with the goal to provide an unbiased (domain- and application-independent) view on reality, which can help the modeler of domain- or application-specific ontologies (information science) to avoid possibly erroneous ontological assumptions encountered in modeling large-scale ontologies.
By maintaining an independent view on reality a formal (upper level) ontology gains the following properties:

indefinite expandability:
the ontology remains consistent with increasing content.
content and context independence:
any kind of 'concept' can find its place.
accommodate different levels of granularity.

Historical background
Theories on how to conceptualize reality date back as far as Plato and Aristotle. The term 'formal ontology' itself was coined by Edmund Husserl in the second edition of his Logical Investigations (1900–01), where it refers to an ontological counterpart of formal logic. Formal ontology for Husserl embraces an axiomatized mereology and a theory of dependence relations, for example between the qualities of an object and the object itself. 'Formal' signifies not the use of a formal-logical language, but rather: non-material, or in other words domain-independent (of universal application). Husserl's ideas on formal ontology were developed especially by his Polish student Roman Ingarden in his Controversy over the Existence of the World. The relations between the Husserlian tradition of formal ontology and the Polish tradition of mereology are set forth in Parts and Moments. Studies in Logic and Formal Ontology, edited by Barry Smith.

Existing formal ontologies (foundational ontologies)
BFO – Basic Formal Ontology
GFO – General Formal Ontology
BORO – Business Objects Reference Ontology
CIDOC Conceptual Reference Model
Cyc (Cyc is not just an upper ontology, it also contains many mid-level and specialized ontologies as well)
UMBEL – Upper Mapping and Binding Exchange Layer, a subset of OpenCyc
DOLCE – Descriptive Ontology for Linguistic and Cognitive Engineering
SUMO – Suggested Upper Merged Ontology
YAMATO - Yet Another More Advanced Top Ontology

Common terms in formal (upper-level) ontologies
The differences in terminology used between separate formal upper-level ontologies can be quite substantial, but most formal upper-level ontologies apply one foremost dichotomy: that between endurants and perdurants.

Endurant
Also known as continuants, or in some cases as ""substance"", endurants are those entities that can be observed-perceived as a complete concept, at no matter which given snapshot of time.
Were we to freeze time we would still be able to perceive/conceive the entire endurant.
Examples include material objects (such as an apple or a human), and abstract ""fiat"" objects (such as an organization, or the border of a country).

Perdurant
Also known as occurrents, accidents or happenings, perdurants are those entities for which only a part exists if we look at them at any given snapshot in time.
When we freeze time we can only see a part of the perdurant."
Which of the following is not a formal (upper-level) ontology?,BFO - Basic Formal Ontology,GFO - General Formal Ontology,CIDOC Conceptual Reference Model,Cyc,YAMATO - Yet Another More Advanced Top Ontology,D,"In philosophy, the term formal ontology is used to refer to an ontology defined by axioms in a formal language with the goal to provide an unbiased (domain- and application-independent) view on reality, which can help the modeler of domain- or application-specific ontologies (information science) to avoid possibly erroneous ontological assumptions encountered in modeling large-scale ontologies.
By maintaining an independent view on reality a formal (upper level) ontology gains the following properties:

indefinite expandability:
the ontology remains consistent with increasing content.
content and context independence:
any kind of 'concept' can find its place.
accommodate different levels of granularity.

Historical background
Theories on how to conceptualize reality date back as far as Plato and Aristotle. The term 'formal ontology' itself was coined by Edmund Husserl in the second edition of his Logical Investigations (1900–01), where it refers to an ontological counterpart of formal logic. Formal ontology for Husserl embraces an axiomatized mereology and a theory of dependence relations, for example between the qualities of an object and the object itself. 'Formal' signifies not the use of a formal-logical language, but rather: non-material, or in other words domain-independent (of universal application). Husserl's ideas on formal ontology were developed especially by his Polish student Roman Ingarden in his Controversy over the Existence of the World. The relations between the Husserlian tradition of formal ontology and the Polish tradition of mereology are set forth in Parts and Moments. Studies in Logic and Formal Ontology, edited by Barry Smith.

Existing formal ontologies (foundational ontologies)
BFO – Basic Formal Ontology
GFO – General Formal Ontology
BORO – Business Objects Reference Ontology
CIDOC Conceptual Reference Model
Cyc (Cyc is not just an upper ontology, it also contains many mid-level and specialized ontologies as well)
UMBEL – Upper Mapping and Binding Exchange Layer, a subset of OpenCyc
DOLCE – Descriptive Ontology for Linguistic and Cognitive Engineering
SUMO – Suggested Upper Merged Ontology
YAMATO - Yet Another More Advanced Top Ontology

Common terms in formal (upper-level) ontologies
The differences in terminology used between separate formal upper-level ontologies can be quite substantial, but most formal upper-level ontologies apply one foremost dichotomy: that between endurants and perdurants.

Endurant
Also known as continuants, or in some cases as ""substance"", endurants are those entities that can be observed-perceived as a complete concept, at no matter which given snapshot of time.
Were we to freeze time we would still be able to perceive/conceive the entire endurant.
Examples include material objects (such as an apple or a human), and abstract ""fiat"" objects (such as an organization, or the border of a country).

Perdurant
Also known as occurrents, accidents or happenings, perdurants are those entities for which only a part exists if we look at them at any given snapshot in time.
When we freeze time we can only see a part of the perdurant."
What is the difference between endurants and perdurants?,"Endurants can be observed at any given snapshot in time, while perdurants can only be partially observed","Endurants are abstract objects, while perdurants are material objects","Endurants exist in the physical world, while perdurants exist in the conceptual world","Endurants are static entities, while perdurants are dynamic entities","Endurants are part of formal logic, while perdurants are part of mereology",A,"In philosophy, the term formal ontology is used to refer to an ontology defined by axioms in a formal language with the goal to provide an unbiased (domain- and application-independent) view on reality, which can help the modeler of domain- or application-specific ontologies (information science) to avoid possibly erroneous ontological assumptions encountered in modeling large-scale ontologies.
By maintaining an independent view on reality a formal (upper level) ontology gains the following properties:

indefinite expandability:
the ontology remains consistent with increasing content.
content and context independence:
any kind of 'concept' can find its place.
accommodate different levels of granularity.

Historical background
Theories on how to conceptualize reality date back as far as Plato and Aristotle. The term 'formal ontology' itself was coined by Edmund Husserl in the second edition of his Logical Investigations (1900–01), where it refers to an ontological counterpart of formal logic. Formal ontology for Husserl embraces an axiomatized mereology and a theory of dependence relations, for example between the qualities of an object and the object itself. 'Formal' signifies not the use of a formal-logical language, but rather: non-material, or in other words domain-independent (of universal application). Husserl's ideas on formal ontology were developed especially by his Polish student Roman Ingarden in his Controversy over the Existence of the World. The relations between the Husserlian tradition of formal ontology and the Polish tradition of mereology are set forth in Parts and Moments. Studies in Logic and Formal Ontology, edited by Barry Smith.

Existing formal ontologies (foundational ontologies)
BFO – Basic Formal Ontology
GFO – General Formal Ontology
BORO – Business Objects Reference Ontology
CIDOC Conceptual Reference Model
Cyc (Cyc is not just an upper ontology, it also contains many mid-level and specialized ontologies as well)
UMBEL – Upper Mapping and Binding Exchange Layer, a subset of OpenCyc
DOLCE – Descriptive Ontology for Linguistic and Cognitive Engineering
SUMO – Suggested Upper Merged Ontology
YAMATO - Yet Another More Advanced Top Ontology

Common terms in formal (upper-level) ontologies
The differences in terminology used between separate formal upper-level ontologies can be quite substantial, but most formal upper-level ontologies apply one foremost dichotomy: that between endurants and perdurants.

Endurant
Also known as continuants, or in some cases as ""substance"", endurants are those entities that can be observed-perceived as a complete concept, at no matter which given snapshot of time.
Were we to freeze time we would still be able to perceive/conceive the entire endurant.
Examples include material objects (such as an apple or a human), and abstract ""fiat"" objects (such as an organization, or the border of a country).

Perdurant
Also known as occurrents, accidents or happenings, perdurants are those entities for which only a part exists if we look at them at any given snapshot in time.
When we freeze time we can only see a part of the perdurant."
Which of the following is an example of an endurant?,An organization,The border of a country,An apple,A human,A concept,C,"In philosophy, the term formal ontology is used to refer to an ontology defined by axioms in a formal language with the goal to provide an unbiased (domain- and application-independent) view on reality, which can help the modeler of domain- or application-specific ontologies (information science) to avoid possibly erroneous ontological assumptions encountered in modeling large-scale ontologies.
By maintaining an independent view on reality a formal (upper level) ontology gains the following properties:

indefinite expandability:
the ontology remains consistent with increasing content.
content and context independence:
any kind of 'concept' can find its place.
accommodate different levels of granularity.

Historical background
Theories on how to conceptualize reality date back as far as Plato and Aristotle. The term 'formal ontology' itself was coined by Edmund Husserl in the second edition of his Logical Investigations (1900–01), where it refers to an ontological counterpart of formal logic. Formal ontology for Husserl embraces an axiomatized mereology and a theory of dependence relations, for example between the qualities of an object and the object itself. 'Formal' signifies not the use of a formal-logical language, but rather: non-material, or in other words domain-independent (of universal application). Husserl's ideas on formal ontology were developed especially by his Polish student Roman Ingarden in his Controversy over the Existence of the World. The relations between the Husserlian tradition of formal ontology and the Polish tradition of mereology are set forth in Parts and Moments. Studies in Logic and Formal Ontology, edited by Barry Smith.

Existing formal ontologies (foundational ontologies)
BFO – Basic Formal Ontology
GFO – General Formal Ontology
BORO – Business Objects Reference Ontology
CIDOC Conceptual Reference Model
Cyc (Cyc is not just an upper ontology, it also contains many mid-level and specialized ontologies as well)
UMBEL – Upper Mapping and Binding Exchange Layer, a subset of OpenCyc
DOLCE – Descriptive Ontology for Linguistic and Cognitive Engineering
SUMO – Suggested Upper Merged Ontology
YAMATO - Yet Another More Advanced Top Ontology

Common terms in formal (upper-level) ontologies
The differences in terminology used between separate formal upper-level ontologies can be quite substantial, but most formal upper-level ontologies apply one foremost dichotomy: that between endurants and perdurants.

Endurant
Also known as continuants, or in some cases as ""substance"", endurants are those entities that can be observed-perceived as a complete concept, at no matter which given snapshot of time.
Were we to freeze time we would still be able to perceive/conceive the entire endurant.
Examples include material objects (such as an apple or a human), and abstract ""fiat"" objects (such as an organization, or the border of a country).

Perdurant
Also known as occurrents, accidents or happenings, perdurants are those entities for which only a part exists if we look at them at any given snapshot in time.
When we freeze time we can only see a part of the perdurant."
What is the purpose of electrochemical reduction of carbon dioxide?,To convert carbon dioxide to organic feedstocks,To produce electricity,To capture and store carbon dioxide,To generate oxygen,To produce hydrogen,A,"The electrochemical reduction of carbon dioxide, also known as electrolysis of carbon dioxide, is the conversion of carbon dioxide (CO2) to more reduced chemical species using electrical energy. It is one possible step in the broad scheme of carbon capture and utilization, nevertheless it is deemed to be one of the most promising approaches.Electrochemical reduction of carbon dioxide represents a possible means of producing chemicals or fuels, converting carbon dioxide (CO2) to organic feedstocks such as formic acid (HCOOH), carbon monoxide (CO), methane (CH4), ethylene (C2H4) and ethanol (C2H5OH). Among the more selective metallic catalysts in this field are tin for formic acid, silver for carbon monoxide and copper for methane, ethylene or ethanol. Methanol, propanol and 1-butanol have also been produced via CO2 electrochemical reduction, albeit in small quantities.  The main challenges are the relatively high cost of electricity (vs petroleum) and the fact that most CO2 is diluted with O2 and thus is an unsuitable substrate.
The first examples of electrochemical reduction of carbon dioxide are from the 19th century, when carbon dioxide was reduced to carbon monoxide using a zinc cathode. Research in this field intensified in the 1980s following the oil embargoes of the 1970s. As of 2021, pilot-scale carbon dioxide electrochemical reduction is being developed by several companies, including Siemens, Dioxide Materials, Twelve and GIGKarasek."
Which metallic catalyst is selective for the production of formic acid?,Silver,Copper,Tin,Zinc,Gold,C,"The electrochemical reduction of carbon dioxide, also known as electrolysis of carbon dioxide, is the conversion of carbon dioxide (CO2) to more reduced chemical species using electrical energy. It is one possible step in the broad scheme of carbon capture and utilization, nevertheless it is deemed to be one of the most promising approaches.Electrochemical reduction of carbon dioxide represents a possible means of producing chemicals or fuels, converting carbon dioxide (CO2) to organic feedstocks such as formic acid (HCOOH), carbon monoxide (CO), methane (CH4), ethylene (C2H4) and ethanol (C2H5OH). Among the more selective metallic catalysts in this field are tin for formic acid, silver for carbon monoxide and copper for methane, ethylene or ethanol. Methanol, propanol and 1-butanol have also been produced via CO2 electrochemical reduction, albeit in small quantities.  The main challenges are the relatively high cost of electricity (vs petroleum) and the fact that most CO2 is diluted with O2 and thus is an unsuitable substrate.
The first examples of electrochemical reduction of carbon dioxide are from the 19th century, when carbon dioxide was reduced to carbon monoxide using a zinc cathode. Research in this field intensified in the 1980s following the oil embargoes of the 1970s. As of 2021, pilot-scale carbon dioxide electrochemical reduction is being developed by several companies, including Siemens, Dioxide Materials, Twelve and GIGKarasek."
What are some of the challenges associated with electrochemical reduction of carbon dioxide?,Low cost of electricity,Suitability of CO2 as a substrate,Abundance of O2 in CO2,Lack of metallic catalysts,High efficiency of the process,B,"The electrochemical reduction of carbon dioxide, also known as electrolysis of carbon dioxide, is the conversion of carbon dioxide (CO2) to more reduced chemical species using electrical energy. It is one possible step in the broad scheme of carbon capture and utilization, nevertheless it is deemed to be one of the most promising approaches.Electrochemical reduction of carbon dioxide represents a possible means of producing chemicals or fuels, converting carbon dioxide (CO2) to organic feedstocks such as formic acid (HCOOH), carbon monoxide (CO), methane (CH4), ethylene (C2H4) and ethanol (C2H5OH). Among the more selective metallic catalysts in this field are tin for formic acid, silver for carbon monoxide and copper for methane, ethylene or ethanol. Methanol, propanol and 1-butanol have also been produced via CO2 electrochemical reduction, albeit in small quantities.  The main challenges are the relatively high cost of electricity (vs petroleum) and the fact that most CO2 is diluted with O2 and thus is an unsuitable substrate.
The first examples of electrochemical reduction of carbon dioxide are from the 19th century, when carbon dioxide was reduced to carbon monoxide using a zinc cathode. Research in this field intensified in the 1980s following the oil embargoes of the 1970s. As of 2021, pilot-scale carbon dioxide electrochemical reduction is being developed by several companies, including Siemens, Dioxide Materials, Twelve and GIGKarasek."
When did research in the field of electrochemical reduction of carbon dioxide intensify?,19th century,1980s,2021,1970s,20th century,B,"The electrochemical reduction of carbon dioxide, also known as electrolysis of carbon dioxide, is the conversion of carbon dioxide (CO2) to more reduced chemical species using electrical energy. It is one possible step in the broad scheme of carbon capture and utilization, nevertheless it is deemed to be one of the most promising approaches.Electrochemical reduction of carbon dioxide represents a possible means of producing chemicals or fuels, converting carbon dioxide (CO2) to organic feedstocks such as formic acid (HCOOH), carbon monoxide (CO), methane (CH4), ethylene (C2H4) and ethanol (C2H5OH). Among the more selective metallic catalysts in this field are tin for formic acid, silver for carbon monoxide and copper for methane, ethylene or ethanol. Methanol, propanol and 1-butanol have also been produced via CO2 electrochemical reduction, albeit in small quantities.  The main challenges are the relatively high cost of electricity (vs petroleum) and the fact that most CO2 is diluted with O2 and thus is an unsuitable substrate.
The first examples of electrochemical reduction of carbon dioxide are from the 19th century, when carbon dioxide was reduced to carbon monoxide using a zinc cathode. Research in this field intensified in the 1980s following the oil embargoes of the 1970s. As of 2021, pilot-scale carbon dioxide electrochemical reduction is being developed by several companies, including Siemens, Dioxide Materials, Twelve and GIGKarasek."
Which companies are developing pilot-scale carbon dioxide electrochemical reduction?,Siemens,Dioxide Materials,Twelve,GIGKarasek,All of the above,E,"The electrochemical reduction of carbon dioxide, also known as electrolysis of carbon dioxide, is the conversion of carbon dioxide (CO2) to more reduced chemical species using electrical energy. It is one possible step in the broad scheme of carbon capture and utilization, nevertheless it is deemed to be one of the most promising approaches.Electrochemical reduction of carbon dioxide represents a possible means of producing chemicals or fuels, converting carbon dioxide (CO2) to organic feedstocks such as formic acid (HCOOH), carbon monoxide (CO), methane (CH4), ethylene (C2H4) and ethanol (C2H5OH). Among the more selective metallic catalysts in this field are tin for formic acid, silver for carbon monoxide and copper for methane, ethylene or ethanol. Methanol, propanol and 1-butanol have also been produced via CO2 electrochemical reduction, albeit in small quantities.  The main challenges are the relatively high cost of electricity (vs petroleum) and the fact that most CO2 is diluted with O2 and thus is an unsuitable substrate.
The first examples of electrochemical reduction of carbon dioxide are from the 19th century, when carbon dioxide was reduced to carbon monoxide using a zinc cathode. Research in this field intensified in the 1980s following the oil embargoes of the 1970s. As of 2021, pilot-scale carbon dioxide electrochemical reduction is being developed by several companies, including Siemens, Dioxide Materials, Twelve and GIGKarasek."
What is confrontation analysis?,A technique used in negotiations,A method of decision-making,An analysis of multi-party interactions,A mathematical basis of drama theory,A game theory approach to resolving conflicts,C,"Confrontation analysis (also known as dilemma analysis) is an operational analysis technique used to structure, understand and think through multi-party interactions such as negotiations. It is the underpinning mathematical basis of drama theory. 
It is derived from game theory but considers that instead of resolving the game, the players often redefine the game when interacting. Emotions triggered from the potential interaction play a large part in this redefinition. So whereas game theory looks on an interaction as a single decision matrix and resolves that, confrontation analysis looks on the interaction as a sequence of linked interactions, where the decision matrix changes under the influence of precisely defined emotional dilemmas.

Derivation and use
Confrontation analysis was devised by Professor Nigel Howard in the early 1990s drawing from his work on game theory and metagame analysis. It has been turned to defence, political, legal, financial and commercial  applications.

Much of the theoretical background to General Rupert Smith's book The Utility of Force drew its inspiration from the theory of confrontation analysis.I am in debt to Professor Nigel Howard, whose explanation of Confrontation Analysis and Game Theory at a seminar in 1998 excited my interest. Our subsequent discussions helped me to order my thoughts and the lessons I had learned into a coherent structure with the result that, for the first time, I was able to understand my experiences within a theoretical model which allowed me to use them further
Confrontation analysis can also be used in a decision workshop as structure to support role-playing for training, analysis and decision rehearsal.

Method
Confrontation analysis looks on an interaction as a sequence of confrontations."
What is the main difference between game theory and confrontation analysis?,"Game theory resolves interactions as a single decision matrix, while confrontation analysis looks at interactions as a sequence of linked interactions","Game theory focuses on emotions triggered from potential interactions, while confrontation analysis considers emotional dilemmas","Game theory is used in negotiations, while confrontation analysis is used in decision workshops","Game theory is a theoretical model, while confrontation analysis is a practical approach","Game theory considers interactions as a sequence of confrontations, while confrontation analysis resolves interactions as a single decision matrix",A,"Confrontation analysis (also known as dilemma analysis) is an operational analysis technique used to structure, understand and think through multi-party interactions such as negotiations. It is the underpinning mathematical basis of drama theory. 
It is derived from game theory but considers that instead of resolving the game, the players often redefine the game when interacting. Emotions triggered from the potential interaction play a large part in this redefinition. So whereas game theory looks on an interaction as a single decision matrix and resolves that, confrontation analysis looks on the interaction as a sequence of linked interactions, where the decision matrix changes under the influence of precisely defined emotional dilemmas.

Derivation and use
Confrontation analysis was devised by Professor Nigel Howard in the early 1990s drawing from his work on game theory and metagame analysis. It has been turned to defence, political, legal, financial and commercial  applications.

Much of the theoretical background to General Rupert Smith's book The Utility of Force drew its inspiration from the theory of confrontation analysis.I am in debt to Professor Nigel Howard, whose explanation of Confrontation Analysis and Game Theory at a seminar in 1998 excited my interest. Our subsequent discussions helped me to order my thoughts and the lessons I had learned into a coherent structure with the result that, for the first time, I was able to understand my experiences within a theoretical model which allowed me to use them further
Confrontation analysis can also be used in a decision workshop as structure to support role-playing for training, analysis and decision rehearsal.

Method
Confrontation analysis looks on an interaction as a sequence of confrontations."
Who devised confrontation analysis?,Professor Nigel Howard,General Rupert Smith,Professor Rupert Smith,Professor Nigel Smith,General Nigel Howard,A,"Confrontation analysis (also known as dilemma analysis) is an operational analysis technique used to structure, understand and think through multi-party interactions such as negotiations. It is the underpinning mathematical basis of drama theory. 
It is derived from game theory but considers that instead of resolving the game, the players often redefine the game when interacting. Emotions triggered from the potential interaction play a large part in this redefinition. So whereas game theory looks on an interaction as a single decision matrix and resolves that, confrontation analysis looks on the interaction as a sequence of linked interactions, where the decision matrix changes under the influence of precisely defined emotional dilemmas.

Derivation and use
Confrontation analysis was devised by Professor Nigel Howard in the early 1990s drawing from his work on game theory and metagame analysis. It has been turned to defence, political, legal, financial and commercial  applications.

Much of the theoretical background to General Rupert Smith's book The Utility of Force drew its inspiration from the theory of confrontation analysis.I am in debt to Professor Nigel Howard, whose explanation of Confrontation Analysis and Game Theory at a seminar in 1998 excited my interest. Our subsequent discussions helped me to order my thoughts and the lessons I had learned into a coherent structure with the result that, for the first time, I was able to understand my experiences within a theoretical model which allowed me to use them further
Confrontation analysis can also be used in a decision workshop as structure to support role-playing for training, analysis and decision rehearsal.

Method
Confrontation analysis looks on an interaction as a sequence of confrontations."
What are the applications of confrontation analysis?,Military strategy and planning,Political decision-making,Legal disputes resolution,Financial analysis,All of the above,E,"Confrontation analysis (also known as dilemma analysis) is an operational analysis technique used to structure, understand and think through multi-party interactions such as negotiations. It is the underpinning mathematical basis of drama theory. 
It is derived from game theory but considers that instead of resolving the game, the players often redefine the game when interacting. Emotions triggered from the potential interaction play a large part in this redefinition. So whereas game theory looks on an interaction as a single decision matrix and resolves that, confrontation analysis looks on the interaction as a sequence of linked interactions, where the decision matrix changes under the influence of precisely defined emotional dilemmas.

Derivation and use
Confrontation analysis was devised by Professor Nigel Howard in the early 1990s drawing from his work on game theory and metagame analysis. It has been turned to defence, political, legal, financial and commercial  applications.

Much of the theoretical background to General Rupert Smith's book The Utility of Force drew its inspiration from the theory of confrontation analysis.I am in debt to Professor Nigel Howard, whose explanation of Confrontation Analysis and Game Theory at a seminar in 1998 excited my interest. Our subsequent discussions helped me to order my thoughts and the lessons I had learned into a coherent structure with the result that, for the first time, I was able to understand my experiences within a theoretical model which allowed me to use them further
Confrontation analysis can also be used in a decision workshop as structure to support role-playing for training, analysis and decision rehearsal.

Method
Confrontation analysis looks on an interaction as a sequence of confrontations."
How can confrontation analysis be used in decision workshops?,To analyze and rehearse decisions,To support role-playing for training,To provide a theoretical model for decision-making,To define emotional dilemmas in multi-party interactions,To structure and understand negotiations,B,"Confrontation analysis (also known as dilemma analysis) is an operational analysis technique used to structure, understand and think through multi-party interactions such as negotiations. It is the underpinning mathematical basis of drama theory. 
It is derived from game theory but considers that instead of resolving the game, the players often redefine the game when interacting. Emotions triggered from the potential interaction play a large part in this redefinition. So whereas game theory looks on an interaction as a single decision matrix and resolves that, confrontation analysis looks on the interaction as a sequence of linked interactions, where the decision matrix changes under the influence of precisely defined emotional dilemmas.

Derivation and use
Confrontation analysis was devised by Professor Nigel Howard in the early 1990s drawing from his work on game theory and metagame analysis. It has been turned to defence, political, legal, financial and commercial  applications.

Much of the theoretical background to General Rupert Smith's book The Utility of Force drew its inspiration from the theory of confrontation analysis.I am in debt to Professor Nigel Howard, whose explanation of Confrontation Analysis and Game Theory at a seminar in 1998 excited my interest. Our subsequent discussions helped me to order my thoughts and the lessons I had learned into a coherent structure with the result that, for the first time, I was able to understand my experiences within a theoretical model which allowed me to use them further
Confrontation analysis can also be used in a decision workshop as structure to support role-playing for training, analysis and decision rehearsal.

Method
Confrontation analysis looks on an interaction as a sequence of confrontations."
What is transportation engineering?,The application of technology and scientific principles to the planning and design of transportation facilities,The study of how to move people and goods efficiently and safely,The management of transportation facilities for any mode of transportation,The analysis of traveler decisions and forecasting of passenger travel,The development of safety protocols and signal timing,B,"Transportation engineering or transport engineering is the application of technology and scientific principles to the planning, functional design, operation and management of facilities for any mode of transportation in order to provide for the safe, efficient, rapid, comfortable, convenient, economical, and environmentally compatible movement of people and goods transport.
The planning aspects of transportation engineering relate to elements of urban planning, and involve technical forecasting decisions and political factors.  Technical forecasting of passenger travel usually involves an urban transportation planning model, requiring the estimation of trip generation (number of purposeful trips), trip distribution (destination choice, where the traveler is going), mode choice (mode that is being taken), and route assignment (the streets or routes that are being used).  More sophisticated forecasting can include other aspects of traveler decisions, including auto ownership, trip chaining (the decision to link individual trips together in a tour) and the choice of residential or business location (known as land use forecasting). Passenger trips are the focus of transportation engineering because they often represent the peak of demand on any transportation system.
A review of descriptions of the scope of various committees indicates that while facility planning and design continue to be the core of the transportation engineering field, such areas as operations planning, logistics, network analysis, financing, and policy analysis are also important, particularly to those working in highway and urban transportation. The National Council of Examiners for Engineering and Surveying (NCEES) list online the safety protocols, geometric design requirements, and signal timing.
Transportation engineering, primarily involves planning, design, construction, maintenance, and operation of transportation facilities. The facilities support air, highway, railroad, pipeline, water, and even space transportation. The design aspects of transportation engineering include the sizing of transportation facilities (how many lanes or how much capacity the facility has), determining the materials and thickness used in pavement designing the geometry (vertical and horizontal alignment) of the roadway (or track).
Before any planning occurs an engineer must take what is known as an inventory of the area or, if it is appropriate, the previous system in place."
What does the planning aspect of transportation engineering involve?,Forecasting of passenger travel,Determining the materials and thickness used in pavement,Analyzing network analysis and financing,Designing the geometry of roadways,Managing transportation facilities,A,"Transportation engineering or transport engineering is the application of technology and scientific principles to the planning, functional design, operation and management of facilities for any mode of transportation in order to provide for the safe, efficient, rapid, comfortable, convenient, economical, and environmentally compatible movement of people and goods transport.
The planning aspects of transportation engineering relate to elements of urban planning, and involve technical forecasting decisions and political factors.  Technical forecasting of passenger travel usually involves an urban transportation planning model, requiring the estimation of trip generation (number of purposeful trips), trip distribution (destination choice, where the traveler is going), mode choice (mode that is being taken), and route assignment (the streets or routes that are being used).  More sophisticated forecasting can include other aspects of traveler decisions, including auto ownership, trip chaining (the decision to link individual trips together in a tour) and the choice of residential or business location (known as land use forecasting). Passenger trips are the focus of transportation engineering because they often represent the peak of demand on any transportation system.
A review of descriptions of the scope of various committees indicates that while facility planning and design continue to be the core of the transportation engineering field, such areas as operations planning, logistics, network analysis, financing, and policy analysis are also important, particularly to those working in highway and urban transportation. The National Council of Examiners for Engineering and Surveying (NCEES) list online the safety protocols, geometric design requirements, and signal timing.
Transportation engineering, primarily involves planning, design, construction, maintenance, and operation of transportation facilities. The facilities support air, highway, railroad, pipeline, water, and even space transportation. The design aspects of transportation engineering include the sizing of transportation facilities (how many lanes or how much capacity the facility has), determining the materials and thickness used in pavement designing the geometry (vertical and horizontal alignment) of the roadway (or track).
Before any planning occurs an engineer must take what is known as an inventory of the area or, if it is appropriate, the previous system in place."
What are the core areas of transportation engineering?,Facility planning and design,Operations planning and logistics,Network analysis and financing,Policy analysis and land use forecasting,Safety protocols and geometric design requirements,A,"Transportation engineering or transport engineering is the application of technology and scientific principles to the planning, functional design, operation and management of facilities for any mode of transportation in order to provide for the safe, efficient, rapid, comfortable, convenient, economical, and environmentally compatible movement of people and goods transport.
The planning aspects of transportation engineering relate to elements of urban planning, and involve technical forecasting decisions and political factors.  Technical forecasting of passenger travel usually involves an urban transportation planning model, requiring the estimation of trip generation (number of purposeful trips), trip distribution (destination choice, where the traveler is going), mode choice (mode that is being taken), and route assignment (the streets or routes that are being used).  More sophisticated forecasting can include other aspects of traveler decisions, including auto ownership, trip chaining (the decision to link individual trips together in a tour) and the choice of residential or business location (known as land use forecasting). Passenger trips are the focus of transportation engineering because they often represent the peak of demand on any transportation system.
A review of descriptions of the scope of various committees indicates that while facility planning and design continue to be the core of the transportation engineering field, such areas as operations planning, logistics, network analysis, financing, and policy analysis are also important, particularly to those working in highway and urban transportation. The National Council of Examiners for Engineering and Surveying (NCEES) list online the safety protocols, geometric design requirements, and signal timing.
Transportation engineering, primarily involves planning, design, construction, maintenance, and operation of transportation facilities. The facilities support air, highway, railroad, pipeline, water, and even space transportation. The design aspects of transportation engineering include the sizing of transportation facilities (how many lanes or how much capacity the facility has), determining the materials and thickness used in pavement designing the geometry (vertical and horizontal alignment) of the roadway (or track).
Before any planning occurs an engineer must take what is known as an inventory of the area or, if it is appropriate, the previous system in place."
What does the design aspect of transportation engineering involve?,Determining the materials and thickness used in pavement,Analyzing network analysis and financing,Managing transportation facilities,Designing the geometry of roadways,Developing safety protocols and signal timing,D,"Transportation engineering or transport engineering is the application of technology and scientific principles to the planning, functional design, operation and management of facilities for any mode of transportation in order to provide for the safe, efficient, rapid, comfortable, convenient, economical, and environmentally compatible movement of people and goods transport.
The planning aspects of transportation engineering relate to elements of urban planning, and involve technical forecasting decisions and political factors.  Technical forecasting of passenger travel usually involves an urban transportation planning model, requiring the estimation of trip generation (number of purposeful trips), trip distribution (destination choice, where the traveler is going), mode choice (mode that is being taken), and route assignment (the streets or routes that are being used).  More sophisticated forecasting can include other aspects of traveler decisions, including auto ownership, trip chaining (the decision to link individual trips together in a tour) and the choice of residential or business location (known as land use forecasting). Passenger trips are the focus of transportation engineering because they often represent the peak of demand on any transportation system.
A review of descriptions of the scope of various committees indicates that while facility planning and design continue to be the core of the transportation engineering field, such areas as operations planning, logistics, network analysis, financing, and policy analysis are also important, particularly to those working in highway and urban transportation. The National Council of Examiners for Engineering and Surveying (NCEES) list online the safety protocols, geometric design requirements, and signal timing.
Transportation engineering, primarily involves planning, design, construction, maintenance, and operation of transportation facilities. The facilities support air, highway, railroad, pipeline, water, and even space transportation. The design aspects of transportation engineering include the sizing of transportation facilities (how many lanes or how much capacity the facility has), determining the materials and thickness used in pavement designing the geometry (vertical and horizontal alignment) of the roadway (or track).
Before any planning occurs an engineer must take what is known as an inventory of the area or, if it is appropriate, the previous system in place."
What must an engineer do before planning occurs in transportation engineering?,Design the geometry of the roadway,Analyze network analysis and financing,Manage transportation facilities,Take an inventory of the area or previous system in place,Develop safety protocols and signal timing,D,"Transportation engineering or transport engineering is the application of technology and scientific principles to the planning, functional design, operation and management of facilities for any mode of transportation in order to provide for the safe, efficient, rapid, comfortable, convenient, economical, and environmentally compatible movement of people and goods transport.
The planning aspects of transportation engineering relate to elements of urban planning, and involve technical forecasting decisions and political factors.  Technical forecasting of passenger travel usually involves an urban transportation planning model, requiring the estimation of trip generation (number of purposeful trips), trip distribution (destination choice, where the traveler is going), mode choice (mode that is being taken), and route assignment (the streets or routes that are being used).  More sophisticated forecasting can include other aspects of traveler decisions, including auto ownership, trip chaining (the decision to link individual trips together in a tour) and the choice of residential or business location (known as land use forecasting). Passenger trips are the focus of transportation engineering because they often represent the peak of demand on any transportation system.
A review of descriptions of the scope of various committees indicates that while facility planning and design continue to be the core of the transportation engineering field, such areas as operations planning, logistics, network analysis, financing, and policy analysis are also important, particularly to those working in highway and urban transportation. The National Council of Examiners for Engineering and Surveying (NCEES) list online the safety protocols, geometric design requirements, and signal timing.
Transportation engineering, primarily involves planning, design, construction, maintenance, and operation of transportation facilities. The facilities support air, highway, railroad, pipeline, water, and even space transportation. The design aspects of transportation engineering include the sizing of transportation facilities (how many lanes or how much capacity the facility has), determining the materials and thickness used in pavement designing the geometry (vertical and horizontal alignment) of the roadway (or track).
Before any planning occurs an engineer must take what is known as an inventory of the area or, if it is appropriate, the previous system in place."
What does aviation engineering deal with?,Construction of buildings and roads,Development of computer software,Design of airports and airspace,Research on marine life,Development of new medical treatments,C,"Aviation engineering is a branch of engineering which deals with airspace development, airport design, aircraft navigation technologies, and aerodrome planning. It also involves the formulation of public policy, regulations, aviation laws pertaining to airspace, airlines, airports, aerodromes and the conduct of air services agreements through treaty.
This branch of engineering is distinct from aerospace engineering which deals with the development of aircraft and spacecraft.

Airspace development
The global airspace is divided into territorial airspace which then belongs to a country. Generally, airspace has to be engineered to benefit both military and civil users. Planning and designing airspace is important so as not to affect military operations and in order to designate air routes for commercial airlines to navigate freely without intervention by military authorities. For instance, not all of China can be used for commercial aeronautical navigation. Certain airspaces are designated as military-use only. Navigating outside commercial airspaces in the country may lead to the risk of the astrayed aircraft.In prior years, airspace has been limited to military and airmail services."
Why is planning and designing airspace important?,To ensure military operations are not affected,To increase commercial airline profits,To encourage more countries to enter the aviation industry,To reduce the risk of aircraft collisions,To improve air traffic control systems,A,"Aviation engineering is a branch of engineering which deals with airspace development, airport design, aircraft navigation technologies, and aerodrome planning. It also involves the formulation of public policy, regulations, aviation laws pertaining to airspace, airlines, airports, aerodromes and the conduct of air services agreements through treaty.
This branch of engineering is distinct from aerospace engineering which deals with the development of aircraft and spacecraft.

Airspace development
The global airspace is divided into territorial airspace which then belongs to a country. Generally, airspace has to be engineered to benefit both military and civil users. Planning and designing airspace is important so as not to affect military operations and in order to designate air routes for commercial airlines to navigate freely without intervention by military authorities. For instance, not all of China can be used for commercial aeronautical navigation. Certain airspaces are designated as military-use only. Navigating outside commercial airspaces in the country may lead to the risk of the astrayed aircraft.In prior years, airspace has been limited to military and airmail services."
What is the purpose of designating military-use only airspaces?,To prevent commercial airlines from flying over certain areas,To provide exclusive space for military aircraft operations,To allow military authorities to control commercial airline routes,To discourage foreign countries from entering the aviation industry,To promote international cooperation in aviation,B,"Aviation engineering is a branch of engineering which deals with airspace development, airport design, aircraft navigation technologies, and aerodrome planning. It also involves the formulation of public policy, regulations, aviation laws pertaining to airspace, airlines, airports, aerodromes and the conduct of air services agreements through treaty.
This branch of engineering is distinct from aerospace engineering which deals with the development of aircraft and spacecraft.

Airspace development
The global airspace is divided into territorial airspace which then belongs to a country. Generally, airspace has to be engineered to benefit both military and civil users. Planning and designing airspace is important so as not to affect military operations and in order to designate air routes for commercial airlines to navigate freely without intervention by military authorities. For instance, not all of China can be used for commercial aeronautical navigation. Certain airspaces are designated as military-use only. Navigating outside commercial airspaces in the country may lead to the risk of the astrayed aircraft.In prior years, airspace has been limited to military and airmail services."
What are the risks of navigating outside commercial airspaces in a country?,Increased air traffic congestion,Higher fuel consumption,Potential interference from military authorities,Unavailability of navigational aids,Lack of passenger comfort amenities,C,"Aviation engineering is a branch of engineering which deals with airspace development, airport design, aircraft navigation technologies, and aerodrome planning. It also involves the formulation of public policy, regulations, aviation laws pertaining to airspace, airlines, airports, aerodromes and the conduct of air services agreements through treaty.
This branch of engineering is distinct from aerospace engineering which deals with the development of aircraft and spacecraft.

Airspace development
The global airspace is divided into territorial airspace which then belongs to a country. Generally, airspace has to be engineered to benefit both military and civil users. Planning and designing airspace is important so as not to affect military operations and in order to designate air routes for commercial airlines to navigate freely without intervention by military authorities. For instance, not all of China can be used for commercial aeronautical navigation. Certain airspaces are designated as military-use only. Navigating outside commercial airspaces in the country may lead to the risk of the astrayed aircraft.In prior years, airspace has been limited to military and airmail services."
What was the historical limitation of airspace?,Limited availability of military aircraft,Limited capacity for airmail services,Limited access to airspace for commercial purposes,Limited technology for air navigation,Limited funding for aviation research,C,"Aviation engineering is a branch of engineering which deals with airspace development, airport design, aircraft navigation technologies, and aerodrome planning. It also involves the formulation of public policy, regulations, aviation laws pertaining to airspace, airlines, airports, aerodromes and the conduct of air services agreements through treaty.
This branch of engineering is distinct from aerospace engineering which deals with the development of aircraft and spacecraft.

Airspace development
The global airspace is divided into territorial airspace which then belongs to a country. Generally, airspace has to be engineered to benefit both military and civil users. Planning and designing airspace is important so as not to affect military operations and in order to designate air routes for commercial airlines to navigate freely without intervention by military authorities. For instance, not all of China can be used for commercial aeronautical navigation. Certain airspaces are designated as military-use only. Navigating outside commercial airspaces in the country may lead to the risk of the astrayed aircraft.In prior years, airspace has been limited to military and airmail services."
What is dark matter?,A form of matter that interacts with the electromagnetic field and can be easily detected,A form of matter that does not interact with the electromagnetic field and is difficult to detect,A form of matter that emits electromagnetic radiation and is easily observable,A form of matter that reflects electromagnetic radiation and can be seen,A form of matter that absorbs electromagnetic radiation and is visible,B,"Dark matter is a hypothetical form of matter thought to account for approximately 85% of the matter in the universe. Dark matter is called ""dark"" because it does not appear to interact with the electromagnetic field, which means it does not absorb, reflect, or emit electromagnetic radiation and is, therefore, difficult to detect. Various astrophysical observations – including gravitational effects which cannot be explained by currently accepted theories of gravity unless more matter is present than can be seen – imply dark matter's presence. For this reason, most experts think that dark matter is abundant in the universe and has had a strong influence on its structure and evolution.The primary evidence for dark matter comes from calculations showing that many galaxies would behave quite differently if they did not contain a large amount of unseen matter. Some galaxies would not have formed at all and others would not move as they currently do. Other lines of evidence include observations in gravitational lensing and the cosmic microwave background, along with astronomical observations of the observable universe's current structure, the formation and evolution of galaxies, mass location during galactic collisions, and the motion of galaxies within galaxy clusters. In the standard Lambda-CDM model of cosmology, the total mass–energy content of the universe contains 5% ordinary matter, 26.8% dark matter, and 68.2% of a form of energy known as dark energy."
"Why is dark matter called ""dark""?",Because it emits a dark-colored light,Because it absorbs all light and appears black,Because it reflects all light and appears white,Because it does not interact with electromagnetic radiation and cannot be seen,Because it produces a dark shadow wherever it is present,D,"Dark matter is a hypothetical form of matter thought to account for approximately 85% of the matter in the universe. Dark matter is called ""dark"" because it does not appear to interact with the electromagnetic field, which means it does not absorb, reflect, or emit electromagnetic radiation and is, therefore, difficult to detect. Various astrophysical observations – including gravitational effects which cannot be explained by currently accepted theories of gravity unless more matter is present than can be seen – imply dark matter's presence. For this reason, most experts think that dark matter is abundant in the universe and has had a strong influence on its structure and evolution.The primary evidence for dark matter comes from calculations showing that many galaxies would behave quite differently if they did not contain a large amount of unseen matter. Some galaxies would not have formed at all and others would not move as they currently do. Other lines of evidence include observations in gravitational lensing and the cosmic microwave background, along with astronomical observations of the observable universe's current structure, the formation and evolution of galaxies, mass location during galactic collisions, and the motion of galaxies within galaxy clusters. In the standard Lambda-CDM model of cosmology, the total mass–energy content of the universe contains 5% ordinary matter, 26.8% dark matter, and 68.2% of a form of energy known as dark energy."
What are some astrophysical observations that imply the presence of dark matter?,Observations of dark matter interacting with the electromagnetic field,Observations of dark matter emitting electromagnetic radiation,Observations of dark matter absorbing electromagnetic radiation,Observations of dark matter reflecting electromagnetic radiation,Observations of gravitational effects that cannot be explained by current theories of gravity,E,"Dark matter is a hypothetical form of matter thought to account for approximately 85% of the matter in the universe. Dark matter is called ""dark"" because it does not appear to interact with the electromagnetic field, which means it does not absorb, reflect, or emit electromagnetic radiation and is, therefore, difficult to detect. Various astrophysical observations – including gravitational effects which cannot be explained by currently accepted theories of gravity unless more matter is present than can be seen – imply dark matter's presence. For this reason, most experts think that dark matter is abundant in the universe and has had a strong influence on its structure and evolution.The primary evidence for dark matter comes from calculations showing that many galaxies would behave quite differently if they did not contain a large amount of unseen matter. Some galaxies would not have formed at all and others would not move as they currently do. Other lines of evidence include observations in gravitational lensing and the cosmic microwave background, along with astronomical observations of the observable universe's current structure, the formation and evolution of galaxies, mass location during galactic collisions, and the motion of galaxies within galaxy clusters. In the standard Lambda-CDM model of cosmology, the total mass–energy content of the universe contains 5% ordinary matter, 26.8% dark matter, and 68.2% of a form of energy known as dark energy."
What is the primary evidence for dark matter?,Observations of dark matter interacting with visible matter,Observations of dark matter emitting visible light,Calculations showing that galaxies would behave differently without the presence of unseen matter,Observations of dark matter absorbing visible light,Observations of dark matter reflecting visible light,C,"Dark matter is a hypothetical form of matter thought to account for approximately 85% of the matter in the universe. Dark matter is called ""dark"" because it does not appear to interact with the electromagnetic field, which means it does not absorb, reflect, or emit electromagnetic radiation and is, therefore, difficult to detect. Various astrophysical observations – including gravitational effects which cannot be explained by currently accepted theories of gravity unless more matter is present than can be seen – imply dark matter's presence. For this reason, most experts think that dark matter is abundant in the universe and has had a strong influence on its structure and evolution.The primary evidence for dark matter comes from calculations showing that many galaxies would behave quite differently if they did not contain a large amount of unseen matter. Some galaxies would not have formed at all and others would not move as they currently do. Other lines of evidence include observations in gravitational lensing and the cosmic microwave background, along with astronomical observations of the observable universe's current structure, the formation and evolution of galaxies, mass location during galactic collisions, and the motion of galaxies within galaxy clusters. In the standard Lambda-CDM model of cosmology, the total mass–energy content of the universe contains 5% ordinary matter, 26.8% dark matter, and 68.2% of a form of energy known as dark energy."
What is the total mass-energy content of the universe according to the standard Lambda-CDM model?,"5% ordinary matter, 26.8% dark matter, and 68.2% dark energy","5% ordinary matter, 68.2% dark matter, and 26.8% dark energy","26.8% ordinary matter, 5% dark matter, and 68.2% dark energy","26.8% ordinary matter, 68.2% dark matter, and 5% dark energy","68.2% ordinary matter, 5% dark matter, and 26.8% dark energy",A,"Dark matter is a hypothetical form of matter thought to account for approximately 85% of the matter in the universe. Dark matter is called ""dark"" because it does not appear to interact with the electromagnetic field, which means it does not absorb, reflect, or emit electromagnetic radiation and is, therefore, difficult to detect. Various astrophysical observations – including gravitational effects which cannot be explained by currently accepted theories of gravity unless more matter is present than can be seen – imply dark matter's presence. For this reason, most experts think that dark matter is abundant in the universe and has had a strong influence on its structure and evolution.The primary evidence for dark matter comes from calculations showing that many galaxies would behave quite differently if they did not contain a large amount of unseen matter. Some galaxies would not have formed at all and others would not move as they currently do. Other lines of evidence include observations in gravitational lensing and the cosmic microwave background, along with astronomical observations of the observable universe's current structure, the formation and evolution of galaxies, mass location during galactic collisions, and the motion of galaxies within galaxy clusters. In the standard Lambda-CDM model of cosmology, the total mass–energy content of the universe contains 5% ordinary matter, 26.8% dark matter, and 68.2% of a form of energy known as dark energy."
What is a bell jar commonly used for in laboratories?,Storing chemicals,Generating electricity,Creating a vacuum,Heating substances,Measuring temperature,C,"A bell jar is a glass jar, similar in shape to a bell (i.e. in its best-known form it is open at the bottom, while its top and sides together are a single piece), and can be manufactured from a variety of materials (ranging from glass to different types of metals). Bell jars are often used in laboratories to form and contain a vacuum. It is a common science apparatus used in experiments. Bell jars have a limited ability to create strong vacuums; vacuum chambers are available when higher performance is needed. They have been used to demonstrate the effect of vacuum on sound propagation.
In addition to their scientific applications, bell jars may also serve as display cases or transparent dust covers. In these situations the bell jar is not usually placed under vacuum.

Vacuum
A vacuum bell jar is placed on a base which is vented to a hose fitting, that can be connected via a hose to a vacuum pump."
What are bell jars made of?,Stainless steel,Plastic,Glass,Copper,Brass,C,"A bell jar is a glass jar, similar in shape to a bell (i.e. in its best-known form it is open at the bottom, while its top and sides together are a single piece), and can be manufactured from a variety of materials (ranging from glass to different types of metals). Bell jars are often used in laboratories to form and contain a vacuum. It is a common science apparatus used in experiments. Bell jars have a limited ability to create strong vacuums; vacuum chambers are available when higher performance is needed. They have been used to demonstrate the effect of vacuum on sound propagation.
In addition to their scientific applications, bell jars may also serve as display cases or transparent dust covers. In these situations the bell jar is not usually placed under vacuum.

Vacuum
A vacuum bell jar is placed on a base which is vented to a hose fitting, that can be connected via a hose to a vacuum pump."
What is the primary shape of a bell jar?,Cylinder,Cone,Cube,Bowl,Bell,E,"A bell jar is a glass jar, similar in shape to a bell (i.e. in its best-known form it is open at the bottom, while its top and sides together are a single piece), and can be manufactured from a variety of materials (ranging from glass to different types of metals). Bell jars are often used in laboratories to form and contain a vacuum. It is a common science apparatus used in experiments. Bell jars have a limited ability to create strong vacuums; vacuum chambers are available when higher performance is needed. They have been used to demonstrate the effect of vacuum on sound propagation.
In addition to their scientific applications, bell jars may also serve as display cases or transparent dust covers. In these situations the bell jar is not usually placed under vacuum.

Vacuum
A vacuum bell jar is placed on a base which is vented to a hose fitting, that can be connected via a hose to a vacuum pump."
What is the purpose of the base of a vacuum bell jar?,To seal the jar,To connect to a vacuum pump,To hold the experiment,To create a vacuum,To display the jar,B,"A bell jar is a glass jar, similar in shape to a bell (i.e. in its best-known form it is open at the bottom, while its top and sides together are a single piece), and can be manufactured from a variety of materials (ranging from glass to different types of metals). Bell jars are often used in laboratories to form and contain a vacuum. It is a common science apparatus used in experiments. Bell jars have a limited ability to create strong vacuums; vacuum chambers are available when higher performance is needed. They have been used to demonstrate the effect of vacuum on sound propagation.
In addition to their scientific applications, bell jars may also serve as display cases or transparent dust covers. In these situations the bell jar is not usually placed under vacuum.

Vacuum
A vacuum bell jar is placed on a base which is vented to a hose fitting, that can be connected via a hose to a vacuum pump."
What is the function of a vacuum pump?,To remove air from the bell jar,To create high pressure in the bell jar,To generate heat inside the bell jar,To rotate the bell jar,To measure the volume of the bell jar,A,"A bell jar is a glass jar, similar in shape to a bell (i.e. in its best-known form it is open at the bottom, while its top and sides together are a single piece), and can be manufactured from a variety of materials (ranging from glass to different types of metals). Bell jars are often used in laboratories to form and contain a vacuum. It is a common science apparatus used in experiments. Bell jars have a limited ability to create strong vacuums; vacuum chambers are available when higher performance is needed. They have been used to demonstrate the effect of vacuum on sound propagation.
In addition to their scientific applications, bell jars may also serve as display cases or transparent dust covers. In these situations the bell jar is not usually placed under vacuum.

Vacuum
A vacuum bell jar is placed on a base which is vented to a hose fitting, that can be connected via a hose to a vacuum pump."
What is digital ecology?,A branch of biology that studies the interaction between digital systems and the natural environment,A field of study that looks at the impact of digital technologies on the environment,A type of pollution caused by electronic waste,A practice that promotes sustainable solutions for digital technologies,A complex and multifaceted field that requires a holistic approach,B,"Digital ecology is a science about the interdependence of digital systems and the natural environment. This field of study looks at the methods in which digital technologies are changing the way how people interact with the environment, as well as how these technologies affects the environment itself. It is a branch of ecology that promotes green practices to fight digital pollution. Currently the total carbon footprint of the internet, our electronic devices, and supporting elements accounts for about 3.7% of global greenhouse gas emissions (including about 1.4 per cent of overall global carbon dioxide emissions).Digital ecology is a complex and multifaceted field that requires a holistic approach to understanding the relationship between digital technologies and the natural world. With the increasing reliance on digital technologies, it is important to consider the environmental consequences of these technologies and work towards more sustainable solutions.

Negative impact on the environment
One of the main areas of focus in digital ecology is the impact of electronic waste, or e-waste. As more and more devices become obsolete and are replaced with newer models, the amount of e-waste being produced is increasing at an alarming rate. This e-waste often ends up in landfills, where it can leach harmful chemicals into the soil and water supply.Another aspect of digital ecology is the energy consumption of digital technologies and the digital pollution in causes."
What is the total carbon footprint of the internet and electronic devices?,1.4% of global greenhouse gas emissions,3.7% of global greenhouse gas emissions,1.4% of overall global carbon dioxide emissions,3.7% of overall global carbon dioxide emissions,Both 1.4% and 3.7%,E,"Digital ecology is a science about the interdependence of digital systems and the natural environment. This field of study looks at the methods in which digital technologies are changing the way how people interact with the environment, as well as how these technologies affects the environment itself. It is a branch of ecology that promotes green practices to fight digital pollution. Currently the total carbon footprint of the internet, our electronic devices, and supporting elements accounts for about 3.7% of global greenhouse gas emissions (including about 1.4 per cent of overall global carbon dioxide emissions).Digital ecology is a complex and multifaceted field that requires a holistic approach to understanding the relationship between digital technologies and the natural world. With the increasing reliance on digital technologies, it is important to consider the environmental consequences of these technologies and work towards more sustainable solutions.

Negative impact on the environment
One of the main areas of focus in digital ecology is the impact of electronic waste, or e-waste. As more and more devices become obsolete and are replaced with newer models, the amount of e-waste being produced is increasing at an alarming rate. This e-waste often ends up in landfills, where it can leach harmful chemicals into the soil and water supply.Another aspect of digital ecology is the energy consumption of digital technologies and the digital pollution in causes."
What is one of the main areas of focus in digital ecology?,The impact of digital technologies on the environment,The energy consumption of digital technologies,The production of electronic waste,The promotion of green practices,The relationship between digital technologies and the natural world,C,"Digital ecology is a science about the interdependence of digital systems and the natural environment. This field of study looks at the methods in which digital technologies are changing the way how people interact with the environment, as well as how these technologies affects the environment itself. It is a branch of ecology that promotes green practices to fight digital pollution. Currently the total carbon footprint of the internet, our electronic devices, and supporting elements accounts for about 3.7% of global greenhouse gas emissions (including about 1.4 per cent of overall global carbon dioxide emissions).Digital ecology is a complex and multifaceted field that requires a holistic approach to understanding the relationship between digital technologies and the natural world. With the increasing reliance on digital technologies, it is important to consider the environmental consequences of these technologies and work towards more sustainable solutions.

Negative impact on the environment
One of the main areas of focus in digital ecology is the impact of electronic waste, or e-waste. As more and more devices become obsolete and are replaced with newer models, the amount of e-waste being produced is increasing at an alarming rate. This e-waste often ends up in landfills, where it can leach harmful chemicals into the soil and water supply.Another aspect of digital ecology is the energy consumption of digital technologies and the digital pollution in causes."
What happens to e-waste that ends up in landfills?,It leaches harmful chemicals into the soil and water supply,It gets recycled and turned into new devices,It is burned and releases toxic fumes into the air,It decomposes naturally without causing any harm,It is buried deep underground and poses no risk to the environment,A,"Digital ecology is a science about the interdependence of digital systems and the natural environment. This field of study looks at the methods in which digital technologies are changing the way how people interact with the environment, as well as how these technologies affects the environment itself. It is a branch of ecology that promotes green practices to fight digital pollution. Currently the total carbon footprint of the internet, our electronic devices, and supporting elements accounts for about 3.7% of global greenhouse gas emissions (including about 1.4 per cent of overall global carbon dioxide emissions).Digital ecology is a complex and multifaceted field that requires a holistic approach to understanding the relationship between digital technologies and the natural world. With the increasing reliance on digital technologies, it is important to consider the environmental consequences of these technologies and work towards more sustainable solutions.

Negative impact on the environment
One of the main areas of focus in digital ecology is the impact of electronic waste, or e-waste. As more and more devices become obsolete and are replaced with newer models, the amount of e-waste being produced is increasing at an alarming rate. This e-waste often ends up in landfills, where it can leach harmful chemicals into the soil and water supply.Another aspect of digital ecology is the energy consumption of digital technologies and the digital pollution in causes."
What is digital pollution?,A type of pollution caused by digital technologies,A type of pollution caused by electronic waste,A practice that promotes sustainable solutions for digital technologies,A complex and multifaceted field that requires a holistic approach,The impact of digital technologies on the environment,A,"Digital ecology is a science about the interdependence of digital systems and the natural environment. This field of study looks at the methods in which digital technologies are changing the way how people interact with the environment, as well as how these technologies affects the environment itself. It is a branch of ecology that promotes green practices to fight digital pollution. Currently the total carbon footprint of the internet, our electronic devices, and supporting elements accounts for about 3.7% of global greenhouse gas emissions (including about 1.4 per cent of overall global carbon dioxide emissions).Digital ecology is a complex and multifaceted field that requires a holistic approach to understanding the relationship between digital technologies and the natural world. With the increasing reliance on digital technologies, it is important to consider the environmental consequences of these technologies and work towards more sustainable solutions.

Negative impact on the environment
One of the main areas of focus in digital ecology is the impact of electronic waste, or e-waste. As more and more devices become obsolete and are replaced with newer models, the amount of e-waste being produced is increasing at an alarming rate. This e-waste often ends up in landfills, where it can leach harmful chemicals into the soil and water supply.Another aspect of digital ecology is the energy consumption of digital technologies and the digital pollution in causes."
What is the main focus of energy engineering?,Developing renewable energy sources,Increasing energy efficiency,Managing facility operations,Ensuring environmental compliance,Improving manufacturing processes,B,"Energy engineering or Energy Systems Engineering is a broad field of engineering dealing with energy efficiency, energy services, facility management, plant engineering, environmental compliance, sustainable energy and renewable energy technologies. Energy engineering is one of the most recent engineering disciplines to emerge. Energy engineering combines knowledge from the fields of physics, math, and chemistry with economic and environmental engineering practices. Energy engineers apply their skills to increase efficiency and further develop renewable sources of energy.
The main job of energy engineers is to find the most efficient and sustainable ways to operate buildings and manufacturing processes. Energy engineers audit the use of energy in those processes and suggest ways to improve the systems. This means suggesting advanced lighting, better insulation, more efficient heating and cooling properties of buildings. Although an energy engineer is concerned about obtaining and using energy in the most environmentally friendly ways, their field is not limited to strictly renewable energy like hydro, solar, biomass, or geothermal."
Which fields of knowledge are combined in energy engineering?,"Physics, math, and chemistry",Economics and environmental engineering,"Physics, math, and biology","Chemistry, economics, and biology","Physics, chemistry, and biology",A,"Energy engineering or Energy Systems Engineering is a broad field of engineering dealing with energy efficiency, energy services, facility management, plant engineering, environmental compliance, sustainable energy and renewable energy technologies. Energy engineering is one of the most recent engineering disciplines to emerge. Energy engineering combines knowledge from the fields of physics, math, and chemistry with economic and environmental engineering practices. Energy engineers apply their skills to increase efficiency and further develop renewable sources of energy.
The main job of energy engineers is to find the most efficient and sustainable ways to operate buildings and manufacturing processes. Energy engineers audit the use of energy in those processes and suggest ways to improve the systems. This means suggesting advanced lighting, better insulation, more efficient heating and cooling properties of buildings. Although an energy engineer is concerned about obtaining and using energy in the most environmentally friendly ways, their field is not limited to strictly renewable energy like hydro, solar, biomass, or geothermal."
What are some examples of systems that energy engineers suggest improvements for?,Transportation and logistics systems,Manufacturing and production systems,Communication and information systems,Healthcare and medical systems,Energy distribution systems,B,"Energy engineering or Energy Systems Engineering is a broad field of engineering dealing with energy efficiency, energy services, facility management, plant engineering, environmental compliance, sustainable energy and renewable energy technologies. Energy engineering is one of the most recent engineering disciplines to emerge. Energy engineering combines knowledge from the fields of physics, math, and chemistry with economic and environmental engineering practices. Energy engineers apply their skills to increase efficiency and further develop renewable sources of energy.
The main job of energy engineers is to find the most efficient and sustainable ways to operate buildings and manufacturing processes. Energy engineers audit the use of energy in those processes and suggest ways to improve the systems. This means suggesting advanced lighting, better insulation, more efficient heating and cooling properties of buildings. Although an energy engineer is concerned about obtaining and using energy in the most environmentally friendly ways, their field is not limited to strictly renewable energy like hydro, solar, biomass, or geothermal."
Are energy engineers limited to renewable energy sources?,"Yes, they only work with renewable energy","No, they also work with non-renewable energy","Yes, but they are not limited to any specific type of renewable energy","No, they only work with non-renewable energy","Yes, but they mainly focus on hydro, solar, biomass, and geothermal energy",B,"Energy engineering or Energy Systems Engineering is a broad field of engineering dealing with energy efficiency, energy services, facility management, plant engineering, environmental compliance, sustainable energy and renewable energy technologies. Energy engineering is one of the most recent engineering disciplines to emerge. Energy engineering combines knowledge from the fields of physics, math, and chemistry with economic and environmental engineering practices. Energy engineers apply their skills to increase efficiency and further develop renewable sources of energy.
The main job of energy engineers is to find the most efficient and sustainable ways to operate buildings and manufacturing processes. Energy engineers audit the use of energy in those processes and suggest ways to improve the systems. This means suggesting advanced lighting, better insulation, more efficient heating and cooling properties of buildings. Although an energy engineer is concerned about obtaining and using energy in the most environmentally friendly ways, their field is not limited to strictly renewable energy like hydro, solar, biomass, or geothermal."
What are some examples of energy efficiency improvements that energy engineers suggest for buildings?,Upgrading energy management systems,Installing energy-efficient lighting,Improving insulation,Optimizing heating and cooling systems,All of the above,E,"Energy engineering or Energy Systems Engineering is a broad field of engineering dealing with energy efficiency, energy services, facility management, plant engineering, environmental compliance, sustainable energy and renewable energy technologies. Energy engineering is one of the most recent engineering disciplines to emerge. Energy engineering combines knowledge from the fields of physics, math, and chemistry with economic and environmental engineering practices. Energy engineers apply their skills to increase efficiency and further develop renewable sources of energy.
The main job of energy engineers is to find the most efficient and sustainable ways to operate buildings and manufacturing processes. Energy engineers audit the use of energy in those processes and suggest ways to improve the systems. This means suggesting advanced lighting, better insulation, more efficient heating and cooling properties of buildings. Although an energy engineer is concerned about obtaining and using energy in the most environmentally friendly ways, their field is not limited to strictly renewable energy like hydro, solar, biomass, or geothermal."
What is the primary motivation for pure mathematicians?,To solve real-world problems,To create practical applications,To work out logical consequences of basic principles,To apply mathematical theories to physics and computer science,To achieve mathematical rigor,C,"Pure mathematics is the study of mathematical concepts independently of any application outside mathematics. These concepts may originate in real-world concerns, and the results obtained may later turn out to be useful for practical applications, but pure mathematicians are not primarily motivated by such applications. Instead, the appeal is attributed to the intellectual challenge and aesthetic beauty of working out the logical consequences of basic principles.
While pure mathematics has existed as an activity since at least ancient Greece, the concept was elaborated upon around the year 1900, after the introduction of theories with counter-intuitive properties (such as non-Euclidean geometries and Cantor's theory of infinite sets), and the discovery of apparent paradoxes (such as continuous functions that are nowhere differentiable, and Russell's paradox). This introduced the need to renew the concept of mathematical rigor and rewrite all mathematics accordingly, with a systematic use of axiomatic methods. This led many mathematicians to focus on mathematics for its own sake, that is, pure mathematics.
Nevertheless, almost all mathematical theories remained motivated by problems coming from the real world or from less abstract mathematical theories. Also, many mathematical theories, which had seemed to be totally pure mathematics, were eventually used in applied areas, mainly physics and computer science. A famous early example is Isaac Newton's demonstration that his law of universal gravitation implied that planets move in orbits that are conic sections, geometrical curves that had been studied in antiquity by Apollonius."
When did the concept of pure mathematics become more elaborate?,In ancient Greece,Around the year 1900,With the introduction of non-Euclidean geometries,With the discovery of Russell's paradox,After the development of axiomatic methods,B,"Pure mathematics is the study of mathematical concepts independently of any application outside mathematics. These concepts may originate in real-world concerns, and the results obtained may later turn out to be useful for practical applications, but pure mathematicians are not primarily motivated by such applications. Instead, the appeal is attributed to the intellectual challenge and aesthetic beauty of working out the logical consequences of basic principles.
While pure mathematics has existed as an activity since at least ancient Greece, the concept was elaborated upon around the year 1900, after the introduction of theories with counter-intuitive properties (such as non-Euclidean geometries and Cantor's theory of infinite sets), and the discovery of apparent paradoxes (such as continuous functions that are nowhere differentiable, and Russell's paradox). This introduced the need to renew the concept of mathematical rigor and rewrite all mathematics accordingly, with a systematic use of axiomatic methods. This led many mathematicians to focus on mathematics for its own sake, that is, pure mathematics.
Nevertheless, almost all mathematical theories remained motivated by problems coming from the real world or from less abstract mathematical theories. Also, many mathematical theories, which had seemed to be totally pure mathematics, were eventually used in applied areas, mainly physics and computer science. A famous early example is Isaac Newton's demonstration that his law of universal gravitation implied that planets move in orbits that are conic sections, geometrical curves that had been studied in antiquity by Apollonius."
What is an example of a famous early application of pure mathematics?,The use of Cantor's theory of infinite sets in computer science,The study of non-Euclidean geometries in physics,The demonstration of conic sections in planetary motion by Isaac Newton,The use of continuous functions in mathematical physics,The development of mathematical rigor in ancient Greece,C,"Pure mathematics is the study of mathematical concepts independently of any application outside mathematics. These concepts may originate in real-world concerns, and the results obtained may later turn out to be useful for practical applications, but pure mathematicians are not primarily motivated by such applications. Instead, the appeal is attributed to the intellectual challenge and aesthetic beauty of working out the logical consequences of basic principles.
While pure mathematics has existed as an activity since at least ancient Greece, the concept was elaborated upon around the year 1900, after the introduction of theories with counter-intuitive properties (such as non-Euclidean geometries and Cantor's theory of infinite sets), and the discovery of apparent paradoxes (such as continuous functions that are nowhere differentiable, and Russell's paradox). This introduced the need to renew the concept of mathematical rigor and rewrite all mathematics accordingly, with a systematic use of axiomatic methods. This led many mathematicians to focus on mathematics for its own sake, that is, pure mathematics.
Nevertheless, almost all mathematical theories remained motivated by problems coming from the real world or from less abstract mathematical theories. Also, many mathematical theories, which had seemed to be totally pure mathematics, were eventually used in applied areas, mainly physics and computer science. A famous early example is Isaac Newton's demonstration that his law of universal gravitation implied that planets move in orbits that are conic sections, geometrical curves that had been studied in antiquity by Apollonius."
What led to the renewed focus on mathematical rigor in pure mathematics?,The introduction of non-Euclidean geometries,The discovery of apparent paradoxes,The need to apply mathematics to physics and computer science,The development of axiomatic methods,The study of ancient Greek mathematicians,B,"Pure mathematics is the study of mathematical concepts independently of any application outside mathematics. These concepts may originate in real-world concerns, and the results obtained may later turn out to be useful for practical applications, but pure mathematicians are not primarily motivated by such applications. Instead, the appeal is attributed to the intellectual challenge and aesthetic beauty of working out the logical consequences of basic principles.
While pure mathematics has existed as an activity since at least ancient Greece, the concept was elaborated upon around the year 1900, after the introduction of theories with counter-intuitive properties (such as non-Euclidean geometries and Cantor's theory of infinite sets), and the discovery of apparent paradoxes (such as continuous functions that are nowhere differentiable, and Russell's paradox). This introduced the need to renew the concept of mathematical rigor and rewrite all mathematics accordingly, with a systematic use of axiomatic methods. This led many mathematicians to focus on mathematics for its own sake, that is, pure mathematics.
Nevertheless, almost all mathematical theories remained motivated by problems coming from the real world or from less abstract mathematical theories. Also, many mathematical theories, which had seemed to be totally pure mathematics, were eventually used in applied areas, mainly physics and computer science. A famous early example is Isaac Newton's demonstration that his law of universal gravitation implied that planets move in orbits that are conic sections, geometrical curves that had been studied in antiquity by Apollonius."
What are some areas where pure mathematics has been applied?,Computer science and physics,Ancient Greece and antiquity,Real-world problems and practical applications,Non-Euclidean geometries and Cantor's theory of infinite sets,Russell's paradox and continuous functions,A,"Pure mathematics is the study of mathematical concepts independently of any application outside mathematics. These concepts may originate in real-world concerns, and the results obtained may later turn out to be useful for practical applications, but pure mathematicians are not primarily motivated by such applications. Instead, the appeal is attributed to the intellectual challenge and aesthetic beauty of working out the logical consequences of basic principles.
While pure mathematics has existed as an activity since at least ancient Greece, the concept was elaborated upon around the year 1900, after the introduction of theories with counter-intuitive properties (such as non-Euclidean geometries and Cantor's theory of infinite sets), and the discovery of apparent paradoxes (such as continuous functions that are nowhere differentiable, and Russell's paradox). This introduced the need to renew the concept of mathematical rigor and rewrite all mathematics accordingly, with a systematic use of axiomatic methods. This led many mathematicians to focus on mathematics for its own sake, that is, pure mathematics.
Nevertheless, almost all mathematical theories remained motivated by problems coming from the real world or from less abstract mathematical theories. Also, many mathematical theories, which had seemed to be totally pure mathematics, were eventually used in applied areas, mainly physics and computer science. A famous early example is Isaac Newton's demonstration that his law of universal gravitation implied that planets move in orbits that are conic sections, geometrical curves that had been studied in antiquity by Apollonius."
What is physical science?,The study of living systems,The study of non-living systems,The study of both living and non-living systems,The study of social systems,The study of historical systems,B,"Physical science is a branch of natural science that studies non-living systems, in contrast to life science. It in turn has many branches, each referred to as a ""physical science"", together called the ""physical sciences"".

Definition
Physical science can be described as all of the following:

A branch of science  (a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe).A branch of natural science – natural science is a major branch of science that tries to explain and predict nature's phenomena, based on empirical evidence. In natural science, hypotheses must be verified scientifically to be regarded as scientific theory.  Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into two main branches: life science (for example biology) and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.

Branches of physical science
Physics – natural and physical science could involve the study of matter and its motion through space and time, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.Branches of physics
Astronomy – study of celestial objects (such as stars, galaxies, planets, moons, asteroids, comets and nebulae), the physics, chemistry, and evolution of such objects, and phenomena that originate outside the atmosphere of Earth, including supernovae explosions, gamma-ray bursts, and cosmic microwave background radiation.
Branches of astronomy
Chemistry – studies the composition, structure, properties and change of matter."
What are the two main branches of natural science?,Life science and physical science,Chemistry and physics,Astronomy and biology,Geology and ecology,Oceanography and meteorology,A,"Physical science is a branch of natural science that studies non-living systems, in contrast to life science. It in turn has many branches, each referred to as a ""physical science"", together called the ""physical sciences"".

Definition
Physical science can be described as all of the following:

A branch of science  (a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe).A branch of natural science – natural science is a major branch of science that tries to explain and predict nature's phenomena, based on empirical evidence. In natural science, hypotheses must be verified scientifically to be regarded as scientific theory.  Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into two main branches: life science (for example biology) and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.

Branches of physical science
Physics – natural and physical science could involve the study of matter and its motion through space and time, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.Branches of physics
Astronomy – study of celestial objects (such as stars, galaxies, planets, moons, asteroids, comets and nebulae), the physics, chemistry, and evolution of such objects, and phenomena that originate outside the atmosphere of Earth, including supernovae explosions, gamma-ray bursts, and cosmic microwave background radiation.
Branches of astronomy
Chemistry – studies the composition, structure, properties and change of matter."
What does physics study?,Living organisms,Celestial objects,The composition of matter,The behavior of the universe,Social systems,D,"Physical science is a branch of natural science that studies non-living systems, in contrast to life science. It in turn has many branches, each referred to as a ""physical science"", together called the ""physical sciences"".

Definition
Physical science can be described as all of the following:

A branch of science  (a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe).A branch of natural science – natural science is a major branch of science that tries to explain and predict nature's phenomena, based on empirical evidence. In natural science, hypotheses must be verified scientifically to be regarded as scientific theory.  Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into two main branches: life science (for example biology) and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.

Branches of physical science
Physics – natural and physical science could involve the study of matter and its motion through space and time, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.Branches of physics
Astronomy – study of celestial objects (such as stars, galaxies, planets, moons, asteroids, comets and nebulae), the physics, chemistry, and evolution of such objects, and phenomena that originate outside the atmosphere of Earth, including supernovae explosions, gamma-ray bursts, and cosmic microwave background radiation.
Branches of astronomy
Chemistry – studies the composition, structure, properties and change of matter."
What does astronomy study?,Living organisms,Celestial objects,The composition of matter,The behavior of the universe,Social systems,B,"Physical science is a branch of natural science that studies non-living systems, in contrast to life science. It in turn has many branches, each referred to as a ""physical science"", together called the ""physical sciences"".

Definition
Physical science can be described as all of the following:

A branch of science  (a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe).A branch of natural science – natural science is a major branch of science that tries to explain and predict nature's phenomena, based on empirical evidence. In natural science, hypotheses must be verified scientifically to be regarded as scientific theory.  Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into two main branches: life science (for example biology) and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.

Branches of physical science
Physics – natural and physical science could involve the study of matter and its motion through space and time, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.Branches of physics
Astronomy – study of celestial objects (such as stars, galaxies, planets, moons, asteroids, comets and nebulae), the physics, chemistry, and evolution of such objects, and phenomena that originate outside the atmosphere of Earth, including supernovae explosions, gamma-ray bursts, and cosmic microwave background radiation.
Branches of astronomy
Chemistry – studies the composition, structure, properties and change of matter."
What does chemistry study?,Living organisms,Celestial objects,The composition of matter,The behavior of the universe,Social systems,C,"Physical science is a branch of natural science that studies non-living systems, in contrast to life science. It in turn has many branches, each referred to as a ""physical science"", together called the ""physical sciences"".

Definition
Physical science can be described as all of the following:

A branch of science  (a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe).A branch of natural science – natural science is a major branch of science that tries to explain and predict nature's phenomena, based on empirical evidence. In natural science, hypotheses must be verified scientifically to be regarded as scientific theory.  Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into two main branches: life science (for example biology) and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.

Branches of physical science
Physics – natural and physical science could involve the study of matter and its motion through space and time, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.Branches of physics
Astronomy – study of celestial objects (such as stars, galaxies, planets, moons, asteroids, comets and nebulae), the physics, chemistry, and evolution of such objects, and phenomena that originate outside the atmosphere of Earth, including supernovae explosions, gamma-ray bursts, and cosmic microwave background radiation.
Branches of astronomy
Chemistry – studies the composition, structure, properties and change of matter."
What is the main goal of earthquake engineering?,To construct inexpensive structures,To design structures that will not be damaged in minor shaking,To make structures extremely strong,To construct structures that will collapse in a major earthquake,To avoid serious damage or collapse in a major earthquake,E,"Earthquake engineering is an interdisciplinary branch of engineering that designs and analyzes structures, such as buildings and bridges, with earthquakes in mind. Its overall goal is to make such structures more resistant to earthquakes. An earthquake (or seismic) engineer aims to construct structures that will not be damaged in minor shaking and will avoid serious damage or collapse in a major earthquake. 
A properly engineered structure does not necessarily have to be extremely strong or expensive. It has to be properly designed to withstand the seismic effects while sustaining an acceptable level of damage.

Definition
Earthquake engineering is a scientific field concerned with protecting society, the natural environment, and the man-made environment from earthquakes by limiting the seismic risk to socio-economically acceptable levels. Traditionally, it has been narrowly defined as the study of the behavior of structures and geo-structures subject to seismic loading; it is considered as a subset of structural engineering, geotechnical engineering, mechanical engineering, chemical engineering, applied physics, etc. However, the tremendous costs experienced in recent earthquakes have led to an expansion of its scope to encompass disciplines from the wider field of civil engineering, mechanical engineering, nuclear engineering, and from the social sciences, especially sociology, political science, economics, and finance.The main objectives of earthquake engineering are:

Foresee the potential consequences of strong earthquakes on urban areas and civil infrastructure.
Design, construct and maintain structures to perform at earthquake exposure up to the expectations and in compliance with building codes.

Seismic loading
Seismic loading means application of an earthquake-generated excitation on a structure (or geo-structure)."
What is the definition of earthquake engineering?,The study of the behavior of structures and geo-structures subject to seismic loading,The study of earthquakes and their impact on society,The study of civil engineering and mechanical engineering,The study of earthquakes and their impact on the environment,The study of protecting society and the environment from earthquakes,E,"Earthquake engineering is an interdisciplinary branch of engineering that designs and analyzes structures, such as buildings and bridges, with earthquakes in mind. Its overall goal is to make such structures more resistant to earthquakes. An earthquake (or seismic) engineer aims to construct structures that will not be damaged in minor shaking and will avoid serious damage or collapse in a major earthquake. 
A properly engineered structure does not necessarily have to be extremely strong or expensive. It has to be properly designed to withstand the seismic effects while sustaining an acceptable level of damage.

Definition
Earthquake engineering is a scientific field concerned with protecting society, the natural environment, and the man-made environment from earthquakes by limiting the seismic risk to socio-economically acceptable levels. Traditionally, it has been narrowly defined as the study of the behavior of structures and geo-structures subject to seismic loading; it is considered as a subset of structural engineering, geotechnical engineering, mechanical engineering, chemical engineering, applied physics, etc. However, the tremendous costs experienced in recent earthquakes have led to an expansion of its scope to encompass disciplines from the wider field of civil engineering, mechanical engineering, nuclear engineering, and from the social sciences, especially sociology, political science, economics, and finance.The main objectives of earthquake engineering are:

Foresee the potential consequences of strong earthquakes on urban areas and civil infrastructure.
Design, construct and maintain structures to perform at earthquake exposure up to the expectations and in compliance with building codes.

Seismic loading
Seismic loading means application of an earthquake-generated excitation on a structure (or geo-structure)."
What disciplines does earthquake engineering encompass?,Structural engineering and geotechnical engineering,Mechanical engineering and chemical engineering,Applied physics and nuclear engineering,"Sociology, political science, economics, and finance",All of the above,E,"Earthquake engineering is an interdisciplinary branch of engineering that designs and analyzes structures, such as buildings and bridges, with earthquakes in mind. Its overall goal is to make such structures more resistant to earthquakes. An earthquake (or seismic) engineer aims to construct structures that will not be damaged in minor shaking and will avoid serious damage or collapse in a major earthquake. 
A properly engineered structure does not necessarily have to be extremely strong or expensive. It has to be properly designed to withstand the seismic effects while sustaining an acceptable level of damage.

Definition
Earthquake engineering is a scientific field concerned with protecting society, the natural environment, and the man-made environment from earthquakes by limiting the seismic risk to socio-economically acceptable levels. Traditionally, it has been narrowly defined as the study of the behavior of structures and geo-structures subject to seismic loading; it is considered as a subset of structural engineering, geotechnical engineering, mechanical engineering, chemical engineering, applied physics, etc. However, the tremendous costs experienced in recent earthquakes have led to an expansion of its scope to encompass disciplines from the wider field of civil engineering, mechanical engineering, nuclear engineering, and from the social sciences, especially sociology, political science, economics, and finance.The main objectives of earthquake engineering are:

Foresee the potential consequences of strong earthquakes on urban areas and civil infrastructure.
Design, construct and maintain structures to perform at earthquake exposure up to the expectations and in compliance with building codes.

Seismic loading
Seismic loading means application of an earthquake-generated excitation on a structure (or geo-structure)."
What is seismic loading?,The application of an earthquake-generated excitation on a structure,The study of seismic waves,The process of constructing earthquake-resistant structures,The measurement of earthquake magnitude,The analysis of earthquake impacts on urban areas,A,"Earthquake engineering is an interdisciplinary branch of engineering that designs and analyzes structures, such as buildings and bridges, with earthquakes in mind. Its overall goal is to make such structures more resistant to earthquakes. An earthquake (or seismic) engineer aims to construct structures that will not be damaged in minor shaking and will avoid serious damage or collapse in a major earthquake. 
A properly engineered structure does not necessarily have to be extremely strong or expensive. It has to be properly designed to withstand the seismic effects while sustaining an acceptable level of damage.

Definition
Earthquake engineering is a scientific field concerned with protecting society, the natural environment, and the man-made environment from earthquakes by limiting the seismic risk to socio-economically acceptable levels. Traditionally, it has been narrowly defined as the study of the behavior of structures and geo-structures subject to seismic loading; it is considered as a subset of structural engineering, geotechnical engineering, mechanical engineering, chemical engineering, applied physics, etc. However, the tremendous costs experienced in recent earthquakes have led to an expansion of its scope to encompass disciplines from the wider field of civil engineering, mechanical engineering, nuclear engineering, and from the social sciences, especially sociology, political science, economics, and finance.The main objectives of earthquake engineering are:

Foresee the potential consequences of strong earthquakes on urban areas and civil infrastructure.
Design, construct and maintain structures to perform at earthquake exposure up to the expectations and in compliance with building codes.

Seismic loading
Seismic loading means application of an earthquake-generated excitation on a structure (or geo-structure)."
What are the main objectives of earthquake engineering?,To predict the exact timing of strong earthquakes,To design structures that perform at earthquake exposure up to expectations,To prevent any damage from occurring in earthquakes,To analyze the behavior of seismic waves,To calculate the cost of earthquake damage,B,"Earthquake engineering is an interdisciplinary branch of engineering that designs and analyzes structures, such as buildings and bridges, with earthquakes in mind. Its overall goal is to make such structures more resistant to earthquakes. An earthquake (or seismic) engineer aims to construct structures that will not be damaged in minor shaking and will avoid serious damage or collapse in a major earthquake. 
A properly engineered structure does not necessarily have to be extremely strong or expensive. It has to be properly designed to withstand the seismic effects while sustaining an acceptable level of damage.

Definition
Earthquake engineering is a scientific field concerned with protecting society, the natural environment, and the man-made environment from earthquakes by limiting the seismic risk to socio-economically acceptable levels. Traditionally, it has been narrowly defined as the study of the behavior of structures and geo-structures subject to seismic loading; it is considered as a subset of structural engineering, geotechnical engineering, mechanical engineering, chemical engineering, applied physics, etc. However, the tremendous costs experienced in recent earthquakes have led to an expansion of its scope to encompass disciplines from the wider field of civil engineering, mechanical engineering, nuclear engineering, and from the social sciences, especially sociology, political science, economics, and finance.The main objectives of earthquake engineering are:

Foresee the potential consequences of strong earthquakes on urban areas and civil infrastructure.
Design, construct and maintain structures to perform at earthquake exposure up to the expectations and in compliance with building codes.

Seismic loading
Seismic loading means application of an earthquake-generated excitation on a structure (or geo-structure)."
What is the main focus of plastics engineering?,Processing and design of plastic products,Development and manufacture of plastic materials,Design and development of plastic machinery,Processing and manufacture of plastic machinery,Design and development of plastic products,A,"Plastics engineering encompasses the processing, design, development, and manufacture of plastics products. A plastic is a polymeric material that is in a semi-liquid state, having the property of plasticity and exhibiting flow. Plastics engineering encompasses plastics material and plastic machinery. Plastic machinery is the general term for all types of machinery and devices used in the plastics processing industry. The nature of plastic materials poses unique challenges to an engineer. Mechanical properties of plastics are often difficult to quantify, and the plastics engineer has to design a product that meets certain specifications while keeping costs to a minimum. Other properties that the plastics engineer has to address include: outdoor weatherability, thermal properties such as upper use temperature, electrical properties, barrier properties, and resistance to chemical attack.
In plastics engineering, as in most engineering disciplines, the economics of a product plays an important role."
What is a plastic?,A polymeric material in a solid state with no flow,A polymeric material in a semi-liquid state with plasticity and flow,A polymeric material with no plasticity and flow,A polymeric material that is in a gaseous state,A polymeric material that is in a liquid state with no plasticity,B,"Plastics engineering encompasses the processing, design, development, and manufacture of plastics products. A plastic is a polymeric material that is in a semi-liquid state, having the property of plasticity and exhibiting flow. Plastics engineering encompasses plastics material and plastic machinery. Plastic machinery is the general term for all types of machinery and devices used in the plastics processing industry. The nature of plastic materials poses unique challenges to an engineer. Mechanical properties of plastics are often difficult to quantify, and the plastics engineer has to design a product that meets certain specifications while keeping costs to a minimum. Other properties that the plastics engineer has to address include: outdoor weatherability, thermal properties such as upper use temperature, electrical properties, barrier properties, and resistance to chemical attack.
In plastics engineering, as in most engineering disciplines, the economics of a product plays an important role."
What is plastic machinery?,Machinery used to process plastic materials,Machinery used to manufacture plastic products,Machinery used to design plastic materials,Machinery used to develop plastic products,Machinery used to design plastic machinery,A,"Plastics engineering encompasses the processing, design, development, and manufacture of plastics products. A plastic is a polymeric material that is in a semi-liquid state, having the property of plasticity and exhibiting flow. Plastics engineering encompasses plastics material and plastic machinery. Plastic machinery is the general term for all types of machinery and devices used in the plastics processing industry. The nature of plastic materials poses unique challenges to an engineer. Mechanical properties of plastics are often difficult to quantify, and the plastics engineer has to design a product that meets certain specifications while keeping costs to a minimum. Other properties that the plastics engineer has to address include: outdoor weatherability, thermal properties such as upper use temperature, electrical properties, barrier properties, and resistance to chemical attack.
In plastics engineering, as in most engineering disciplines, the economics of a product plays an important role."
What are some challenges faced by plastics engineers?,Difficulty in quantifying mechanical properties of plastics,Difficulty in designing products that meet specifications and minimize costs,Addressing properties such as outdoor weatherability and thermal properties,Addressing properties such as electrical properties and barrier properties,All of the above,E,"Plastics engineering encompasses the processing, design, development, and manufacture of plastics products. A plastic is a polymeric material that is in a semi-liquid state, having the property of plasticity and exhibiting flow. Plastics engineering encompasses plastics material and plastic machinery. Plastic machinery is the general term for all types of machinery and devices used in the plastics processing industry. The nature of plastic materials poses unique challenges to an engineer. Mechanical properties of plastics are often difficult to quantify, and the plastics engineer has to design a product that meets certain specifications while keeping costs to a minimum. Other properties that the plastics engineer has to address include: outdoor weatherability, thermal properties such as upper use temperature, electrical properties, barrier properties, and resistance to chemical attack.
In plastics engineering, as in most engineering disciplines, the economics of a product plays an important role."
Why is the economics of a product important in plastics engineering?,To ensure high-quality products,To minimize costs,To maximize profits,To meet certain specifications,To address properties such as resistance to chemical attack,B,"Plastics engineering encompasses the processing, design, development, and manufacture of plastics products. A plastic is a polymeric material that is in a semi-liquid state, having the property of plasticity and exhibiting flow. Plastics engineering encompasses plastics material and plastic machinery. Plastic machinery is the general term for all types of machinery and devices used in the plastics processing industry. The nature of plastic materials poses unique challenges to an engineer. Mechanical properties of plastics are often difficult to quantify, and the plastics engineer has to design a product that meets certain specifications while keeping costs to a minimum. Other properties that the plastics engineer has to address include: outdoor weatherability, thermal properties such as upper use temperature, electrical properties, barrier properties, and resistance to chemical attack.
In plastics engineering, as in most engineering disciplines, the economics of a product plays an important role."
What is the Global Digital Mathematics Library (GDML)?,A digital library focused on science,A project organized under the International Mathematical Union,A collection of prominent and fundamental theorems in mathematics,A workshop on semantic representation of mathematical knowledge,A report on developing a 21st century global library for mathematics research,B,"The Global Digital Mathematics Library (GDML) is a project organized under the auspices of the International Mathematical Union (IMU) to establish a digital library focused on mathematics.
A working group was convened in September 2014, following the 2014 International Congress of Mathematicians, by former IMU President Ingrid Daubechies and Chair Peter J. Olver of the IMU’s Committee on Electronic Information and Communication (CEIC). Currently the working group has eight members, namely:

Thierry Bouche, Institut Fourier & Cellule MathDoc, Grenoble, France
Bruno Buchberger, RISC, Hagenberg/Linz, Austria
Patrick Ion, Mathematical Reviews/AMS, Ann Arbor, MI, US
Michael Kohlhase, Jacobs University, Bremen, Germany
Jim Pitman, University of California, Berkeley, CA, US
Olaf Teschke, zbMATH/FIZ, Berlin, Germany
Stephen M. Watt, University of Waterloo, Waterloo, ON, Canada
Eric Weisstein, Wolfram Research, McAllen, TX, US

Background
In the spring of 2014, the Committee on Planning a Global Library of the Mathematical Sciences released a comprehensive study entitled “Developing a 21st Century Global Library for Mathematics Research.” This report states in its Strategic Plan section, “There is a compelling argument that through a combination of machine learning methods and editorial effort by both paid and volunteer editors, a significant portion of the information and knowledge in the global mathematical corpus could be made available to researchers as linked open data through the GDML.""

Workshop
A workshop titled ""Semantic Representation of Mathematical Knowledge"" was held at the Fields Institute in Toronto during February 3–5, 2016. The goal of the workshop was to lay down the foundations of a prototype semantic representation language for the GDML.  The workshop's organizers recognized that the extremely wide scope of mathematics as a whole made it unrealistic to map out the detailed concepts, structures, and operations needed and used in individual mathematical subjects.  The workshop therefore limited itself to surveys of the status quo in mathematical representation languages including representation of prominent and fundamental theorems in certain areas that could serve as building blocks for additional mathematical results, and to discussing ways to best identify and design semantic components for individual disciplines of mathematics.
The workshop organizers are presently preparing a report summarizing the workshop's conclusions and making recommendations for further progress towards a GDML.

References
See also
Mathematical knowledge management."
Who organized the GDML project?,Thierry Bouche,Bruno Buchberger,Ingrid Daubechies,Stephen M. Watt,Eric Weisstein,C,"The Global Digital Mathematics Library (GDML) is a project organized under the auspices of the International Mathematical Union (IMU) to establish a digital library focused on mathematics.
A working group was convened in September 2014, following the 2014 International Congress of Mathematicians, by former IMU President Ingrid Daubechies and Chair Peter J. Olver of the IMU’s Committee on Electronic Information and Communication (CEIC). Currently the working group has eight members, namely:

Thierry Bouche, Institut Fourier & Cellule MathDoc, Grenoble, France
Bruno Buchberger, RISC, Hagenberg/Linz, Austria
Patrick Ion, Mathematical Reviews/AMS, Ann Arbor, MI, US
Michael Kohlhase, Jacobs University, Bremen, Germany
Jim Pitman, University of California, Berkeley, CA, US
Olaf Teschke, zbMATH/FIZ, Berlin, Germany
Stephen M. Watt, University of Waterloo, Waterloo, ON, Canada
Eric Weisstein, Wolfram Research, McAllen, TX, US

Background
In the spring of 2014, the Committee on Planning a Global Library of the Mathematical Sciences released a comprehensive study entitled “Developing a 21st Century Global Library for Mathematics Research.” This report states in its Strategic Plan section, “There is a compelling argument that through a combination of machine learning methods and editorial effort by both paid and volunteer editors, a significant portion of the information and knowledge in the global mathematical corpus could be made available to researchers as linked open data through the GDML.""

Workshop
A workshop titled ""Semantic Representation of Mathematical Knowledge"" was held at the Fields Institute in Toronto during February 3–5, 2016. The goal of the workshop was to lay down the foundations of a prototype semantic representation language for the GDML.  The workshop's organizers recognized that the extremely wide scope of mathematics as a whole made it unrealistic to map out the detailed concepts, structures, and operations needed and used in individual mathematical subjects.  The workshop therefore limited itself to surveys of the status quo in mathematical representation languages including representation of prominent and fundamental theorems in certain areas that could serve as building blocks for additional mathematical results, and to discussing ways to best identify and design semantic components for individual disciplines of mathematics.
The workshop organizers are presently preparing a report summarizing the workshop's conclusions and making recommendations for further progress towards a GDML.

References
See also
Mathematical knowledge management."
Which organization is the GDML project organized under?,International Congress of Mathematicians,Committee on Electronic Information and Communication,International Mathematical Union,Fields Institute,Mathematical Reviews/AMS,C,"The Global Digital Mathematics Library (GDML) is a project organized under the auspices of the International Mathematical Union (IMU) to establish a digital library focused on mathematics.
A working group was convened in September 2014, following the 2014 International Congress of Mathematicians, by former IMU President Ingrid Daubechies and Chair Peter J. Olver of the IMU’s Committee on Electronic Information and Communication (CEIC). Currently the working group has eight members, namely:

Thierry Bouche, Institut Fourier & Cellule MathDoc, Grenoble, France
Bruno Buchberger, RISC, Hagenberg/Linz, Austria
Patrick Ion, Mathematical Reviews/AMS, Ann Arbor, MI, US
Michael Kohlhase, Jacobs University, Bremen, Germany
Jim Pitman, University of California, Berkeley, CA, US
Olaf Teschke, zbMATH/FIZ, Berlin, Germany
Stephen M. Watt, University of Waterloo, Waterloo, ON, Canada
Eric Weisstein, Wolfram Research, McAllen, TX, US

Background
In the spring of 2014, the Committee on Planning a Global Library of the Mathematical Sciences released a comprehensive study entitled “Developing a 21st Century Global Library for Mathematics Research.” This report states in its Strategic Plan section, “There is a compelling argument that through a combination of machine learning methods and editorial effort by both paid and volunteer editors, a significant portion of the information and knowledge in the global mathematical corpus could be made available to researchers as linked open data through the GDML.""

Workshop
A workshop titled ""Semantic Representation of Mathematical Knowledge"" was held at the Fields Institute in Toronto during February 3–5, 2016. The goal of the workshop was to lay down the foundations of a prototype semantic representation language for the GDML.  The workshop's organizers recognized that the extremely wide scope of mathematics as a whole made it unrealistic to map out the detailed concepts, structures, and operations needed and used in individual mathematical subjects.  The workshop therefore limited itself to surveys of the status quo in mathematical representation languages including representation of prominent and fundamental theorems in certain areas that could serve as building blocks for additional mathematical results, and to discussing ways to best identify and design semantic components for individual disciplines of mathematics.
The workshop organizers are presently preparing a report summarizing the workshop's conclusions and making recommendations for further progress towards a GDML.

References
See also
Mathematical knowledge management."
What was the goal of the workshop held at the Fields Institute?,To develop a prototype semantic representation language for the GDML,"To map out the detailed concepts, structures, and operations in individual mathematical subjects",To summarize the workshop's conclusions and make recommendations for further progress,To discuss ways to best identify and design semantic components for individual disciplines of mathematics,"To release a comprehensive study entitled ""Developing a 21st Century Global Library for Mathematics Research""",A,"The Global Digital Mathematics Library (GDML) is a project organized under the auspices of the International Mathematical Union (IMU) to establish a digital library focused on mathematics.
A working group was convened in September 2014, following the 2014 International Congress of Mathematicians, by former IMU President Ingrid Daubechies and Chair Peter J. Olver of the IMU’s Committee on Electronic Information and Communication (CEIC). Currently the working group has eight members, namely:

Thierry Bouche, Institut Fourier & Cellule MathDoc, Grenoble, France
Bruno Buchberger, RISC, Hagenberg/Linz, Austria
Patrick Ion, Mathematical Reviews/AMS, Ann Arbor, MI, US
Michael Kohlhase, Jacobs University, Bremen, Germany
Jim Pitman, University of California, Berkeley, CA, US
Olaf Teschke, zbMATH/FIZ, Berlin, Germany
Stephen M. Watt, University of Waterloo, Waterloo, ON, Canada
Eric Weisstein, Wolfram Research, McAllen, TX, US

Background
In the spring of 2014, the Committee on Planning a Global Library of the Mathematical Sciences released a comprehensive study entitled “Developing a 21st Century Global Library for Mathematics Research.” This report states in its Strategic Plan section, “There is a compelling argument that through a combination of machine learning methods and editorial effort by both paid and volunteer editors, a significant portion of the information and knowledge in the global mathematical corpus could be made available to researchers as linked open data through the GDML.""

Workshop
A workshop titled ""Semantic Representation of Mathematical Knowledge"" was held at the Fields Institute in Toronto during February 3–5, 2016. The goal of the workshop was to lay down the foundations of a prototype semantic representation language for the GDML.  The workshop's organizers recognized that the extremely wide scope of mathematics as a whole made it unrealistic to map out the detailed concepts, structures, and operations needed and used in individual mathematical subjects.  The workshop therefore limited itself to surveys of the status quo in mathematical representation languages including representation of prominent and fundamental theorems in certain areas that could serve as building blocks for additional mathematical results, and to discussing ways to best identify and design semantic components for individual disciplines of mathematics.
The workshop organizers are presently preparing a report summarizing the workshop's conclusions and making recommendations for further progress towards a GDML.

References
See also
Mathematical knowledge management."
"What is the purpose of the GDML according to the report ""Developing a 21st Century Global Library for Mathematics Research""?",To provide researchers with linked open data,To establish a digital library focused on mathematics,To develop a prototype semantic representation language,"To map out the detailed concepts, structures, and operations in individual mathematical subjects",To identify and design semantic components for individual disciplines of mathematics,A,"The Global Digital Mathematics Library (GDML) is a project organized under the auspices of the International Mathematical Union (IMU) to establish a digital library focused on mathematics.
A working group was convened in September 2014, following the 2014 International Congress of Mathematicians, by former IMU President Ingrid Daubechies and Chair Peter J. Olver of the IMU’s Committee on Electronic Information and Communication (CEIC). Currently the working group has eight members, namely:

Thierry Bouche, Institut Fourier & Cellule MathDoc, Grenoble, France
Bruno Buchberger, RISC, Hagenberg/Linz, Austria
Patrick Ion, Mathematical Reviews/AMS, Ann Arbor, MI, US
Michael Kohlhase, Jacobs University, Bremen, Germany
Jim Pitman, University of California, Berkeley, CA, US
Olaf Teschke, zbMATH/FIZ, Berlin, Germany
Stephen M. Watt, University of Waterloo, Waterloo, ON, Canada
Eric Weisstein, Wolfram Research, McAllen, TX, US

Background
In the spring of 2014, the Committee on Planning a Global Library of the Mathematical Sciences released a comprehensive study entitled “Developing a 21st Century Global Library for Mathematics Research.” This report states in its Strategic Plan section, “There is a compelling argument that through a combination of machine learning methods and editorial effort by both paid and volunteer editors, a significant portion of the information and knowledge in the global mathematical corpus could be made available to researchers as linked open data through the GDML.""

Workshop
A workshop titled ""Semantic Representation of Mathematical Knowledge"" was held at the Fields Institute in Toronto during February 3–5, 2016. The goal of the workshop was to lay down the foundations of a prototype semantic representation language for the GDML.  The workshop's organizers recognized that the extremely wide scope of mathematics as a whole made it unrealistic to map out the detailed concepts, structures, and operations needed and used in individual mathematical subjects.  The workshop therefore limited itself to surveys of the status quo in mathematical representation languages including representation of prominent and fundamental theorems in certain areas that could serve as building blocks for additional mathematical results, and to discussing ways to best identify and design semantic components for individual disciplines of mathematics.
The workshop organizers are presently preparing a report summarizing the workshop's conclusions and making recommendations for further progress towards a GDML.

References
See also
Mathematical knowledge management."
What is a butter stamp used for?,Stamping a design onto a block of warm bread,Stamping a design onto a block of warm butter,Stamping a design onto a block of warm cheese,Stamping a design onto a block of warm chocolate,Stamping a design onto a block of warm ice cream,B,"A butter stamp is a device for stamping a design onto a block of warm butter. Butter stamps were sometimes commercial but usually purely decorative and applied in homes. They were typically made of wood and feature simple designs of cows, flowers or geometric patterns and, if commercial, the name of the retailer. Often, they formed part of a box-like mould for forming the whole block. Other designs achieved the same effect by carving the design at the bottom of a butter mould. Part of the intent for commercial moulds and stamps was to demonstrate consistency in the quantity of butter sold. Both moulds and stamps are now normally inexpensive antiques.


== References ==."
What were butter stamps typically made of?,Plastic,Metal,Glass,Wood,Ceramic,D,"A butter stamp is a device for stamping a design onto a block of warm butter. Butter stamps were sometimes commercial but usually purely decorative and applied in homes. They were typically made of wood and feature simple designs of cows, flowers or geometric patterns and, if commercial, the name of the retailer. Often, they formed part of a box-like mould for forming the whole block. Other designs achieved the same effect by carving the design at the bottom of a butter mould. Part of the intent for commercial moulds and stamps was to demonstrate consistency in the quantity of butter sold. Both moulds and stamps are now normally inexpensive antiques.


== References ==."
What kind of designs were typically featured on butter stamps?,Animals,Plants,Geometric patterns,All of the above,None of the above,D,"A butter stamp is a device for stamping a design onto a block of warm butter. Butter stamps were sometimes commercial but usually purely decorative and applied in homes. They were typically made of wood and feature simple designs of cows, flowers or geometric patterns and, if commercial, the name of the retailer. Often, they formed part of a box-like mould for forming the whole block. Other designs achieved the same effect by carving the design at the bottom of a butter mould. Part of the intent for commercial moulds and stamps was to demonstrate consistency in the quantity of butter sold. Both moulds and stamps are now normally inexpensive antiques.


== References ==."
What was the purpose of commercial butter moulds and stamps?,To demonstrate consistency in the quality of butter sold,To make the butter block easier to handle,To add flavor to the butter,To increase the price of the butter,To create unique butter designs,A,"A butter stamp is a device for stamping a design onto a block of warm butter. Butter stamps were sometimes commercial but usually purely decorative and applied in homes. They were typically made of wood and feature simple designs of cows, flowers or geometric patterns and, if commercial, the name of the retailer. Often, they formed part of a box-like mould for forming the whole block. Other designs achieved the same effect by carving the design at the bottom of a butter mould. Part of the intent for commercial moulds and stamps was to demonstrate consistency in the quantity of butter sold. Both moulds and stamps are now normally inexpensive antiques.


== References ==."
What are butter moulds and stamps commonly considered as now?,Valuable antiques,Modern kitchen utensils,Decorative items,Historical artifacts,Children toys,A,"A butter stamp is a device for stamping a design onto a block of warm butter. Butter stamps were sometimes commercial but usually purely decorative and applied in homes. They were typically made of wood and feature simple designs of cows, flowers or geometric patterns and, if commercial, the name of the retailer. Often, they formed part of a box-like mould for forming the whole block. Other designs achieved the same effect by carving the design at the bottom of a butter mould. Part of the intent for commercial moulds and stamps was to demonstrate consistency in the quantity of butter sold. Both moulds and stamps are now normally inexpensive antiques.


== References ==."
Where was Saferworld founded?,Manchester,Bristol,London,Edinburgh,Cardiff,B,"Saferworld is an international non-governmental organisation with conflict prevention and peacebuilding programmes in over 20 countries and territories in the Horn of Africa, the African Great Lakes region, Asia, the Middle East, Central Asia and the Caucasus. It was founded in Bristol, UK in 1989 and now has its main office in London.
Much of Saferworld's work includes obtaining research and conducting specific analyses on current conflicts. One of the main objectives of the organisation is to enhance the power and influence of individuals, by encouraging vocalisation of grievances and social injustices. Saferworld achieves this through careful analysis of conflicts and arranging sit-down discussions between stakeholders.
Saferworld addresses many community-based issues through in-depth analysis and objective examination. One of the organisation’s strengths is its ability to delve into conflicts, rather than scraping the surface and gauging assistance on an underdeveloped perspective. Saferworld also takes special interest in understanding how conflicts affect women, children, and other demographics.

History
Saferworld was founded in 1989 in Bristol, UK. After changing its name from the Nuclear Freeze Campaign to Saferworld, its focus moved from nuclear weapons proliferation to campaigning for more effective controls on the proliferation and misuse of conventional arms."
In which regions does Saferworld have conflict prevention and peacebuilding programmes?,North America and Europe,Africa and Asia,South America and Oceania,Asia and Europe,Africa and Europe,B,"Saferworld is an international non-governmental organisation with conflict prevention and peacebuilding programmes in over 20 countries and territories in the Horn of Africa, the African Great Lakes region, Asia, the Middle East, Central Asia and the Caucasus. It was founded in Bristol, UK in 1989 and now has its main office in London.
Much of Saferworld's work includes obtaining research and conducting specific analyses on current conflicts. One of the main objectives of the organisation is to enhance the power and influence of individuals, by encouraging vocalisation of grievances and social injustices. Saferworld achieves this through careful analysis of conflicts and arranging sit-down discussions between stakeholders.
Saferworld addresses many community-based issues through in-depth analysis and objective examination. One of the organisation’s strengths is its ability to delve into conflicts, rather than scraping the surface and gauging assistance on an underdeveloped perspective. Saferworld also takes special interest in understanding how conflicts affect women, children, and other demographics.

History
Saferworld was founded in 1989 in Bristol, UK. After changing its name from the Nuclear Freeze Campaign to Saferworld, its focus moved from nuclear weapons proliferation to campaigning for more effective controls on the proliferation and misuse of conventional arms."
What is one of Saferworld's main objectives?,Promoting nuclear disarmament,Improving healthcare access,Enhancing individual power and influence,Protecting the environment,Promoting economic development,C,"Saferworld is an international non-governmental organisation with conflict prevention and peacebuilding programmes in over 20 countries and territories in the Horn of Africa, the African Great Lakes region, Asia, the Middle East, Central Asia and the Caucasus. It was founded in Bristol, UK in 1989 and now has its main office in London.
Much of Saferworld's work includes obtaining research and conducting specific analyses on current conflicts. One of the main objectives of the organisation is to enhance the power and influence of individuals, by encouraging vocalisation of grievances and social injustices. Saferworld achieves this through careful analysis of conflicts and arranging sit-down discussions between stakeholders.
Saferworld addresses many community-based issues through in-depth analysis and objective examination. One of the organisation’s strengths is its ability to delve into conflicts, rather than scraping the surface and gauging assistance on an underdeveloped perspective. Saferworld also takes special interest in understanding how conflicts affect women, children, and other demographics.

History
Saferworld was founded in 1989 in Bristol, UK. After changing its name from the Nuclear Freeze Campaign to Saferworld, its focus moved from nuclear weapons proliferation to campaigning for more effective controls on the proliferation and misuse of conventional arms."
What is one of Saferworld's strengths?,Superficial analysis of conflicts,Focusing on underdeveloped perspectives,Delving into conflicts and conducting in-depth analysis,"Ignoring the impact of conflicts on women, children, and other demographics",Lack of understanding of social injustices,C,"Saferworld is an international non-governmental organisation with conflict prevention and peacebuilding programmes in over 20 countries and territories in the Horn of Africa, the African Great Lakes region, Asia, the Middle East, Central Asia and the Caucasus. It was founded in Bristol, UK in 1989 and now has its main office in London.
Much of Saferworld's work includes obtaining research and conducting specific analyses on current conflicts. One of the main objectives of the organisation is to enhance the power and influence of individuals, by encouraging vocalisation of grievances and social injustices. Saferworld achieves this through careful analysis of conflicts and arranging sit-down discussions between stakeholders.
Saferworld addresses many community-based issues through in-depth analysis and objective examination. One of the organisation’s strengths is its ability to delve into conflicts, rather than scraping the surface and gauging assistance on an underdeveloped perspective. Saferworld also takes special interest in understanding how conflicts affect women, children, and other demographics.

History
Saferworld was founded in 1989 in Bristol, UK. After changing its name from the Nuclear Freeze Campaign to Saferworld, its focus moved from nuclear weapons proliferation to campaigning for more effective controls on the proliferation and misuse of conventional arms."
What was Saferworld's initial focus when it was founded?,Campaigning for nuclear disarmament,Advocating for better education systems,Addressing poverty and inequality,Promoting renewable energy,Fighting against political corruption,A,"Saferworld is an international non-governmental organisation with conflict prevention and peacebuilding programmes in over 20 countries and territories in the Horn of Africa, the African Great Lakes region, Asia, the Middle East, Central Asia and the Caucasus. It was founded in Bristol, UK in 1989 and now has its main office in London.
Much of Saferworld's work includes obtaining research and conducting specific analyses on current conflicts. One of the main objectives of the organisation is to enhance the power and influence of individuals, by encouraging vocalisation of grievances and social injustices. Saferworld achieves this through careful analysis of conflicts and arranging sit-down discussions between stakeholders.
Saferworld addresses many community-based issues through in-depth analysis and objective examination. One of the organisation’s strengths is its ability to delve into conflicts, rather than scraping the surface and gauging assistance on an underdeveloped perspective. Saferworld also takes special interest in understanding how conflicts affect women, children, and other demographics.

History
Saferworld was founded in 1989 in Bristol, UK. After changing its name from the Nuclear Freeze Campaign to Saferworld, its focus moved from nuclear weapons proliferation to campaigning for more effective controls on the proliferation and misuse of conventional arms."
Who is considered the originator of queueing theory?,Agner Krarup Erlang,Albert Einstein,Isaac Newton,Marie Curie,Thomas Edison,A,"Queueing theory is the mathematical study of waiting lines, or queues. A queueing model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service.
Queueing theory has its origins in research by Agner Krarup Erlang, who created models to describe the system of incoming calls at the Copenhagen Telephone Exchange Company. These ideas have since seen applications in telecommunication, traffic engineering, computing, project management, and particularly industrial engineering, where they are applied in the design of factories, shops, offices, and hospitals.

Spelling
The spelling ""queueing"" over ""queuing"" is typically encountered in the academic research field. In fact, one of the flagship journals of the field is Queueing Systems.

Single queueing nodes
A queue or queueing node can be thought of as nearly a black box. Jobs (also called customers or requests, depending on the field) arrive to the queue, possibly wait some time, take some time being processed, and then depart from the queue.

However, the queueing node is not quite a pure black box since some information is needed about the inside of the queuing node. The queue has one or more servers which can each be paired with an arriving job."
Which field does queueing theory find applications in?,Telecommunication,Traffic engineering,Computing,Project management,All of the above,E,"Queueing theory is the mathematical study of waiting lines, or queues. A queueing model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service.
Queueing theory has its origins in research by Agner Krarup Erlang, who created models to describe the system of incoming calls at the Copenhagen Telephone Exchange Company. These ideas have since seen applications in telecommunication, traffic engineering, computing, project management, and particularly industrial engineering, where they are applied in the design of factories, shops, offices, and hospitals.

Spelling
The spelling ""queueing"" over ""queuing"" is typically encountered in the academic research field. In fact, one of the flagship journals of the field is Queueing Systems.

Single queueing nodes
A queue or queueing node can be thought of as nearly a black box. Jobs (also called customers or requests, depending on the field) arrive to the queue, possibly wait some time, take some time being processed, and then depart from the queue.

However, the queueing node is not quite a pure black box since some information is needed about the inside of the queuing node. The queue has one or more servers which can each be paired with an arriving job."
What is a queueing node?,A box with black color,A box with white color,A nearly black box,A red box,A green box,C,"Queueing theory is the mathematical study of waiting lines, or queues. A queueing model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service.
Queueing theory has its origins in research by Agner Krarup Erlang, who created models to describe the system of incoming calls at the Copenhagen Telephone Exchange Company. These ideas have since seen applications in telecommunication, traffic engineering, computing, project management, and particularly industrial engineering, where they are applied in the design of factories, shops, offices, and hospitals.

Spelling
The spelling ""queueing"" over ""queuing"" is typically encountered in the academic research field. In fact, one of the flagship journals of the field is Queueing Systems.

Single queueing nodes
A queue or queueing node can be thought of as nearly a black box. Jobs (also called customers or requests, depending on the field) arrive to the queue, possibly wait some time, take some time being processed, and then depart from the queue.

However, the queueing node is not quite a pure black box since some information is needed about the inside of the queuing node. The queue has one or more servers which can each be paired with an arriving job."
What is the purpose of a queueing node?,To process jobs,To store jobs,To transport jobs,To create jobs,To destroy jobs,A,"Queueing theory is the mathematical study of waiting lines, or queues. A queueing model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service.
Queueing theory has its origins in research by Agner Krarup Erlang, who created models to describe the system of incoming calls at the Copenhagen Telephone Exchange Company. These ideas have since seen applications in telecommunication, traffic engineering, computing, project management, and particularly industrial engineering, where they are applied in the design of factories, shops, offices, and hospitals.

Spelling
The spelling ""queueing"" over ""queuing"" is typically encountered in the academic research field. In fact, one of the flagship journals of the field is Queueing Systems.

Single queueing nodes
A queue or queueing node can be thought of as nearly a black box. Jobs (also called customers or requests, depending on the field) arrive to the queue, possibly wait some time, take some time being processed, and then depart from the queue.

However, the queueing node is not quite a pure black box since some information is needed about the inside of the queuing node. The queue has one or more servers which can each be paired with an arriving job."
Which spelling is typically used in the academic research field?,Queuing,Queueing,Quing,Quuing,Qing,B,"Queueing theory is the mathematical study of waiting lines, or queues. A queueing model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service.
Queueing theory has its origins in research by Agner Krarup Erlang, who created models to describe the system of incoming calls at the Copenhagen Telephone Exchange Company. These ideas have since seen applications in telecommunication, traffic engineering, computing, project management, and particularly industrial engineering, where they are applied in the design of factories, shops, offices, and hospitals.

Spelling
The spelling ""queueing"" over ""queuing"" is typically encountered in the academic research field. In fact, one of the flagship journals of the field is Queueing Systems.

Single queueing nodes
A queue or queueing node can be thought of as nearly a black box. Jobs (also called customers or requests, depending on the field) arrive to the queue, possibly wait some time, take some time being processed, and then depart from the queue.

However, the queueing node is not quite a pure black box since some information is needed about the inside of the queuing node. The queue has one or more servers which can each be paired with an arriving job."
What is the definition of applied science?,The branch of science that applies existing scientific knowledge to develop more practical applications.,The systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.,"The process and product of planning, designing, and construction.",The study concerned with all technical aspects of foods.,"The art and science of managing forests, tree plantations, and related natural resources.",A,"The following outline is provided as an overview of and topical guide to applied science:
Applied science – the branch of science that applies existing scientific knowledge to develop more practical applications, including inventions and other technological advancements. Science itself is the  systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.

Branches of applied science
Applied cryptography – applications of cryptography.
Applied science – application of scientific knowledge transferred into a physical environment.
Actuarial science — applies mathematical and statistical methods to assess risk in the insurance, finance, and other industries
Agricultural science
Agronomy – science and technology of producing and using plants for food, fuel, feed, fiber, and reclamation.
Animal husbandry – agricultural practice of breeding and raising livestock.
Aquaculture – also known as aquafarming, is the farming of aquatic organisms such as fish, crustaceans, molluscs and aquatic plants.Algaculture – form of aquaculture involving the farming of species of algae.
Mariculture – cultivation of marine organisms for food and other products in the open ocean, an enclosed section of the ocean, or in tanks, ponds or raceways which are filled with seawater.
Agriculture – science of farming
Cuniculture – also known as rabbit farming, is the breeding and raising domestic rabbits, usually for their meat or fur.
Fungiculture –  process of producing food, medicine, and other products by the cultivation of mushrooms and other fungi.
Heliciculture – also called snail farming, is the process of farming or raising land snails specifically for human consumption, and more recently, to obtain snail slime for cosmetics use.
Olericulture – science of vegetable growing, dealing with the culture of non-woody (herbaceous) plants for food.
Sericulture – also called silk farming, is the rearing of silkworms for the production of silk. Although there are several commercial species of silkworms, Bombyx mori is the most widely used and intensively studied.
Food science – study concerned with all technical aspects of foods, beginning with harvesting or slaughtering, and ending with its cooking and consumption, an ideology commonly referred to as ""from field to fork"". It is the discipline in which the engineering, biological, and physical sciences are used to study the nature of foods, the causes of deterioration, the principles underlying food processing, and the improvement of foods for the consuming public.
Forestry – art and science of managing forests, tree plantations, and related natural resources.
Arboriculture – cultivation, management, and study of individual trees, shrubs, vines, and other perennial woody plants.
Silviculture – practice of controlling the establishment, growth, composition, health, and quality of forests to meet diverse needs and values. It includes regenerating, tending and harvesting techniques.
Horticulture – art, science, technology and business of intensive plant cultivation for human use
Floriculture – discipline of horticulture concerned with the cultivation of flowering and ornamental plants for gardens and for floristry, comprising the floral industry.
Hydroculture – growing of plants in a soilless medium, or an aquatic based environment. Plant nutrients are distributed via water. Hydroculture is aquatic horticulture.
Hydroponics – subset of hydroculture and is a method of growing plants using mineral nutrient solutions, in water, without soil.
Permaculture – branch of ecological design and ecological engineering, which develop sustainable human settlements and self-maintained agricultural systems modeled from natural ecosystems.
Architecture – process and product of planning, designing and construction."
What is the definition of actuarial science?,The branch of science that applies existing scientific knowledge to develop more practical applications.,The systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.,"The process and product of planning, designing, and construction.","The application of mathematical and statistical methods to assess risk in the insurance, finance, and other industries.","The art and science of managing forests, tree plantations, and related natural resources.",D,"The following outline is provided as an overview of and topical guide to applied science:
Applied science – the branch of science that applies existing scientific knowledge to develop more practical applications, including inventions and other technological advancements. Science itself is the  systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.

Branches of applied science
Applied cryptography – applications of cryptography.
Applied science – application of scientific knowledge transferred into a physical environment.
Actuarial science — applies mathematical and statistical methods to assess risk in the insurance, finance, and other industries
Agricultural science
Agronomy – science and technology of producing and using plants for food, fuel, feed, fiber, and reclamation.
Animal husbandry – agricultural practice of breeding and raising livestock.
Aquaculture – also known as aquafarming, is the farming of aquatic organisms such as fish, crustaceans, molluscs and aquatic plants.Algaculture – form of aquaculture involving the farming of species of algae.
Mariculture – cultivation of marine organisms for food and other products in the open ocean, an enclosed section of the ocean, or in tanks, ponds or raceways which are filled with seawater.
Agriculture – science of farming
Cuniculture – also known as rabbit farming, is the breeding and raising domestic rabbits, usually for their meat or fur.
Fungiculture –  process of producing food, medicine, and other products by the cultivation of mushrooms and other fungi.
Heliciculture – also called snail farming, is the process of farming or raising land snails specifically for human consumption, and more recently, to obtain snail slime for cosmetics use.
Olericulture – science of vegetable growing, dealing with the culture of non-woody (herbaceous) plants for food.
Sericulture – also called silk farming, is the rearing of silkworms for the production of silk. Although there are several commercial species of silkworms, Bombyx mori is the most widely used and intensively studied.
Food science – study concerned with all technical aspects of foods, beginning with harvesting or slaughtering, and ending with its cooking and consumption, an ideology commonly referred to as ""from field to fork"". It is the discipline in which the engineering, biological, and physical sciences are used to study the nature of foods, the causes of deterioration, the principles underlying food processing, and the improvement of foods for the consuming public.
Forestry – art and science of managing forests, tree plantations, and related natural resources.
Arboriculture – cultivation, management, and study of individual trees, shrubs, vines, and other perennial woody plants.
Silviculture – practice of controlling the establishment, growth, composition, health, and quality of forests to meet diverse needs and values. It includes regenerating, tending and harvesting techniques.
Horticulture – art, science, technology and business of intensive plant cultivation for human use
Floriculture – discipline of horticulture concerned with the cultivation of flowering and ornamental plants for gardens and for floristry, comprising the floral industry.
Hydroculture – growing of plants in a soilless medium, or an aquatic based environment. Plant nutrients are distributed via water. Hydroculture is aquatic horticulture.
Hydroponics – subset of hydroculture and is a method of growing plants using mineral nutrient solutions, in water, without soil.
Permaculture – branch of ecological design and ecological engineering, which develop sustainable human settlements and self-maintained agricultural systems modeled from natural ecosystems.
Architecture – process and product of planning, designing and construction."
What is the definition of agriculture?,The branch of science that applies existing scientific knowledge to develop more practical applications.,The systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.,"The process and product of planning, designing, and construction.","The science and technology of producing and using plants for food, fuel, feed, fiber, and reclamation.","The art and science of managing forests, tree plantations, and related natural resources.",D,"The following outline is provided as an overview of and topical guide to applied science:
Applied science – the branch of science that applies existing scientific knowledge to develop more practical applications, including inventions and other technological advancements. Science itself is the  systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.

Branches of applied science
Applied cryptography – applications of cryptography.
Applied science – application of scientific knowledge transferred into a physical environment.
Actuarial science — applies mathematical and statistical methods to assess risk in the insurance, finance, and other industries
Agricultural science
Agronomy – science and technology of producing and using plants for food, fuel, feed, fiber, and reclamation.
Animal husbandry – agricultural practice of breeding and raising livestock.
Aquaculture – also known as aquafarming, is the farming of aquatic organisms such as fish, crustaceans, molluscs and aquatic plants.Algaculture – form of aquaculture involving the farming of species of algae.
Mariculture – cultivation of marine organisms for food and other products in the open ocean, an enclosed section of the ocean, or in tanks, ponds or raceways which are filled with seawater.
Agriculture – science of farming
Cuniculture – also known as rabbit farming, is the breeding and raising domestic rabbits, usually for their meat or fur.
Fungiculture –  process of producing food, medicine, and other products by the cultivation of mushrooms and other fungi.
Heliciculture – also called snail farming, is the process of farming or raising land snails specifically for human consumption, and more recently, to obtain snail slime for cosmetics use.
Olericulture – science of vegetable growing, dealing with the culture of non-woody (herbaceous) plants for food.
Sericulture – also called silk farming, is the rearing of silkworms for the production of silk. Although there are several commercial species of silkworms, Bombyx mori is the most widely used and intensively studied.
Food science – study concerned with all technical aspects of foods, beginning with harvesting or slaughtering, and ending with its cooking and consumption, an ideology commonly referred to as ""from field to fork"". It is the discipline in which the engineering, biological, and physical sciences are used to study the nature of foods, the causes of deterioration, the principles underlying food processing, and the improvement of foods for the consuming public.
Forestry – art and science of managing forests, tree plantations, and related natural resources.
Arboriculture – cultivation, management, and study of individual trees, shrubs, vines, and other perennial woody plants.
Silviculture – practice of controlling the establishment, growth, composition, health, and quality of forests to meet diverse needs and values. It includes regenerating, tending and harvesting techniques.
Horticulture – art, science, technology and business of intensive plant cultivation for human use
Floriculture – discipline of horticulture concerned with the cultivation of flowering and ornamental plants for gardens and for floristry, comprising the floral industry.
Hydroculture – growing of plants in a soilless medium, or an aquatic based environment. Plant nutrients are distributed via water. Hydroculture is aquatic horticulture.
Hydroponics – subset of hydroculture and is a method of growing plants using mineral nutrient solutions, in water, without soil.
Permaculture – branch of ecological design and ecological engineering, which develop sustainable human settlements and self-maintained agricultural systems modeled from natural ecosystems.
Architecture – process and product of planning, designing and construction."
What is the definition of food science?,The branch of science that applies existing scientific knowledge to develop more practical applications.,The systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.,"The process and product of planning, designing, and construction.","The study concerned with all technical aspects of foods, beginning with harvesting or slaughtering, and ending with its cooking and consumption.","The art and science of managing forests, tree plantations, and related natural resources.",D,"The following outline is provided as an overview of and topical guide to applied science:
Applied science – the branch of science that applies existing scientific knowledge to develop more practical applications, including inventions and other technological advancements. Science itself is the  systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.

Branches of applied science
Applied cryptography – applications of cryptography.
Applied science – application of scientific knowledge transferred into a physical environment.
Actuarial science — applies mathematical and statistical methods to assess risk in the insurance, finance, and other industries
Agricultural science
Agronomy – science and technology of producing and using plants for food, fuel, feed, fiber, and reclamation.
Animal husbandry – agricultural practice of breeding and raising livestock.
Aquaculture – also known as aquafarming, is the farming of aquatic organisms such as fish, crustaceans, molluscs and aquatic plants.Algaculture – form of aquaculture involving the farming of species of algae.
Mariculture – cultivation of marine organisms for food and other products in the open ocean, an enclosed section of the ocean, or in tanks, ponds or raceways which are filled with seawater.
Agriculture – science of farming
Cuniculture – also known as rabbit farming, is the breeding and raising domestic rabbits, usually for their meat or fur.
Fungiculture –  process of producing food, medicine, and other products by the cultivation of mushrooms and other fungi.
Heliciculture – also called snail farming, is the process of farming or raising land snails specifically for human consumption, and more recently, to obtain snail slime for cosmetics use.
Olericulture – science of vegetable growing, dealing with the culture of non-woody (herbaceous) plants for food.
Sericulture – also called silk farming, is the rearing of silkworms for the production of silk. Although there are several commercial species of silkworms, Bombyx mori is the most widely used and intensively studied.
Food science – study concerned with all technical aspects of foods, beginning with harvesting or slaughtering, and ending with its cooking and consumption, an ideology commonly referred to as ""from field to fork"". It is the discipline in which the engineering, biological, and physical sciences are used to study the nature of foods, the causes of deterioration, the principles underlying food processing, and the improvement of foods for the consuming public.
Forestry – art and science of managing forests, tree plantations, and related natural resources.
Arboriculture – cultivation, management, and study of individual trees, shrubs, vines, and other perennial woody plants.
Silviculture – practice of controlling the establishment, growth, composition, health, and quality of forests to meet diverse needs and values. It includes regenerating, tending and harvesting techniques.
Horticulture – art, science, technology and business of intensive plant cultivation for human use
Floriculture – discipline of horticulture concerned with the cultivation of flowering and ornamental plants for gardens and for floristry, comprising the floral industry.
Hydroculture – growing of plants in a soilless medium, or an aquatic based environment. Plant nutrients are distributed via water. Hydroculture is aquatic horticulture.
Hydroponics – subset of hydroculture and is a method of growing plants using mineral nutrient solutions, in water, without soil.
Permaculture – branch of ecological design and ecological engineering, which develop sustainable human settlements and self-maintained agricultural systems modeled from natural ecosystems.
Architecture – process and product of planning, designing and construction."
What is the definition of architecture?,The branch of science that applies existing scientific knowledge to develop more practical applications.,The systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.,"The process and product of planning, designing, and construction.","The study concerned with all technical aspects of foods, beginning with harvesting or slaughtering, and ending with its cooking and consumption.","The art and science of managing forests, tree plantations, and related natural resources.",C,"The following outline is provided as an overview of and topical guide to applied science:
Applied science – the branch of science that applies existing scientific knowledge to develop more practical applications, including inventions and other technological advancements. Science itself is the  systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.

Branches of applied science
Applied cryptography – applications of cryptography.
Applied science – application of scientific knowledge transferred into a physical environment.
Actuarial science — applies mathematical and statistical methods to assess risk in the insurance, finance, and other industries
Agricultural science
Agronomy – science and technology of producing and using plants for food, fuel, feed, fiber, and reclamation.
Animal husbandry – agricultural practice of breeding and raising livestock.
Aquaculture – also known as aquafarming, is the farming of aquatic organisms such as fish, crustaceans, molluscs and aquatic plants.Algaculture – form of aquaculture involving the farming of species of algae.
Mariculture – cultivation of marine organisms for food and other products in the open ocean, an enclosed section of the ocean, or in tanks, ponds or raceways which are filled with seawater.
Agriculture – science of farming
Cuniculture – also known as rabbit farming, is the breeding and raising domestic rabbits, usually for their meat or fur.
Fungiculture –  process of producing food, medicine, and other products by the cultivation of mushrooms and other fungi.
Heliciculture – also called snail farming, is the process of farming or raising land snails specifically for human consumption, and more recently, to obtain snail slime for cosmetics use.
Olericulture – science of vegetable growing, dealing with the culture of non-woody (herbaceous) plants for food.
Sericulture – also called silk farming, is the rearing of silkworms for the production of silk. Although there are several commercial species of silkworms, Bombyx mori is the most widely used and intensively studied.
Food science – study concerned with all technical aspects of foods, beginning with harvesting or slaughtering, and ending with its cooking and consumption, an ideology commonly referred to as ""from field to fork"". It is the discipline in which the engineering, biological, and physical sciences are used to study the nature of foods, the causes of deterioration, the principles underlying food processing, and the improvement of foods for the consuming public.
Forestry – art and science of managing forests, tree plantations, and related natural resources.
Arboriculture – cultivation, management, and study of individual trees, shrubs, vines, and other perennial woody plants.
Silviculture – practice of controlling the establishment, growth, composition, health, and quality of forests to meet diverse needs and values. It includes regenerating, tending and harvesting techniques.
Horticulture – art, science, technology and business of intensive plant cultivation for human use
Floriculture – discipline of horticulture concerned with the cultivation of flowering and ornamental plants for gardens and for floristry, comprising the floral industry.
Hydroculture – growing of plants in a soilless medium, or an aquatic based environment. Plant nutrients are distributed via water. Hydroculture is aquatic horticulture.
Hydroponics – subset of hydroculture and is a method of growing plants using mineral nutrient solutions, in water, without soil.
Permaculture – branch of ecological design and ecological engineering, which develop sustainable human settlements and self-maintained agricultural systems modeled from natural ecosystems.
Architecture – process and product of planning, designing and construction."
What is the objective of predictive engineering analytics (PEA)?,To react to issues that may arise during the design process,To predict product behavior and let simulation drive the design,To improve collaboration between analysis teams,To introduce new software tools and refine testing processes,To install a process that lets design continue after product delivery,B,"Predictive engineering analytics (PEA) is a development approach for the manufacturing industry that helps with the design of complex products (for example, products that include smart systems). It concerns the introduction of new software tools, the integration between those, and a refinement of simulation and testing processes to improve collaboration between analysis teams that handle different applications. This is combined with intelligent reporting and data analytics. The objective is to let simulation drive the design, to predict product behavior rather than to react on issues which may arise, and to install a process that lets design continue after product delivery.

Industry needs
In a classic development approach, manufacturers deliver discrete product generations. Before bringing those to market, they use extensive verification and validation processes, usually by combining several simulation and testing technologies. But this approach has several shortcomings when looking at how products are evolving. Manufacturers in the automotive industry, the aerospace industry, the marine industry or any other mechanical industry all share similar challenges: they have to re-invent the way they design to be able to deliver what their customers want and buy today.

Complex products that include smart systems
Products include, besides the mechanics, ever more electronics, software and control systems."
What are the shortcomings of the classic development approach?,It does not involve extensive verification and validation processes,It does not consider customer demands and preferences,It does not include smart systems in the product design,It does not focus on collaboration between analysis teams,It does not predict product behavior and let simulation drive the design,E,"Predictive engineering analytics (PEA) is a development approach for the manufacturing industry that helps with the design of complex products (for example, products that include smart systems). It concerns the introduction of new software tools, the integration between those, and a refinement of simulation and testing processes to improve collaboration between analysis teams that handle different applications. This is combined with intelligent reporting and data analytics. The objective is to let simulation drive the design, to predict product behavior rather than to react on issues which may arise, and to install a process that lets design continue after product delivery.

Industry needs
In a classic development approach, manufacturers deliver discrete product generations. Before bringing those to market, they use extensive verification and validation processes, usually by combining several simulation and testing technologies. But this approach has several shortcomings when looking at how products are evolving. Manufacturers in the automotive industry, the aerospace industry, the marine industry or any other mechanical industry all share similar challenges: they have to re-invent the way they design to be able to deliver what their customers want and buy today.

Complex products that include smart systems
Products include, besides the mechanics, ever more electronics, software and control systems."
Which industries face similar challenges in terms of product design?,Automotive industry,Aerospace industry,Marine industry,Mechanical industry,All of the above,E,"Predictive engineering analytics (PEA) is a development approach for the manufacturing industry that helps with the design of complex products (for example, products that include smart systems). It concerns the introduction of new software tools, the integration between those, and a refinement of simulation and testing processes to improve collaboration between analysis teams that handle different applications. This is combined with intelligent reporting and data analytics. The objective is to let simulation drive the design, to predict product behavior rather than to react on issues which may arise, and to install a process that lets design continue after product delivery.

Industry needs
In a classic development approach, manufacturers deliver discrete product generations. Before bringing those to market, they use extensive verification and validation processes, usually by combining several simulation and testing technologies. But this approach has several shortcomings when looking at how products are evolving. Manufacturers in the automotive industry, the aerospace industry, the marine industry or any other mechanical industry all share similar challenges: they have to re-invent the way they design to be able to deliver what their customers want and buy today.

Complex products that include smart systems
Products include, besides the mechanics, ever more electronics, software and control systems."
What components are included in complex products?,Mechanics only,Electronics and software,Control systems,All of the above,None of the above,D,"Predictive engineering analytics (PEA) is a development approach for the manufacturing industry that helps with the design of complex products (for example, products that include smart systems). It concerns the introduction of new software tools, the integration between those, and a refinement of simulation and testing processes to improve collaboration between analysis teams that handle different applications. This is combined with intelligent reporting and data analytics. The objective is to let simulation drive the design, to predict product behavior rather than to react on issues which may arise, and to install a process that lets design continue after product delivery.

Industry needs
In a classic development approach, manufacturers deliver discrete product generations. Before bringing those to market, they use extensive verification and validation processes, usually by combining several simulation and testing technologies. But this approach has several shortcomings when looking at how products are evolving. Manufacturers in the automotive industry, the aerospace industry, the marine industry or any other mechanical industry all share similar challenges: they have to re-invent the way they design to be able to deliver what their customers want and buy today.

Complex products that include smart systems
Products include, besides the mechanics, ever more electronics, software and control systems."
What does predictive engineering analytics aim to improve?,Simulation and testing processes,Intelligent reporting,Data analytics,Collaboration between analysis teams,All of the above,E,"Predictive engineering analytics (PEA) is a development approach for the manufacturing industry that helps with the design of complex products (for example, products that include smart systems). It concerns the introduction of new software tools, the integration between those, and a refinement of simulation and testing processes to improve collaboration between analysis teams that handle different applications. This is combined with intelligent reporting and data analytics. The objective is to let simulation drive the design, to predict product behavior rather than to react on issues which may arise, and to install a process that lets design continue after product delivery.

Industry needs
In a classic development approach, manufacturers deliver discrete product generations. Before bringing those to market, they use extensive verification and validation processes, usually by combining several simulation and testing technologies. But this approach has several shortcomings when looking at how products are evolving. Manufacturers in the automotive industry, the aerospace industry, the marine industry or any other mechanical industry all share similar challenges: they have to re-invent the way they design to be able to deliver what their customers want and buy today.

Complex products that include smart systems
Products include, besides the mechanics, ever more electronics, software and control systems."
What can posture provide in humans?,Information through verbal communication,Information through nonverbal communication,Information through written communication,Information through auditory communication,Information through visual communication,B,"In humans, posture can provide a significant amount of important information through nonverbal communication.  Psychological studies have also demonstrated the effects of body posture on emotions.  This research can be traced back to Charles Darwin's studies of emotion and movement in humans and animals.  Currently, many studies have shown that certain patterns of body movements are indicative of specific emotions.   Researchers studied sign language and found that even non-sign language users can determine emotions from only hand movements. Another example is the fact that anger is characterized by forward whole body movement.   The theories that guide research in this field are the self-validation or perception theory and the embodied emotion theory.
Self-validation theory is when a participant's posture has a significant effect on their self-evaluation of their emotions."
Who can determine emotions from hand movements?,Only sign language users,Only non-sign language users,Both sign language users and non-sign language users,Neither sign language users nor non-sign language users,None of the above,C,"In humans, posture can provide a significant amount of important information through nonverbal communication.  Psychological studies have also demonstrated the effects of body posture on emotions.  This research can be traced back to Charles Darwin's studies of emotion and movement in humans and animals.  Currently, many studies have shown that certain patterns of body movements are indicative of specific emotions.   Researchers studied sign language and found that even non-sign language users can determine emotions from only hand movements. Another example is the fact that anger is characterized by forward whole body movement.   The theories that guide research in this field are the self-validation or perception theory and the embodied emotion theory.
Self-validation theory is when a participant's posture has a significant effect on their self-evaluation of their emotions."
Which theory suggests that a participant's posture has an effect on their self-evaluation of their emotions?,Perception theory,Emotion theory,Self-validation theory,Embodied emotion theory,None of the above,C,"In humans, posture can provide a significant amount of important information through nonverbal communication.  Psychological studies have also demonstrated the effects of body posture on emotions.  This research can be traced back to Charles Darwin's studies of emotion and movement in humans and animals.  Currently, many studies have shown that certain patterns of body movements are indicative of specific emotions.   Researchers studied sign language and found that even non-sign language users can determine emotions from only hand movements. Another example is the fact that anger is characterized by forward whole body movement.   The theories that guide research in this field are the self-validation or perception theory and the embodied emotion theory.
Self-validation theory is when a participant's posture has a significant effect on their self-evaluation of their emotions."
"According to the research, what type of body movement is anger characterized by?",Sideways body movement,Backward body movement,Upward body movement,Downward body movement,Forward body movement,E,"In humans, posture can provide a significant amount of important information through nonverbal communication.  Psychological studies have also demonstrated the effects of body posture on emotions.  This research can be traced back to Charles Darwin's studies of emotion and movement in humans and animals.  Currently, many studies have shown that certain patterns of body movements are indicative of specific emotions.   Researchers studied sign language and found that even non-sign language users can determine emotions from only hand movements. Another example is the fact that anger is characterized by forward whole body movement.   The theories that guide research in this field are the self-validation or perception theory and the embodied emotion theory.
Self-validation theory is when a participant's posture has a significant effect on their self-evaluation of their emotions."
Who conducted studies on emotion and movement in humans and animals?,Charles Darwin,Sigmund Freud,Ivan Pavlov,Albert Bandura,None of the above,A,"In humans, posture can provide a significant amount of important information through nonverbal communication.  Psychological studies have also demonstrated the effects of body posture on emotions.  This research can be traced back to Charles Darwin's studies of emotion and movement in humans and animals.  Currently, many studies have shown that certain patterns of body movements are indicative of specific emotions.   Researchers studied sign language and found that even non-sign language users can determine emotions from only hand movements. Another example is the fact that anger is characterized by forward whole body movement.   The theories that guide research in this field are the self-validation or perception theory and the embodied emotion theory.
Self-validation theory is when a participant's posture has a significant effect on their self-evaluation of their emotions."
What is the fresco technique?,A technique of mural painting executed upon freshly laid cement,A technique of mural painting executed upon freshly laid lime plaster,A technique of mural painting executed upon dried plaster,A technique of mural painting executed upon freshly laid gypsum plaster,A technique of mural painting executed upon freshly laid clay plaster,B,"Fresco (plural frescos or frescoes) is a technique of mural painting executed upon freshly laid (""wet"") lime plaster. Water is used as the vehicle for the dry-powder pigment to merge with the plaster, and with the setting of the plaster, the painting becomes an integral part of the wall. The word fresco (Italian: affresco) is derived from the Italian adjective fresco meaning ""fresh"", and may thus be contrasted with fresco-secco or secco mural painting techniques, which are applied to dried plaster, to supplement painting in fresco. The fresco technique has been employed since antiquity and is closely associated with Italian Renaissance painting.The word fresco is commonly and inaccurately used in English to refer to any wall painting regardless of the plaster technology or binding medium. This, in part, contributes to a misconception that the most geographically and temporally common wall painting technology was the painting into wet lime plaster. Even in apparently Buon fresco technology, the use of supplementary organic materials was widespread, if underrecognized.

Technology
Buon fresco pigment is mixed with room temperature water and is used on a thin layer of wet, fresh plaster, called the intonaco (after the Italian word for plaster).  Because of the chemical makeup of the plaster, a binder is not required, as the pigment mixed solely with the water will sink into the intonaco, which itself becomes the medium holding the pigment."
What is the meaning of fresco in Italian?,Stale,Antique,Dry,Fresh,Wet,D,"Fresco (plural frescos or frescoes) is a technique of mural painting executed upon freshly laid (""wet"") lime plaster. Water is used as the vehicle for the dry-powder pigment to merge with the plaster, and with the setting of the plaster, the painting becomes an integral part of the wall. The word fresco (Italian: affresco) is derived from the Italian adjective fresco meaning ""fresh"", and may thus be contrasted with fresco-secco or secco mural painting techniques, which are applied to dried plaster, to supplement painting in fresco. The fresco technique has been employed since antiquity and is closely associated with Italian Renaissance painting.The word fresco is commonly and inaccurately used in English to refer to any wall painting regardless of the plaster technology or binding medium. This, in part, contributes to a misconception that the most geographically and temporally common wall painting technology was the painting into wet lime plaster. Even in apparently Buon fresco technology, the use of supplementary organic materials was widespread, if underrecognized.

Technology
Buon fresco pigment is mixed with room temperature water and is used on a thin layer of wet, fresh plaster, called the intonaco (after the Italian word for plaster).  Because of the chemical makeup of the plaster, a binder is not required, as the pigment mixed solely with the water will sink into the intonaco, which itself becomes the medium holding the pigment."
What is the binder used in fresco technique?,Water,Room temperature water,Pigment mixed with lime,Pigment mixed with gypsum,Pigment mixed with clay,A,"Fresco (plural frescos or frescoes) is a technique of mural painting executed upon freshly laid (""wet"") lime plaster. Water is used as the vehicle for the dry-powder pigment to merge with the plaster, and with the setting of the plaster, the painting becomes an integral part of the wall. The word fresco (Italian: affresco) is derived from the Italian adjective fresco meaning ""fresh"", and may thus be contrasted with fresco-secco or secco mural painting techniques, which are applied to dried plaster, to supplement painting in fresco. The fresco technique has been employed since antiquity and is closely associated with Italian Renaissance painting.The word fresco is commonly and inaccurately used in English to refer to any wall painting regardless of the plaster technology or binding medium. This, in part, contributes to a misconception that the most geographically and temporally common wall painting technology was the painting into wet lime plaster. Even in apparently Buon fresco technology, the use of supplementary organic materials was widespread, if underrecognized.

Technology
Buon fresco pigment is mixed with room temperature water and is used on a thin layer of wet, fresh plaster, called the intonaco (after the Italian word for plaster).  Because of the chemical makeup of the plaster, a binder is not required, as the pigment mixed solely with the water will sink into the intonaco, which itself becomes the medium holding the pigment."
What is the medium that holds the pigment in fresco technique?,Water,Lime,Plaster,Gypsum,Clay,C,"Fresco (plural frescos or frescoes) is a technique of mural painting executed upon freshly laid (""wet"") lime plaster. Water is used as the vehicle for the dry-powder pigment to merge with the plaster, and with the setting of the plaster, the painting becomes an integral part of the wall. The word fresco (Italian: affresco) is derived from the Italian adjective fresco meaning ""fresh"", and may thus be contrasted with fresco-secco or secco mural painting techniques, which are applied to dried plaster, to supplement painting in fresco. The fresco technique has been employed since antiquity and is closely associated with Italian Renaissance painting.The word fresco is commonly and inaccurately used in English to refer to any wall painting regardless of the plaster technology or binding medium. This, in part, contributes to a misconception that the most geographically and temporally common wall painting technology was the painting into wet lime plaster. Even in apparently Buon fresco technology, the use of supplementary organic materials was widespread, if underrecognized.

Technology
Buon fresco pigment is mixed with room temperature water and is used on a thin layer of wet, fresh plaster, called the intonaco (after the Italian word for plaster).  Because of the chemical makeup of the plaster, a binder is not required, as the pigment mixed solely with the water will sink into the intonaco, which itself becomes the medium holding the pigment."
Which wall painting technology is commonly and inaccurately referred to as fresco?,Painting into wet lime plaster,Painting into dried plaster,Painting into gypsum plaster,Painting into clay plaster,Painting into cement,B,"Fresco (plural frescos or frescoes) is a technique of mural painting executed upon freshly laid (""wet"") lime plaster. Water is used as the vehicle for the dry-powder pigment to merge with the plaster, and with the setting of the plaster, the painting becomes an integral part of the wall. The word fresco (Italian: affresco) is derived from the Italian adjective fresco meaning ""fresh"", and may thus be contrasted with fresco-secco or secco mural painting techniques, which are applied to dried plaster, to supplement painting in fresco. The fresco technique has been employed since antiquity and is closely associated with Italian Renaissance painting.The word fresco is commonly and inaccurately used in English to refer to any wall painting regardless of the plaster technology or binding medium. This, in part, contributes to a misconception that the most geographically and temporally common wall painting technology was the painting into wet lime plaster. Even in apparently Buon fresco technology, the use of supplementary organic materials was widespread, if underrecognized.

Technology
Buon fresco pigment is mixed with room temperature water and is used on a thin layer of wet, fresh plaster, called the intonaco (after the Italian word for plaster).  Because of the chemical makeup of the plaster, a binder is not required, as the pigment mixed solely with the water will sink into the intonaco, which itself becomes the medium holding the pigment."
What is the focus of digital sociology?,Understanding the use of digital media as part of everyday life,Analyzing the social implications of technology,Studying the emergence of online communities,Investigating cyber crime,Examining the transformation from industrial to informational society,A,"The sociology of the Internet involves the application of sociological theory and method to the Internet as a source of information and communication. The overlapping field of digital sociology focuses on understanding the use of digital media as part of everyday life, and how these various technologies contribute to patterns of human behavior, social relationships, and concepts of the self. Sociologists are concerned with the social implications of the technology; new social networks, virtual communities and ways of interaction that have arisen, as well as issues related to cyber crime.
The Internet—the newest in a series of major information breakthroughs—is of interest for sociologists in various ways: as a tool for research, for example, in using online questionnaires instead of paper ones, as a discussion platform, and as a research topic. The sociology of the Internet in the stricter sense concerns the analysis of online communities (e.g. as found in newsgroups), virtual communities and virtual worlds, organizational change catalyzed through new media such as the Internet, and social change at-large in the transformation from industrial to informational society (or to information society). Online communities can be studied statistically through network analysis and at the same time interpreted qualitatively, such as through virtual ethnography. Social change can be studied through statistical demographics or through the interpretation of changing messages and symbols in online media studies.

Emergence of the discipline
The Internet is a relatively new phenomenon."
How can the Internet be used as a tool for sociological research?,Conducting online questionnaires instead of paper ones,Analyzing changing messages and symbols in online media,Investigating cyber crime,Studying the emergence of online communities,Interpreting statistical demographics,A,"The sociology of the Internet involves the application of sociological theory and method to the Internet as a source of information and communication. The overlapping field of digital sociology focuses on understanding the use of digital media as part of everyday life, and how these various technologies contribute to patterns of human behavior, social relationships, and concepts of the self. Sociologists are concerned with the social implications of the technology; new social networks, virtual communities and ways of interaction that have arisen, as well as issues related to cyber crime.
The Internet—the newest in a series of major information breakthroughs—is of interest for sociologists in various ways: as a tool for research, for example, in using online questionnaires instead of paper ones, as a discussion platform, and as a research topic. The sociology of the Internet in the stricter sense concerns the analysis of online communities (e.g. as found in newsgroups), virtual communities and virtual worlds, organizational change catalyzed through new media such as the Internet, and social change at-large in the transformation from industrial to informational society (or to information society). Online communities can be studied statistically through network analysis and at the same time interpreted qualitatively, such as through virtual ethnography. Social change can be studied through statistical demographics or through the interpretation of changing messages and symbols in online media studies.

Emergence of the discipline
The Internet is a relatively new phenomenon."
What can be studied through virtual ethnography?,Social change,Network analysis,Emergence of online communities,Transformation from industrial to informational society,Use of digital media in everyday life,A,"The sociology of the Internet involves the application of sociological theory and method to the Internet as a source of information and communication. The overlapping field of digital sociology focuses on understanding the use of digital media as part of everyday life, and how these various technologies contribute to patterns of human behavior, social relationships, and concepts of the self. Sociologists are concerned with the social implications of the technology; new social networks, virtual communities and ways of interaction that have arisen, as well as issues related to cyber crime.
The Internet—the newest in a series of major information breakthroughs—is of interest for sociologists in various ways: as a tool for research, for example, in using online questionnaires instead of paper ones, as a discussion platform, and as a research topic. The sociology of the Internet in the stricter sense concerns the analysis of online communities (e.g. as found in newsgroups), virtual communities and virtual worlds, organizational change catalyzed through new media such as the Internet, and social change at-large in the transformation from industrial to informational society (or to information society). Online communities can be studied statistically through network analysis and at the same time interpreted qualitatively, such as through virtual ethnography. Social change can be studied through statistical demographics or through the interpretation of changing messages and symbols in online media studies.

Emergence of the discipline
The Internet is a relatively new phenomenon."
What is the sociology of the Internet concerned with?,Social change in the transformation from industrial to informational society,Virtual communities and virtual worlds,Cyber crime,The analysis of online communities,All of the above,E,"The sociology of the Internet involves the application of sociological theory and method to the Internet as a source of information and communication. The overlapping field of digital sociology focuses on understanding the use of digital media as part of everyday life, and how these various technologies contribute to patterns of human behavior, social relationships, and concepts of the self. Sociologists are concerned with the social implications of the technology; new social networks, virtual communities and ways of interaction that have arisen, as well as issues related to cyber crime.
The Internet—the newest in a series of major information breakthroughs—is of interest for sociologists in various ways: as a tool for research, for example, in using online questionnaires instead of paper ones, as a discussion platform, and as a research topic. The sociology of the Internet in the stricter sense concerns the analysis of online communities (e.g. as found in newsgroups), virtual communities and virtual worlds, organizational change catalyzed through new media such as the Internet, and social change at-large in the transformation from industrial to informational society (or to information society). Online communities can be studied statistically through network analysis and at the same time interpreted qualitatively, such as through virtual ethnography. Social change can be studied through statistical demographics or through the interpretation of changing messages and symbols in online media studies.

Emergence of the discipline
The Internet is a relatively new phenomenon."
Why is the Internet of interest to sociologists?,It is a tool for research,It is a discussion platform,It is a research topic,All of the above,None of the above,D,"The sociology of the Internet involves the application of sociological theory and method to the Internet as a source of information and communication. The overlapping field of digital sociology focuses on understanding the use of digital media as part of everyday life, and how these various technologies contribute to patterns of human behavior, social relationships, and concepts of the self. Sociologists are concerned with the social implications of the technology; new social networks, virtual communities and ways of interaction that have arisen, as well as issues related to cyber crime.
The Internet—the newest in a series of major information breakthroughs—is of interest for sociologists in various ways: as a tool for research, for example, in using online questionnaires instead of paper ones, as a discussion platform, and as a research topic. The sociology of the Internet in the stricter sense concerns the analysis of online communities (e.g. as found in newsgroups), virtual communities and virtual worlds, organizational change catalyzed through new media such as the Internet, and social change at-large in the transformation from industrial to informational society (or to information society). Online communities can be studied statistically through network analysis and at the same time interpreted qualitatively, such as through virtual ethnography. Social change can be studied through statistical demographics or through the interpretation of changing messages and symbols in online media studies.

Emergence of the discipline
The Internet is a relatively new phenomenon."
What is the main focus of Social Software Engineering (SSE)?,Improving software quality through automation,Enhancing the user interface of software,Addressing the social aspects of software development,Optimizing software performance,Reducing software development costs,C,"Social software engineering (SSE) is a branch of software engineering that is concerned with the social aspects of software development and the developed software.
SSE focuses on the socialness of both software engineering and developed software. On the one hand, the consideration of social factors in software engineering activities, processes and CASE tools is deemed to be useful to improve the quality of both development process and produced software. Examples include the role of situational awareness and multi-cultural factors in collaborative software development. On the other hand, the dynamicity of the social contexts in which software could operate (e.g., in a cloud environment) calls for engineering social adaptability as a runtime iterative activity. Examples include approaches which enable software to gather users' quality feedback and use it to adapt autonomously or semi-autonomously.
SSE studies and builds socially-oriented tools to support collaboration and knowledge sharing in software engineering. SSE also investigates the adaptability of software to the dynamic social contexts in which it could operate and the involvement of clients and end-users in shaping software adaptation decisions at runtime. Social context includes norms, culture, roles and responsibilities, stakeholder's goals and interdependencies, end-users perception of the quality and appropriateness of each software behaviour, etc.
The participants of the 1st International Workshop on Social Software Engineering and Applications (SoSEA 2008) proposed the following characterization:

Community-centered: Software is produced and consumed by and/or for a community rather than focusing on individuals
Collaboration/collectiveness: Exploiting the collaborative and collective capacity of human beings
Companionship/relationship: Making explicit the various associations among people
Human/social activities: Software is designed consciously to support human activities and to address social problems
Social inclusion: Software should enable social inclusion enforcing links and trust in communitiesThus, SSE can be defined as ""the application of processes, methods, and tools to enable community-driven creation, management, deployment, and use of software in online environments"".One of the main observations in the field of SSE is that the concepts, principles, and technologies made for social software applications are applicable to software development itself as software engineering is inherently a social activity."
What are some examples of social factors considered in SSE?,The role of artificial intelligence in software development,The impact of cultural diversity on collaborative software development,The influence of hardware components on software performance,The use of machine learning algorithms in software testing,The importance of data security in software engineering,B,"Social software engineering (SSE) is a branch of software engineering that is concerned with the social aspects of software development and the developed software.
SSE focuses on the socialness of both software engineering and developed software. On the one hand, the consideration of social factors in software engineering activities, processes and CASE tools is deemed to be useful to improve the quality of both development process and produced software. Examples include the role of situational awareness and multi-cultural factors in collaborative software development. On the other hand, the dynamicity of the social contexts in which software could operate (e.g., in a cloud environment) calls for engineering social adaptability as a runtime iterative activity. Examples include approaches which enable software to gather users' quality feedback and use it to adapt autonomously or semi-autonomously.
SSE studies and builds socially-oriented tools to support collaboration and knowledge sharing in software engineering. SSE also investigates the adaptability of software to the dynamic social contexts in which it could operate and the involvement of clients and end-users in shaping software adaptation decisions at runtime. Social context includes norms, culture, roles and responsibilities, stakeholder's goals and interdependencies, end-users perception of the quality and appropriateness of each software behaviour, etc.
The participants of the 1st International Workshop on Social Software Engineering and Applications (SoSEA 2008) proposed the following characterization:

Community-centered: Software is produced and consumed by and/or for a community rather than focusing on individuals
Collaboration/collectiveness: Exploiting the collaborative and collective capacity of human beings
Companionship/relationship: Making explicit the various associations among people
Human/social activities: Software is designed consciously to support human activities and to address social problems
Social inclusion: Software should enable social inclusion enforcing links and trust in communitiesThus, SSE can be defined as ""the application of processes, methods, and tools to enable community-driven creation, management, deployment, and use of software in online environments"".One of the main observations in the field of SSE is that the concepts, principles, and technologies made for social software applications are applicable to software development itself as software engineering is inherently a social activity."
What does SSE aim to support in software engineering?,Individual creativity in software development,Efficient project management in software development,Collaboration and knowledge sharing in software engineering,Optimization of software algorithms,Automation of software testing,C,"Social software engineering (SSE) is a branch of software engineering that is concerned with the social aspects of software development and the developed software.
SSE focuses on the socialness of both software engineering and developed software. On the one hand, the consideration of social factors in software engineering activities, processes and CASE tools is deemed to be useful to improve the quality of both development process and produced software. Examples include the role of situational awareness and multi-cultural factors in collaborative software development. On the other hand, the dynamicity of the social contexts in which software could operate (e.g., in a cloud environment) calls for engineering social adaptability as a runtime iterative activity. Examples include approaches which enable software to gather users' quality feedback and use it to adapt autonomously or semi-autonomously.
SSE studies and builds socially-oriented tools to support collaboration and knowledge sharing in software engineering. SSE also investigates the adaptability of software to the dynamic social contexts in which it could operate and the involvement of clients and end-users in shaping software adaptation decisions at runtime. Social context includes norms, culture, roles and responsibilities, stakeholder's goals and interdependencies, end-users perception of the quality and appropriateness of each software behaviour, etc.
The participants of the 1st International Workshop on Social Software Engineering and Applications (SoSEA 2008) proposed the following characterization:

Community-centered: Software is produced and consumed by and/or for a community rather than focusing on individuals
Collaboration/collectiveness: Exploiting the collaborative and collective capacity of human beings
Companionship/relationship: Making explicit the various associations among people
Human/social activities: Software is designed consciously to support human activities and to address social problems
Social inclusion: Software should enable social inclusion enforcing links and trust in communitiesThus, SSE can be defined as ""the application of processes, methods, and tools to enable community-driven creation, management, deployment, and use of software in online environments"".One of the main observations in the field of SSE is that the concepts, principles, and technologies made for social software applications are applicable to software development itself as software engineering is inherently a social activity."
"What is meant by ""social context"" in SSE?",The political environment in which software operates,The economic factors influencing software development,The cultural norms and roles in the software development community,The technological infrastructure supporting software deployment,The legal regulations governing software licensing,C,"Social software engineering (SSE) is a branch of software engineering that is concerned with the social aspects of software development and the developed software.
SSE focuses on the socialness of both software engineering and developed software. On the one hand, the consideration of social factors in software engineering activities, processes and CASE tools is deemed to be useful to improve the quality of both development process and produced software. Examples include the role of situational awareness and multi-cultural factors in collaborative software development. On the other hand, the dynamicity of the social contexts in which software could operate (e.g., in a cloud environment) calls for engineering social adaptability as a runtime iterative activity. Examples include approaches which enable software to gather users' quality feedback and use it to adapt autonomously or semi-autonomously.
SSE studies and builds socially-oriented tools to support collaboration and knowledge sharing in software engineering. SSE also investigates the adaptability of software to the dynamic social contexts in which it could operate and the involvement of clients and end-users in shaping software adaptation decisions at runtime. Social context includes norms, culture, roles and responsibilities, stakeholder's goals and interdependencies, end-users perception of the quality and appropriateness of each software behaviour, etc.
The participants of the 1st International Workshop on Social Software Engineering and Applications (SoSEA 2008) proposed the following characterization:

Community-centered: Software is produced and consumed by and/or for a community rather than focusing on individuals
Collaboration/collectiveness: Exploiting the collaborative and collective capacity of human beings
Companionship/relationship: Making explicit the various associations among people
Human/social activities: Software is designed consciously to support human activities and to address social problems
Social inclusion: Software should enable social inclusion enforcing links and trust in communitiesThus, SSE can be defined as ""the application of processes, methods, and tools to enable community-driven creation, management, deployment, and use of software in online environments"".One of the main observations in the field of SSE is that the concepts, principles, and technologies made for social software applications are applicable to software development itself as software engineering is inherently a social activity."
How can SSE be defined?,The application of social media in software development,The use of crowdsourcing in software testing,The implementation of artificial intelligence in software engineering,"The application of processes, methods, and tools to enable community-driven creation, management, deployment, and use of software in online environments",The use of blockchain technology in software development,D,"Social software engineering (SSE) is a branch of software engineering that is concerned with the social aspects of software development and the developed software.
SSE focuses on the socialness of both software engineering and developed software. On the one hand, the consideration of social factors in software engineering activities, processes and CASE tools is deemed to be useful to improve the quality of both development process and produced software. Examples include the role of situational awareness and multi-cultural factors in collaborative software development. On the other hand, the dynamicity of the social contexts in which software could operate (e.g., in a cloud environment) calls for engineering social adaptability as a runtime iterative activity. Examples include approaches which enable software to gather users' quality feedback and use it to adapt autonomously or semi-autonomously.
SSE studies and builds socially-oriented tools to support collaboration and knowledge sharing in software engineering. SSE also investigates the adaptability of software to the dynamic social contexts in which it could operate and the involvement of clients and end-users in shaping software adaptation decisions at runtime. Social context includes norms, culture, roles and responsibilities, stakeholder's goals and interdependencies, end-users perception of the quality and appropriateness of each software behaviour, etc.
The participants of the 1st International Workshop on Social Software Engineering and Applications (SoSEA 2008) proposed the following characterization:

Community-centered: Software is produced and consumed by and/or for a community rather than focusing on individuals
Collaboration/collectiveness: Exploiting the collaborative and collective capacity of human beings
Companionship/relationship: Making explicit the various associations among people
Human/social activities: Software is designed consciously to support human activities and to address social problems
Social inclusion: Software should enable social inclusion enforcing links and trust in communitiesThus, SSE can be defined as ""the application of processes, methods, and tools to enable community-driven creation, management, deployment, and use of software in online environments"".One of the main observations in the field of SSE is that the concepts, principles, and technologies made for social software applications are applicable to software development itself as software engineering is inherently a social activity."
What is an oil spill?,The release of a liquid petroleum hydrocarbon into the environment due to human activity,The release of gasoline and diesel fuel into the environment due to human activity,"The release of crude oil from tankers, offshore platforms, drilling rigs, and wells",The release of any oily refuse or waste oil into the environment,The release of oil into the atmosphere due to human activity,A,"An oil spill is the release of a liquid petroleum hydrocarbon into the environment, especially the marine ecosystem, due to human activity, and is a form of pollution. The term is usually given to marine oil spills, where oil is released into the ocean or coastal waters, but spills may also occur on land. Oil spills may be due to releases of crude oil from tankers, offshore platforms, drilling rigs and wells, as well as spills of refined petroleum products (such as gasoline and diesel fuel) and their by-products, heavier fuels used by large ships such as bunker fuel, or the spill of any oily refuse or waste oil.Oil spills penetrate into the structure of the plumage of birds and the fur of mammals, reducing its insulating ability, and making them more vulnerable to temperature fluctuations and much less buoyant in the water. Cleanup and recovery from an oil spill is difficult and depends upon many factors, including the type of oil spilled, the temperature of the water (affecting evaporation and biodegradation), and the types of shorelines and beaches involved. Spills may take weeks, months or even years to clean up.Oil spills can have disastrous consequences for society; economically, environmentally, and socially. As a result, oil spill accidents have initiated intense media attention and political uproar, bringing many together in a political struggle concerning government response to oil spills and what actions can best prevent them from happening.

Human impact
An oil spill represents an immediate negative effects on human health, including respiratory and reproductive problems as well as liver, and immune system damage. Oil spills causing future oil supply to decline also effects the everyday life of humans such as the potential closure of beaches, parks, fisheries and fire hazards."
Which ecosystem is most affected by oil spills?,Forest ecosystems,Desert ecosystems,Marine ecosystems,Grassland ecosystems,Tundra ecosystems,C,"An oil spill is the release of a liquid petroleum hydrocarbon into the environment, especially the marine ecosystem, due to human activity, and is a form of pollution. The term is usually given to marine oil spills, where oil is released into the ocean or coastal waters, but spills may also occur on land. Oil spills may be due to releases of crude oil from tankers, offshore platforms, drilling rigs and wells, as well as spills of refined petroleum products (such as gasoline and diesel fuel) and their by-products, heavier fuels used by large ships such as bunker fuel, or the spill of any oily refuse or waste oil.Oil spills penetrate into the structure of the plumage of birds and the fur of mammals, reducing its insulating ability, and making them more vulnerable to temperature fluctuations and much less buoyant in the water. Cleanup and recovery from an oil spill is difficult and depends upon many factors, including the type of oil spilled, the temperature of the water (affecting evaporation and biodegradation), and the types of shorelines and beaches involved. Spills may take weeks, months or even years to clean up.Oil spills can have disastrous consequences for society; economically, environmentally, and socially. As a result, oil spill accidents have initiated intense media attention and political uproar, bringing many together in a political struggle concerning government response to oil spills and what actions can best prevent them from happening.

Human impact
An oil spill represents an immediate negative effects on human health, including respiratory and reproductive problems as well as liver, and immune system damage. Oil spills causing future oil supply to decline also effects the everyday life of humans such as the potential closure of beaches, parks, fisheries and fire hazards."
What are some sources of oil spills?,Tankers and offshore platforms,Drilling rigs and wells,Refined petroleum products,Bunker fuel and waste oil,All of the above,E,"An oil spill is the release of a liquid petroleum hydrocarbon into the environment, especially the marine ecosystem, due to human activity, and is a form of pollution. The term is usually given to marine oil spills, where oil is released into the ocean or coastal waters, but spills may also occur on land. Oil spills may be due to releases of crude oil from tankers, offshore platforms, drilling rigs and wells, as well as spills of refined petroleum products (such as gasoline and diesel fuel) and their by-products, heavier fuels used by large ships such as bunker fuel, or the spill of any oily refuse or waste oil.Oil spills penetrate into the structure of the plumage of birds and the fur of mammals, reducing its insulating ability, and making them more vulnerable to temperature fluctuations and much less buoyant in the water. Cleanup and recovery from an oil spill is difficult and depends upon many factors, including the type of oil spilled, the temperature of the water (affecting evaporation and biodegradation), and the types of shorelines and beaches involved. Spills may take weeks, months or even years to clean up.Oil spills can have disastrous consequences for society; economically, environmentally, and socially. As a result, oil spill accidents have initiated intense media attention and political uproar, bringing many together in a political struggle concerning government response to oil spills and what actions can best prevent them from happening.

Human impact
An oil spill represents an immediate negative effects on human health, including respiratory and reproductive problems as well as liver, and immune system damage. Oil spills causing future oil supply to decline also effects the everyday life of humans such as the potential closure of beaches, parks, fisheries and fire hazards."
What are the consequences of oil spills on wildlife?,Increased buoyancy in the water,Improved insulating ability of plumage and fur,Reduced vulnerability to temperature fluctuations,Damage to liver and immune system,None of the above,D,"An oil spill is the release of a liquid petroleum hydrocarbon into the environment, especially the marine ecosystem, due to human activity, and is a form of pollution. The term is usually given to marine oil spills, where oil is released into the ocean or coastal waters, but spills may also occur on land. Oil spills may be due to releases of crude oil from tankers, offshore platforms, drilling rigs and wells, as well as spills of refined petroleum products (such as gasoline and diesel fuel) and their by-products, heavier fuels used by large ships such as bunker fuel, or the spill of any oily refuse or waste oil.Oil spills penetrate into the structure of the plumage of birds and the fur of mammals, reducing its insulating ability, and making them more vulnerable to temperature fluctuations and much less buoyant in the water. Cleanup and recovery from an oil spill is difficult and depends upon many factors, including the type of oil spilled, the temperature of the water (affecting evaporation and biodegradation), and the types of shorelines and beaches involved. Spills may take weeks, months or even years to clean up.Oil spills can have disastrous consequences for society; economically, environmentally, and socially. As a result, oil spill accidents have initiated intense media attention and political uproar, bringing many together in a political struggle concerning government response to oil spills and what actions can best prevent them from happening.

Human impact
An oil spill represents an immediate negative effects on human health, including respiratory and reproductive problems as well as liver, and immune system damage. Oil spills causing future oil supply to decline also effects the everyday life of humans such as the potential closure of beaches, parks, fisheries and fire hazards."
What factors affect the cleanup and recovery from an oil spill?,Type of oil spilled,Temperature of the water,Type of shorelines and beaches involved,All of the above,None of the above,D,"An oil spill is the release of a liquid petroleum hydrocarbon into the environment, especially the marine ecosystem, due to human activity, and is a form of pollution. The term is usually given to marine oil spills, where oil is released into the ocean or coastal waters, but spills may also occur on land. Oil spills may be due to releases of crude oil from tankers, offshore platforms, drilling rigs and wells, as well as spills of refined petroleum products (such as gasoline and diesel fuel) and their by-products, heavier fuels used by large ships such as bunker fuel, or the spill of any oily refuse or waste oil.Oil spills penetrate into the structure of the plumage of birds and the fur of mammals, reducing its insulating ability, and making them more vulnerable to temperature fluctuations and much less buoyant in the water. Cleanup and recovery from an oil spill is difficult and depends upon many factors, including the type of oil spilled, the temperature of the water (affecting evaporation and biodegradation), and the types of shorelines and beaches involved. Spills may take weeks, months or even years to clean up.Oil spills can have disastrous consequences for society; economically, environmentally, and socially. As a result, oil spill accidents have initiated intense media attention and political uproar, bringing many together in a political struggle concerning government response to oil spills and what actions can best prevent them from happening.

Human impact
An oil spill represents an immediate negative effects on human health, including respiratory and reproductive problems as well as liver, and immune system damage. Oil spills causing future oil supply to decline also effects the everyday life of humans such as the potential closure of beaches, parks, fisheries and fire hazards."
